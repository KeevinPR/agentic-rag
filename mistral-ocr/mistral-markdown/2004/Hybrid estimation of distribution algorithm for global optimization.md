# Hybrid estimation of distribution algorithm for global optimization 

Qingfu Zhang, Jianyong Sun, Edward Tsang and John Ford Department of Computer Science, University of Essex, Colchester, UK

Keywords Algorithmic languages, Optimization techniques, Optimized production scheduling


#### Abstract

This paper introduces a new hybrid evolutionary algorithm (EA) for continuous global optimization problems, called estimation of distribution algorithm with local search (EDA/L). Like other EAs, EDA/L maintains and improves a population of solutions in the feasible region. Initial candidate solutions are generated by uniform design, these solutions evenly scatter over the feasible solution region. To generate a new population, a marginal histogram model is built based on the global statistical information extracted from the current population and then new solutions are sampled from the model thus built. The incomplete simplex method applies to every new solution generated by uniform design or sampled from the histogram model. Unconstrained optimization by diagonal quadratic approximation applies to several selected resultant solutions of the incomplete simplex method at each generation. We study the effectiveness of main components of EDA/L. The experimental results demonstrate that EDA/L is better than four other recent EAs in terms of the solution quality and the computational cost.


## 1. Introduction

In this paper, we are considering the following global optimization problem:

$$
\begin{array}{ll}
\operatorname{minimize} & f(x) \\
\text { subject to } & x \in D
\end{array}
$$

where $x=\left(x_{1}, x_{2}, \ldots, x_{n}\right) \in R^{n}, f: R^{n} \rightarrow R$ is the objective function and $D=\left\{x \mid a_{i} \leq x_{i} \leq b_{i}\right.$ for $\left.i=1,2, \ldots, n\right\}$ is the feasible region. This problem arises in almost every field of science, engineering and business. Often, the objective function $f(x)$ is non-convex and has many local minima in the feasible region that are not the global minimum. A traditional optimization algorithm (Fletcher, 1987) such as the conjugate gradient method and the downhill simplex method could fail in locating the global minimum since such an algorithm has no mechanism to escape from a local minimum once it has been trapped in it. The global optimization problem becomes even more difficult if derivatives of $f(x)$ are unavailable as in many applications. Unless special assumptions are made about the objective function $f$, any algorithm, in general, has to evaluate $f$ on a dense subset of the feasible region to find the global

[^0]This work was supported by EPSRC under Grant GR/R64742/01 and a Research Promotion Fund grant from the University of Essex.


[^0]:    This work was supported by EPSRC under Grant GR/R64742/01 and a Research Promotion Fund grant from the University of Essex.

minimum (Torn and Zilinskas, 1989). This intractability makes room for developing heuristic algorithms such as evolutionary algorithms (EAs) for the global optimization problem. Indeed, many EAs have been developed for this problem in the past (Angeline, 1998; Chellapilla, 1998; Leung and Wang, 2001; Yao and Liu, 1999).

Estimation of distribution algorithms (EDAs) (Bosman and Thierens, 2000; Larrañaga and Lozano, 2001; Müehlenbein and Paaß, 1996; Pelikan et al., 1999; Zhang, 1999) are a new class of EAs. Like other EAs, EDAs maintain and successively improve a population of potential solutions until some stopping condition is met. However, EDAs do not use crossover or mutation. Instead, they select the best solutions from the current population and explicitly extract global statistical information from the selected solutions. A posterior probability distribution model of promising solutions is built, based on the extracted information. Then new solutions are sampled from the model thus built and fully or in part replace solutions in the current population. More precisely, EDAs work as follows.

Step 0. Randomly pick a set of solutions to form the initial population.
Step 1. Select some solutions from the current population according to a selection method. Build the probability model of the selected solutions.

Step 2. Replace some or all of the members of the current population by new solutions sampled from the probability model.

Step 3. If the stopping condition IS not met, go to Step 1.
Several EDAs have been proposed for solving global optimization problems. In these existing algorithms, the probability distribution of the promising solutions are modelled by a Gaussian distribution (Larrañaga and Lozano, 2001), a Gaussian mixture (Bosman and Thierens, 2001) or a histogram (Tsutsui et al., 2001). Since many points are needed to build a good probability model, these algorithms are often very time-consuming in practice.

A very natural way to improve the performance of EAs is to hybridize local search with EAs. Indeed, the hybrid evolutionary approach, often called the memetic algorithm in the literature, has proved to be a very efficient algorithmic framework for solving hard optimization problems, particularly, hard combinatorial optimization problems (Baluja and Davies, 1997; Merz and Freisleben, 2000; Robles et al., 2001). In a hybrid EA, EA operators (such as crossover and mutation) generate a starting point, a local search method starts from this point and locates the corresponding local minimum. Due to the diversity provided by EA operators, the algorithm is less likely to be trapped in a local minimum, while the local search speeds up the convergence of the algorithm greatly. Very recently, Bosman and Thierens have hybridized their iterated density estimation evolutionary algorithm (IEDA) with the conjugate gradient algorithm for solving the global optimization problem (Bosman and Thierens, 2001). Their algorithm requires calculation of derivatives so it cannot

be used in the case when the derivatives of the objective function $f(x)$ are not available.

In this paper, we introduce a new hybrid EDA for global optimization which has been successful on a number of test problems. We refer to the algorithm as estimation of distribution algorithm with local search (EDA/L). Most existing hybrid EAs utilize one local search technique. In EDA/L approach, however, we use two local search algorithms. One is the incomplete simplex method (Nelder and Mead, 1965) and the other is unconstrained optimization by diagonal quadratic approximation (UOBDQA) (Powell, 2002). For every new solution that is generated in the initialization step or sampled from a probability model in EDA/L, we apply the downhill simplex method to it and only allow the simplex method to run $O(n)$ steps. Then, if the resultant solution of the incomplete simplex method is good enough, we apply UOBDQA to it. The incomplete simplex method helps to locate promising solutions more accurately while the role of UOBDQA is to find good local minima and hopefully the global minimum. The other feature of EDA/L is that it generates its initial population from $D$ by the uniform design technique (Fang and Wang, 1994). The uniform design technique can generate points in $D$ that are more evenly distributed in $D$ than points generated randomly. We study the effectiveness of main components of our proposed algorithm in this paper. The experimental results show that our algorithm is better than four recent EAs in terms of the solution quality and computational cost.

## 2. The algorithm

### 2.1 The framework of the algorithm

The framework of the proposed algorithm is as follows.
Step 0. Parameter setting. Population size: $N$, the best solution value found so far: $F$ best $=\infty$, the number of new solutions sampled from the probability model at each iteration: $K$, the maximal number of function evaluations in the simplex method: $S$, and the number of solutions undergoing UOBDQA at each iteration: $J . J<K$.

Step 1. Initialization. Generate $N$ solutions $\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{N}$ from $D$, using the uniform design technique. Apply the simplex method with at most $S$ function evaluations to each solution $\tilde{x}^{i}$ and obtain $\tilde{x}^{i}(1 \leq i \leq N)$. Then let $x^{1}, x^{2}, \ldots, x^{N}$ constitute the initial population.

Step 2. Reproduction. Build a probability model based on the statistical information extracted from some selected solutions in the current population. Sample $K$ new solutions $\tilde{x}^{N+1}, \ldots, \tilde{x}^{N+K}$ from this model and then apply the simplex method with at most $S$ function evaluations to each solution $\tilde{x}^{i}$ and obtain $x^{i}(N+1 \leq i \leq N+K)$.

Step 3. Comparison. Compare the function values of all $x^{i}(1 \leq i \leq N+K)$, order and relabel them such that

$$
f\left(x^{1}\right) \leq f\left(x^{2}\right) \leq \cdots \leq f\left(x^{N+K}\right)
$$

Step 4. Update of Fbest. Apply UOBDQA to $x^{i}(1 \leq i \leq J)$ and obtain $J$ local minima $y^{i}(1 \leq i \leq J)$. If Fbest $>\min _{1 \leq i \leq J} f\left(y^{i}\right)$, set Fbest $=$ $\min _{1 \leq i \leq J} f\left(y^{i}\right)$.

Step 5. Stopping condition. If the stopping condition is met, stop.
Step 6. Update of the population. Let $x^{J+1}, \ldots, x^{J+N}$ constitute new population. Go to Step 2.
Experimental design techniques (Montgomery, 1997) such as orthogonal design and uniform design have been proposed to improve the performance of genetic algorithms (Leung and Wang, 2001; Zhang and Leung, 1999). The total number of orthogonal design points is often much more than $2^{n}$ while the number of uniform design points is relatively smaller. This is the reason why we use the uniform design in initialization (Step 1). The probability model built in reproduction (Step 2) step models the distribution of the best solutions in the current population. Therefore, sampling solutions from this model should fall in promising areas with high probability. In Step 3 (comparison), since all the solutions have undergone the incomplete simplex method with at most $S$ function value evaluations, the best ones, i.e. $x^{1}, x^{2}, \ldots, x^{J}$ in Step 4, should be more likely to be close to the global optimum than other solutions. We apply UOBQDA only to these best solutions in Step 4. The details of the main ingredients of EDA/L are explained in the following.

# 2.2 Initialization 

Before we solve a global optimization, we often have no information about the location of the global minimum of the objective function. Therefore, the solutions $\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{N}$ in Step 1 should scatter over the feasible region $D$ as uniformly as possible. The most popular measure of uniformity is $L_{\mathrm{p}}$-discrepancy. Let $\mathscr{P}=\left\{z^{1}, z^{2}, \ldots, z^{N}\right\}$ be a set of $N$ points in $C=[0,1]^{n}$, The $L_{\mathrm{p}}$-discrepancy of $P$ over $C$ is defined as follows (Fang and Wang, 1994):

$$
D_{\mathrm{p}}(\mathscr{P})=\left\{\int_{C}\left|\frac{\mathscr{N}(\mathscr{P},[0, x))}{n}-\operatorname{Vol}([0, x))\right|^{p} \mathrm{~d} x\right\}^{1 / p}
$$

where $[0, x)=\left[0, x_{1}\right) \times\left[0, x_{2}\right) \times \cdots \times\left[0, x_{n}\right), \mathscr{N}(\mathscr{P},[0, x))$ is the number of points of $\mathscr{P}$ falling in $[0, x)$ and $\operatorname{Vol}([0, x))=\prod_{i=1}^{n} x_{i}$ is the volume of $[0, x)$. The goal of uniform design is to generate $z^{1}, z^{2}, \ldots, z^{N}$ to minimize $D_{\mathrm{p}}(\mathscr{P})$. Uniform design provides a series of uniform arrays for different $n$ and $q$. A uniform array is often denoted by $U_{N}\left(q^{n}\right)$. The following is an example of uniform arrays:

$$
U_{9}\left(9^{6}\right)=\left[\begin{array}{llllll}
1 & 2 & 4 & 5 & 7 & 8 \\
2 & 4 & 8 & 1 & 5 & 7 \\
3 & 6 & 3 & 6 & 3 & 6 \\
4 & 8 & 7 & 2 & 1 & 5 \\
5 & 1 & 2 & 7 & 8 & 4 \\
6 & 3 & 6 & 3 & 6 & 3 \\
7 & 5 & 1 & 8 & 4 & 2 \\
8 & 7 & 5 & 4 & 2 & 1 \\
9 & 9 & 9 & 9 & 9 & 9
\end{array}\right]
$$

Here are $N$ rows in $U_{N}\left(q^{n}\right)$ and each row represents a point in $R^{n}$. All these points are integer points in $[1, q]^{n}$. Let $u^{k}=\left(u_{1}^{k}, \ldots, u_{n}^{k}\right)(1 \leq k \leq N)$ be points (rows) in $U_{N}\left(q^{n}\right)$, we can generate $z^{1}, z^{2}, \ldots, z^{N}$ in $[0,1]^{n}$ in the following way:

$$
z^{i}=\left(\frac{2 u_{1}^{i}-1}{2 q}, \frac{2 u_{2}^{i}-1}{2 q}, \ldots, \frac{2 u_{n}^{i}-1}{2 q}\right), \quad 1 \leq i \leq N
$$

The discrepancy of these points generated in this way is much smaller than that of $s$ randomly generated points over $C$. In the initialization step, based on $U_{N}\left(q^{n}\right), N$ points $\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{N}$ can be generated as follows:

$$
\begin{gathered}
z^{i}=\left(a_{1}+\frac{2 u_{1}^{i}-1}{2 q}\left(b_{1}-a_{1}\right), a_{2}+\frac{2 u_{2}^{i}-1}{2 q}\left(b_{2}-a_{2}\right), \ldots, a_{n}+\frac{2 u_{n}^{i}-1}{2 q}\left(b_{n}-a_{n}\right)\right) \\
1 \leq i \leq N
\end{gathered}
$$

Many uniform design arrays can be found from http://www.math.hkbu.edu. hk/UniformDesign. There are several algorithms for generating uniform design arrays (Fang and Wang, 1994).

# 2.3 Reproduction 

The role of reproduction operators such as crossover and mutation in EAs is to generate new promising solutions in the search space. The reproduction operator in EDAs generates new solutions by sampling from a probability model which characterizes the distribution of promising solutions. Several different probability models have been proposed in EDAs for continuous optimization problems. A Gaussian model can be built with very low computational cost but cannot be used to cope with the multimodal objective

functions (Larrañaga and Lozano, 2001). A Gaussian mixture model can deal with any multimodal objective function in theory but the cost of building such a model is prohibitively high, particularly for large-scale problems (Bosman and Thierens, 2001). We employ the marginal histogram model (Tsutsui et al., 2001) in EDA/L. The overhead of building this model is $O(n)$. Therefore, it is suitable for solving large-scale problems. Since the variables are independent of one another in the marginal histogram model, its modelling ability is poorer than that of a Gaussian mixture model. However, the utilization of local search in EDA/L can offset this shortcoming.
2.3.1 Histogram model. Let $x^{1}, x^{2}, \ldots, x^{N}$ be the current population, we select the $M$ best solutions from the current population. Without loss of generality, we assume that the $M$ selected solutions are $x^{1}, x^{2}, \ldots, x^{M}$. To model the distribution of these $M$ best solutions by a fixed-width histogram distribution (Tsutsui et al., 2001), we divide the search space $\left[a_{j}, b_{i}\right]$ of each variable $x_{i}$ into $H$ subintervals with the same length. The $j$ th subinterval is $[1]$

$$
\left[a_{i}+\frac{j-1}{H}\left(b_{i}-a_{i}\right), a_{i}+\frac{j}{H}\left(b_{i}-a_{i}\right)\right), \quad(1 \leq j \leq H)
$$

Then we count $W(i, j)$, the number of selected solutions whose values of $x_{i}$ fall in the $j$ th subinterval. The marginal probability density of $x_{i}$ is modelled by

$$
p_{i}\left(x_{i}\right)=\left\{\begin{array}{cc}
\frac{W(i, 1)}{M} \frac{H}{b_{i}-a_{i}} & a_{i} \leq x_{i}<a_{i}+\frac{1}{H}\left(b_{i}-a_{i}\right) \\
\frac{W(i, 2)}{M} \frac{H}{b_{i}-a_{i}} & a_{i}+\frac{1}{H}\left(b_{i}-a_{i}\right) \leq x_{i}<a_{i}+\frac{2}{H}\left(b_{i}-a_{i}\right) \\
\vdots & \vdots \\
\frac{W(i, H)}{M} \frac{H}{b_{i}-a_{i}} & a_{i}+\frac{H-1}{H}\left(b_{i}-a_{i}\right) \leq x_{i} \leq b_{i}
\end{array}\right.
$$

Figure 1 shows an example of marginal distribution.
2.3.2 Sampling method. A new solution $\tilde{x}=\left(\tilde{x}_{1}, \tilde{x}_{2}, \ldots, \tilde{x}_{n}\right)$ is generated by sampling the value of each $x_{i}$ from equation (2) independently. To generate $\tilde{x}_{i}$, we first randomly select a subinterval

$$
\left[a_{i}+\frac{j-1}{H}\left(b_{i}-a_{i}\right), a_{i}+\frac{j}{H}\left(b_{i}-a_{i}\right)\right)
$$

with probability $W(i, j) / M$, and then pick up a number from this subinterval with uniform distribution. In reproduction, we generate $K$ new solutions in this way.

![img-0.jpeg](img-0.jpeg)

Hybrid estimation

# 97 

Figure 1.
An example of marginal distribution

### 2.4 Simplex method

A simplex in $R^{n}$ is the convex polytope with $n+1$ affinely independent vertices. A simplex in $R^{2}$ is a triangle, in $R^{3}$ a tetrahedron, and so on. The downhill simplex method (Fletcher, 1987; Nelder and Mead, 1965), introduced by Nelder and Mead in 1965, maintains at each iteration a simplex in $R^{n}$. Starting with a point $v$, this method generates other $n$ points to form the initial simplex. Let $v^{1}, v^{2}, \ldots, v^{n+1}$ be the vertices of the current simplex, the simplex method will first order and relabel these vertices such that

$$
f\left(v^{1}\right) \leq f\left(v^{2}\right) \leq \ldots \leq f\left(v^{n+1}\right)
$$

$v^{1}$ is referred to as the best point, $v^{n+1}$ the worst point and $v^{n}$ the second worst point. Then a single new point, often called the accepted point, will be produced to replace the current worst vertex $v^{n+1}$ in the set of the vertices, or $n$ points will be generated, together with the current best point $v^{1}$, to form the simplex at the next iteration. One iteration of the simplex method is given as follows.

Step 1. Ordering. Order the $n+1$ vertices such that $f\left(v^{1}\right) \leq f\left(v^{2}\right) \leq \cdots \leq$ $f\left(v^{n+1}\right)$.

Step 2. Reflection. Compute the reflection point $v^{\text {ref }}$

$$
v^{\mathrm{ref}}=\bar{v}+\left(\bar{v}-v^{n+1}\right)
$$

where

$$
\bar{v}=\frac{1}{n} \sum_{i=1}^{n} v^{i}
$$

is the centroid of the $n$ best vertices. If $f\left(v^{1}\right) \leq f\left(v^{\text {ref }}\right)<f\left(v^{n}\right)$, replace $v^{n+1}$ by $v^{\text {ref }}$ and terminate the iteration.

Step 3. Expansion. If $v^{\text {ref }}$ is better than $v^{1}$, i.e. $f\left(v^{\text {ref }}\right)<f\left(v^{1}\right)$, compute the expansion point $v^{\text {ex }}$ :

$$
v^{\mathrm{ex}}=\bar{v}+2\left(v^{\mathrm{ref}}-\bar{v}\right)
$$

replace $v^{n+1}$ by the better of $v^{\text {ex }}$ and $v^{\text {ref }}$ and terminate the iteration.
Step 4. Inside contraction. If $v^{\text {ref }}$ is not better than $v^{n+1}$, i.e. $f\left(v^{\text {ref }}\right) \geq f\left(v^{n+1}\right)$, compute

$$
v^{\mathrm{ic}}=\bar{v}-\frac{1}{2}\left(\bar{v}-v^{n+1}\right)
$$

if $f\left(v^{\text {ic }}\right)<f\left(v^{n+1}\right)$, replace $v^{n+1}$ by $v^{\text {ic }}$ and terminate the iteration; otherwise, go to Step 6.

Step 5. Outside contraction. If $v^{\text {ref }}$ is better than $v^{n+1}$, i.e. $f\left(v^{n}\right) \leq f\left(v^{\text {ref }}\right)<$ $f\left(v^{n+1}\right)$, compute

$$
v^{\mathrm{oc}}=\bar{v}+\frac{1}{2}\left(v^{\mathrm{ref}}-\bar{v}\right)
$$

if $f\left(v^{\text {oc }}\right) \leq f\left(v^{\text {ref }}\right)$, replace $v^{n+1}$ by $v^{\text {oc }}$ and terminate the iteration; otherwise go to Step 6.

Step 6. Shrinking. Compute

$$
u^{i}=v^{1}+\frac{1}{2}\left(v^{i}-v^{1}\right), \quad i=2,3, \ldots, n+1
$$

Replace $v^{2}, v^{3}, \ldots, v^{n+1}$ by $u^{2}, u^{3}, \ldots, u^{n+1}$ and terminate the iteration.

# 2.5 UOBDQA 

Trust region methods using quadratic interpolation model, developed by Powell (2002), is a new class of derivative free local optimization algorithms for finding a local minimum of the objective function $f(x)$. Such algorithms start with an initial point $v, \rho_{\text {beg }}$ and $\rho_{\text {end }}$, the initial and final values of a trust region radius $\rho$. The algorithms generate a set of interpolation points (including $v$ ) in a neighbourhood of $v$. Then an initial quadratic model is formed by interpolating these points. The algorithm generates a new point, either by minimizing the current quadratic model within a trust region, or by a procedure that improves the accuracy of the model. One of the interpolation points is replaced by the resultant point. The typical distance between successive points at which $f$ is calculated are of magnitude of the trust region radius $\rho$. The initial value of $\rho$ is set to be $\rho_{\text {beg. }} \rho$ will be reduced when the objective function stops decreasing for such changes to the points. The algorithms stop when $\rho$ is smaller than $\rho_{\text {end }}$.

Several instances of the trust region methods have been implemented by Powell (2002). Unconstrained optimization by quadratic approximation (UOBYQA), as its name suggests, builds a general quadratic model in a trust region. It needs to maintain $(n+1)(n+2) / 2$ interpolation points at each iteration. The amount of routine work of an iteration is $O\left(n^{4}\right)$. Consequently, UOBYQA will be prohibitive for the large value of $n$. To overcome this shortcoming, UOBDQA was proposed, which chooses a quadratic of the form

$$
Q\left(v^{*}+d\right)=f\left(v^{*}\right)+g^{\mathrm{T}} d+d^{\mathrm{T}} D d
$$

where $v^{*}$ is the point with the least objective function among the current interpolation points. $g$ is a vector and $D$ is a diagonal matrix of dimension $n$. Therefore, $2 n+1$ interpolation points are needed to determine this quadratic model. The amount of routine work of each iteration is $O\left(n^{2}\right)$ instead of $O\left(n^{4}\right)$ in UOBYQA, which makes UOBDQA more useful than UOBYQA for solving large-scale optimization problems. For the detailed description of UOBYQA and UOBDQA, refer to Powell (2002).

# 3. Numerical experiments and results 

### 3.1 Test suite

The following well-known functions are used in our experimental studies. All these functions are non-convex functions with many local minima.

$$
\begin{aligned}
f_{1} & =\sum_{i=1}^{n}\left(-x_{1} \sin \left(\sqrt{\left|x_{i}\right|}\right)\right. \\
f_{2} & =\sum_{i=1}^{n}\left(-x_{i}^{2}-10 \cos \left(2 \pi x_{i}\right)+10\right) \\
f_{3} & =-20 \exp \left(-0.2 \sqrt{\frac{1}{n} \sum_{i=1}^{n} x_{i}^{2}}\right)-\exp \left(\sum_{i=1}^{n} \cos \left(2 \pi x_{i}\right)\right) \\
f_{4} & =\frac{1}{4,000} \sum_{i=1}^{n}\left(-x_{i}^{2}-\prod_{i=1}^{n} \cos \left(\frac{x_{i}}{\sqrt{i}}\right)+1\right) \\
f_{5}= & \frac{\pi}{n}\left\{10 \sin ^{2}\left(\pi y_{i}\right)+\sum_{i=1}^{n-1}\left(y_{i}-1\right)^{2}\left[1+10 \sin ^{2}\left(\pi y_{i+1}\right)\right]+\left(y_{n}-1\right)^{2}\right\} \\
& +\sum_{i=1}^{n} u\left(x_{i}, 10,100,4\right)
\end{aligned}
$$

where

$$
y_{i}=1+\frac{1}{4}\left(x_{i}+1\right)
$$

and

$$
\begin{aligned}
& u\left(x_{i}, a, k, m\right)= \begin{cases}k\left(x_{i}-a\right)^{m} & x_{i}>a \\
0 & -a \leq x_{i} \leq a \\
k\left(-x_{i}-a\right)^{m} & x_{i}<-a\end{cases} \\
& f_{6}=\frac{1}{10}\left\{\sin ^{2}\left(3 \pi x_{1}\right)+\sum_{n=1}^{n-1}\left(x_{i}-1\right)^{2}\left[1+\sin ^{2}\left(3 \pi x_{i+1}\right)\right]\right. \\
& \left.+\left(x_{n}-1\right)^{2}\left(\left[1+\sin ^{2}\left(2 \pi x_{n}\right)\right]\right)\right\}+\sum_{i=1}^{n} u\left(x_{i}, 5,100,4\right) \\
& f_{7}=-\sum_{i=1}^{n} \sin \left(x_{i}\right) \sin ^{m}\left(\frac{i \times x_{i}^{2}}{\pi}\right) \\
& f_{8}=-\sum_{i=1}^{n}\left[\sum_{j=1}^{n}\left(a_{i j} \sin \omega_{j}+b_{i j} \cos \omega_{j}\right)-\sum_{j=1}^{n}\left(a_{i j} \sin x_{j}+b_{i j} \cos \omega_{j}\right)\right]
\end{aligned}
$$

where $a_{i j}$ and $b_{i j}$ are random integers in $[-100,100]$, and $\omega_{j}$ is a random number in $[-\pi, \pi]$.

$$
\begin{aligned}
& f_{9}=\frac{1}{n} \sum_{i=1}^{n}\left(x_{i}^{4}-16 x_{i}^{2}+5 x_{i}\right) \\
& f_{10}=\sum_{i=1}^{n-1}\left[100\left(x_{i}^{2}-x_{i+1}\right)^{2}+\left(x_{j}-1\right)^{2}\right]
\end{aligned}
$$

The basic characteristics of these test functions are given in Table I.

# 3.2 Parameter setting 

We set the parameter values as follows.

- The size of population:

$$
N= \begin{cases}101 & n=100 \\ 31 & n=30\end{cases}
$$

| Test function | Globally minimal function value | Feasible solution region | $n$ | Hybrid estimation |
| :--: | :--: | :--: | :--: | :--: |
| $f_{1}$ | $-12569.5$ | $[-500,500]^{n}$ | 30 |  |
| $f_{2}$ | 0 | $[-5.12,5.12]^{n}$ | 30 |  |
| $f_{3}$ | 0 | $[-32,32]^{n}$ | 30 |  |
| $f_{4}$ | 0 | $[-600,600]^{n}$ | 30 |  |
| $f_{5}$ | 0 | $[-50,50]^{n}$ | 30 | 101 |
| $f_{6}$ | 0 | $[-50,50]^{n}$ | 30 |  |
| $f_{7}$ | $-99.2784$ | $[0, \pi]^{n}$ | 100 | Table I. |
| $f_{8}$ | 0 | $[-\pi, \pi]^{n}$ | 100 | The basic |
| $f_{9}$ | $-78.33236$ | $[-5,5]^{n}$ | 100 | characteristics of the test functions |
| $f_{10}$ | 0 | $[-5,10]^{n}$ | 100 |  |

- We use the good lattice point method (Fang and Wang, 1994) to construct the uniform arrays $U_{101}\left(101^{100}\right)$ and $U_{31}\left(31^{30}\right)$, which is used in initialization step.
- The number of solutions generated in reproduction step at each generation is $K=12$, the number of solutions undergoing UOBQDA is $J=2$. The maximal number of the function value evaluations in the simplex method $S=[1.5 n]$.
- In reproduction step, the $M=[N / 2]$ best solutions from the current population are selected to construct the probability model, $H=100$, used in the histogram model.
- In the simplex method, the initial vertices $v^{1}, v^{2}, \ldots, v^{n+1}$ are generated in the following way:

$$
v^{1}=v
$$

and

$$
v^{i}=v+\lambda e^{i}, i=2,3, \ldots, n+1
$$

where $e^{i}(i=1,2, \ldots, n)$ are $n$ unit vectors, and $\lambda$ is a constant, we set $\lambda=0.01$ in our experiments.

- In UOBDQA, we set $\rho_{\text {neg }}=0.01$ and $\rho_{\text {end }}=10^{-8}$.
- Stopping condition. When the current best function value cannot be further reduced in five successive generations after 30 generations, then the execution of the algorithm is stopped.
In our experimental study, we perform 30 independent runs for each algorithm on each test problem and record the mean number of function evaluations and the mean of the smallest function values found in 30 runs.

3.3 Effectiveness of the components in the algorithm

The main features of our algorithm are uniform design, utilization of two different local search techniques and EDA reproduction. Therefore, we would like to address the following three questions.
(1) Does the uniform design in initialization have any benefit?
(2) Is it necessary to use two local search techniques in EDA/L? and
(3) Is EDA reproduction operator better than commonly-used crossovers?

To investigate the above three questions, we compare EDA/L with the following three algorithms.

Algorithm $A$. It is the same as EDA/L, except that it randomly generates $N$ solutions $\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{N}$ from $D$ in initialization.

Algorithm $B$. It is the same as EDA/L, except that it does not apply the simplex method to the new solutions generated in initialization and reproduction, i.e. it simply allows $x^{i}=\tilde{x}^{i}$ in initialization and reproduction.

Algorithm $C$. It is the same as EDA/L, except that it uses crossovers instead of the EDA operator to generate new solutions in reproduction. We have implemented three commonly used crossovers: one point crossover, uniform crossover, and intermediate crossover. Let $a=\left(a_{1}, a_{2}, \ldots, a_{n}\right)$ and $b=\left(b_{1}, b_{2}, \ldots, b_{n}\right)$ be the two parents, the one point crossover randomly picks up a crossover position $j \in\{2,3, \ldots, n\}$, and the resultant offspring is $c=$ $\left(a_{1}, \ldots, a_{j-1}, b_{j}, \ldots b_{n}\right)$. The resultant offspring of the uniform crossover is $c=\left(c_{1}, c_{2}, \ldots, c_{n}\right)$ where $c_{i}$ is randomly picked up from $\left\{a_{i}, b_{i}\right\}$. In intermediate crossover, the offspring is $c=\left(\left(a_{1}+b_{1}\right) / 2, \ldots,\left(a_{n}+b_{n}\right) / 2\right)$. In reproduction in Algorithm C, we randomly pick up two distinct solutions from the current population, then apply a crossover to them to generate a new solution. Repeating this procedure $K$ times will generate $K$ new solutions.

We use $f_{2}, f_{7}$ and $f_{10}$ as the test functions to compare the performances of these three algorithms and our proposed algorithm. The results are listed in Tables II-IV.

Since all the algorithms adopted the same framework and stopping condition, it is hardly surprising that all these algorithms require the same number of function evaluations. EDA/L outperforms other algorithms in terms of solution quality. These results indicate that we can have positive answers to the earlier three questions.

### 3.4 Performance of EDA/L

We have tested EDA/L on $f_{1} \cdot f_{10}$. Table V shows the experimental results.
We can see that the means of the smallest function value found are equal or close to the globally minimal function values. At the first glance, one may judge that the performance of EDA/L is poor on $f_{7}$. The mean of the smallest function found to $f_{7}$ is about 4.9 per cent above the global minimal function value. However, there are 100! local minima for this function in its feasible region.

Therefore, it is extremely hard to find the global minimum. To our best knowledge, only the orthogonal genetic algorithm was tested on $f_{7}$ with 100 variables (Leung and Wang, 2001). In the following comparison, we will see that our algorithm is better than the orthogonal genetic algorithm on this problem.

### 3.5 Comparison with other algorithms

The functions $f_{1}: f_{10}$ have been tested by the following EAs recently.

- Orthogonal genetic algorithm with quantization (OGA/Q) (Leung and Wang, 2001). This algorithm applies the orthogonal design to improve the performance of the genetic algorithm for global optimization. $f_{1}: f_{10}$ and some other unimodel functions were tested by OGA/Q.

| Algorithm | Mean no. of function evaluations | Mean of the smallest function values found |
| :--: | :--: | :--: |
| EDA/L | 75,014 | 0 |
| Algorithm A | 77,870 | 0.198992 |
| Alogrithm B | 72,718 | 2.026529 |
| Algorithm C with one point crossover | 81,706 | 0.021804 |
| Algorithm C with uniform crossover | 88,141 | 0.0014333 |
| Algorithm C with intermediate crossover | 77,477 | 2.586894 |


| Algorithm | Mean no. of function evaluations | Mean of the smallest function values found |
| :--: | :--: | :--: |
| EDA/L | 169,887 | $-94.3757$ |
| Algorithm A | 183,891 | $-87.1928$ |
| Alogrithm B | 179,810 | $-80.2910$ |
| Algorithm C with one point crossover | 189,103 | $-83.4810$ |
| Algorithm C with uniform crossover | 198,381 | $-90.3381$ |
| Algorithm C with intermediate crossover | 183,491 | $-90.2874$ |


| Algorithm | Mean no. of function evaluations | Mean of the smallest function values found |
| :--: | :--: | :--: |
| EDA/L | 1,281,401 | $4.324 \times 10^{-3}$ |
| Algorithm A | 1,294,983 | $5.981 \times 10^{-2}$ |
| Alogrithm B | 1,592,820 | $1.395 \times 10^{-1}$ |
| Algorithm C with one point crossover | 1,509,894 | 12.987412 |
| Algorithm C with uniform crossover | 1,300,304 | 1.292349 |
| Algorithm C with intermediate crossover | 1,582,901 | 2.183849 |

Hybrid estimation

Table II.
The effectiveness of the main components of the algorithm on $f_{2}$

Table III.
The effectiveness of the main components of the algorithm on $f_{7}$

Table IV.
The effectiveness of the main components of the algorithm on $f_{10}$

| $\begin{aligned} & \text { EC } \\ & 21,1 \end{aligned}$ | Test function | Mean number of function evaluations | Mean of the smallest function values found | Globally minimal function value |
| :--: | :--: | :--: | :--: | :--: |
|  | $f_{1}$ | 52,216 | $-12569.48$ | $-12569.5$ |
|  | $f_{2}$ | 75,014 | 0 | 0 |
|  | $f_{3}$ | 106,061 | $4.141 \times 10^{-15}$ | 0 |
| 104 | $f_{4}$ | 79,096 | 0 | 0 |
|  | $f_{5}$ | 89,925 | $3.654 \times 10^{-21}$ | 0 |
|  | $f_{6}$ | 114,570 | $3.485 \times 10^{-21}$ | 0 |
|  | $f_{7}$ | 169,887 | $-94.3757$ | $-99.2784$ |
| Table V. | $f_{8}$ | 124,417 | $3.294 \times 10^{-8}$ | 0 |
| The performance of EDA/L | $f_{9}$ | 153,116 | $-78.31077$ | $-78.33236$ |
|  | $f_{10}$ | 128,140 | $4.324 \times 10^{-3}$ | 0 |

- Fast evolution strategy (FSA) (Yao and Liu, 1999). This algorithm uses Cauchy mutation instead of Gaussian mutation in generating new test points. $f_{1}-f_{6}$ was tested by FSA.
- Particle swarm optimization (PSO) (Angeline, 1998). It is a algorithm inspired by bird flocking. $f_{2}, f_{4}$, and $f_{10}$ were tested by PSO.
- Evolution programming with Gaussian mutation (EP/GM) (Chellapilla, 1998). $f_{2}, f_{3}, f_{4}$ and $f_{10}$ were tested by EP/GM.

Table VI compares the quality of the solutions found by EDA/L with the above four algorithms, while Table VII compares the computational costs.

We can see that EDA/L can give better solutions than other algorithms on all the test functions except $f_{3}$ in which the solution obtained by OGA/Q is slightly better than EDA/L. We also see that the computational cost of our algorithm is the smallest on all the test problems. For $f_{5}$ as an example, EDA/L gives a mean of the smallest function values of $3.654 \times 10^{-21}$ which is much more accurate

| Test functions | EDA/L | OGA/Q | FSA | PSO | EP/GM |
| :--: | :--: | :--: | :--: | :--: | :--: |
| $f_{1}$ | $-12569.4811$ | $-12569.4537$ | $-12554.5$ | N/A | N/A |
| $f_{2}$ | 0 | 0 | $4.6 \times 10^{-2}$ | 46.4689 | 120.00 |
| $f_{3}$ | $4.141 \times 10^{-15}$ | $4.440 \times 10^{-16}$ | $1.8 \times 10^{-2}$ | N/A | 9.10 |
| $f_{4}$ | 0 | 0 | $1.6 \times 10^{-2}$ | 0.4498 | $2.52 \times 10^{-7}$ |
| $f_{5}$ | $3.654 \times 10^{-21}$ | $6.019 \times 10^{-6}$ | $9.2 \times 10^{-6}$ | N/A | N/A |
| $f_{6}$ | $3.485 \times 10^{-21}$ | $1.869 \times 10^{-4}$ | $1.6 \times 10^{-4}$ | N/A | N/A |
| $f_{7}$ | $-94.3757$ | $-92.83$ | N/A | N/A | N/A |
| Comparison of the mean of the smallest function value found | $3.294 \times 10^{-8}$ | $4.672 \times 10^{-7}$ | N/A | N/A | N/A |
|  | $-78.31077$ | $-78.3000$ | N/A | N/A | N/A |
|  | $f_{10}$ | $4.324 \times 10^{-3}$ | $7.520 \times 10^{-1}$ | N/A | 1911.598 | 82.70 |

| Test functions | EDA/L | OGA/Q | FSA | PSO | EP/GM | Hybrid estimation |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| $f_{1}$ | 52,216 | 302,166 | 900,030 | N/A | N/A |  |
| $f_{2}$ | 75,014 | 224,710 | 500,030 | 250,000 | 250,000 |  |
| $f_{3}$ | 106,061 | 114,421 | 150,030 | N/A | 150,000 |  |
| $f_{4}$ | 79,096 | 134,000 | 200,030 | 250,000 | 250,000 |  |
| $f_{5}$ | 89,925 | 134,556 | 150,030 | N/A | N/A | 105 |
| $f_{6}$ | 114,570 | 134,143 | 150,030 | N/A | N/A |  |
| $f_{7}$ | 169.887 | 302,773 | N/A | N/A | N/A | Table VII. |
| $f_{8}$ | 124.417 | 190,031 | N/A | N/A | N/A | Comparison of the mean |
| $f_{9}$ | 153,116 | 245,930 | N/A | N/A | N/A | no. of the function |
| $f_{10}$ | 128,140 | 167,863 | N/A | 250,000 | 250,000 | evaluations |

than the solutions obtained by OGA/Q and FSA, but the mean number of the function evaluations is about 67 per cent of that of OGA/Q, and about 60 per cent of that of FSA. These results show that EDA/L can give a better solution at a lower computational cost than the existing algorithms. The C++ code of EDA/L can be obtained from the authors at qzhang@essex.ac.uk

# 4. Conclusions 

In this paper, we have introduced a hybrid estimation of distribution algorithm, called EDA/L, for global optimizations. The main features of the EDA/L are as follows.

- Initial points are generated by uniform design technique. Therefore, these points are evenly scattered over the feasible region.
- Two local search techniques are used in EDA/L. The incomplete simplex method is applied to every new point generated by uniform design and by EDA reproduction operator. UOBDQA applies to some selected resultant points of the incomplete simplex method. The incomplete simplex method helps to determine promising points while UOBDQA locates the local minima.
- Our EDA reproduction operator is based on the histogram model, which is easy to build and sample. Although the modelling ability of the histogram model is limited, the local search techniques in EDA/L can compensate it.
The empirical studies show that the uniform design and utilization of two local searches are beneficial to the algorithm. We also show that our EDA reproduction operator is better than the three commonly-used crossovers. We tested EDA/L on ten well-known benchmark problems. The results show that EDA/L are significantly better than four existing EAs.

# References 

Angeline, P.J. (1998), "Evolutionary optimization versus particle swarm optimization: philosophy and performance differences", in Porto, V.W., Saravanan, N., Waagen, D. and Eiben, A.E. (Eds), Proceedings of Evolutionary Programming, VII, Springer-Verlag, Berlin, Germany, pp. 601-10.
Baluja, S. and Davies, S. (1997), "Combing multiple optimization runs with optimal dependency trees", CMU-CS-97-157, Carnegie Mellon University.
Bosman, P.A.N. and Thierens, D. (2000), "Expanding from discrete to continuous EDAs: the IEDA", Proceedings of Parallel Problem Solving from Nature, PPSN-VI, pp. 767-76.
Bosman, P.A.N. and Thierens, D. (2001), "Exploiting gradient information in continuous iterated density estimation evolutionary algorithms", Working Report, UU-CS-2001-53, Universiteit Utrecht.
Chellapilla, K. (1998), "Combining mutation operators in evolutionary programming", IEEE Transactions on Evolutionary Computation, Vol. 2, pp. 91-6.
Fang, K.T. and Wang, Y. (1994), Number-Theoretic Methods in Statistics, Chapman and Hall, London.
Fletcher, R. (1987), Practical Methods of Optimization, Wiley, New York, NY.
Larrañaga, P. and Lozano, J.A. (2001), Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation, Kluwer Academic Publishers, Dordrecht.
Leung, Y.W. and Wang, Y. (2001), "An orthogonal genetic algorithm with quantization for global numerical optimization", IEEE Transactions on Evolutionary Computation, Vol. 5, pp. 41-51.
Merz, P. and Freisleben, B. (2000), "Fitness landscape analysis and memetic algorithms for the quadratic assignment problem", IEEE Transactions on Evolutionary Computation, Vol. 4, pp. 337-52.
Montgomery, D.C. (1997), Design and Analysis of Experiments, 5th ed., Wiley, New York, NY.
Müehlenbein, H. and Paa $\beta$, G. (1996), "From recombination of genes to the estimation of distribution Part 1, binary parameter", Parallel Problem Solving from Nature, Lecture Notes in Computer Science 1141, pp. 178-87.
Nelder, J.A. and Mead, R. (1965), "A simplex method for function minimization", Computer Journal, Vol. 7, pp. 308-13.
Pelikan, M., Goldberg, D.E. and Cantú-Paz, E. (1999), "BOA: the Bayesian optimization algorithm", in Banzhaf, W., Daida, J., Eiben, A.E., Garzon, M.H., Honavar, V., Jakiela, M. and Smith, R.E. (Eds), Proceedings of the Genetic and Evolutionary Computation Conference GECCO-99, Morgan Kaufmann Publishers, San Francisco, CA, Orlando, FL, Vol. 1, pp. 525-32.
Powell, M.J.D. (2002), "On trust region methods for unconstrained minimization without derivatives", Working Report, DAMTP 2002/NA02, Department of Applied Mathematics and Theoretical Physics, University of Cambridge.
Robles, V., de Miguel, P. and Larrañaga, P. (2001), "Solving the travelling salesman problem with estimation of distribution algorithms", in Larrañaga, P. and Lozano, J.A. (Eds), Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation, Kluwer Academic Publishers, Dordrecht.

Torn, A. and Zilinskas, A. (1989), Global Optimization, Springer-Verlag, Berlin.
Tsutsui, S., Pelikan, M. and Goldberg, D.E. (2001), "Evolutionary algorithm using marginal histogram models in continuous domain", IlliGAL Report 2001019, University of Illinois at Urbana-Champaign.
Yao, X. and Liu, Y. (1999), "Evolutionary programming made faster", IEEE Transactions on Evolutionary Computation, Vol. 3 No. 2, pp. 82-102.
Zhang, B.T. (1999), "A Bayesian framework for evolutionary computation", Proceedings of the 1999 Congress on Evolutionary Computation, Vol. 1, pp. 228-722.
Zhang, Q. and Leung, Y.W. (1999), "An orthogonal genetic algorithm for multimedia multicast routing", IEEE Transactions on Evolutionary Computation, Vol. 3, pp. 53-62.