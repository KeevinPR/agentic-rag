# A matrix cube-based estimation of distribution algorithm for the energy-efficient distributed assembly permutation flow-shop scheduling problem 

Zi-Qi Zhang ${ }^{\mathrm{a}, \mathrm{b}}$, Rong $\mathrm{Hu}^{\mathrm{a}, \mathrm{c}, *}$, Bin Qian ${ }^{\mathrm{a}, \mathrm{b}, \mathrm{c}}$, Huai-Ping Jin ${ }^{\mathrm{a}}$, Ling Wang ${ }^{\mathrm{d}}$, Jian-Bo Yang ${ }^{\mathrm{e}}$<br>${ }^{a}$ School of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China<br>${ }^{\text {b }}$ School of Mechanical and Electrical Engineering, Kunming University of Science and Technology, Kunming 650500, China<br>${ }^{\text {c }}$ Yunnan Key Laboratory of Artificial Intelligence, Kunming University of Science and Technology, Kunming 650500, China<br>${ }^{d}$ Department of Automation, Tsinghua University, Beijing 100084, China<br>${ }^{e}$ Alliance Manchester Business School, The University of Manchester, Manchester M15 6PB, United Kingdom

## A R T I C L E I N F O

Keywords:
Energy-efficient scheduling
Estimation of distribution algorithm
Distributed flowshop scheduling
Assembly line
Low-carbon manufacturing

## A B STR A C T

In this paper, a matrix-cube-based estimation of distribution algorithm (MCEDA) is proposed to solve the energyefficient distributed assembly permutation flow-shop scheduling problem (EE_DAPFSP) that minimizes both the maximum completion time $\left(C_{\max }\right)$ and the total carbon emission (TCE) simultaneously. Firstly, a high-quality and diverse initial population is constructed via a hybrid initialization method. Secondly, a matrix-cube-based probabilistic model and its update mechanism are designed to appropriately accumulate the valuable pattern information from superior solutions. Thirdly, a suitable sampling strategy is developed to sample the probabilistic model to generate a new population per generation, so as to guide the search direction toward promising regions in solution space. Fourthly, a problem-dependent neighborhood search based on critical path is provided to perform an in-depth local search around the promising regions found by the global search. Fifthly, two types of speed adjustment strategies based on problem properties are also embedded to further improve the quality of the obtained solutions. Sixthly, the influence of the parameters is investigated based on the multi-factor analysis of variance of Design-of-Experiments. Finally, extensive experiments and comprehensive comparisons with several recent state-of-the-art multi-objective algorithms are carried out based on the well-known benchmark instances, and the statistical results demonstrate the efficiency and effectiveness of the proposed MCEDA in addressing the EE_DAPFSP.

## 1. Introduction

With growing worldwide concern about the global warming and climate change, it is imperative that all of the developed and developing countries around the world should enact laws and take energy-efficient measures or technologies to relief global environment and energy crisis (May et al., 2015). Low-carbon or energy-efficient manufacturing is the key measure and inexorable choice to realize the integration of environmental sustainability and economic development. Meanwhile, along with the deepening of economic globalization, modern production pattern of many enterprises has a tendency to change from the traditional centralized manufacturing mode to the trans-regional decentralized manufacturing mode. Under these backgrounds, the research on energy-efficient and distributed production scheduling problems has
great practical and engineering significance.
Among the distributed scheduling problems, the distributed assembly permutation flow-shop scheduling problem (DAPFSP) is widely encountered in advanced manufacturing systems and modern supply chains. In fact, many real-life scheduling problems in manufacturing enterprises can be modeled as the DAPFSP. For example, in some large Chinese enterprises manufacturing automobile engines (e.g., Wei chai Power Co., Ltd. and Yu chai Group), all parts of each engine, such as cylinder block, cylinder head, and crankshaft, are first allocated to different factories or flow shops for processing, and then these parts are assembled into the final automobile engine in an assembly shop. Because the DAPFSP has been proved to be NP-hard with strong sense (Hatami et al., 2013), the relationship between its inherent geometric structure and the optimal solution is still an open problem, and there is

[^0]
[^0]:    ${ }^{a}$ Corresponding author.
    E-mail address: ronghu@vip.163.com (R. Hu).

no algorithm that can obtain the optimal solution in polynomial time. The mathematical programming algorithms need to traverse or partially traverse the DAPFSP's solution space, which makes them limited due to the long running time in solving medium and large-scale problems. To tackle this issue, the following metaheuristics have been presented in recent years to obtain satisfactory solutions for the DAPFSP and its variant under different scales within several seconds or tens of seconds.

As for DAPFSP, a fast variable neighborhood descent (VND) method (Hatami et al., 2013), an estimation of distribution algorithm-based memetic algorithm (EDAMA) (Wang \& Wang, 2016), an effective hybrid biogeography-based optimization (HIBBO) (Lin \& Zhang, 2016), a backtracking search hyper-heuristic (BS-HH) (Lin et al., 2017), three improved discrete invasive weed optimization algorithms (DIWOs) (Sang et al., 2019) and a matrix cube-based estimation of distribution algorithm (MCEDA) (Zhang et al., 2021) have been proposed. As for the variant of the DAPFSP, Fan et al. (2019) considered a series of identical factories in which each factory consists of a flow shop for job processing and an assembly line for product processing, and then presented three effective constructive heuristics and an enhance iterated greedy algorithm (IG). However, the above studies of DAPFSP only consider the efficiency-oriented criteria, and no existing studies involve energy conservation. Therefore, this paper aims to solve the energy-efficient DAPFSP (EE_DAPFSP) with the criteria of minimizing the maximum completion time $\left(C_{\max }\right)$ and the total carbon emission (TCE) at the same time.

The considered EE_DAPFSP is more complex and general than the DAPFSP, and the latter reduces to the former. This means that the EE_DAPFSP is also a NP-hard problem in strong sense. Obviously, it is a challenge to design an effective algorithm to address this problem. Among the existing metaheuristics, the estimation of distribution algorithm (EDA) is a special one. Unlike the crossover and mutation operators in most traditional metaheuristics, EDA builds one or more probabilistic models to learn the valuable information of the structure patterns of superior solutions, and generates the next offspring population by sampling these models. Such a new population generation mechanism can avoid the destruction of the building blocks (the partial structure patterns) in superior individuals or solutions to a certain extent (Larra√±aga \& Lozano, 2001). Due to its good exploration ability, inherent parallelism and quick convergence, EDA has been applied to deal with different kinds of scheduling problem, e.g., the permutation flow-shop scheduling problem (PFSP) (Jarboui et al., 2009), the multiobjective PFSP (Tiwari et al., 2014), the lot-streaming flow-shop scheduling problem (Pan \& Ruiz, 2012), the flexible job-shop scheduling problem (Wang et al., 2012), and the DAPFSP (Wang \& Wang, 2016; Zhang et al., 2021).

In the above EDAs, the two-dimensional probability model or matrix is used to store the information of the blocks and the order of jobs from each superior solution or individual. Here one block consists of any two consecutive jobs in a solution. Obviously, the structure of matrix determines that only the matrix elements and the subscripts of these elements can be used to store information. For the two-dimensional matrix, each element is used to save the occurrence frequency or probability that the job $C$ appears immediately after the job $R$, and its subscript $[R$, $C$ ) is only enough to save the corresponding block's pattern. There is no extra space to record the position of this block. This causes the sampling procedure may misplace the blocks in new individuals. As a result, the search direction cannot be reasonably guided, and the actual performance of these existing EDAs is limited (refer to Subsection 3.2 for more details). To overcome this defect, a novel EDA with a matrix-cube-based or three-dimensional probability model, namely MCEDA, is designed for the EE_DAPFSP. The test results on the instances with different scales demonstrate that MCEDA can obtain better solution than state-of-the-art algorithms under the same running time.

The main features of MCEDA lie in four aspects: the high-quality initial population generated by a hybrid initialization strategy, the global search guided by a three-dimensional probabilistic model, the

Table 1
Difference between this work and the previous literature.

| Difference | The previous literature | This work |
| :--: | :--: | :--: |
| Problem formulation | The existing formulations of the DAPFSP only take account of traditional efficiencydependent criteria (Hatami et al., 2013; Lin \& Zhang, 2016; Wang \& Wang, 2016; Lin et al., 2017; Fan et al., 2019; Zhang et al., 2021). | The sequence model of the EE_DAPFSP is established by minimizing both efficiencydependent and energy-saving criteria. This is the first time that the TCE (i.e., an energysaving criterion) has been treated as a separate criterion in the DAPFSPs. |
| Global search framework | The traditional metaheuristics for the DAPFSP only employ conventional genetic operators (i.e., the selection, crossover and mutation) or common neighborhood operators (i.e., insert, interchange and swap) to generate offspring to execute exploration (Lin \& Zhang, 2016; Lin et al., 2017; Fan et al., 2019; Sang et al., 2019). Furthermore, most of existing EDAs for scheduling problem use the two-dimensional probability model to save the information of superior individuals (Jarboui et al., 2009; Pan \& Ruiz, 2012; Tiwari et al., 2014; Wang \& Wang, 2016). | A novel EDA with a matrix-cube-based or three-dimensional probability model is utilized to reasonably reserve the valuable information of superior individuals and effectively guide the search direction. |
| Speed adjustment strategy | Most literatures only consider designing energy- saving speed adjustment strategies to reduce energy-saving criteria without reducing efficiency-dependent criteria, so as to enhance the quality of current non-dominated solutions (Ding et al., 2016; Chen et al., 2019; Abedi et al., 2020; Jiang \& Zhang, 2019; Wang \& Wang, 2020). | In addition to designing an energy-saving speed adjustment strategy to enhance the quality of each current non-dominated solution, a reduced-time speed adjustment strategy is also designed to generate possible non-dominated solutions, so as to further increase the diversity and number of non-dominated solutions. |

deep local search driven by a multi-neighborhood search, and the solution quality enhanced via two speed adjustment strategies. In terms of the initial population, a hybrid initialization strategy combining an effective constructive heuristic method and a randomization method is devised to generate high-quality initial population. This strategy can make the algorithm's search start from some regions close to the promising regions. In terms of the global search, a three-dimensional probabilistic model with an update mechanism is designed to reasonably reserve the valuable information of superior individuals, and a special sampling strategy is devised to guide the search to the promising regions in solution space. Since the three-dimensional structure is utilized to accurately record the job blocks with their exact positions, the global search can be effectively guided to the truly promising regions. In terms of the local search, an efficient neighborhood search adopting four critical path-based neighborhood structures is developed to execute deep search from the promising regions obtained by the global search. In terms of the solution quality, the problem properties are analyzed and two types of the problem-specific speed adjustment strategies are proposed to further improve the quality of the obtained non-dominated solutions. The novelties of this paper are summarized in Table 1.

The remainder of this paper is organized as follows. Section 2 describes and formulates the EE_DAPFSP. Section 3 introduces the proposed MCEDA in detail. Section 4 carries out computational comparisons and statistical analyses. Finally, some conclusions and future works are provided in Section 5.

Table 2
The notations used in the permutation-based model of the EE_DAPFSP.

| Indices |  |
| :--: | :--: |
| $i$ | The index for jobs where $i=1,2, \ldots, n$. |
| $j$ | The index for machines where $j=1,2, \ldots, m$. |
| $h$ | The index for products where $h=1,2, \ldots, S$. |
| $f$ | The index for factories where $f=1,2, \ldots, F$. |
| $k$ | The index for velocities where $k=1,2, \ldots, \mathrm{~d}$. |
| Parameters |  |
| n | The total number of jobs. |
| $m$ | The total number of machines. |
| $F$ | The total number of factories. |
| $S$ | The total number of products. |
| d | The total number of velocities. |
| $J$ | The set of jobs, i.e., $J=\left\{J_{1}, J_{2}, \ldots, J_{n}\right\}$. |
| $M$ | The set of machines, i.e., $M=\left\{M_{1}, M_{2}, \ldots, M_{m}\right\}$ where $\left|M_{i}\right| \geqslant 2$. |
| $P$ | The set of products, i.e., $P=\left\{P_{1}, P_{2}, \ldots, P_{S}\right\}$. |
| $O$ | The set of operations, i.e., $O=\left\{O_{i, 1}, O_{i, 2}, \ldots, O_{i, m}\right\}$. |
| $V$ | The set of velocities, i.e., $V=\left\{v_{1}, v_{2}, \ldots, v_{d}\right\}$. |
| $N_{h}$ | The number of jobs belongs to product $P_{h}$. |
| $p_{i, j}$ | The processing time of operation $O_{i, j}$ on machine $M_{j}$. |
| $p_{k}^{h}$ | The assembling time of product $h$ on machine $M_{k}$. |
| Variables |  |
| $n_{f}$ | The total number of jobs assigned to factory $f$, where $\sum_{j=1}^{T} n_{f}=n$. |
| $\omega_{h}$ | The total number of jobs of product $P_{h}$, where $\sum_{k=1}^{T} \omega_{h}=n$. |
| $\pi$ | The total sequence of jobs, i.e., $\pi=\left\{e_{1}, e_{2}, \ldots, e_{n}\right\}$. |
| $\pi_{f}$ | The sub-sequence of jobs in factory $f$, i.e., $\pi^{f}=\left\{e_{1}^{f}, e_{2}^{f}, \ldots, e_{n}^{f}\right\}$. |
| $\pi_{p}$ | The sequence of assembled products. |
| $\lambda$ | The assembly sequence of products, i.e., $\lambda=\left\{i_{1}, i_{2}, \ldots, i_{d}\right\}$. |
| $\hat{p}_{i, j}^{h}$ | The actual processing time of $O_{i, j}$ on $M_{j}$ at the speed $v_{0}$ where $\hat{p}_{i, j}^{h}=\hat{p}_{i, 1} / v_{0}$. |
| $C_{i, j}$ | The completion time of $O_{i, j}$ on $M_{j}$. |
| $S_{k}^{h}$ | The earliest possible assembly time of product $h$ on the assembly line. |
| $C_{k}^{h}$ | The completion time of product $h$ on the assembly line. |
| $E_{j k}$ | The energy consumption per unit time of machine $M_{j}$ running at speed $v_{k}$. |
| $S E_{j}$ | The energy consumption per unit time when machine $M_{j}$ is in standby state. |
| $c$ | The coefficient between energy consumption and carbon emission, where $c=0.7559$. |
|  | $c$ refers to the carbon emission per unit of consumed energy (kilogram $\mathrm{CO}_{2}$ equivalent/kiloWatthour). |
| $C_{\max }(\pi, V)$ | The makespan of a feasible solution $(\pi, V)$. |
| $\operatorname{ACE}_{( }(e, V)$ | The total carbon emission of a feasible solution $(\pi, V)$. |
| (II. 2) | A set of feasible scheduling schemes for the problem considered. |

## 2. Problem statement

### 2.1. Energy-efficient distributed assembly permutation flow-shop scheduling problem

The EE_DAPFSP can be briefly described as follows. There are $S$ products which consist of a set of $n$ jobs. Each of the $n$ jobs from the set $J=\left\{J_{1}, J_{2}, \ldots, J_{n}\right\}$ is allocated to one of the $F$ factories and is to be processed sequentially through $m$ machines $M=\left\{M_{1}, M_{2}, \ldots, M_{m}\right\}$, and then these $n$ jobs are to be assembled into $S$ products. Each product consists of a series of specific jobs and each job belongs to one certain product. The production process mainly consists of two stages, namely the processing stage and the assembly stage. The processing stage consists of $F$ factories, and each factory is regarded as a flow shop composed of the same number of machines, i.e., $m$ heterogeneous machines with different functions. Each machine has $d$ discrete and adjustable processing speeds, i.e., $V=\left\{v_{1}, v_{2}, \ldots, v_{d}\right\}$. Each job $J_{i} \in J$ has a predetermined processing time $p_{i, j}$ on every machine $M_{j} \in M$, and a series of $m$ operations $\left(O_{i, 1}, O_{i, 2}\right.$, $\ldots, O_{i, m}$ ) of $J_{i}$ can be completed in any factory, where the actual processing time of $O_{i, 1}$ on $M_{j}$ at speed $v_{k} \in V$ is $\hat{p}_{i, j}^{h}=p_{i, 1} / v_{k}$. Then, the corresponding energy consumption is produced whether the machine is in the processing state or in the standby state. Once all of the jobs of the specific product have been processed in factories, the assembly process can be started on the assembly machine. The EE_DAPFSP considered contains three subproblems, namely, appropriately assign jobs to factories, suitable select the processing speed of machines, and reasonably determine the processing order and the assembly order of jobs and products. The notations description for the EE_DAPFSP are provided in Table 2, and the illustration of the EE_DAPFSP is shown in Fig. 1.

In addition, all assumptions of FSP also meet herein. (i) No release time is considered. Each job and machine are available and independent, and each job can be processed immediately in time; (ii) No machine breakdown or setup time is considered. Preemption and interruption are not allowed in the processing; (iii) No transportation time is considered. Each product can be assembled once its jobs have been processed; (iv) Each job cannot be changed the factory and all operations of each job should be processed in the same factory, once the job assignment has been determined. At any time, each job can only be processed on at most one machine, and each machine is allowed to process no more than one job; (v) The processing time is deterministic, and the processing speed of machines remain unchanged during the processing process. The EE_DAPFSP aim to determine the allocation of jobs in the factories, the processing order of jobs on machines and the assembly order of each product, and the processing speed of all the machines. The production efficiency criterion needs to minimize the makespan, which is defined as the completion time of the last product on the assembly machine. The energy consumption criterion needs to minimize the total carbon
![img-0.jpeg](img-0.jpeg)

Fig. 1. Illustration of the EE_DAPFSP.

![img-2.jpeg](img-2.jpeg)

Fig. 2. The Gantt chart of a feasible solution of the EE_DAPFSP.
emission throughout the manufacturing period. The goal of the EE_DAPFSP is to find a group of optimal scheduling schemes with minimizing of both makespan and total carbon emission. According to the above description, the permutation-based model of the EE_DAPFSP can be given as follows.

$$
\begin{aligned}
& C_{d_{i, 1}^{\prime}}=\bar{p}_{d_{i, 1}^{\prime}, f}^{k}, f=1,2, \ldots, F ; k=1,2, \ldots, d .} \\
& C_{d_{i, 1}^{\prime \prime}}=C_{d_{i, 1}^{\prime}, 1}+\bar{p}_{d_{i, 1}^{\prime}, i}^{k}, i=2,3, \ldots, n_{f} ; f=1,2, \ldots, F ; k=1,2, \ldots, d . \\
& C_{d_{i, j}^{\prime}}=C_{d_{i, j}^{\prime}-1}+\bar{p}_{d_{i, j}^{\prime}, j}^{k}=2,3, \ldots, m ; f=1,2, \ldots, F ; k=1,2, \ldots, d . \\
& C_{d_{i, j}^{\prime}}=\max \backslash\left\{C_{d_{i, j}^{\prime}, i} C_{d_{i, j}^{\prime}-1} \backslash\right\}+\bar{p}_{d_{i, j}^{\prime}, j}^{k} \text {, } \\
& i=2,3, \ldots, n_{f} ; j=2,3, \ldots, m ; f=1,2, \ldots, F ; k=1,2, \ldots, d .
\end{aligned}
$$

![img-2.jpeg](img-2.jpeg)

Fig. 3. The real-time power consumption curve of the EE_DAPFSP.

$\left\{\boldsymbol{x}_{i_{1}}^{A}=\max _{\boldsymbol{x}_{i}^{t} \in \Omega_{i}^{A}} C_{x_{i}^{t}, a i} C_{x_{i}^{t}, a t} \mid i=1,2, \ldots, n_{f} ; f=1,2, \ldots, F ; h=1,2, \ldots, S\right.$.
$S_{x_{1}}^{A}=\max \left\{C_{x_{1}}^{A}, \max \left\{C_{x_{0}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{f}}^{A}, \max \left\{C_{n_{n_{f}}^{A}, \max \left\{C_{n_{n_{f}}^{A}, \max \left\{C_{n_{n_{f}}^{A}, \max \left\{C_{n_{n_{f}}^{A}, \max \left\{C_{n_{n_{n_{f}}^{A}, \max \left\{C_{n_{n_{n_{n_{n_{f}}^{A}, \max \left\{C_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n_{n

- Pareto front: The Pareto optimal front is aggregated by all of the solutions in Pareto optimal set $\Omega^{*}$, that is $P F=\backslash\{F \mid x\}=$ $\left[f_{1}(x), f_{2}(x), \ldots, f_{m}(x)\right]^{\mathrm{T}}\left[x \in \Omega^{*}\right]$.
- Non-dominated solution: In the feasible solution subset $\mathrm{A} \subseteq \Omega$, for each solution $x \in \mathrm{~A}$, if and only if $\neg \exists y \in \mathrm{~A}: x \prec y$ or $x \succ y$.
- Non-dominated set: For a feasible solution set $\Omega^{*} \subseteq \Omega$, if there exists another non-dominated subset $\Omega$ such that $\Omega \subseteq \Omega^{*}, \forall a \subseteq \Omega$ and $b \subseteq \Omega^{*}$, $\neg \exists b \prec a$ and $a \neq b$, then $\Omega$ is a non-dominated solution set.
- Non-dominated sorting: It is used to sort all obtained nondominated solutions and divide them into different levels according to the Pareto dominance relationship (Deb et al., 2002).
- Crowded distance: It is used to measure the degree of dispersion among the non-dominated solutions on the same level (Ding et al., 2016; Deb et al., 2002). In general, the larger the crowding distance of a set of non-dominated solutions is, the better the dispersion of the non-dominated solution set is. The crowding distance calculation method is given in Algorithm 1.

In many practical production processes, decision makers usually choose their preferred scheduling solution from the obtained Pareto optimal front based on a reasonable trade-off between preferences or priorities of objectives. Therefore, the aim of MOPs is to obtain a variety of non-dominated solutions with good proximity and diversity with respect to the true Pareto optimal front.

Algorithm 1: Crowded distance calculation method
belongs must be taken into account in the processing stage. Thus, the achievement of objectives and the handling of constraints must not only take into account the optimal production efficiency, but also meet the production requirements of low-carbon manufacturing. Conflicting constraints complicate the feasible solution space of the considered problem, resulting in a great challenge that the algorithm being more difficult to obtain a satisfactory solution within an acceptable time. Therefore, when designing EDA-based algorithms to address the EE_DAPFSP, it is not only necessary to establish effective probabilistic models to quickly guide directions to truly promising regions, but also need to analyze some problem-dependent properties and then utilize them to narrow the search scope by avoiding invalid searches.

Ding et al. (2016) proposed two assumptions about the relationship between job processing speed and machine energy consumption based on the problem properties of the low-carbon PFSP. That is, when job $J_{i}$ is processed at a higher speed on a machine $M_{j}$, the processing time of the job would be shorter and the total energy consumption would be increased. In other words, if $\forall v_{k 1}>v_{k 2}\left(v_{k 1}, v_{k 2} \in V\right)$, we have $\hat{p}_{i j}^{k 1}<\hat{p}_{i j}^{k 2}$, then $\hat{p}_{i j}^{k 1}-E_{j k 1}^{k 2}-E_{j k 2}$. Obviously, there is an inevitable contradiction relation between the two objectives $C_{\max }$ and TCE in the EE_DAPFSP, and there is no optimal scheduling solution can be found in the absolute sense. For any two feasible scheduling solutions $\left(\pi^{\prime}, V^{\prime}\right)$ and $\left(\pi^{\prime \prime}, V^{\prime \prime}\right)$, when $C_{\max }\left(\pi^{\prime}, V^{\prime}\right) \geqslant C_{\max }\left(\pi^{\prime \prime}, V^{\prime \prime}\right), \operatorname{TCE}\left(\pi^{\prime}, V^{\prime}\right) \geqslant \operatorname{TCE}\left(\pi^{\prime \prime}, V^{\prime \prime}\right)$ and $\left(C_{\max }\left(\pi^{\prime}\right.\right.$, $\left.\left.V^{\prime}\right), \operatorname{TCE}\left(\pi^{\prime}, V^{\prime}\right)\right) \neq\left(C_{\max }\left(\pi^{\prime \prime}, V^{\prime \prime}\right), \operatorname{TCE}\left(\pi^{\prime \prime}, V^{\prime \prime}\right)\right)$, it can be concluded that the solution $\left(\pi^{\prime \prime}, V^{\prime \prime}\right)$ dominates the solution $\left(\pi^{\prime}, V^{\prime}\right)$ (denoted as $\left(\pi^{\prime}, V^{\prime}\right) \prec\left(\pi^{\prime \prime}\right.$, $\left.V^{\prime \prime}\right)$ ). According to the above assumptions and the definition of multi-

Input: $t$ non-dominated solutions where $\mathbf{C}=\left(C_{\max }^{1}, C_{\max }^{2}, \ldots, C_{\max }^{t}\right)^{T}$ and $\mathbf{E}=\left(\mathrm{TCE}^{1}, \mathrm{TCE}^{2}, \ldots, \mathrm{TCE}^{t}\right)^{T}$.
1: Sort $\mathbf{C}$ in ascending order, i.e., $\left(C_{\max }^{I(1)}, C_{\max }^{I(2)}, \ldots, C_{\max }^{I(t)}\right)^{T}$, where $\boldsymbol{I}=(I(1), I(2), \ldots, I(t))^{T}$ and the vector $I$ is the index of the ascending order.
2: Initialization: $C_{d i s}(I(1)):=+\infty$ and $C_{d i s}(I(t)):=+\infty$.
3: for $i=2$ to $t-1$ do
4: $\quad C_{\max }-C_{d i s}(I(i)) \leftarrow\left|C_{\max }^{I(i+1)}-C_{\max }^{I(i-1)}\right| /\left(\max \{\mathbf{C}\}-\min \{\mathbf{C}\}\right)$.
5: $\quad \operatorname{TCE}_{-} C_{d i s}(I(i)) \leftarrow\left|\operatorname{TCE}^{I(i+1)}-\operatorname{TCE}^{I(i-1)}\right| /\left(\max \{\mathbf{E}\}-\min \{\mathbf{E}\}\right)$.
6: $\quad C_{d i s}(I(i)) \leftarrow C_{\max }-C_{d i s}(I(i))+\operatorname{TCE}_{-} C_{d i s}(I(i))$.
7: end for
Output: Crowded distance for each feasible solution $C_{d i s}(i), i=1,2, \ldots, t$.

### 2.3. Problem property analysis

The EE_DAPFSP is a typical complex PFSP with adjustable machine speed and there is a close coupling relationship between the production and assembly stages. In EE_DAPFSP, the machine speed directly determines the processing operation, and the processing stage affects the assembly stage, that is, to determine the start time of each product in the assembly stage, the completion time of all parts to which the product
objective domination, two properties of the EE_DAPFSP can be put forward as follows.

Property 1. Let us assume that the machine speed is fixed in the processing stage. That is, the processing speed matrix remains constant. Under this assumption, for any two feasible scheduling solutions $\left(\pi^{\prime}, V^{\prime}\right)$ and $\left(\pi^{\prime \prime}, V^{\prime \prime}\right)$, if $C_{\max }\left(\pi^{\prime}, V^{\prime}\right)>C_{\max }\left(\pi^{\prime \prime}, V^{\prime \prime}\right)$ and $V=V^{\prime \prime}$, then it has $\left(\pi^{\prime}, V^{\prime}\right) \prec\left(\pi^{\prime \prime}, V^{\prime \prime}\right)$.

Proof: Assume that the two feasible solutions of the EE_DAPFSP are $\left(\pi^{\prime}, V^{\prime}\right)$ and $\left(\pi^{\prime \prime}, V^{\prime \prime}\right)$, respectively. According to Eq. (9) in Subsection 2.1, the total carbon emission of these two solutions $\left(\pi^{\prime}, V^{\prime}\right)$ and $\left(\pi^{\prime \prime}, V^{\prime \prime}\right)$ can be represented as $\operatorname{TCE}\left(\pi^{\prime}, V^{\prime}\right)=\varepsilon\left(\mathrm{E}\left(\pi^{\prime}, V^{\prime}\right)+\mathrm{SE}\left(\pi^{\prime}, V^{\prime}\right)\right)$ and $\operatorname{TCE}\left(\pi^{\prime \prime}, V^{\prime \prime}\right)$ $=\varepsilon\left(\mathrm{E}\left(\pi^{\prime \prime}, V^{\prime \prime}\right)+\mathrm{SE}\left(\pi^{\prime \prime}, V^{\prime \prime}\right)\right)$, respectively. $\mathrm{E}(\cdot)$ and $\mathrm{SE}(\cdot)$ indicate the total

![img-6.jpeg](img-6.jpeg)
(a) Identify critical path for a feasible solution.
![img-4.jpeg](img-4.jpeg)
(b) Calculate time margin between adjacent jobs on the non-critical path.
![img-5.jpeg](img-5.jpeg)
(c) Perform speed-down operations and update time margin from back to front.
![img-6.jpeg](img-6.jpeg)
(d) Perform speed-down operations on all jobs in non-critical path.

Fig. 4. An illustrative example of the energy-saving speed adjustment strategy for EE_DAPFSP.

energy consumption of machines in running state and the energy consumption of machines in standby state. Since $\dot{V}=\boldsymbol{V}^{\prime \prime}$, the two solutions $\left(\dot{x}, \dot{V}\right)$ and $\left(x^{\prime \prime}, V^{\prime \prime}\right)$ have exactly the same processing processes of jobs in the processing stage. That is, the processing time and energy consumption of the processing operations are also the same, and obviously $\mathrm{E}\left(\dot{x}, \dot{V}\right)=\mathrm{E}\left(x^{\prime \prime}, V^{\prime \prime}\right)$ holds. In addition, each product $\lambda_{h}(h=1, \ldots, S)$ in the product order $\lambda=\left[\lambda_{1}, \lambda_{2}, \ldots, \lambda_{S}\right]$ corresponding to $x$ in the solution $\left(x .\right.$ $V$ ) must wait until it's all parts are completed before starting the product assembly process.

According to Eq. (5) in Subsection 2.1, denote $\max _{\dot{x}_{i}^{l} \leq \lambda_{h}} C_{\dot{x}_{i}^{l}, m}$ be the start assembly time of product $\lambda_{h}\left(i=1, \ldots, n_{f}, f=1, \ldots, F\right.$ and $\left.h=1, \ldots, S\right)$, $\bar{x}_{h}=\left[\bar{x}_{h, 1}, \bar{x}_{h, 2}, \ldots, \bar{x}_{h, n_{f}}\right]$ be the job order belonging to product $\lambda_{h}$. Let $P_{x_{h}, j}^{f}$ and $S t_{x_{h}, j}^{f}$ respectively represent the total duration of the processing state and the total duration of the standby state of the job $\bar{x}_{h, i}$ ( $I=1, \ldots, m_{S}$ ) contained in product $\lambda_{h}$ in the product order $\lambda$ on the machine $M_{i}$ in factory $f$, and then the start assembly time of product $\lambda_{h}$ can be represented as $\max _{\dot{x}_{i}^{l} \leq \lambda_{h}} C_{\dot{x}_{i}^{l}, m}=\max _{f, i} \sum_{x_{h}, m}^{f}+S t_{x_{h}, m}^{f}$ 1. The makespan $C_{\max }\left(\boldsymbol{x}, \boldsymbol{V}\right)$ of the feasible solution $\left(\boldsymbol{x}, \boldsymbol{V}\right)$ is determined by the start assembly time of each product. Let $\lambda$ and $\lambda^{\prime \prime}$ be the product orders corresponding to $\dot{x}^{\prime}$ and $\dot{x}^{\prime \prime}$ in the solutions $\left(\dot{x}^{\prime}, \dot{V}\right)$ and $\left(x^{\prime \prime}, V^{\prime \prime}\right)$. Since $\dot{x}^{\prime}$ and $\dot{x}^{\prime \prime}$ in $\left(\dot{x}^{\prime}, V^{\prime}\right)$ and $\left(x^{\prime \prime}, V^{\prime \prime}\right)$ correspond to the same product, and the job order $\bar{x}_{h}$ in $\lambda_{h}$ is the same as that in the job order $\bar{x}^{\prime \prime}{ }_{h}$ and $\mathrm{E}\left(\dot{x}^{\prime}, \dot{V}\right)=$ $\mathrm{E}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$. It is clear that $P t_{x_{h}, j}^{f}=P t_{x_{h}, j}^{f} \forall j \in M$ and $C_{\max }\left(\dot{x}^{\prime}, \dot{V}\right)>$ $C_{\max }\left(x^{\prime \prime}, V^{\prime \prime}\right)$. It can be deduced that if $\exists \lambda_{h^{\prime}} \in \lambda$, it has $S t_{x_{h^{\prime}, j}}^{f}>\mathrm{St}_{x_{h^{\prime}, j}^{\prime}, j}^{f}$ $\exists j \in M$. Thus, for the standby state, the total energy consumption can be given as follows:
$S E\left(\dot{x}^{\prime}, V^{\prime}\right)=\sum_{j} \sum_{j \in M} S t_{x_{h_{j}}}^{f} \times S E_{j}>\sum_{j} \sum_{j \in M} S t_{x_{h_{j}}}^{f} \times S E_{j}=S E\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$
Since there is $\operatorname{TCE}\left(\dot{x}^{\prime}, \dot{V}\right)=\epsilon\left(\mathrm{E}\left(\dot{x}^{\prime}, \dot{V}\right)+\mathrm{SE}\left(\dot{x}^{\prime}, \dot{V}\right)\right), \operatorname{TCE}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)=$ $\epsilon\left(\mathrm{E}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)+\mathrm{SE}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)\right)$ and $\mathrm{E}\left(\dot{x}^{\prime}, V^{\prime}\right)=\mathrm{E}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$ is met, it has $\operatorname{TCE}\left(\dot{x}^{\prime}\right.$, $\left.\dot{V}\right)>\operatorname{TCE}\left(\dot{x}^{\prime \prime}\right)$ and $C_{\max }\left(\dot{x}^{\prime}, \dot{V}\right)>C_{\max }\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$. Thus, we can get $\left(\dot{x}^{\prime}\right.$, $\left.\dot{V}\right) \prec\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$. Property 1 is proved.
Property 2. For any two feasible solutions $\left(\dot{x}^{\prime}, \dot{V}\right)$ and $\left(x^{\prime \prime}, V^{\prime \prime}\right)$, if they have the same maximum completion time, then the solution with the slower processing speed dominates the other solution. That is, if $C_{\max }\left(\dot{x}^{\prime}, V^{\prime}\right)=$ $C_{\max }\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$ when $V_{i j} \geqslant V_{i j}^{\prime}(\forall i=1, \ldots, n ; j=1, \ldots, m)$ and $\dot{V} \neq V^{\prime \prime}$, then it has $\left(\dot{x}^{\prime}, \dot{V}\right) \prec\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$.

Proof: Since $V_{i j} \geqslant V_{i j}^{\prime}(\forall i=1, \ldots, n ; j=1, \ldots, m), C_{\max }\left(\dot{x}^{\prime}, V^{\prime}\right)=C_{\max }\left(\dot{x}^{\prime \prime}\right.$, $\left.V^{\prime \prime}\right)$ and $\dot{V} \neq V^{\prime \prime}$, it is clear that the production process of the solution $\left(\dot{x}^{\prime}, \dot{V}\right)$ is faster than that of the solution $\left(x^{\prime \prime}, V^{\prime \prime}\right)$. For the job orders $\dot{x}^{\prime}$ and $\dot{x}^{\prime \prime}$ in any two solutions $\left(\dot{x}^{\prime}, \dot{V}\right)$ and $\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$, each product in the product orders $\lambda$ and $\lambda^{\prime \prime}$ corresponding to the job orders $\dot{x}^{\prime}$ and $\dot{x}^{\prime \prime}$ must wait for the completion of all the parts belonging to the product before it can be assembled. In addition, the job orders $\bar{x}_{h}$ and $\bar{x}^{\prime \prime}{ }_{h}$ corresponding to the same product $\lambda_{h}$ in the job orders $\dot{x}^{\prime}$ and $\dot{x}^{\prime \prime}$ are the same. Since $P t_{x_{h}, j}^{f} \leqslant$
$P t_{x_{h}, j}^{f} \quad \forall j \in M$ and $C_{\max }\left(\dot{x}^{\prime}, \dot{V}\right)=C_{\max }\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$, then it has $S t_{x_{h}, j}^{f} \geqslant S t_{x_{h}^{\prime}, j}^{f}$ $\forall j \in M$. Since $\dot{V} \neq V^{\prime \prime}$, then $\exists j \in M$ makes $P t_{x_{h}^{\prime}, j}^{f}<P t_{x_{h}^{\prime}, j}^{f}$, that is, it has $S t_{x_{h}, j}^{f}>\mathrm{St}_{x_{h}, j}^{f} \quad$. Therefore, the total energy consumption in Eq. (12) for the standby state is satisfied. According to $\operatorname{TCE}\left(\dot{x}^{\prime}, \dot{V}\right)=\epsilon\left(\mathrm{E}\left(\dot{x}^{\prime}, \dot{V}\right)+\mathrm{SE}\left(\dot{x}^{\prime}, \dot{V}\right)\right)$ and $\operatorname{TCE}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)=\epsilon\left(\mathrm{E}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)+\right.$ $\mathrm{SE}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$ ) in Property 1, the total energy consumption in the processing state satisfies $\mathrm{E}\left(\dot{x}^{\prime}, \dot{V}\right)>\mathrm{E}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$, so it has $\operatorname{TCE}\left(\dot{x}^{\prime}, \dot{V}\right)>\operatorname{TCE}\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$. Considering that $C_{\max }\left(\dot{x}^{\prime}, \dot{V}\right)=C_{\max }\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$, the dominance relationship $\left(\dot{x}^{\prime}, \dot{V}\right)<\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$ between $\left(\dot{x}^{\prime}, \dot{V}\right)$ and $\left(\dot{x}^{\prime \prime}, V^{\prime \prime}\right)$ is met. Property 2 is proved.

According to above properties, a reasonable compromise between the two objectives of $C_{\max }$ and TCE can be achieved, that is, the total carbon emissions can be reduced by appropriately adjusting the speed of the machines in each factory, while the maximum completion time remains unchanged.

It should be clear that for all types of flow shop scheduling problems, the critical path directly determines the maximum completion time of any feasible solution (Wang \& Wang, 2016). In this section, according to Property 2, we present an energy-saving speed adjustment strategy that can adjust the processing speed of some jobs on non-critical paths while keeping the processing speed of jobs on the critical path unchanged. The proposed energy-saving speed adjustment strategy can ensure that the maximum completion time of the scheduling solution remains unchanged, while effectively reducing the total carbon emissions, and improving the algorithm's ability to obtain high-quality solutions with low-energy consumption. The energy-saving speed adjustment strategy is provided as follows.

Step 1: Identify a critical path of the feasible solution $\left(\boldsymbol{x}, \boldsymbol{V}\right)$. The critical path directly determines the value of the production efficiency criterion (i.e., the maximum completion time $C_{\max }$ ). If more than one critical path exists for the same $C_{\max }$, then one of the critical paths is randomly selected.

Step 2: Determine whether the job on the non-critical path meets the speed-down operation conditions: there is a certain amount of time margin between the completion time of the current job and the start time of the next job on the machine, and the processing speed of the job is not in the lowest gear. If the speed-down operation conditions are not met, then no speed reduction is required, otherwise continue to the next step.

Step 3: Reduce the processing speed of the job by one level. Since all processing speeds considered are a series of discrete values, the speeddown operation needs to be performed to ensure that the increment of the job processing time is within the time margin and does not affect the critical path identified in Step 1. If the critical path is affected, the speeddown operation is not performed, otherwise skip to Step 2 and continue to perform speed reduction on the remaining jobs until all jobs on the non-critical path have been executed the speed-down operation.

For ease of understanding, Fig. 4 provides a diagram of the energysaving speed adjustment strategy for EE_DAPFSP when $n=8, m=$ $2, F=2, S=2$. Firstly, a critical path for the feasible scheduling solution is determined, as shown in Fig. 4(a). It can be seen that the critical factory is Factory 1, and the corresponding critical operations are $\mathrm{O}_{3,1}$, $O_{9,1}, O_{9,2}$. Then, the time margins for a series of non-critical operations $O_{3,2}, O_{2,2}, O_{5,2}, O_{5,1}$ and $O_{1,2}, O_{6,2}, O_{7,2}, O_{4,1}, O_{4,2}$ in factory 1 and factory

2 are calculated as shown in Fig. 4(b). Finally, the speed-down operations are performed and the time margins are updated for the jobs on the non-critical path from back to front, as illustrated in Fig. 4(c). Fig. 4(d) gives the Gantt chart corresponding to the obtained solution with lowenergy consumption by using the energy-saving speed adjustment strategy. Then, the Pareto archive is updated after the speed adjustment.

Algorithm 2: Multi-objective initialization method
updating mechanism and sampling strategy are proposed, respectively. Then, the critical path based local search and the speed adjustment strategies are designed. Finally, the overall process of MCEDA is described and the computational complexity of MCEDA is briefly analyzed.

### 3.1. Solution representation and population initialization

The solution representation is of great importance to the meta-

Input: Non-dominated solution set $N S$.
1: All products are sorted according to their corresponding assembly time in ascending order, and the product order is $\lambda^{\prime}=\left[\lambda_{1}^{\prime}, \lambda_{2}^{\prime}, \ldots, \lambda_{n}^{\prime}\right]$.
2: Initialize processing speed matrix $\mathbf{V}:=\mathbf{V}^{\prime}$. Initialize non-dominated solution set $N S:=\varnothing$.
3: for $h=1$ to $S$ do
4: The jobs of product $\lambda_{h}^{\prime}$ in product order $\lambda^{\prime}$ are sorted in ascending order by the total completion time. Then the job order of product $\lambda_{h}^{\prime}$ after sorting is $\hat{\pi}_{h}$.
5: end for
6: The complete job order $\pi^{\prime}$ is constructed by combining each sub-sequence $\hat{\pi}_{h}$ according to the product order $\lambda^{\prime}=\left[\lambda_{1}^{\prime}, \lambda_{2}^{\prime}, \ldots, \lambda_{n}^{\prime}\right]$.
7: Evaluate $\left(\pi^{\prime}, \mathbf{V}^{\prime}\right)$ and update $N S \leftarrow\left(\pi^{\prime}, \mathbf{V}^{\prime}\right)$.
8: for $h=1$ to $S$ do
9: $\quad$ Select the job order $\hat{\pi}_{h}$ of product $\lambda_{h}^{\prime}$, where $\omega_{h}$ is the number of jobs in $\hat{\pi}_{h} . N S_{l}:=\varnothing$.
10: for $l=1$ to $\omega_{h}$ do
11: $\quad$ Remove the $l$ th job from $\hat{\pi}_{h}$ and reinsert it into the $\omega_{h}+1$ positions of $\hat{\pi}_{h}$ in turn.
12: Evaluate the solution to obtain $C_{\max }$ and TCE . The non-dominated solution set obtained after insertion operation is $N S_{l}$, and update $N S: N S \leftarrow N S \quad N S_{l}$.
13: end for
14: end for
15: while $|N S|>10 \% \times p s$ do
16: Calculate the crowded distance of solutions in $N S$ according to Algorithm 1.
17: Remove solutions with the smallest crowded distance from $N S$ until the condition is met.
18: end while
Output: Non-dominated solution set $N S$.

## 3. MCEDA for EE_DAPFSP

In this section, a multidimensional distribution estimation algorithm (MCEDA) is presented to solve the EE_DAPFSP. Firstly, the solution representation and population initialization are provided. Secondly, the matrix-cube-based multidimensional probabilistic model and its
heuristics. The PFSP usually uses a coding sequence of jobs $\boldsymbol{x}=|\boldsymbol{x}| 1$ ). $\boldsymbol{x}(2), \ldots, \boldsymbol{x}(n)$ ) that can directly determine the processing priority of $n$ jobs to represent a feasible solution of the problem (Ding et al., 2016; Lu et al., 2017). For the job order $\boldsymbol{x}$ of the EE_DAPFSP, in order to obtain a feasible scheduling scheme, the factory allocation rule $\mathrm{NR}_{2}$ proposed by Hatami et al. (2013) is employed to decode the job order $\boldsymbol{x}$ and assign each job to each factory in turn. Once the job order $\boldsymbol{x}$ is determined and the corresponding processing speeds for all operations are assigned, and then a feasible solution is obtained. Therefore, the feasible solution representation of the EE_DAPFSP studied in this paper needs to consider not only the processing sequence $\boldsymbol{x}$ for jobs but also the processing speed

![img-7.jpeg](img-7.jpeg)

Fig. 5. An illustrative example of the accumulation process of job blocks by $M C_{4 \rightarrow 4}^{8}$.
matrix $\boldsymbol{V}$ for machines, so a feasible solution can be denoted as $(\boldsymbol{\pi}, \boldsymbol{V})$. Thus, the production efficiency and the energy consumption criteria are denoted as $\mathrm{C}_{\max }(\pi, V)$ and $\operatorname{TCE}(\pi, V)$, respectively.

It needs to be noted that the size of the set $\boldsymbol{\Pi}$ of feasible coding sequences is $n^{t}$, the size of the set of feasible speed matrices $\boldsymbol{\Sigma}$ is $d^{\text {ten }}$, and the solution spaces of $\boldsymbol{\Pi}$ and $\boldsymbol{\Sigma}$ are independent of each other. Compared with the solution space size $n^{t}$ of the traditional PFSP, the feasible solution space size of the EE_DAPFSP is as large as $S!\times \prod_{h=1}^{n}\left|N_{h}\right|!\times d^{\text {ten }}$. Obviously, the significant expansion of the feasible search space for the considered problem requires much more computational efforts to find the global optimal solutions or at least the near-optimal solutions, and it is need to develop some high-performing algorithms for solving the EE_DAPFSP. To be specific, in order to decode a feasible solution and obtain a scheduling scheme, firstly, each job in the job order $\boldsymbol{\pi}$ is sequentially assigned to machines of different factories by means of the NR2 rule. That is, the completion time of each job in each factory is determined respectively according to the corresponding processing speed in $V$. Secondly, each job is allocated to the factory that can complete all processing processes of the job at the earliest time, and then the sub-sequence of jobs $\left[\boldsymbol{\pi}_{1}, \boldsymbol{\pi}_{2}, \ldots, \boldsymbol{\pi}_{F}\right]$ in all factories is obtained. Then, the assembly order of the products is determined according to the processing completion time of the corresponding jobs for each product. Finally, the maximum completion time $C_{\max }(\boldsymbol{\pi}, \boldsymbol{V})$ and the total carbon emission $\operatorname{TCE}(\boldsymbol{\pi}, \boldsymbol{V})$ of the feasible solution $(\boldsymbol{\pi}, \boldsymbol{V})$ can be calculated according to Eq. (8) and Eq. (9) in Section 2.

In order to take into account both the quality and the diversity of the feasible solutions in the initial population, we employ a hybrid initialization strategy that combines the effective constructive heuristic method and the randomization method to generate the initial population. To be specific, $10 \%$ of the feasible solutions in the initial population are produced by using the multi-objective initialization method given in Algorithm 2, and the remaining $90 \%$ of the other solutions are generated
randomly. For the multi-objective optimization problems, the nondominated set $N S$ is usually used to record and reserve all of the obtained non-dominated solutions. The expression $N S \leftarrow N S \cup N S_{i}$ in Algorithm 2 represents the set of non-dominated solutions picked from the union of the two sets $N S$ and $N S_{i}$. The notation $|N S|$ refers to the number of elements in the non-dominated set $N S$. Note that the initialization of the processing speed matrix $V$ for each non-dominated solution $(\boldsymbol{\pi}, \boldsymbol{V})$ also has a certain impact on the performance of the algorithm. If a higher initial processing speed is set, it would be more favorable to optimize the maximum completion time $C_{\max }(\boldsymbol{\pi}, \boldsymbol{V})$; while if a lower initial processing speed is set, it would be more inclined to optimize the total energy consumption $\operatorname{TCE}(\boldsymbol{\pi}, \boldsymbol{V})$. Therefore, in order to achieve a reasonable trade-off between these two criteria, different speed levels should be adopted to initialize the processing speed matrix $V$ in the initialization process of the population, and then all non-dominated solutions obtained at different initial processing speeds are merged. Then, the top $10 \%$ of high-quality solutions are selected as a part of the initial population via Algorithm 1, while the remaining $90 \%$ of the solutions in the initial population are generated by using randomization method. Meanwhile, to ensure the fairness of the computational comparisons, the proposed population initialization method is used for both the presented MCEDA and the compared algorithms in the subsequent experimental sections.

### 3.2. Multi-dimensional probabilistic model

Since most of EDAs were proposed based on the two-dimensional probabilistic models, these two-dimensional probabilistic models cannot learn the promising patterns adequately. In this subsection, a multi-dimensional probabilistic model is designed to reasonably learn and accumulate the structural characteristics and promising patterns of the superior solutions, i.e., the order relation information of jobs and the position information of job blocks. which can effectively guide the search direction toward the potential regions in the solution space. Then, the framework of the multi-dimensional probabilistic model is provided by introducing the block structure and the matrix cube, the updating mechanism and the sampling strategy, respectively.

### 3.2.1. Block structure and matrix cube

For the feasible scheduling solution $(\boldsymbol{\pi}, \boldsymbol{V})$ of the EE_DAPFSP, the job block is first defined as the two consecutive adjacent jobs in the job order $\boldsymbol{\pi}$. Obviously, $\boldsymbol{\pi}$ can be composed of all the job blocks that appear at different positions in the job order. For $n$ different jobs, there is a total of $(n-1)^{2}$ job blocks. The same job blocks appearing at different positions in the job order $\boldsymbol{\pi}$ of each feasible solution $(\boldsymbol{\pi}, \boldsymbol{V})$ are defined as the similar blocks. For example, for two job orders $\boldsymbol{\pi}=\{3.2 .1 .4\}$ and $\boldsymbol{\pi}^{\prime \prime}=$ $\{4.3 .2 .1\}$, there are a total of four job blocks, i.e., $\{3.2\},\{2.1\},\{1.4\}$ and $\{4$. 3]. Since the two job blocks $\{3.2\}$ and $\{2.1\}$ appear both in $\boldsymbol{\pi}$ and $\boldsymbol{\pi}^{\prime \prime}$, then the job blocks $\{3.2\}$ and $\{2.1\}$ are similar blocks. In order to investigate the distribution characteristics of the total job blocks in the high-quality solutions of the considered problem, in this subsection, a data structure of three-dimensional matrix cube is designed to reasonably record and reserve the total order relation information of the jobs and the distribution information of the job blocks. Moreover, the matrix cube can also appropriately learn and accumulate the valuable structural characteristics of superior solutions in a statistical way, and then can be used to construct a more effective probabilistic model. Without loss of generality, let $\operatorname{Pop}(\boldsymbol{G})$ be the population at $G$ th generation, where $G=0.1 \ldots$

. $\operatorname{Max} G$ and $p s$ is the size of $\operatorname{Pop}(G)$. Let $\operatorname{SPop}(G)=\left\{\pi_{\text {shor }}^{G, 1}, \pi_{\text {shor }}^{G, 2}, \cdots \pi_{\text {shor }}^{G, q s}\right\}$ be the promising solutions or the superior subpopulation extracted from $\operatorname{Pop}(G)$, where $s p s$ is the size of $\operatorname{SPop}(G)$. It is clear that $\pi_{\text {shor }}^{G, k}$ is the $k$ th individual in $\operatorname{SPop}(G)$, which can be denoted as $\pi_{\text {shor }}^{G, k}=\left[\pi_{\text {shor }, 1}^{G, k}, \pi_{\text {shor }, 2}^{G, k} \cdots\right.$ $\left.\cdot \pi_{\text {shor }, n}^{G, k}\right](k=1, \ldots, s p s)$. Let $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ denote the matrix cube at $G$ th generation, where $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{z})(x=1, \ldots, n-1 ; y, z=1, \ldots, n)$ with the subscript $(x, y, z)$ represents the element in $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ is used to record and reserve the information of the job relations and the distribution of similar blocks of high-quality subpopulation in $G$ th generation. $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ is described as follows.

$$
\begin{aligned}
& l_{\boldsymbol{M} C_{\alpha=\alpha}^{G, 1}(x, y, z)}^{(i)}=\left\{\begin{array}{r}
1, y=\pi_{\text {shor }, x}^{G, k} \text { and } z=\pi_{\text {shor }, x+1}^{G, k} \\
0, \text { else }
\end{array}\right. \\
& x=1,2, \ldots, n-1 ; y, z=1,2, \ldots, n ; k=1,2, \ldots, \quad \text { sps. } \\
& M C_{\alpha=\alpha=\alpha}^{G}(x, y, z)=\sum_{k=1}^{s p s} l_{\boldsymbol{M} C_{\alpha=\alpha}^{G, 1}(x, y, z)}^{G, k}, x=1,2, \ldots, n-1 ; y, z=1,2, \ldots, n . \\
& \boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{G}(x, y)=\left[M C_{\alpha=\alpha=\alpha}^{G}(x, y, 1), M C_{\alpha=\alpha=\alpha}^{G}(x, y, 2), \ldots, M C_{\alpha=\alpha=\alpha}^{G}(x, y, n)\right]_{n \rightarrow \alpha},
\end{aligned}
$$

where $l_{\boldsymbol{M} C_{\alpha=\alpha=\alpha}^{G}(x, y, z)}$ in Eq. (13) is an indicator function, which is used to record the information of job blocks of the $k$ th solution $\pi_{\text {shor }}^{G, k}$ in $\operatorname{SPop}(G)$, that is, the number of occurrences of job blocks $\left[\pi_{\text {shor }, x}^{G, k}, \pi_{\text {shor }, x+1}^{G, k}\right]$ at the $x$ th position in the $\pi_{\text {shor }}^{G, k}$ (i.e., $y=\pi_{\text {shor }, x}^{G, k}, z=\pi_{\text {shor }, x+1}^{G, k}$ ). The element $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{G}(x, y, z)$ in Eq. (14) is used to accumulate the number of job blocks, i.e., to count the distribution of job blocks among all individuals in $\operatorname{SPop}(G)$. Eqs. (15) and (16) give the specific hierarchical structure of the matrix cube, where the two-dimensional submatrix $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(x)$ is used to store the total frequency information of the job block $\left[\pi_{\text {shor }, x}^{G, k}\right.$. $\left.\pi_{\text {shor }, x+1}^{G, k}\right]$ at the $x$ th position of all individuals in the $\operatorname{SPop}(G)$. Obviously, by using the matrix cube $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ composed of a series of the position relation based two-dimensional matrices $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(\mathbf{1}), \boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(\mathbf{2}), \ldots$, $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(\boldsymbol{n})$ in Eq. (16), the proposed multi-dimensional probabilistic model can accurately and effectively learn and preserve all the information of both the ordinal relation of jobs and the distribution of job blocks of high-quality individuals in an intuitive way. Considering five high-quality individuals with $s p s=5$, i.e., $\pi_{\text {shor }}^{1,1}=[1.2 .3 .4], \pi_{\text {shor }}^{1,2}=[2.3$, $1,4], \pi_{\text {shor }}^{1,3}=[3.2,1,4], \pi_{\text {shor }}^{1,4}=[4.3,2,1]$ and $\pi_{\text {shor }}^{1,5}=[4.3,1,2]$, an illustration of the accumulation process of job blocks from these five high-quality individuals is given below, as shown in Fig. 5. Firstly, for the first position $(x=1)$ of all individuals from $\pi_{\text {shor }}^{1,1}$ to $\pi_{\text {shor }}^{1,2}$, the existing job blocks $[1,2]$ (i.e., $y=1, z=2$ ), $[2,3]$ (i.e., $y=2, z=3$ ), $[3,2]$ (i.e., $y=3, z=2$ ), and $[4,3]$ (i.e., $y=4, z=3$ ) can be recorded and reserved by $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ (1). Since the job block $[4,3]$ appears twice while the other job blocks appear only once, it can be concluded that $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(1,1,2)=$ $1, M C_{\alpha=\alpha=4}^{G}(1.2 .3)=1, M C_{\alpha=\alpha=4}^{G}(1.3 .2)=1$, and $M C_{\alpha=\alpha=4}^{G}(1.4 .3)=2$. Secondly, for the second position $(x=2)$ of all individuals, the existing
job blocks $[2,1]$ (i.e., $y=2, z=1$ ), $[2,3]$ (i.e., $y=2, z=3$ ), $[3,1]$ (i.e., $y=3, z=1$ ), and $[3,2]$ (i.e., $y=3, z=2$ ) can be recorded and reserved by $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(2)$. Meanwhile, we have $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(2,2,1)=1, \boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(2,2$, $3)=1, M C_{\alpha=\alpha=4}^{G}(2,3,1)=2$, and $M C_{\alpha=\alpha=4}^{G}(2,3,2)=1$, respectively. Finally, for the third position $(x=3)$ of all individuals, the existing job blocks $[1,2]$ (i.e., $y=1, z=2$ ), $[1,4]$ (i.e., $y=1, z=4$ ), $[2,1]$ (i.e., $y=2$, $z=1$ ), and $[3,4]$ (i.e., $y=3, z=4$ ) can be recorded and reserved by $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(3)$, that is, $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(3,1,2)=1, \boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(3,1,4)=2$, $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(3,2,1)=1$, and $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(3,3,4)=1$. It should be noted that since the useful information of the job block in the last position $(x=4)$ is already contained in the previous position $(x=3)$, all elements in $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ are set to zero directly.

It can be seen from Fig. 5 that all of the job blocks or similar blocks of each excellent individual located at different positions can be fully learned and preserved in $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$. For the two-dimensional probabilistic model based EDA commonly used in the literature (Pan \& Ruiz, 2012; Jarboui et al., 2009; Wang \& Wang, 2016), the structural information of similar blocks $[2,3],[4,3]$, and $[1,4]$ existed in $\boldsymbol{\pi}_{\text {shor }}^{1,1}$ to $\boldsymbol{\pi}_{\text {shor }}^{1,5}$ is only stored in the same subscript $(2,3),(4,3)$, and $(1,4)$ by using the two-dimensional matrices, which may not be able to accurately distinguish the specific location of each job block in the elite solutions and inevitably lead to confusion about the location of the promising blocks. As a result, the two-dimensional probabilistic model based EDA is unable to effectively determine the proper positions to place these promising similar blocks when sampling the two-dimensional probabilistic model to generate new individuals. However, for the designed matrix cube $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$, these valuable similar blocks can be respectively record and retained in different layers of $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ depending on their specific positions. It is clear that the similar block $[2,3]$ in $\boldsymbol{\pi}_{\text {shor }}^{1,1}=[1,2,3,4]$ is recorded in $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ (2) (i.e., the second layer of $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ ), while the similar block $[2,3]$ in $\boldsymbol{\pi}_{\text {shor }}^{1,2}=[2,3,1,4]$ is reserved in $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}(1)$ (i.e., the first layer of $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$ ), respectively. That is, the promising patterns of all job blocks or similar blocks are properly learned by employing the proposed matrix cube, which can be used to bulid more effective probabilistic model, while also avoiding the destruction or improper fusion of promising patterns.

### 3.2.2. Updating mechanism

The probabilistic models are crucial to the EDAs, and the effectiveness and reasonableness of the designed probabilistic model directly affects the performance of the algorithm (Zhang et al., 2021). Different from the two-dimensional probabilistic models, in this subsection, a novel matrix-cube-based multidimensional probabilistic is presented to learn and accumulate the valuable information of the relation of jobs and the distribution of similar blocks in high-quality subpopulation. For descriptive convenience, define $\boldsymbol{P M}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{G}$ as a multidimensional probabilistic model based on $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{\alpha}=\boldsymbol{\alpha}=\boldsymbol{\alpha}}^{(i)}$, where $\boldsymbol{P M}_{\alpha=\alpha=\alpha}^{G}(x, y, z)$ $(x=1, \ldots, n-1 ; y, z=1, \ldots, n)$ is the element of $\boldsymbol{P M}_{\alpha=\alpha=\alpha}^{G}$. Then, the formal definition of the probability distribution of the job blocks at the $x$ th position in the job order $x$ of the selected superior solutions is shown in Eq. (17).

![img-8.jpeg](img-8.jpeg)

Fig. 6. An illustrative example of the update process of the proposed probabilistic model.
$\boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}(\boldsymbol{x})=\left[\begin{array}{c}\boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}(\boldsymbol{x}, \boldsymbol{1}) \\ \vdots \\ \boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}(\boldsymbol{x}, \boldsymbol{n})\end{array}\right]$

$$
=\left[\begin{array}{cccc}
\boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}(\boldsymbol{x}, \boldsymbol{1}, \boldsymbol{1}) & \cdots & \boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}(\boldsymbol{x}, \boldsymbol{1}, \boldsymbol{n}) \\
\vdots & \ddots & \vdots \\
\boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}(\boldsymbol{x}, \boldsymbol{n}, \boldsymbol{1}) & \cdots & \boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}(\boldsymbol{x}, \boldsymbol{n}, \boldsymbol{n})
\end{array}\right]_{\boldsymbol{a}=\boldsymbol{a}}
$$

where the probability value in $\boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}(\boldsymbol{x})$ indicates the probability of occurrence of the job blocks $\left|a_{\text {sbest } . \boldsymbol{x}}^{G, k}, \boldsymbol{a}_{\text {sbest } . \boldsymbol{x}+1}^{G, k}\right|$ at the $x$ th position in the $\boldsymbol{a}_{\text {sbest }}^{G, k}\left(\right.$ i.e., $\left.\boldsymbol{y}=a_{\text {sbest }, \boldsymbol{x}}^{G, k}, \boldsymbol{z}=a_{\text {sbest }, \boldsymbol{x}+1}^{G, k}\right)$. In order to update the probabilistic model, let $N_{M C}^{G}(\boldsymbol{x})$ be the total number of job blocks that have appeared at the $x$ th position in the superior subpopulation $\boldsymbol{S P q q}(G)$, i.e., $N_{M C}^{G}(\boldsymbol{x})=$ $\sum_{i=1}^{n} \sum_{j=-1}^{n} M C_{N-i \cdot n}^{G}(x, y, z), N_{P M}^{G}(x)$ be the sum of all the probabilities of different job blocks appearing at the $x$ th position in $\boldsymbol{S P q q}(G)$, i.e., $N_{P M}^{G}(\boldsymbol{x})=\sum_{i=1}^{n} \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots$ $P M_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}(x, y, z)=\left\{\begin{array}{cccc}0, & 1 & 0 & 2 \\ 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0\end{array}\right\}\left[\begin{array}{cccc}0 & 1 & 0 & 2 \\ 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 2 & 0\end{array}\right]\left[\begin{array}{cccc}0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{array}\right], x \geq 2 ; 1\}$.
$\boldsymbol{\pi}_{\text {sbest }}^{G, 1}=\left[\begin{array}{llll}2,3,1,4 \\ \pi_{\text {sbest }}^{\boldsymbol{G}, 1} & 0,063 & 0.063 \\ 0.063 & 0.063 & 0.063 \\ 0.063 & 0.063 & 0.063 \\ 0.063 & 0.063 & 0.063 & 0.063\end{array}\right]$
$\boldsymbol{\pi}_{\text {sbest }}^{G, 3}=\left[\begin{array}{llll}2,3,1,4 \\ \pi_{\text {sbest }}^{\boldsymbol{G}, 1} & 0,063 & 0.063 \\ 0.063 & 0.063 & 0.063 \\ 0.063 & 0.063 & 0.063 & 0.063\end{array}\right]$
$\boldsymbol{\pi}_{\text {sbest }}^{G, 1}=\left[\begin{array}{llll}2,3,1,4 \\ \pi_{\text {sbest }}^{\boldsymbol{G}, 1} & 0,063 & 0.063 \\ 0.063 & 0.063 & 0.063 \\ 0.063 & 0.063 & 0.063 & 0.063\end{array}\right]$
$\boldsymbol{\pi}_{\text {sbest }}^{G, 3}=\left[\begin{array}{llll}2,3,1,4 \\ \pi_{\text {sbest }}^{\boldsymbol{G}, 1} & 0,063 & 0.063 \\ 0.063 & 0.063 & 0.063 \\ 0.063 & 0.063 & 0.063 & 0.063\end{array}\right]$

Step 4: Set $G=G+1$. If $G<\operatorname{Max} G$, then go to Step 3.
Note that all probability values in the in the first layer of $\boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{B}}$ are set to 0 while others are set to $1 / n^{2}$ in Eq. (18), which can learn the initial structural features of superior sub-population and increase the guidance toward potential regions at the initial phase. Moreover, the probabilistic model $\boldsymbol{P M}_{\boldsymbol{a}=\boldsymbol{a}=\boldsymbol{a}}^{\boldsymbol{G}}$ in step 3 can keep learning the promising patterns extracted from the superior subpopulation and progressively accumulate the useful information of similar blocks by using an

adjustable learning rate to make a trade-off between historical and current information. At the end of the update process, the normalization is necessary for the probabilistic model $\boldsymbol{P M}_{n \times n \times n}^{G}$. In order to illustrate the proposed probabilistic model $\boldsymbol{P M}_{n \times n \times n}^{G}$ clearly, the update process of $\boldsymbol{P M}_{n \times n \times n}^{G}$ is given in Fig. 6, where the learning rate $r$ is set to 0.5 .

### 3.2.3. Sampling strategy

The probabilistic model is able to implicitly estimate the distribution characteristics of the superior solutions in the solution space by converting characteristic information into corresponding probability values. In order to appropriately apply these stored promising patterns from the high-quality solutions by means of $\boldsymbol{M} \boldsymbol{C}_{n \times n \times n}^{G}$, it is necessary to design an effective sampling strategy for the proposed multidimensional probabilistic model $\boldsymbol{P M}_{n \times n \times n}^{G}$. Let $\boldsymbol{\pi}^{G, k}=\left[\pi_{1}^{G, k}, \pi_{2}^{G, k}, \ldots, \pi_{n}^{G, k}\right]$ denote the $k$ th individual in $\boldsymbol{P o p}(\boldsymbol{G})$, and $\boldsymbol{R}_{\boldsymbol{P M}}^{G}=\left[R_{\boldsymbol{P M}}^{G}(1), R_{\boldsymbol{P M}}^{G}(2), \ldots\right.$ $\left.R_{\boldsymbol{P M}}^{G}(n)\right]$ denote the temporary row vector. Define SelectJob $\left(\boldsymbol{\pi}^{G, k}, i\right)(i>1)$ as the job selection function, which is used to determine one candidate job $J_{s}$ at the $i$ th position of $\boldsymbol{\pi}^{G, k}$. Since the probability information that the job block $\left[\pi_{1}^{G, k}, \pi_{i}^{G, k}\right]$ is selected to locate in the $(i-1)$ th position in $\boldsymbol{\pi}^{G, k}$ is stored in the $\boldsymbol{P M}_{n \times n \times n}^{G}(\boldsymbol{i}-\mathbf{1})$, where $i>1$, the job selection function SelectJob $\left(\boldsymbol{\pi}^{G, k}, i\right)$ is used to sample only by means of the $(i-1)$ th
regions found by the superior solutions, a specially designed sampling strategy is used to determine the first job of $\boldsymbol{\pi}^{G, k}$, namely first position limited sampling strategy (FPLSS), which is described in Algorithm 4. It is clear that the $\boldsymbol{P M}_{n \times n \times n}^{G}(\mathbf{1})$ holds the total probability information of various job blocks that appeared in the first position of all the selected superior solutions in $\boldsymbol{S P o p}(\boldsymbol{G})$ before the $G$ th generation. In Algorithm 4, lines $1-4$ are used to calculate the cumulative probability of each row vector in $\boldsymbol{P M}_{n \times n \times n}^{G}(\mathbf{1})$, and lines $5-13$ are used to generate the first job $\pi_{1}^{G, k}$ of $\boldsymbol{\pi}^{G, k}$ by means of the roulette wheel selection, which is helpful in controlling the search direction reasonably. The procedure of new population generation is provided in Algorithm 5.

```
Algorithm 3:SelectJob \(\left(\boldsymbol{P M}_{n \times n \times n}^{G}(\boldsymbol{i}-\mathbf{1}), \boldsymbol{\pi}^{G, k}, i\right)\)
```

```
Input: \(\mathrm{PM}_{\mathrm{n}=\mathrm{n}=\mathrm{n}}^{G}(\mathbf{i}-\mathbf{1}), \pi^{G, \mathrm{k}}\) and \(i\).
    Produce a random value \(p_{r}\) where \(p_{r} \in\left[0, \sum_{h=1}^{n} P M_{n=n=n}^{G}\left(i-1, \pi_{i-1}^{G, k}, h\right)\right)\).
        if \(p_{r} \in\left[0, P M_{n=n=n}^{G}\left(i-1, \pi_{i-1}^{G, k}, 1\right)\right)\) then
            \(J_{s} \leftarrow 1\).
        else
        for \(t=1\) to \(n-1\) do //Roulette wheel selection.
            if \(p_{r} \in\left[\sum_{h=1}^{t} P M_{n=n=n}^{G}\left(i-1, \pi_{i-1}^{G, k}, h\right), \sum_{h=1}^{t+1} P M_{n=n=n}^{G}\left(i-1, \pi_{i-1}^{G, k}, h\right)\right)\) then
                \(J_{s} \leftarrow t+1\), break.
            end if
        end for
    end if
    for \(t=i\) to \(n-1\) do //Avoid repeated selection of jobs.
        for \(j=1\) to \(n\) do
            \(P M_{n=n=n}^{G}\left(t, j, J_{s}\right)=0\).
        end for
    end for
```

Output: the candidate job $J_{s}$.
layer of the probabilistic model $\boldsymbol{P M}_{n \times n \times n}^{G}$, and then the procedure of SelectJob $\left(\boldsymbol{\pi}^{G, k}, i\right)$ is described in Algorithm 3. Note that the job selection function SelectJob $\left(\boldsymbol{\pi}^{G, k}, i\right)$ depends on the job $\pi_{i-1}^{G, k}$ at the $(i-1)$ th position when selecting the job $\pi_{i}^{G, k}$ at the $i$ th position in $\boldsymbol{\pi}^{G, k}$. Because the job $\pi_{0}^{G, k}$ does not exist, SelectJob $\left(\boldsymbol{\pi}^{G, k}, i\right)$ cannot be adopted to determine the first job $\pi_{1}^{G, k}$ of $\boldsymbol{\pi}^{G, k}$. In order to guide the search direction toward promising

It should be pointed out that line 6 in Algorithm 5 is used to build the promising job block (i.e., $\left.\left|\pi_{i-1}^{G, k}, \pi_{i}^{G, k}\right|\right)$ at positions $i-1$ and $i$ by using the roulette wheel selection rule on the row vector $\boldsymbol{P M}_{n \times n \times n}^{G}\left(i-1, \pi_{i-1}^{G, k}\right)$. Meanwhile, the promising job blocks at different positions can be linked together via order relation information in lines $2-9$ to produce a new individual, which is a key step of MCEDA's global exploration. The larger the probability value corresponding to the job block, the more

likely it is to be selected during the sampling process. Since the large values and their subscripts in $\boldsymbol{P M}_{\boldsymbol{n}=\boldsymbol{n}=\boldsymbol{n}}^{\mathrm{G}}$ are determined by the excellent individuals, new generated individuals can inherit more promising patterns or blocks at suitable positions. Therefore, MCEDA can better guide the search to promising regions in solution space. In Algorithm 3, the computational complexity in lines $2-10$ is $O(n)$ and the complexity in lines $11-15$ is $O\left(n^{2}\right)$. In Algorithm 4, the computational complexity in

In Algorithm 5, the computational complexity in line 4, line 6, and lines $1-11$ are $O\left(n^{2}\right), O\left(n^{2}\right)$ and $O\left(p s \times n^{2}\right)$, respectively. Thus, the total complexity of Algorithm 5 is $O\left(p s \times n^{2}\right)$.

```
Algorithm 4:FPBSS \(\left(\boldsymbol{P M}_{\boldsymbol{n}=\boldsymbol{n}=\boldsymbol{n}}^{\mathrm{G}}(\mathbf{1}), \boldsymbol{\pi}^{\mathrm{G}, \mathbf{k}}\right)\)
```

Input: $\mathbf{P M}_{\mathbf{n}=\mathbf{n}=\mathbf{n}}^{\mathrm{G}}(\mathbf{1}), \boldsymbol{\pi}^{\mathrm{G}, \mathbf{k}}$.
for $y=1$ to $n$ do
$R_{P M}^{G}(y)=\sum_{z=1}^{n} P M_{n \times n \times n}^{G}(1, y, z) . / /$ Calculate the cumulative probability.
end for
4: $\quad$ Produce a random value $p_{r}$ where $p_{r} \in\left[0, \sum_{y=1}^{n} R_{P M}^{G}(y)\right)$.
5: if $p_{r} \in\left[0, R_{P M}^{G}(1)\right)$ then
6: $\quad J_{s} \leftarrow 1$.
7: else
8: for $t=1$ to $n-1$ do //Roulette wheel selection.
9: if $p_{r} \in\left[\sum_{y=1}^{t} R_{P M}^{G}(y), \sum_{y=1}^{t+1} R_{P M}^{G}(y)\right)$ then
$J_{s} \leftarrow t+1$, break.
end if
end for
13: end if
14: for $t=1$ to $n-1$ do //Avoid repeated selection of jobs.
15: for $j=1$ to $n$ do
$P M_{n \times n \times n}^{G}\left(t, j, J_{s}\right)=0$.
17: end for
18: end for
Output: the candidate job $J_{s}$.
line 2, lines $5-13$ and lines $14-18$ are $O\left(n^{2}\right), O(n)$, and $O\left(n^{2}\right)$, respectively. Thus, the complexity of Algorithm 3 and Algorithm 4 are $O\left(n^{2}\right)$.

```
Algorithm 5: New Population Generation
```

```
Input: \(\mathbf{P M}_{\mathbf{n}=\mathbf{n}=\mathbf{n}}^{\mathbf{G}}, \boldsymbol{\pi}^{\mathbf{G}, \mathbf{k}}, \operatorname{Pop}(\mathbf{G}+\mathbf{1})\).
    for \(k=1\) to ps do
    for \(i=1\) to \(n\) do
        if \(i=1\) then
            \(J_{s} \leftarrow F P B S S\left(\mathbf{P M}_{\mathbf{n}=\mathbf{n}=\mathbf{n}}^{\mathbf{G}}(\mathbf{1}), \boldsymbol{\pi}^{\mathbf{G}, \mathbf{k}}\right) . / /\) Algorithm 4
        else
            \(J_{s} \leftarrow \operatorname{SelectJob}\left(\mathbf{P M}_{\mathbf{n}=\mathbf{n}=\mathbf{n}}^{\mathbf{G}}(\mathbf{i}-\mathbf{1}), \boldsymbol{\pi}^{\mathbf{G}, \mathbf{k}}, i\right) . / /\) Algorithm 3
        end if
            \(\pi_{i}^{G, k}=J_{s}\).
        end for
        \(\operatorname{Pop}(\mathbf{G}+\mathbf{1}) \leftarrow \operatorname{Pop}(\mathbf{G}+\mathbf{1}) \quad \boldsymbol{\pi}^{\mathbf{G}, \mathbf{k}}, \boldsymbol{\pi}^{\mathbf{G}, \mathbf{k}} \leftarrow \varnothing\).
```

### 3.3. Critical path-based local search

According to the property analysis of the considered problem in Subsection 2.3, it can be seen that the landscape of the feasible solution space of the EE_DAPFSP is complex and varied, resulting in a large number of high-quality non-dominated solutions are non-uniformly scattered in several local regions near the bottom of the feasible solution space. In order to enhance the depth search capability and achieve a satisfactory trade-off between exploration and exploitation, it is very necessary to conduct a deeper exploitation (local search) on the nearest neighbor regions of these non-dominated solutions found by the exploration (global search) of MCEDA. It is well known that the ability of the local search largely depends on the development of the neighborhood structure and the design of the neighborhood order. For the sequence model of PFSP, there are several commonly used neighborhood search operations, such as Insert, Swap, Interchange and Inverse. Notice that, for the DAPFSP, the intra-factory swap or insert operations and the interfactory swap or insert operations are commonly used neighborhood operations (Wang \& Wang, 2016; Zhang et al., 2021). Since the maximum completion time $C_{\max }(\pi, \boldsymbol{V})$ of each feasible solution $(\pi, \boldsymbol{V})$ is directly determined by the critical path (Wang \& Wang, 2016; Zhang
et al., 2021), it is possible to shorten the maximum completion time only by adjusting all jobs on the critical path. In this subsection, four kinds of the neighborhood structures are designed based on the problem-specific critical path, and then a variable neighborhood search method based on these neighborhood structures is performed for the high-quality solutions in the non-dominated set $\Omega$ obtained in MCEDA's global search. To be specific, the swap and insert operations within the factory are performed separately for each non-dominated solution, and then the swap and insert operations between factories are performed separately. It should be noted that the job on the critical path of the feasible solution $(\pi, \boldsymbol{V})$ is the critical job, the factory where the critical job is located is the critical factory, denoted as $f_{c}$. Let $n_{C}$ be the total number of jobs assigned to the critical factory $f_{c}$, and $n_{c}$ be the number of critical jobs in the critical factory. To make it more intuitive, an illustration of critical pathbased neighborhood operations is given in Fig. 7. Moreover, the four types of critical path-based neighborhood structures are described in detail as follows.
(1) Intra-factory Swap: Randomly select a critical job $\pi_{n}^{L}\left(u \in\{1.2 \ldots\right.$, $\left.n_{c}\right\})$ and exchange the positions of the critical job $\pi_{n}^{L}$ with each non-critical job $\pi_{n}^{L}\left(v=n_{C}+1, \ldots, n_{f_{c}}\right)$ within the critical factory $f_{c}$.
(2) Intra-factory Insertion: Randomly select a critical job $\pi_{n}^{L}\left(u \in\{1.2\right.$, $\left.\ldots, n_{c}\}\right)$ and insert the critical job $\pi_{n}^{L}$ before or after the position of
![img-9.jpeg](img-9.jpeg)

Critical job
(a) Intra-factory swap within the critical factory.
![img-10.jpeg](img-10.jpeg)

Critical job
(b) Intra-factory insertion within the critical factory.
![img-11.jpeg](img-11.jpeg)

Critical job
(c) Inter-factory swap between the critical factory and the other factory.
![img-12.jpeg](img-12.jpeg)

Critical job
(d) Inter-factory insertion between the critical factory and the other factory.

each non-critical job $x_{t}^{L}\left(v=n_{c}+1, \ldots, n_{f_{c}}\right)$ in the critical factory $f_{c}$, respectively.
(3) Inter-factory Swap: Randomly select a critical job $x_{t_{c}}^{L}\left(u \in\{1,2, \ldots\right.$, $\left.n_{c_{i}}\right\rangle$ ) and a non-critical job $x_{t_{c}}^{L}\left(v \in\{1,2, \ldots, n_{f_{c}}\}\right)$ in $F-1$ noncritical factories $f_{c}$, and exchange the positions of the critical job $x_{t_{c}}^{L}$ with each non-critical job $x_{t_{c}}^{L}$, respectively.
(4) Inter-factory Insertion: Randomly select a critical job $x_{t_{c}}^{L}\left(u \in\{1.2\right.$, $\left.\ldots, n_{c_{i}}\right\rangle$ ) and a non-critical job $x_{t_{c}}^{L}\left(v \in\left\{1,2, \ldots, n_{f_{c}}\right\}\right)$ in $F-1$ noncritical factories $f_{c}$, and insert the critical job $x_{t_{c}}^{L}$ before or after the position of each non-critical job $x_{t_{c}}^{L}$, respectively.

### 3.4. Speed adjustment strategy

In order to achieve a satisfactory tradeoff between the maximum completion time $C_{\max }(x, V)$ and total carbon emission $\operatorname{TCE}(x, V)$, the proposed algorithm not only needs to conduct the critical path based local search for the corresponding job order $x$ of each non-dominated solution $(x, V)$ obtained in each generation, but also needs to regulate the corresponding speed assignment matrix $V$ for each non-dominated solution $(x, V)$ appropriately. Since the processing speed of each machine in all factories is adjustable, it is necessary to further perform a series of speed adjustment strategies for each non-dominated solution $(x, V)$ obtained from the critical path based local search in Subsection 3.3. According to the problem property analysis in Subsection 2.3, two types of speed adjustment strategies are given as follows.
(1) The energy-saving speed adjustment strategy. As for the total carbon emission (TCE) criterion, the processing speed of each job on the critical path of feasible solution is kept unchanged, and the processing speed of the other jobs on the non-critical path should be suitably shortened to avoid excessive energy consumption and carbon emission as much as possible. This speed adjustment strategy (see Subsection 3.2) is given in Algorithm 6.
(2) The reduced-time speed adjustment strategy. As for the maximum completion time ( $C_{\max }$ ) criterion, the processing state of each job on the non-critical path of each feasible non-dominated solutions is kept unchanged, and the processing speed of each job on the critical path can be increased to further reduce the maximum completion time ( $C_{\max }$ ) as much as possible. To be specific, if there is a slightly larger time margin between the critical operation $O_{i, j}$ and the non-critical operation $O_{i-1, j}$ on any one machine $M_{j}$, the processing speed of the critical operation $O_{i, j-1}$ can be increased appropriately, so the corresponding processing time of $O_{i, j-1}$ would be shortened. Then, all related operations after $O_{i, j-1}$ can be shifted forward, and these operations can be completed as early as possible, which may directly advance the assembly completion time of the first product and all other subsequent products. As introduced in Table 1, this speed adjustment strategy can generate possible new non-dominated solutions, thereby enhancing the diversity and number of the obtained nondominated solutions.

The procedure of the reduced-time speed adjustment strategy is shown in Algorithm 7. For example, three operations $O_{2.1}, O_{8.1}, O_{8.2}$ on the critical path in Fig. 4(a) correspond to three critical operations, and there is a certain time margin between the critical operation $O_{8.2}$ and the non-critical operation $O_{3.2}$. Then, the processing speed of the critical operation $O_{8.1}$ can be increased, so that the processing time of this critical operation is shortened accordingly. All of operations after the critical operation $O_{8.1}$ can be moved from the backward to the forward in turn, which may allow the jobs and the product $P_{2}$ to be processed and
assembled as early as possible. In order to ensure that the nondominated solutions obtained by the proposed algorithm have good dispersion and diversity, different speed adjustment strategies are further carried out to selectively optimize different criteria, i.e., the makespan and the total carbon emission. For each non-dominated solution $(x, V)$ in Pareto archive $\Omega^{*}$, the maximum completion time $\left(C_{\max }\right)$ and the total carbon emission (TCE) are first normalized separately for each non-dominated solution, that is, $\bar{C}_{\max }(x, V)=\left(C_{\max }(x\right.$, $\left.V)-C_{\max }^{\min } /\left(C_{\max }^{\max }-C_{\max }^{\min } ; \overline{\operatorname{TCE}}(x, V)=\left(\operatorname{TCE}(x, V)-\operatorname{TCE}^{\min }\right) / \overline{\operatorname{TCE}}^{\max }\right.\right.$ $-\mathrm{TCE}^{\min }$, where $C_{\max }^{\min }, C_{\max }^{\max }$, $\mathrm{TCE}^{\min }$ and $\mathrm{TCE}^{\max }$ respectively represent the smallest $C_{\max }$, the largest $C_{\max }$, the smallest TCE and the largest TCE of all feasible solutions in the obtained non-dominated set. Then, the computational expression $\alpha(x, V)=(\overline{\operatorname{TCE}}(x, V)+e) /\left(\bar{C}_{\max }(x, V)+e\right)$ is defined as the preference level of each feasible solution $(x, V)$, where $e=$ 0.01 and the range of $\alpha$ is $(0.+\infty)$. Obviously, the larger value of $\alpha$ implies that the total carbon emission of the feasible solution $(x, V)$ are higher while the maximum completion time of this solution is smaller, so it is necessary to focus on reducing the total carbon emission rather than the maximum completion time. Conversely, the smaller value of $\alpha$ suggests that the total carbon emission of the feasible solution $(x, V)$ is lower but its maximum completion time is larger, then more attentions should be paid to optimize the production efficiency criterion. According to the different values of preference level $\alpha$ for each feasible solution $(x, V)$, two kinds of speed adjustment strategies are provided as follows.

Step 1: Identify a critical path for each non-dominated solution in non-dominated solution set $\Omega$. If there are multiple critical paths, one critical path is selected randomly.

Step 2: Perform the presented four types of critical path-based neighborhood search operations for each non-dominated solution in turn. If the obtained new solution dominates old solution, the new nondominated solution is used to replace the old one and re-determine the critical path. Then, continue to perform the subsequent neighborhood search operations. If they are not dominated by either of them, the obtained new solution is added to the non-dominated solution set $\Omega$, and other remaining neighborhood search operations continue to be performed on the current solution.

Step 3: Calculate $\bar{C}_{\max }(x, V)$ and $\overline{\operatorname{TCE}}(x, V)$ for each solution $(x, V)$ in the non-dominated set $\Omega$, and then $\alpha(x, V)=(\overline{\operatorname{TCE}}(x, V)+e) /$ $\left(\bar{C}_{\max }(x, V)+e\right)$ is obtained.

Step 4: Sort all non-dominated solutions in ascending value of $\alpha$, and divide the non-dominated solution set $\Omega$ into two parts, namely $\Omega_{c}$ and $\Omega_{e}$, where $|\Omega|=\left|\Omega_{c}\right|+\left|\Omega_{e}\right|$. The obtained non-dominated solutions in $\Omega_{c}$ with a small value of $\alpha$ need to be optimized for $C_{\max }$, while the nondominated solutions in $\Omega_{e}$ have a large value of $\alpha$ and these solutions should to be optimized for TCE.

Step 5: Perform the reduced-time speed adjustment strategy for all non-dominated solutions in $\Omega_{c}$. That is, the processing state of all jobs on the non-critical path is kept unchanged and the processing speed of some jobs on the critical path is increased, so as to reduce the maximum completion time ( $C_{\max }$ ) of these non-dominated solutions.

Step 6: Perform the energy-saving speed adjustment strategy for all non-dominated solutions in $\Omega_{e}$. That is, the processing speed of all jobs on the critical path is kept unchanged and the processing speed of each job on the non-critical path is adjusted, so as to achieve the lowest possible total carbon emission (TCE) under the same makespan ( $C_{\max }$ ) and further improve the quality of each non-dominated solution.

Step 7: Update the Pareto archive $\Omega^{*}$. If the number of the nondominated solutions in the non-dominated set $\Omega$ is larger than $p s$, then all of the feasible solutions are sorted by means of their crowded distances according to Algorithm 1, so as to eliminate some solutions with the smallest crowded distance until the number of feasible solutions in the non-dominated set (population) reaches $p s$.

Input: The processing speed matrix $\mathbf{V}$, and non-dominated set $\Omega$.
1: Identify a critical path for each non-dominated solution in non-dominated set $\Omega$.
2: Put all of the critical jobs into a set $C J$.
3: for $f=1$ to $F$ do
4: for $j=m$ downto 1 do
5: for $i=n_{f}$ downto 1 do
6: if $\pi_{i}^{f} \in C J$ then goto Next Job.
7: if $j=m$ and $i=n_{f}$ then
Determine the product $P_{h}$ to which job $i$ belongs.
$T_{\text {margin }}=S_{P_{h}}^{A}-C_{i, j}-p_{i, j} / v_{i, j}$.
else if $j=m$ then
$T_{\text {margin }}=C_{i+1, j}-p_{i+1, j} / v_{i+1, j}-\left(C_{i, j}-p_{i, j} / v_{i, j}\right)$.
else if $i=n_{f}$ then
$T_{\text {margin }}=C_{i, j+1}-p_{i, j+1} / v_{i, j+1}-\left(C_{i, j}-p_{i, j} / v_{i, j}\right)$.
else
$T_{\text {margin }}=\min \left\{C_{i, j+1}-p_{i, j+1} / v_{i, j+1}, C_{i+1, j}-p_{i+1, j} / v_{i+1, j}\right\}-\left(C_{i, j}-p_{i, j} / v_{i, j}\right)$.
end if
$v=v_{i, j}$.
for $t=4$ downto 1 do
if $v_{i, j}>v_{t}$ then
if $T_{\text {margin }} \geq p_{i, j} / v_{t}$ then
$v_{i, j}=v_{t} . / /$ Obtain the minimum allowable speed within $T_{\text {margin }}$.
end if
end if
end for
if $v \neq v_{i, j}$ then
$C_{i, j}=C_{i, j}-p_{i, j} / v_{i, j}-T_{\text {margin }}$.
end if
Next Job:
end for
end for
31: end for
Output: The processing speed matrix $\mathbf{V}$, non-dominated set $\Omega$.

## Algorithm 7: Reduced-time speed adjustment strategy

Algorithm 6) and the reduced-time speed adjustment strategy (see Algorithm 7), for all of the non-dominated solutions according to different preference levels. Then, update the Pareto archive $\Omega^{*}$.

Step 7: Determine whether the termination condition is satisfied. If

Input: The processing speed matrix $\mathbf{V}$, and non-dominated set $\Omega$.
1: Identify a critical path for each non-dominated solution in non-dominated set $\Omega$.
2: Put all of the critical jobs into a set $C J$. Execute operations in critical factory $f_{c}$ as follows.
for $j=1$ to $m$ do
for $i=1$ to $n_{j}$ do
if $\pi_{i}^{f_{c}} \notin C J$ then goto Next Job.
if $i=1$ and $j=1$ then
$C_{i, j}=C_{i, j}-\hat{p}_{i, j}^{k}+\hat{p}_{i, j}^{5} . / /$ Adjust to maximum speed.
else if $j=1$ then
$C_{i, j}=C_{i-1, j}+\hat{p}_{i, j}^{5}$.
else
$C_{i, j}=\max \left\{C_{i-1, j}, C_{i, j-1}\right\}+\hat{p}_{i, j}^{5}$.
end if
Next Job:
end for
15: end for
Output: The processing speed matrix $\mathbf{V}$, non-dominated set $\Omega$.

### 3.5. The framework of MCEDA

According to the design above, the specific process of the proposed MCEDA for solving the EE_DAPFSP is as follows.

Step 1: (Initialization) Initialize the population $\boldsymbol{P o p}(\mathbf{0})$ via Algorithm 2, the multi-dimensional probabilistic model $\boldsymbol{P M}_{\boldsymbol{n}<\boldsymbol{n}<\boldsymbol{n}}^{\mathbf{n}}$ by using Eq. (18), and key parameters of MCEDA, i.e., $p s, q s, r$.

Step 2: Evaluate each individual in $\operatorname{Pop}(\boldsymbol{G})$ by using Eqs. (1-9), and calculate the matrix cube $\boldsymbol{M} \boldsymbol{C}_{\boldsymbol{n}<\boldsymbol{n}<\boldsymbol{n}}^{G}$ according to Eqs. (13-14).

Step 3: Determine the Pareto dominance relationship of all individuals in $\operatorname{Pop}(\boldsymbol{G})$. Calculate the corresponding dominance level and crowding distance of these individuals via Algorithm 1. The top $p s \times \varphi$ excellent individuals are selected to form a high-quality subpopulation $\operatorname{SPop}(\boldsymbol{G})$ after sorting all individuals in $\operatorname{Pop}(\boldsymbol{G})$. Then, update the Pareto archive $\Omega^{*}$.

Step 4: (Global exploration) Update the multi-dimensional probabilistic model $\boldsymbol{P M}_{\boldsymbol{n}<\boldsymbol{n}<\boldsymbol{n}}^{\mathrm{G}}$ by means of the incremental learning mechanism in Subsection 3.2.2, where the superior subpopulation $\operatorname{SPop}(\boldsymbol{G})$ is selected in $\operatorname{Pop}(\boldsymbol{G})$.

Step 5: (Global exploration) Sample the multi-dimensional probabilistic model $\boldsymbol{P M}_{\boldsymbol{n}<\boldsymbol{n}<\boldsymbol{n}}^{\mathrm{G}}$ to generate new population $\operatorname{Pop}(\boldsymbol{G}+\mathbf{1})$ by using the specific sampling strategy in Subsection 3.2.3.

Step 6: (Local exploitation) Perform the critical path based local search in Subsection 3.3 for each non-dominated solution, respectively, and further execute two types of speed adjustment strategies in Subsection 3.4, i.e., the energy-saving speed adjustment strategy (see
not, then the program goes to Step 2, otherwise terminate the loop and output the currently obtained Pareto archive $\Omega^{*}$.

The flowchart of the proposed MCEDA for the EE_DAPFSP is illustrated in Fig. 8. According to the above steps, it can be seen that the MCEDA proposed in Section 3 effectively integrates many advantages of both the EDA-specific global exploration and the problem-dependent local exploitation. During each iteration of MCEDA, each new solution is generated by sampling from the promising regions in the solution space based on the multi-dimensional probabilistic model, and then the critical-path based local search with two speed adjustment strategies are performed for high-quality solutions, respectively. Since the multidimensional probabilistic model is updated based on high-quality subpopulation, the total characteristic distribution information of promising patterns from superior solutions can be well learned and stored, so that the sampling process can be concentrated around more potential regions in the solution space. Because of the performance of the scheduling schedule is well stressed and balanced by taking into account the benefit of both global exploration and local exploitation, it can be expected to achieve better results for solving the EE_DAPFSP.

## 4. Experimental comparisons and statistical analysis

The following subsections are devoted to evaluate the overall performance of the proposed MCEDA for the considered EE_DAPFSP. First of all, some information about experiments is provided, which includes experimental setup and performance metrics. Then, the parameters of MCEDA are calibrated. Afterwards, the advantages of each improvement strategy in MCEDA are investigated. Lastly, comprehensive comparisons are conducted and experimental results are analyzed by comparing

![img-13.jpeg](img-13.jpeg)

Fig. 8. The flowchart of MCEDA for the EE_DAPFSP.

MCEDA with several state-of-the-art multi-objective algorithms.

### 4.1. Experimental setup

In order to effectively and reasonably investigate the performance of MCEDA for addressing the considered EE_DAPFSP, in this section, extensive experiments and computational comparisons are carried out for the proposed MCEDA with several high-performing algorithms in the literature. Since the performance of the algorithm is usually greatly affected by different problem scales, we employ two well-known benchmark data sets presented by Hatami et al. (2013) and extend them by adding speed levels of machines in the processing stage to obtain two suitable testing sets for the EE_DAPFSP. The first testing set consists of 900 small-scale instances with 180 groups and each group contains 5 different instances, where $n=\{8.12 .16 .20 .24\}, m=\{2.3 .4$. $5\}, F=\{2.3 .4\}$, and $S=\{2.3 .4\}$. The second testing set is composed of 810 large-scale instances with 81 groups and each group includes 10 different instances, where $n=\{100.200 .500\}, m=\{5.10 .20\}, F=\{4$. $6,8\}$, and $S=\{30.40 .50\}$. So, the total number of testing instances is 1710 and datasets are available at http://soa.iti.es. Since the running state of each machine in processing stage is variable and adjustable, the processing speed is selected from a series of discrete values $\{1.1 .3,1.55,1.75,2.10\}$, that is, there are total five adjustable speed levels for each machine. The processing power consumption of machine $M_{l}$ running at speed $v_{k}$ is $E_{f k}=4 \times v_{k}^{2}(\mathrm{~kW})$ while the standby power
consumption of machine $M_{l}$ is $S E_{f i}=1(\mathrm{~kW})$, and the speed adjustment is not considered in the assembly stage. In addition, it should be pointed out that calibrating algorithms by using the same benchmark instances that will later be adopted for computational comparisons constitute poor practices, which would result in over-fitting or biased experimental results (Pan \& Ruiz, 2012). Therefore, it is of great importance to definitely distinguish between the calibrating instances and the final testing benchmark instances. For this reason, two types of new calibrating set that contain 180 instances for small-scale problems and 81 instances for large-scale problems are independently yielded based on the problem generation method provided by Hatami et al. (2013) for parameter calibration. To be specific, the small-scale calibrating instances contain complete combinations of $n=\{8.12 .16 .20 .24\}, m=\{2.3 .4 .5\}, F=\{2$. $3.4\}$, and $S=\{2.3 .4\}$ and the large-scale calibrating instances consist of $n=\{100.200 .500\}, m=\{5.10 .20\}, F=\{4.6 .8\}$, and $S=\{30.40 .50\}$. The processing times of jobs are randomly sampled from a uniform distribution in the range $[1.99]$, and the assembly times of each product $P_{h}$ are generated according to a uniform distribution in the range $\left[1 \times N_{h}\right.$, $\left.99 \times N_{h}\right]$.

In order to conduct computational comparisons fairly, the same experimental settings are adopted for all compared algorithms, including the same CPU Gigahertz frequency, programming language and termination criteria. All algorithms involved in this study are coded by Pascal language and compiled on Embarcadero RAD Studio XE8. The numerical experiments are independently executed on a PC with Inter

(R) Core(TM) i7-8700 M CPU @ 3.2 GHz processor and 16 G of RAM under Microsoft Windows 7 OS. According to the analysis of the problem properties in Subsection 2.3, it is difficult to find the Pareto optimal frontier within a finite period of time since the feasible solution space of the considered problem is complex and multivariate. The performance of the proposed MCEDA are measured by comparisons with four related state-of-the-art multi-objective evolutionary algorithms (MOEAs), namely, non-dominated sorting genetic algorithm (NSGA-II) (Deb et al., 2002), modified multi-objective iterated greedy (MMOIG) algorithm (Ding et al., 2016), knowledge-based cooperative algorithm (KCA) (Wang \& Wang, 2020), and multi-objective whale swarm algorithm (MOWSA) (Wang et al., 2020). The NSGA-II was first proposed by Deb et al. (2002). Due to its sample and efficient optimization performance, NSGA-II has been widely applied in a variety of multi-objective optimization fields, and it is recognized as one of the most effective algorithms for solving various multi-objective shop scheduling problems. The iterated greedy (IG) was first proposed by Ruiz and Stutzle (2008), it combined with the high efficiency of constructive heuristics and the advantages of simulated annealing approach, which is also regarded as a simple and effective algorithm for tackling different kinds of flow shop scheduling problems. The MMOIG is an effective extension of the typical IG algorithm in a multi-objective optimization perspective, where an extended NEH-Insertion is incorporated in the framework of MMOIG and several problem-dependent multi-neighborhood local searches are adopted to achieve satisfactory results in addressing the low-carbon PFSP (Ding et al., 2016). KCA is a high-performing algorithm recently proposed for handling the energy-efficient DPFSP. KCA can adopt a series of searching operators based on problem's characteristics and achieve multi-neighborhood cooperative search in the feasible solution space through control factors. MOWSA is another newly presented multi-objective algorithm for the energy-efficient DPFSP (Wang et al., 2020). In the literature, KCA and MOWSA have shown relatively good performance than other well-known MOEAs in solving the low-carbon DPFSP.

Taking into account the fairness, all tested algorithms are performed under the same termination conditions. That is, the maximum elapsed CPU time of the proposed MCEDA with NSGA-II, MMOIG, KCA and MOWSA is set to the same time commonly used in the literature, i.e., $n \times$ $m \times f \times 10$ milliseconds. In order to ensure the stability and reliability of the experimental results, computational comparisons are independently conducted 30 and 10 replications for small-scale instances and largescale instances respectively, and all of the numerical results are collected and statistically averaged to eliminate random errors. It's worth noting that, for the largest testing instances of 500 jobs, 20 machines and 8 factories, the program needs to run about 800 s . Due to the proposed MCEDA and four compared algorithms are independently tested in 30 and 10 replications for small-scale and large-scale instances, a total number of $900 \times 5 \times 30+810 \times 5 \times 10=175500$ results for each performance metric would be yielded. As a result, for this larger dataset, most factors are statistically significant, which may allow us to draw strong conclusions.

### 4.2. Performance metrics

The metrics of MOEAs are significantly different from that of singleobjective algorithms, so it is of great importance to comprehensively evaluate the convergence and diversity (distribution) of MOEAs (Wang \& Wang, 2020). On the one hand, the high-performing MOEAs need to find more non-dominated solutions that should approximate to the true Pareto optimal front as closely as possible within an acceptable time. On the other hand, the distribution characteristics of the non-dominated solution sets obtained by MOEAs should be as widely as possible, so as to facilitate the decision maker to choose some suitable schemes by preferences. To verify the effectiveness and efficiency of MOEAs, the non-dominated solution set obtained by all algorithms are measured by some general metrics as follows.
(1) Coverage Metric: It can be used to measure the relatively quality between two Pareto archives obtained by two different algorithms $A$ and $B$, denoted as $C(A, B) . C(A, B)$ gives a mapping from a pair of algorithms $(A, B)$ into the interval $[0,1]$, which is represented as follows.
$C(A, B)=\frac{1}{|B|}\left\{|\boldsymbol{b} \in B| \exists \boldsymbol{a} \in A: \boldsymbol{a} \succ \boldsymbol{b}\right.$ or $\left.\boldsymbol{a}=\boldsymbol{b}\right)\} \cdot$
From Eq. (21), the coverage metric reflects the dominance relationship of feasible solutions from two non-dominated solution sets. If all solutions obtained by $B$ are dominated by some solutions obtained by $A$, then $C(A, B)=1$. Conversely, if all solutions obtained by $B$ are not dominated by any solution obtained by $A$, then we have $C(A, B)=0$. Since the solutions obtained by $A$ and $B$ do not necessarily dominate each other, it holds that $C(A, B)+C(B, A) \neq 1$.
(2) Reference Distance $\left(\mathrm{DI}_{R}\right)$ : The reference distance metric $\mathrm{DI}_{R}$ is used to measure the distance of the elements in non-dominated solution set $\Omega$ with respect to the reference set $\Omega^{*}$, where $\Omega^{*}$ is composed of high-quality solutions in the Pareto archives yielded by all algorithms. Due to the NP-hard in strong sense with high complexity of the EE_DAPFSP, it is usually difficult to obtain the true Pareto optimal set, so the reference set $\Omega^{*}$ is composed of the Pareto archives aggregated jointly by all algorithms. The distance metric $\mathrm{DI}_{R}$ can be expressed as follows.
$\mathrm{DI}_{R}\left(\Omega^{*}\right)=\frac{1}{|\Omega^{*}|} \sum_{y \in \Omega^{*}} \min \left\{d(\boldsymbol{x}, \boldsymbol{y}) \mid \boldsymbol{x} \in \Omega^{*}\right\}$
where $d(\boldsymbol{x}, \boldsymbol{y})$ is the Euclidean distance between $\boldsymbol{x} \in \Omega^{*}$ and $\boldsymbol{y} \in \Omega^{*}$ in the normalized objective space (Ishibuchi et al., 2003), which can be calculated in Eq. (23).
$d(\boldsymbol{x}, \boldsymbol{y})=\sqrt{\sum_{i=1}^{N}\left[\left(f_{i}(\boldsymbol{x})-f_{i}(\boldsymbol{y})\right) /\left(f_{i}^{\min }-f_{i}^{\min }\right)\right]^{2}}$.
In Eq. (23), $f_{i}$ is the $i$ th objective function, and $f_{i}^{\min }$ and $f_{i}^{\min }$ are the maximum and minimum values of the objective function $f_{i}$, respectively. Obviously, $\mathrm{DI}_{R}\left(\Omega^{*}\right)$ is expected to be as small as possible, indicating that $\Omega^{*}$ is closer to reference set $\Omega^{*}$, which means the algorithm has better performance.
(3) Distribution Spacing (DS): The distribution spacing metric DS is adopted to measure the distribution uniformity of solutions obtained by a specified algorithm. The distribution spacing metric can be calculated as follows.
$D S=\sqrt{\frac{1}{|\Omega|} \sum_{i=1}^{|\Omega|}\left(D_{i}-\bar{D}\right)^{2} / \bar{D}}$
where $\bar{D}=\sum_{i=1}^{|\Omega|} D_{i} /|\Omega| . D_{i}$ denotes the minimum Euclidean distance between the $i$ th feasible solution in the solution set $\Omega^{*}$ and the other feasible solutions in $\Omega^{*}$. From Eq. (24), it is obvious that the smaller the value of $D S$ is, the more evenly distributed of the solutions in $\Omega^{*}$ are.
(4) Non-dominance Ratio $\left(\rho_{y}\right)$ : It is used to measure the proportion of the non-dominated solution set $\Omega^{*}$ obtained by a specific algorithm in the reference set $\Omega^{*}$. Obviously, the larger the $\rho_{y}$, the better the performance of the algorithm.

Furthermore, to increase the soundness of our conclusion, two state-of-the-art quality metrics, i.e., Hypervolume (Zitzler \& Thiele, 1999) and Unary epsilon (Zitzler et al., 2003), are also adopted to evaluate quality of the found non-dominated solution sets, which are described as

follows:
(5) Hypervolume $\left(I_{H}\right)$ : The hypervolume represents the volume of the hypercube enclosed by all solutions in a given Pareto front $P F_{G}$ and the reference point $z^{\prime}=\left(z_{1}^{\prime}, z_{2}^{\prime}\right)$ in the objective space. Each objective value of each solution $x \in P F_{G}$ is normalized into $[0,1]$ before calculating the $I_{H}$ value. The normalized value $f_{1}(\boldsymbol{x})$ for the ith objective of solution $x$ can be calculated as
$f_{1}^{\prime}(\boldsymbol{x})=\left(f_{1}(\boldsymbol{x})-f_{1}^{\min }\right) /\left(f_{1}^{\max }-f_{1}^{\min }\right)$.
where $f_{1}^{\min }, f_{1}^{\max }$ and $f_{2}(\boldsymbol{x})$ have the same meanings with Eq.(23). Then, the hypervolume indicator $I_{H}$ for the bi-objective problems can be calculated using Lebesgue measurement in Eq.(26).
$I_{H}\left(P F_{G}\right)=\sum_{k=1}^{[P F_{G}]}\left(z_{1}^{\prime}-f_{1}^{\prime}\left(x_{k}\right)\right) \times\left(f_{2}^{\prime}\left(x_{k}\right)-f_{2}^{\prime}\left(x_{k-1}\right)\right)$.
The accuracy of calculating $I_{H}$ depends on the choice of the reference point, i.e., when evaluating the same Pareto front (non-dominated solution set), selecting different reference points will result in different results. According to the references (Minella et al., 2008; Ciavotta et al., 2013), the reference point is usually selected as (1.2.1.2). The hypervolume is a Pareto-compliant evaluation metric, which means that if one Pareto front $P F_{G}^{\prime}$ is better than another Pareto front $P F_{G}^{\prime \prime}$, then the Hypervolume metric of $P F_{G}^{\prime}$ will be greater than that of $P F_{G}^{\prime \prime}$. The larger value of the hypervolume indicator $I_{H}$, the better convergence as well as a good coverage of the optimal Pareto front.
(6) Unary Epsilon $\left(I_{+}^{\mathrm{L}}\right)$ : It is used to measure the minimum distance between a given Pareto front $P F_{G}$ and the reference (optimal) Pareto front $P F_{R}$. To avoid errors arising from dividing by zero in the calculation of $I_{+}^{\mathrm{L}}$, each objective value $f_{1}(\boldsymbol{x})$ should be normalized into $[1,2]$ as follows.
$f_{1}^{\prime}(\boldsymbol{x})=\left(f_{1}(\boldsymbol{x})-f_{1}^{\min }\right) /\left(f_{1}^{\max }-f_{1}^{\min }\right)+1$.
According to Eq.(27), the $I_{+}^{\mathrm{L}}$ value varies between 1 and 2 . If the $I_{+}^{\mathrm{L}}$ value close to 1 implies that the given Pareto front is close to the reference Pareto front, whereas the $I_{+}^{\mathrm{L}}$ value close to 2 means that it is distant. Then, the bi-objective $I_{+}^{\mathrm{L}}$ can be calculated as

Table 3
The levels of parameters.

| Parameters | Factor level |  |  |  |  |  |  |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|  | 1 | 2 | 3 | 4 | 5 |  |  |
| $p s$ | 10 | 30 | 60 | 90 | 120 |  |  |
| $\varphi$ | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 |  |  |
| $r$ | 0.05 | 0.1 | 0.2 | 0.3 | 0.4 |  |  |

$I_{+}^{\mathrm{L}}\left(P F_{G}, P F_{R}\right)=\max _{x \in P F_{R} \in P F_{G}(x) \in \mathrm{~L}}\left(f_{1}^{\prime}(x) / f_{1}^{\prime}(y)\right)$.
Since the true Pareto front for each instance is unknown, a union set constituted by aggregated all non-dominated solutions obtained by all algorithms, is regarded as the reference (optimal) Pareto front $P F_{R}$. It is clear that a smaller $I_{+}^{\mathrm{L}}$ value means a better approximation to the reference Pareto front.

Notice that, it is unnecessary to immediately evaluate the Pareto archives obtained by each algorithm after finishing each iteration. The Pareto archives yielded by each of the algorithms are measured if and only if all of the algorithms have been run and all non-dominated solutions have been collected. Then the maximum and minimum values of each objective function can be determined, so the minimum and maximum values are fixed, which is fair for the evaluation of each nondominated solution.

### 4.3. Parameter calibration

Since parameter calibration plays an important role in the development of high performing metaheuristics, the reasonable values of parameters have remarkable impact on the effectiveness and efficiency of the designed algorithms. Meanwhile, it should be point out that the statistical calibration is only a fine-tuning process and stochastic algorithms are not expected to behave entirely different after calibration. In this section, the Design-of-Experiments (DOE) methodology (Montgomery, 2008) is employed to analyze the sensitivity of main parameters and further investigate the effects of each parameter on the performance of the proposed MCEDA. There are three control parameters in MCEDA, i.e., the population size ( $p s$ ), the percentage of superior subpopulation $(\varphi)$, and the learning rate $(r)$. A series of potential values (levels) of parameters (factors) are firstly considered through summarizing previous relevant literature (Wang \& Wang, 2016; Zhang et al., 2021), and then the suitable scope of each parameter is determined according to some preliminary experiments. The considered levels for all factors are listed in Table 3. As seen in Table 3, a total of $5 \times 5 \times 5=125$ different configurations for the proposed MCEDA are yielded. Thus, a full factorial experimental design is considered for all configurations, and some additional instances generated by ourselves are adopted as the test bed, which includes 180 basic instances for small-scale problems and 81 basic instances for large-scale problems. Each configuration is tested on these 261 instances where 10 independent replications are performed under the same parameter combination for each instance. As a result, there are a total of $125 \times 261 \times 10=326250$ treatments, which implies that a total of 326250 results (non-dominated sets) would be yielded. The hypervolume $\left(I_{H}\right)$, unary epsilon $\left(I_{+}^{\mathrm{L}}\right)$, and non-dominance ratio $\left(\rho_{+}\right)$indicators are regarded as three response values, respectively. The maximum elapsed CPU time of $n \times m \times f \times 10$ milliseconds is used as the termination criterion, so it requires at least 74.375 CPU days to complete all calibration experiments. Due to multi-cores in our personal

Table 4
The results of ANOVA over calibrating the parameters of MCEDA for small-scale instances.

| Source | Hypervolume $\left(I_{H}\right)$ |  |  |  |  |  | eUnary Epsilon $\left(I_{+}^{\mathrm{L}}\right)$ |  |  |  |  | Non-dominance ratio $\left(\rho_{+}\right)$ |  |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  | Sum of squares | $D f$ | Mean square | $F$-radio | $p$-value | Sum of squares | $D f$ | Mean square | $F$-radio | $p$-value | Sum of squares | $D f$ | Mean square | $F$-radio | $p$-value |
| $p s$ | 0.03916 | 4 | 0.00979 | 1271.58 | 0.0000 | 0.11476 | 4 | 0.02869 | 4379.97 | 0.0000 | 0.04759 | 4 | 0.01190 | 1180.86 | 0.0000 |
| $\varphi$ | 0.01868 | 4 | 0.00467 | 606.39 | 0.0000 | 0.04666 | 4 | 0.01166 | 1780.89 | 0.0000 | 0.01199 | 4 | 0.00300 | 297.49 | 0.0000 |
| $r$ | 0.03730 | 4 | 0.00933 | 1211.06 | 0.0000 | 0.04469 | 4 | 0.01117 | 1705.77 | 0.0000 | 0.00841 | 4 | 0.00210 | 208.75 | 0.0000 |
| $p s^{*} \varphi$ | 0.00012 | 16 | 0.00001 | 1.00 | 0.4683 | 0.00011 | 16 | 0.00001 | 1.04 | 0.4310 | 0.00020 | 16 | 0.00001 | 1.21 | 0.2847 |
| $p s^{*} r$ | 0.00006 | 16 | 0.00000 | 0.48 | 0.9480 | 0.00008 | 16 | 0.00000 | 0.73 | 0.7506 | 0.00005 | 16 | 0.00000 | 0.32 | 0.9933 |
| $\varphi^{*} r$ | 0.00019 | 16 | 0.00001 | 1.52 | 0.1205 | 0.00025 | 16 | 0.00002 | 2.41 | 0.0067 | 0.00009 | 16 | 0.00001 | 0.57 | 0.8977 |
| Residual | 0.00049 | 64 | 0.00001 |  |  | 0.00042 | 64 | 0.00001 |  |  | 0.00064 | 64 | 0.00001 |  |  |
| Total | 0.09600 | 124 |  |  |  | 0.20696 | 124 |  |  |  | 0.06897 | 124 |  |  |  |

Table 5
The results of ANOVA over calibrating the parameters of MCEDA for large-scale instances.

| Source | Hypervolume $\left(I_{\mathrm{H}}\right)$ |  |  |  |  |  | Unary Epsilon $\left(I_{\mathrm{L}}^{0}\right)$ |  |  |  |  |  | Non-dominance ratio $\left(\rho_{c}\right)$ |  |  |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  | Sum of squares | $D f$ | Mean square | $F$-radio | $p$-value |  | Sum of squares | $D f$ | Mean square | $F$ radio | $p$-value |  | Sum of squares | $D f$ | Mean square | $F$-radio | $p$-value |
| $p s$ | 0.02195 | 4 | 0.00549 | 1662.67 | 0.0000 | 0.00421 | 4 | 0.00105 | 109.99 | 0.0000 | 0.18919 | 4 | 0.04730 | 6306.40 | 0.0000 |  |  |
| $\varphi$ | 0.02188 | 4 | 0.00547 | 1657.21 | 0.0000 | 0.00760 | 4 | 0.00190 | 198.56 | 0.0000 | 0.04943 | 4 | 0.01236 | 1647.73 | 0.0000 |  |  |
| $r$ | 0.02144 | 4 | 0.00536 | 1624.48 | 0.0000 | 0.00305 | 4 | 0.00076 | 79.71 | 0.0000 | 0.04838 | 4 | 0.01209 | 1612.53 | 0.0000 |  |  |
| $p s^{*} \varphi$ | 0.00004 | 16 | 0.00000 | 0.85 | 0.6279 | 0.00021 | 16 | 0.00001 | 1.38 | 0.1812 | 0.00018 | 16 | 0.00001 | 1.47 | 0.1408 |  |  |
| $p s^{*} r$ | 0.00036 | 16 | 0.00002 | 6.76 | 0.0000 | 0.00004 | 16 | 0.00000 | 0.28 | 0.9966 | 0.00023 | 16 | 0.00001 | 1.93 | 0.0330 |  |  |
| $\varphi^{*} r$ | 0.00011 | 16 | 0.00001 | 2.06 | 0.0217 | 0.00013 | 16 | 0.00001 | 0.86 | 0.6193 | 0.00003 | 16 | 0.00000 | 0.27 | 0.9975 |  |  |
| Residual | 0.00021 | 64 | 0.00000 |  |  | 0.00061 | 64 | 0.00001 |  |  | 0.00048 | 64 | 0.00001 |  |  |  |  |
| Total | 0.06599 | 124 |  |  |  | 0.01587 | 124 |  |  |  | 0.28792 | 124 |  |  |  |  |  |

Note: All F-ratios are based on the residual mean square error. Boldface indicates that is significant at the 0.05 level.
computer, actually almost 2.5 days are adopted to execute the whole experiment.

All experimental results are analyzed by means of the multi-factor Analysis of Variance (ANOVA), which has been widely applied as a powerful parametric statistical technique in many scheduling literatures (Shao et al., 2018; Sang et al., 2019; Pan et al., 2019). In the ANOVA, three main hypotheses (i.e., normality, homoscedasticity, and independence of residuals), have to be checked and accepted. According to the residual analysis of the experimental results, all assumptions are easily satisfied. The ANOVA results of three parameters of MCEDA are reported in Tables 4 and 5 . For the results of ANOVA, the $F$-ratio is regard as a clear indicator of significance when $p$-value is less than the confidence level. A large $F$-ratio means that the analyzed factor has a considerable effect on the response variable. As seen in Tables 4 and 5, all parameters are statistically significant since their $p$-values are smaller than the 0.05 confidence level for three performance metrics, i.e., hypervolume, unary epsilon, and non-dominance ratio. Among these three parameters, the allowable maximum number of population (ps) achieves the largest $F$ ratio, which indicates that $p s$ has the most significant effect on the performance of the proposed MCEDA for both the small-scale instances and the large-scale instances.

Fig. 9 provides the main effects plots of all parameters for different scale testing sets. It is clearly observed from this figure that the choice of $p s=30$ yields the best performance while $p s=120$ obtains the worst results. To be specific, a small population is favorable to perform more iterations and it is beneficial to achieve a deeper exploitation (local search) in the feasible solution space. However, if the population size is too small, then the size of the superior subpopulation may be affected, resulting the failure of the multi-dimensional probabilistic model to fully learn the excellent structural characteristics of the superior solutions. The results of the proposed MCEDA degrades with the increasing of the population size, and it can be observed from Fig. 9 that the performance has a considerable change, especially from $p s=30$ to $p s=120$. Although a larger population is helpful to prompt the diversity of the obtained non-dominated solutions, it also consumes much more computational cost and reduces the convergence speed, and thereby reduce the searching efficiency. In fact, if we adopt the large population, it may affect the algorithm's ability to perform more iterations, especially for addressing the large instances. Thus, the population size $p s$ should be set as a relatively small value, i.e., $p s=30$. The second largest $F$-ratio value corresponds to the percentage of superior subpopulation $\varphi$. It can be observed in Fig. 9 that the value $\varphi=0.2$ yields the best results, while $\varphi=0.5$ results the worst performance. Moreover, a small-scale superior subpopulation is more conducive to accurately learn the information of both structural features and promising patterns from high-
quality solutions, so the probabilistic model can be updated effectively.
From Tables 4-5, we can see that the factor $r$ is the last significant parameter. The learning rate can control the balance of information fusion between the matrix cube and the multi-dimensional probabilistic model. To be specific, the larger $r$ tends to learn more valuable information from the selected superior solutions in each generation, while the smaller $r$ reinforces the accumulated information of superior solutions during the overall iterative process. Thus, $r$ should be determined by considering the trade-off between the current knowledge and historical experience, and a suitable learning rate helps the algorithm to avoid premature convergence or slow convergence as much as possible (Wang \& Wang, 2016). Fig. 9 reveals that the algorithm has good performance when $r$ is equal to 0.2 , which verifies the conclusion above. According to the parametric experiment results and analysis above, for two different scale testing sets, the best configuration of parameters for MCEDA is suggested as: $p s=30, \varphi=0.2, r=0.2$.

In order to ensure the fairness of the computational comparisons, this section further performs some additional parameter calibrations for NSGA-II, MMOIG, KCA and MOWSA by using the same multi-factor ANOVA technique. For all of the five compared algorithms, it should be noted that the population size $p s$ is a common parameter which is selected as $p s=30$ to make a fair comparison. In addition, to ensure the diversity of the initial population, five different discrete speed values of $1,1.3,1.55,1.75$, and 2.10 are adopted to yield the initial speed matrix, respectively. In NSGA-II, crossover probability $\left(p_{\mathrm{c}}\right)$ and mutation probability $\left(p_{\mathrm{m}}\right)$ are two crucial parameters, which are set as $p_{\mathrm{c}}=0.8$ and $p_{\mathrm{m}}=0.1$. In MMOIG, the number of destructed jobs $(d)$ and mutation probability $(\rho)$ in the destruction phase are two key parameters, which are set as $d=3$ and $\rho=0.4$ following the original literature. The best parameter combination of KCA is set as: the depth of local intensification $L S=100$, and the proportion of EENEHFF2-based initialization $P E=$ 60. The crossover probability $(\alpha)$ and mutation probability $(\beta)$ in MOWSA are set as: $\alpha=0.9$ and $\beta=0.2$. Notice that, it is meaningless to determine all parameters by means of the main effects plot, if some significant interactions are existed between factors. Due to the space of the paper, 2-level interaction plots of pair factors (i.e., $p s^{*} \varphi, p s^{*} r$ and $\varphi^{*} r$ ) on three performance metrics are provided only for the large-scale instances. It is clear in Fig. 10 that the significance of these 2-level interactions is relatively weak and these results are in accordance with the conclusions above. Meanwhile, it can be observed from Tables 4-5 that the $F$-ratio of each parameter is larger than that of their interactions, which further reveals the proposed MCEDA obtains the best performance when $p s=30, \varphi=0.2, r=0.2$.

Main Effects Plot for Hypervolume Data Means
![img-14.jpeg](img-14.jpeg)
(a) The level trend for small-scale instances.
![img-15.jpeg](img-15.jpeg)
(b) The level trend for large-scale instances.

Main Effects Plot for Unary Epsilon Data Means
![img-16.jpeg](img-16.jpeg)
(c) The level trend for small-scale instances.
![img-17.jpeg](img-17.jpeg)
(d) The level trend for large-scale instances.

Main Effects Plot for Non-dominance Ratio
Data Means
![img-18.jpeg](img-18.jpeg)
(e) The level trend for small-scale instances.
![img-19.jpeg](img-19.jpeg)
(f) The level trend for large-scale instances.

Fig. 9. Main effects plots of parameters for hypervolume, unary epsilon, and non-dominance ratio.

# Interaction Plot for Hypervolume Data Means 

![img-20.jpeg](img-20.jpeg)

Fig. 10. Interaction plots for $p s^{*} \varphi, p s^{*} r$ and $\varphi^{*} r$ for large-scale instances.

### 4.4. Effectiveness of probabilistic models

In MCEDA, the matrix-cube-based multi-dimensional probabilistic model is used to learn and estimate the characteristic distribution of promising pattern from some selected superior solutions, so as to guide the searching directions toward potential regions. Since various highperforming EDAs usually employ one or more two-dimensional probabilistic models to guide the global search direction (Jarboui et al., 2009; Pan \& Ruiz, 2012; Tiwari et al., 2014; Wang \& Wang, 2016), it is of necessity to make a fair investigation on the performance of EDA's
global exploration. In this subsection, the global search framework of the proposed MCEDA (denoted as MCEDA ${ }_{n l s}$ ) is compared with three effective two-dimensional model-based EDAs, i.e., an effective EDA designed by Wang and Wang (2016) (denoted as $\mathrm{EEDA}_{n l s}$ ), a state-of-theart EDA presented by Jarboui et al. (2009) (denoted as JEDA $_{n l s}$ ), and a modified PEDA developed by Pan and Ruiz (2012) (denoted as PEDA $_{n l s}$ ). Notice that the corresponding local search parts are removed from each of these three types of EDAs and only the global search is retained. Moreover, the parameters of three compared algorithms are set the same as in the original literature. The comparison of MCEDA $_{n l s}$ against

![img-21.jpeg](img-21.jpeg)

![img-22.jpeg](img-22.jpeg)

Fig. 11. The box plots of $\mathrm{MCEDA}_{\mathrm{v} 1}, \mathrm{MCEDA}_{\mathrm{v} 2}, \mathrm{MCEDA}_{\mathrm{v} 3}$, and MCEDA.

EEDA $_{\text {nib }}$, JEDA $_{\text {nib }}$ and PEDA $_{\text {nib }}$ is executed on small-scale instances by using the same elapse CPU time as a termination criterion. All algorithms independently perform 30 times for each instance, and the average $\mathrm{DI}_{R}, \rho_{r}, D S, I_{H}, I_{s}^{1}$ obtained are used as the measure metrics.

The statistical results grouped by the number of jobs are reported in Table 6, where each cell is averaged across 180 small-scale instances and 30 replicates per instance ( 5400 values in total). The best value of each group is highlighted with boldface in Table 6. It is clear from Table 6 that the the average $\mathrm{DI}_{R}, \rho_{r}$, and $D S$ values obtained by $\mathrm{MCEDA}_{\text {nib }}$ are obviously better than those obtained by $\mathrm{PEDA}_{\text {nib }}, \mathrm{JEDA}_{\text {nib }}$ and $\mathrm{EEDA}_{\text {nib }}$ for all instances. As seen in Table 6, the MCEDA $_{\text {nib }}$ significantly outperforms other three probabilistic model based algorithms on the overwhelming major of instances in terms of both $I_{H}$ and $I_{s}^{1}$ indicators, which indicates the obtained approximated Pareto set is closer to the referenced Pareto front and better quality. The main reason is that the three-dimensional probabilistic model in $\mathrm{MCEDA}_{\text {nib }}$ can save the valuable information of each superior individual in a more accurate and reasonable way. Nevertheless, the two-dimensional probabilistic models in the compared algorithms cannot save the position of each similar block and just simply record all of the same similar blocks in one place of the two-dimensional matrix. As a result, the similar blocks cannot be placed in the right positions when generating new individual, which leads a relatively poor search ability of these compared algorithms.

### 4.5. Effectiveness of improvement strategies

As stated in Section 3, the proposed MCEDA has three main improvement strategies: (1) the population initialization method in subsection 3.1; (2) the critical path based local search in Subsection 3.3; (3) the speed adjustment strategy in Subsection 3.4. To investigate the effectiveness and efficiency of these improvement strategies, some variants of MCEDA are implemented in this subsection. To be specific, for the population initialization method, a variant of MCEDA (denoted as $\mathrm{MCEDA}_{\mathrm{v} 1}$ ) is developed. $\mathrm{MCEDA}_{\mathrm{v} 1}$ does not adpot the presented population initialization method, and it removes the heuristic method in Algorithm 1 and only use the random initialization to produce the initial population. For the critical path based local search, another variant of MCEDA named $\mathrm{MCEDA}_{\mathrm{v} 2}$ is developed. $\mathrm{MCEDA}_{\mathrm{v} 2}$ does not apply the the critical path based local search, and it is otherwise the same as MCEDA. For the speed adjustment strategy, we implement a variant of MCEDA without the speed adjustment strategy (denoted as $\mathrm{MCEDA}_{\mathrm{v} 3}$ ) which is used to confirm the presented speed adjustment strategy whether promotes the quality of the non-dominated solutions obtained in the local search. It should be clarified that each variant only modifies a single component of the proposed MCEDA, and the performance of MCEDA, $\mathrm{MCEDA}_{\mathrm{v} 1}, \mathrm{MCEDA}_{\mathrm{v} 2}$, and $\mathrm{MCEDA}_{\mathrm{v} 3}$ is compared based on small-scale testing set in the identical elapse CPU time as a termination criterion.

The same parameter settings are used for MCEDA and its three variants, and all algorithms are independently performed for 30 times on each instance, and the average $\mathrm{DI}_{R}, \rho_{r}$, and $D S$ obtained are used as the measure metrics. All of the test instances are grouped according to the number of jobs, and the statistical results for MCEDA and its variants are reported in Table 7, in which each value is averaged across 180 test instances and 30 replicates per instance ( 5400 values). The best value of each group is highlighted with boldface in Table 7.

According to Table 7, it can be seen that the MCEDA significantly outperforms the other three variants on the overwhelming major of five different types of job sizes in terms of three metrics, $\mathrm{DI}_{R}, \rho_{r}$, and $D S$, especially significantly better in the $\rho_{r}$ metric, which indicates that all improvement strategies can effectively improve the quality of solutions. As can be observed from Table 7, MCEDA is better than $\mathrm{MCEDA}_{\mathrm{v} 2}$ in all instance groups, which demonstrates that the effectiveness of the critical path based local search. In fact, since the critical path of the considered problem directly determines the maximum completion time, it is necessary to allocate the limited computing resource of local exploitation along the critical path direction to adjust the critical jobs instead of non-critical ones. The critical path-based neighborhood search effectively enhances the local intensification ability of the algorithm, and exhaustively exploit the potential area around promising solutions, making them as close to the optimal Pareto front as possible. As revealed in Table 7, the results of MCEDA outperform $\mathrm{MCEDA}_{\mathrm{v} 3}$, which indicates the effectiveness of the speed adjustment strategy for the considered problem. The presented two types of speed adjustment strategies not only can control the production process according to different situations, but also can complement each other with regard to the two criteria to jointly improve the quality of the obtained non-dominated solutions. In addition, the energy-saving speed adjustment strategy can enable the algorithm to obtain more high-quality non-dominated solutions with low energy consumption during each iteration. As regards $\mathrm{MCEDA}_{\mathrm{v} 1}$, its performance is inferior to MCEDA, which suggests that the adopted heuristic method make the initial candidate solutions converge toward a possible promising region during the early stage. Meanwhile, a random initialization is added to ensure the wide distribution of the initial solutions in the solution space. Thus, we can obtain initial solutions with higher quality and better diversity. In Table 7, we can also see that the results of $\mathrm{MCEDA}_{\mathrm{v} 2}$ and $\mathrm{MCEDA}_{\mathrm{v} 3}$ are inferior to MCEDA at all test instances, which implies that better performance can be reached by combining the critical path based local search and the speed adjustment strategy.

Moreover, the statistical results on DS metrics show the dispersion of the non-dominated solutions obtained by MCEDA is better. To analyze the results from a statistical perspective, Fig. 11 shows the box plots of MCEDA and its variants corresponding to the three performance metrics. As seen in these figures, MCEDA is significantly better than other variants on all test instances, which indicates the proposed MCEDA has the

Table 8
Statistical results on $C$ metric of MCEDA with NSGA-II, MMOIG, KCA and MOWSA for small-scale instances.

| $F \times n$ | C(MCEDA, NSGA- <br> II) | C(NSGA-II, <br> MCEDA) | C(MCEDA, <br> MMOIG) | C(MMOIG, <br> MCEDA) | C(MCEDA, <br> KCA) | C(KCA, <br> MCEDA) | C(MCEDA, <br> MOWSA) | C(MOWSA, <br> MCEDA) |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| $2 \times 8$ | 0.99 | 0.00 | 0.98 | 0.01 | 0.95 | 0.02 | 0.91 | 0.06 |
| $2 \times 12$ | 0.97 | 0.00 | 0.97 | 0.01 | 0.93 | 0.04 | 0.87 | 0.08 |
| $2 \times 16$ | 0.96 | 0.01 | 0.94 | 0.02 | 0.88 | 0.05 | 0.83 | 0.07 |
| $2 \times 20$ | 0.94 | 0.01 | 0.92 | 0.03 | 0.84 | 0.08 | 0.81 | 0.11 |
| $2 \times 24$ | 0.92 | 0.01 | 0.87 | 0.06 | 0.86 | 0.10 | 0.82 | 0.13 |
| $3 \times 8$ | 0.96 | 0.00 | 0.96 | 0.01 | 0.95 | 0.04 | 0.91 | 0.07 |
| $3 \times 12$ | 0.95 | 0.02 | 0.93 | 0.03 | 0.91 | 0.07 | 0.85 | 0.09 |
| $3 \times 16$ | 0.93 | 0.01 | 0.91 | 0.05 | 0.87 | 0.09 | 0.84 | 0.14 |
| $3 \times 20$ | 0.91 | 0.01 | 0.88 | 0.04 | 0.85 | 0.11 | 0.82 | 0.13 |
| $3 \times 24$ | 0.90 | 0.03 | 0.85 | 0.06 | 0.82 | 0.13 | 0.78 | 0.15 |
| $4 \times 8$ | 0.95 | 0.00 | 0.94 | 0.00 | 0.91 | 0.04 | 0.89 | 0.07 |
| $4 \times 12$ | 0.93 | 0.01 | 0.91 | 0.02 | 0.88 | 0.07 | 0.84 | 0.09 |
| $4 \times 16$ | 0.91 | 0.00 | 0.87 | 0.04 | 0.83 | 0.11 | 0.81 | 0.14 |
| $4 \times 20$ | 0.86 | 0.02 | 0.83 | 0.05 | 0.79 | 0.12 | 0.74 | 0.15 |
| $4 \times 24$ | 0.87 | 0.03 | 0.84 | 0.07 | 0.81 | 0.15 | 0.77 | 0.18 |
| Average | 0.93 | 0.01 | 0.91 | 0.03 | 0.87 | 0.08 | 0.83 | 0.11 |

best search capability and further confirms the conclusion drawn above. Therefore, according to the above results and analysis, it can be concluded that these improvement strategies have great promotion on improving the performance of MCEDA.

### 4.6. Comparisons of MCEDA and existing algorithms

To further validate the effectiveness of the proposed algorithm, MCEDA is compared with four state-of-the-art multi-objective algorithms, i.e., NSGA-II, MMOIG, KCA, and MOWSA. Then, the relative quality of the non-dominated sets yielded by each algorithm is measured and evaluated on the $C$ metric, respectively. To the best of our knowledge, no algorithms are recently presented in the literature to tackle the considered EE_DAPFSP. Since these high-performing compared algorithms are not directly designed for the EE_DAPFSP, we reimplement these four algorithms and adjust their objective evaluation functions in the original literature to make them be able to solve this problem. Notice that all of these algorithms are adapted to the sequence-based model, so they can be easily extended and participated in comparisons. All algorithms are performed on the same experiment environment which has same CPU power available. Each algorithm independently runs 30 and 5 replicates on two benchmark sets of different problem sizes. The Pareto reference set of each instance is jointly composed of all non-dominated sets obtained by all algorithms. The computational results are grouped according to different numbers of factories and jobs, denoted by $F \times n$, where 60 instances per average for each group of the small-scale benchmark set and 90 instances per average for each group of the large-scale benchmark set.

The statistical results of the coverage metric of the proposed MCEDA with NSGA-II, MMOIG, KCA and MOWSA on two benchmark sets of different sizes are reported in Tables 8 to 9 , respectively. As seen from

Tables 8-9, MCEDA yields good results that are, on average, almost better than that obtained by its counterparts for all testing instances. For the 900 small-scale instances, it is clear that almost $87 \%$ and $83 \%$ of the feasible solutions in the non-dominated set obtained by KCA and MOWSA are dominated by some of the feasible solutions in the nondominated set obtained by MCEDA in the average sense. In other words, only an average of $8 \%$ and $11 \%$ of solutions obtained by MCEDA are dominated by some feasible solutions yielded by KCA and MOWSA, respectively. In addition, for these non-dominated sets obtained by other two types of classical multi-objective algorithms, i.e., NSGA-II and MMOIG, almost average of $93 \%$ and $91 \%$ of feasible solutions are dominated by some solutions in the non-dominated set produced through MCEDA, which indicates that MCEDA has good performance in solving the small-scale instances of the EE_DAPFSP. Similarly, it can be clearly seen from Table 9 that, for the 810 large-scale instances, the performance of MCEDA is overwhelming on the coverage metric $C$. That is, the average $94 \%, 90 \%, 79 \%$, and $74 \%$ of the non-dominated solutions obtained by four counterparts, i.e., NSGA-II, MMOIG, KCA and MOWSA, are dominated by some of solutions in non-dominated set obtained by MCEDA, while only $1 \%-9 \%$ of the solutions in the non-dominated set of MCEDA are dominated by some solutions obtained by NSGA-II, MMOIG, KCA and MOWSA in an average sense, which further demonstrates that the proposed MCEDA can also be addressing the large-scale instances of the EE_DAPFSP. Furthemore, it is noted that the recently proposed KCA and MOWSA are also two relatively excellent multi-objective algorithms compared with NSGA-II and MMOIG. According to above analysis, we can conclude that the proposed MCEDA is significantly better than four high-performing multi-objective algorithms, i.e., NSGA-II, MMOIG, KCA and MOWSA, in terms of the quality of the obtained non-dominated solutions, which verifies the effectiveness of MCEDA for solving the EE_DAPFSP.

Table 9
Statistical results on $C$ metric of MCEDA with NSGA-II, MMOIG, KCA and MOWSA for large-scale instances.

| $F \times n$ | C(MCEDA, NSGA- <br> II) | C(NSGA-II, <br> MCEDA) | C(MCEDA, <br> MMOIG) | C(MMOIG, <br> MCEDA) | C(MCEDA, <br> KCA) | C(KCA, <br> MCEDA) | C(MCEDA, <br> MOWSA) | C(MOWSA, <br> MCEDA) |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| $4 \times 100$ | 0.97 | 0.00 | 0.93 | 0.01 | 0.86 | 0.03 | 0.81 | 0.06 |
| $4 \times 200$ | 0.94 | 0.00 | 0.89 | 0.00 | 0.83 | 0.02 | 0.77 | 0.05 |
| $4 \times 500$ | 0.93 | 0.00 | 0.87 | 0.01 | 0.78 | 0.04 | 0.73 | 0.07 |
| $6 \times 100$ | 0.98 | 0.01 | 0.94 | 0.02 | 0.87 | 0.06 | 0.82 | 0.09 |
| $6 \times 200$ | 0.95 | 0.00 | 0.91 | 0.01 | 0.81 | 0.05 | 0.79 | 0.08 |
| $6 \times 500$ | 0.92 | 0.00 | 0.85 | 0.03 | 0.74 | 0.07 | 0.65 | 0.14 |
| $8 \times 100$ | 0.94 | 0.00 | 0.94 | 0.00 | 0.81 | 0.02 | 0.78 | 0.06 |
| $8 \times 200$ | 0.92 | 0.01 | 0.90 | 0.02 | 0.76 | 0.02 | 0.72 | 0.11 |
| $8 \times 500$ | 0.93 | 0.00 | 0.83 | 0.01 | 0.69 | 0.05 | 0.63 | 0.13 |
| Average | 0.94 | 0.00 | 0.90 | 0.01 | 0.79 | 0.04 | 0.74 | 0.09 |

![img-23.jpeg](img-23.jpeg)

Table 12
Statistical results for all compared algorithms for the large-scale instances grouped by $F, S$, and $n$.

|  |  | NSGA-II |  |  |  |  |  | MMOIG |  |  |  |  |  | KCA |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | 

![img-24.jpeg](img-24.jpeg)

Fig. 12. The means plots and $95 \%$ Tukey HSD confidence intervals for the interaction between the type of algorithm and the number of factories for NSGA-II, MMOIG, KCA, MOWSA and MCEDA.
non-dominated sets obtained by NSGA-II, MMOIG, KCA, MOWSA and MCEDA for solving the nine typical instances with respect to different sizes (i.e., small, medium and large scale), respectively. It can be clearly seen from Fig. 13 that the non-dominated solutions obtained by MCEDA almost dominate the other solutions yielded by its counterparts regardless of both the small-scale instances and the large-scale instances, which indicates that the superiority of the proposed MCEDA is obvious. The main reason is that MCEDA has a powerful search engine to drive both global exploration and local exploitation. Furthermore, the distribution of the union Pareto fronts found by MCEDA is more decentralized and diversified, and the quality of the non-dominated solutions yielded by MCEDA is relatively high, which can provide a variety of satisfactory scheduling schemes for decision makers and can achieve a reasonable compromise between the maximum completion time and total energy consumption. In conclusion, the MCEDA proposed in this paper can effectively and efficiently solve the energy-efficient DAPFSP.

## 5. Conclusions

Energy saving has become a hot issue of global concern. Energyefficient production scheduling problem is one of the most fundamental and difficult scheduling problems encountered in many kinds of real-life manufacturing industries. This paper considered the energyefficient distributed assembly permutation flow-shop scheduling problem (EE_DAPFSP), whose objectives are to minimize the maximum completion time and the total carbon emission at the same time. To deal with this strongly NP-hard problem, a novel matrix-cube-based distribution estimation algorithm (MCEDA) was proposed. Based on the characteristics of the EE_DAPFSP, the hybrid initialization strategy, the more guided global search, the deep local search, and the speed adjustment strategies were designed, respectively. The effectiveness of these strategies was analyzed. Extensive computational experiments reveal that our MCEDA statically outperforms several state-of-the-art algorithms. To the best of our knowledge, this is the first report to propose an EDA-based algorithm for the energy-saving production scheduling problem.

![img-25.jpeg](img-25.jpeg)
(a) $n=20, m=5, F=3, S=3$.
![img-26.jpeg](img-26.jpeg)
(d) $n=100, m=20, F=6, S=50$.
![img-27.jpeg](img-27.jpeg)
(g) $n=500, m=5, F=8, S=40$.
![img-28.jpeg](img-28.jpeg)
(b) $n=24, m=5, F=3, S=2$.
![img-29.jpeg](img-29.jpeg)
(e) $n=200, m=5, F=6, S=30$.
![img-30.jpeg](img-30.jpeg)
(h) $n=500, m=10, F=6, S=40$.
![img-31.jpeg](img-31.jpeg)
(c) $n=100, m=5, F=4, S=30$.
![img-32.jpeg](img-32.jpeg)
(f) $n=200, m=20, F=8, S=40$.
![img-33.jpeg](img-33.jpeg)
(i) $n=500, m=20, F=8, S=50$.

Fig. 13. The Pareto front distribution of non-dominated solutions obtained by NSGA-II, MMOIG, KCA, MOWSA and MCEDA.

There are mainly two important directions for future research. First, we would like to develop several knowledge-based stargates to further enhance the guidance ability of MCEDA's global search. Second, it would be meaningful to extend the proposed MCEDA to the dynamical DAPFSP as well as the distributed production and transportation integrated scheduling problems.

## CRediT authorship contribution statement

Zi-Qi Zhang: Investigation, Methodology, Software, Writing - original draft. Rong Hu: Methodology, Funding acquisition, Supervision, Writing - review \& editing. Bin Qian: Methodology, Funding acquisition, Investigation, Writing - review \& editing. Huai-Ping Jin: . Ling Wang: Supervision, Project administration. Jian-Bo Yang: Supervision.

## Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## Acknowledgements

This research is partially supported by the National Natural Science Foundation of China (62173169, 61963022, 61873328), , and the Basic Research Key Project of Yunnan Province (202101AS070097).

## References

Abedi, M., Chiong, R., Noman, N., \& Zhang, R. (2020). A multi-population, multiobjective memetic algorithm for energy-efficient job-shop scheduling with deteriorating machines. Expert Systems with Applications, 157, 113348.
Chen, J. F., Wang, L., \& Peng, Z. P. (2019). A collaborative optimization algorithm for energy-efficient multi-objective distributed no-idle flow-shop scheduling. Swarm and Evolutionary Computation, 50, 100557.
Ciavotta, M., Minella, G., \& Ruiz, R. (2013). Multi-objective sequence dependent setup times permutation flowshop: A new algorithm and a comprehensive study. European Journal of Operational Research, 227, 301-313.
Dels, K., Pratap, A., Agarwal, S., \& Meyarivan, T. (2002). A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation, 6, 182-197.
Ding, J. Y., Song, S. J., \& Wu, C. (2016). Carbon-efficient scheduling of flow shops by multi-objective optimization. European Journal of Operational Research, 248, 758-771.
Hatami, S., Ruiz, R., \& Andres-Romano, C. (2013). The Distributed Assembly Permutation Flowshop Scheduling Problem. International Journal of Production Research, 51, 5292-5308.

Ishibuchi, H., Yoshida, T., \& Murata, T. (2003). Balance between genetic search and local search in memetic algorithms for multiobjective permutation flowshop scheduling. IEEE Transactions on Evolutionary Computation, 7, 204-223.
Jarboui, B., Eddaly, M., \& Siarry, P. (2009). An estimation of distribution algorithm for minimizing the total flowtime in permutation flowshop scheduling problems. Computers \& Operations Research, 36, 2638-2646.
Jiang, S. L., \& Zhang, L. (2019). Energy-oriented Scheduling for Hybrid Flow Shop With Limited Buffers Through Efficient Multi-Objective Optimization. IEEE Access, 7, 34477-34487.
Larra√±aga, P., \& Lozano, J. A. (2001). Estimation of distribution algorithms: A new tool for evolutionary computation. Springer Science \& Business Media.
Lin, J., Wang, Z. J., \& Li, X. D. (2017). A backtracking search hyper-heuristic for the distributed assembly flow-shop scheduling problem. Swarm and Evolutionary Computation, 36, 124-135.
Lin, J., \& Zhang, S. (2016). An effective hybrid biogeography-based optimization algorithm for the distributed assembly permutation flow-shop scheduling problem. Computers \& Industrial Engineering, 97, 128-136.
Lu, C., Gao, L., Li, X. Y., Pan, Q. K., \& Wang, Q. (2017). Energy-efficient permutation flow shop scheduling problem using a hybrid multi-objective backtracking search algorithm. Journal of Cleaner Production, 144, 228-238.
May, G., Stahl, B., Taisch, M., \& Prabhu, V. (2015). Multi-objective genetic algorithm for energy-efficient job shop scheduling. International Journal of Production Research, 53, 7071-7089.
Minella, G., Ruiz, R., \& Ciavotta, M. (2008). A Review and Evaluation of Multiobjective Algorithms for the Flowshop Scheduling Problem. Informs Journal on Computing, 20, 451-471.
Montgomery, D. C. (2008). Design and Analysis of Experiments (Second ed.). United States: John Wiley \& Sons, United States.
Pan, Q. K., Gao, L., Li, X. Y., \& Jose, F. M. (2019). Effective constructive heuristics and meta-heuristics for the distributed assembly permutation flowshop scheduling problem. Applied Soft Computing, 81, 105492.
Pan, Q. K., \& Ruiz, R. (2012). An estimation of distribution algorithm for lot-streaming flow shop problems with setup times. Omega-International Journal of Management Science, 40, 166-180.

Ruiz, R., \& Stutzle, T. (2008). An Iterated Greedy heuristic for the sequence dependent setup times flowshop problem with makospan and weighted tardiness objectives. European Journal of Operational Research, 187, 1143-1159.
Sang, H. Y., Pan, Q. K., Li, J. Q., Wang, P., Han, Y. Y., Gao, K. Z., \& Duan, P. (2019). Effective invasive weed optimization algorithms for distributed assembly permutation flowshop problem with total flowtime criterion. Swarm and Evolutionary Computation, 44, 64-73.
Shao, Z., Pi, D., \& Shao, W. (2018). A multi-objective discrete invasive weed optimization for multi-objective blocking flow-shop scheduling problem. Expert Systems with Applications, 113, 77-99.
Tiwari, A., Chang, P.-C., Tiwari, M. K., \& Kollanoor, N. J. (2014). A Pareto block-based estimation and distribution algorithm for multi-objective permutation flow shop scheduling problem. International Journal of Production Research, 53, 793-834.
Wang, G., Gao, L., Li, X., Li, P., \& Tangerinen, M. F. (2020). Energy-efficient distributed permutation flow shop scheduling problem using a multi-objective whale swarm algorithm. Swarm and Evolutionary Computation, 57, 100716.
Wang, J. J., \& Wang, L. (2020). A Knowledge-Based Cooperative Algorithm for EnergyEfficient Scheduling of Distributed Flow-Shop. IEEE Transactions on Systems, Man, Cybernetics: Systems, 50, 1805-1819.
Wang, L., Wang, S. Y., Xu, Y., Zhou, G., \& Liu, M. (2012). A bi-population based estimation of distribution algorithm for the flexible job-shop scheduling problem. Computers \& Industrial Engineering, 62, 917-926.
Wang, S. Y., \& Wang, L. (2016). An Estimation of Distribution Algorithm-Based Memetic Algorithm for the Distributed Assembly Permutation Flow-Shop Scheduling Problem. IEEE Transactions on Systems, Man, Cybernetics: Systems, 46, 139-149.
Zhang, Z. Q., Qian, B., Hu, R., Jin, H. P., \& Wang, L. (2021). A matrix-cube-based estimation of distribution algorithm for the distributed assembly permutation flowshop scheduling problem. Swarm and Evolutionary Computation, 60, 100785.
Zitzler, E., \& Thiele, L. (1999). Multiobjective evolutionary algorithms: A comparative case study and the strength Pareto approach. IEEE Transactions on Evolutionary Computation, 3, 257-271.
Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., \& Fonseca, V. G.d. (2003). Performance assessment of multiobjective optimizers: An analysis and review. IEEE Transactions on Evolutionary Computation, 7, 117-132.