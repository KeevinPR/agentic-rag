# Neural Architecture Search by Estimation of Network Structure Distributions 

ANTON MURAVEV ${ }^{\odot 1}$, (Member, IEEE), JENNI RAITOHARJU ${ }^{\odot 1,2}$, (Member, IEEE), AND MONCEF GABBOUJ ${ }^{\odot 1}$, (Fellow, IEEE)<br>${ }^{1}$ Faculty of Information Technology and Communication Sciences, Tampere University, FI-33100 Tampere, Finland<br>${ }^{2}$ Programme for Environmental Information, Finnish Environment Institute, FI-40500 Jyväskylä, Finland<br>Corresponding author: Anton Muravev (anton.muravev@tuni.fi)

This work was supported by the European Union's Horizon 2020 Research and Innovation Programme under Grant 871449 (OpenDR).


#### Abstract

The influence of deep learning is continuously expanding across different domains, and its new applications are ubiquitous. The question of neural network design thus increases in importance, as traditional empirical approaches are reaching their limits. Manual design of network architectures from scratch relies heavily on trial and error, while using existing pretrained models can introduce redundancies or vulnerabilities. Automated neural architecture design is able to overcome these problems, but the most successful algorithms operate on significantly constrained design spaces, assuming the target network to consist of identical repeating blocks. While such approach allows for faster search, it does so at the cost of expressivity. We instead propose an alternative probabilistic representation of a whole neural network structure under the assumption of independence between layer types. Our matrix of probabilities is equivalent to the population of models, but allows for discovery of structural irregularities, while being simple to interpret and analyze. We construct an architecture search algorithm, inspired by the estimation of distribution algorithms, to take advantage of this representation. The probability matrix is tuned towards generating high-performance models by repeatedly sampling the architectures and evaluating the corresponding networks, while gradually increasing the model depth. Our algorithm is shown to discover non-regular models which cannot be expressed via blocks, but are competitive both in accuracy and computational cost, while not utilizing complex dataflows or advanced training techniques, as well as remaining conceptually simple and highly extensible.


#### Abstract

INDEX TERMS Automatic architecture design, estimation of distribution algorithm, deep learning, convolutional neural network.


## I. INTRODUCTION

The recent successes of deep learning have attracted significant interest in numerous fields of knowledge [1]. Computer vision in particular has witnessed the development of multiple successful models, based on convolutional neural networks (CNNs), for tasks such as classification [2], [3], semantic segmentation [4], and detection [5]. While the growth of deep learning solutions over the years is impressive, their adoption brings many significant challenges of both theoretical and practical nature. In addition to well-known problems such as overfitting or vanishing gradients, which have been subjects of extensive research over the years, new issues are being discovered, which are not yet fully understood. For example,

The associate editor coordinating the review of this manuscript and approving it for publication was Mitra Mirhassani ${ }^{\circledR}$.
the lack of interpretability of decisions made by deep models [6], [7] is a difficult problem to tackle, but has attracted increasing research attention recently [8]. Further concerns have been raised regarding secure practical use of common deep models, as they were shown to be vulnerable to attacks utilizing malicious data [9].

One aspect of the neural networks, intricately tied to these challenges, is the architectural design: the choice of layer count, connection patterns, neuron operations, and their hyperparameters (convolution filter sizes, channel depth). It is well-known that some structural choices are associated with training difficulties; for example, a depth increase causes the vanishing gradient problem [10]. Meanwhile, on a system level, the design guidelines of creating a deep network for a particular practical problem are not well established. The network design task thus becomes a

time-demanding process, involving extensive trial and error. In practice, this issue is commonly avoided by using an already established pre-trained model of the same or related data domain as a feature extractor [11]. While effective, the latter approach presents problems of its own. Pre-trained models tend to be large and can lead to resource-consuming, largely redundant systems, whereas a too small network may produce insufficient accuracy for the problem at hand. Specific features of the data may require specific layer types to be fully exploited [12]. Pre-trained models can carry undesired biases from their original datasets [13]. Additionally, sharing the foundation means that such systems will naturally be more vulnerable to adversarial attacks. A promising way to avoid these problems lies in developing appropriate methods for automated task-specific network design.

The idea of automated neural network design dates back to the early 1990s [14]. The following decade saw a large volume of research on this problem, primarily focusing on evolutionary algorithms as solvers, both due to their gradient-free nature and shared biological inspirations [15]. This family of approaches would later be coined neuroevolution [16]. The research continued into the 2000s, with both improved evolutionary algorithms [16], [17] and other metaheuristic approaches, such as particle swarm optimization [18], as the search method. However, all of these algorithms share the need to perform many evaluations of intermediate solutions, which, in the case of automated architecture design, requires training numerous candidate networks from scratch. Therefore, these approaches were computationally restricted to rather limited model complexity, and their practical applications remained primarily in control tasks and robotics, where these limitations have a smaller impact [19].

The advent of deep learning, where training a single model can take days or weeks, caused manual design to once again become the primary approach. However, as architectural discoveries paved the way to models with a small number of parameters and superior performance [2], [3], the interest in automated design reemerged, taking advantage of both evolutionary optimization [20]-[24] and newer approaches, such as reinforcement learning [25]-[27]. The computational demand remains a major limitation and has hence been the focus of most recent works in the area [27]-[29].

Another concern is the growing semantic complexity of such algorithms. While they may yield successful architectures [23], their search behaviour is hard to analyze, which obscures the effects (whether positive or negative) of individual algorithmic steps and hinders comparisons. For instance, Zoph et al. [26] proposed a reduced search space: the network is represented as a repeating sequence of cells of a few types, where the internal structure of cells of the same type is identical and subject to optimization. Consequently, this design space has been adopted by a multitude of other approaches (see [23], [27], [30]), as it allowed significant speedups. However, the random search has recently been shown to be highly competitive in this space as well, suggesting that previous
successes may come not from algorithm specifics but from the expressivity constraints [31].

Despite the variety of the proposed solutions for the architecture search problem, the majority of them rely on the repeating blocks, cells, or motifs. While effective in reducing the search space and thus (indirectly) the computation required, such an approach is by design biased towards deeper and uniformly structured models. This brings them closer to handcrafted networks, but limits the possibilities to discover irregular architectures with non-repetitive patterns, which can potentially be superior in performance or have other valuable properties, such as faster inference time or lower memory footprint.

We propose a conceptually simple and extensible architecture search method that runs on a less structured, dynamically growing search space. Our solution is based on the estimation of distribution algorithms. We specifically draw from Population-Based Incremental Learning [32] and Univariate Marginal Distribution Algorithm [33]. We utilize a set of discrete probability distributions to describe the choice of layers in a feedforward deep network, assuming their independence. Together they form a network prototype, which is then iteratively updated by sampling and evaluating network models, until convergence is reached. The contributions of this article can be summarized as follows:

- We propose a probabilistic representation of deep neural networks by expressing their structure as a set of layer type probabilities. A single prototype of the proposed form corresponds to not a single network, but a family of models, which can span entire regions of the design space. This representation is less constrained compared to cells in that it does not force repeating patterns to appear, while still allowing the regularity to emerge by itself (if it proves competitive).
- We propose a CNN architecture search method based on the optimization of the above prototype, denoted Architecture Search by Estimation of network structure Distributions (ASED). As candidate networks are sampled and individual probabilities converge to their extreme values, the algorithm naturally transitions from global to local search, avoiding suboptimal areas. Unlike cell-based methods, we dynamically expand the search space to gradually explore deeper and deeper architectures over the runtime.
- We propose additional techniques to introduce non-linear connectivity patterns to solutions and to control the speed of search convergence. Evaluation results confirm that these techniques lead to performance improvements.
- We experimentally demonstrate the comparable performance of our method (in our growing search space) to existing approaches in terms of both model performance and computational requirements. We notably use only feedforward structures without explicitly defined repeated motifs or advanced techniques for regularization and data pre-processing.

The rest of the paper is structured as follows. In Section 2, we review the related developments in the field of neural architecture optimization as well as elaborate on our inspirations. Section 3 describes our approach to architecture search and the additional related techniques. Section 4 contains the experimental results and their analysis. Finally, Section 5 concludes the work and outlines some potential future studies.

## II. RELATED WORK

Ever since the wider adoption of multilayer perceptron structures [34], their architectures became subject to optimization. While general neural network design involved heuristic rules and empirical tests, a promising alternative was found in evolutionary optimization methods due to their ability to solve problems defined only by the target function, without requiring any gradient information [35], [36]. Early works on neuroevolution included [37]-[39]. However, all of these works shared common issues of prohibitive computational requirements and lack of robustness due to the highly noisy nature of the search space [15].
The small-scale neuroevolution reached a new peak when NEAT (NeuroEvolution of Augmented Topologies) [16] was introduced in 2002. The techniques that made NEAT differ from its predecessors are historical gene markings, allowing for straightforward and meaningful crossover, and speciation with fitness sharing, which allows promising individuals to more consistently reach their full potential. Despite the advantages and the flexibility it offered, NEAT remained limited to small-scale applications, such as control tasks with limited inputs (a problem which would later be tackled within reinforcement learning). Multiple subsequent variants of NEAT [17], [40], [41] aimed at efficient generation and representation of more complex networks with repeatable structural patterns. For example, HyperNEAT [17] encodes network architectures via a metric space, called substrate, where every neuron is mapped to a point with fixed coordinates. A separate hypermodel, called a Compositional Pattern-Producing Network (CPPN), takes as inputs the coordinates of a pair of neurons and outputs the connection weight between them, thus defining a network structure. An evolved substrate variant of HyperNEAT, or es-HyperNEAT [41], avoids the need to explicitly define node geometry, allowing for discovery of a wider scope of structures. However, despite the greater representational expression of these methods, the overarching problems remained and the use was limited to specific small-scale applications [42].
The resurgence of interest for architecture optimization started in 2016, after the introduction of reinforcement learning driven Neural Architecture Search (NAS) in [25]. An LSTM-based recurrent neural network (a controller) is trained to output sequences of tokens (symbols), which correspond to specific values of convolutional layers' parameters, such as filter size, count, and stride. The resulting neural network can be trained and evaluated. The controller can then be updated by the REINFORCE rule to maximize the expected accuracy of the generated networks. The major weakness
of NAS is the computational cost of over 22000 GPU days on a standard CIFAR-10 image classification dataset. The follow-up work NAS-Net [26] represents the target network as a predefined sequence of repeating elements, known as cells. Each cell type shares the same internal structure, which is optimized in a graph form and can contain different convolution and pooling operations. During the search process, the total number of cells in the network is reduced to speed up computation, while the final discovered architecture is evaluated in a full-length sequence. Such a reduction of the design space has proven effective in guiding the search, thus boosting the accuracy and reducing the running time to 2000 GPU days, and has since been used in other works, despite the limitations of the representation. Efficient NAS, or ENAS [27], achieves further speed-up (to less than 16 GPU hours) at the cost of some accuracy loss. It utilizes weight sharing, where the convolutional filter weights are identical between the cells and depend only on the position of the corresponding edge in the structural graph. Thus, training from scratch (which was necessary for the network evaluation) is no longer needed, and the tensor of shared weights can be finetuned via gradient updates in-between controller updates. While weight sharing technique allows for significant reduction in computations, it also tends to distort the performance ranking of intermediate candidate networks, leading to potentially inferior models being chosen [31].

Evolutionary algorithms arose once again as a primary competitor to reinforcement learning based solutions. CoDeepNEAT [43] adapts the well-known NEAT procedure for deep networks by using two separate populations - blueprints and modules - for easier representation of repeating patterns. Genetic CNN [20] encodes layer connectivity in a population of binary strings and runs a standard genetic algorithm. EvoCNN [24] instead opts for variable-length gene encoding to represent networks of arbitrary depth, where crossover is made possible via matching the genes that share a type (e.g. a pooling gene can only be matched with another pooling gene). Real et al. [21] run a distributed large-scale evolutionary process directly on a population of networks, where mutations can alter the network structure, parameters, or training process. The following work of Real et al. [23] combines the evolutionary approach with the NAS-Net search space, surpassing reinforcement learning in anytime accuracy and setting a new state-of-the-art performance on the popular CIFAR-10 dataset, as well as generating comparatively simpler models. However, the computational cost remains extensive, clocking above 3000 GPU days. The similar approach is taken by the automatically evolving CNN (AE-CNN) [44], which runs a genetic algorithm on the population of networks composed of customized ResNet and DenseNet blocks, achieving competitive results.

While deep networks can be difficult for the neuroevolution to handle, a viable alternative can be found in expanding the operation set of the shallow networks, allowing for more powerful representations. Generalized Operational Perceptron (GOP) [45] model substitutes the standard neuron

by offering a wider choice of nodal and pooling operations instead of the standard multiplication and addition. The choice of operations can be optimized simultaneously with the network architecture by a greedy incremental procedure. Operational Neural Networks (ONNs) [46], composed of such units, have been shown to achieve superior performance to CNNs on some practical problems. Most recently heterogeneous GOP structures, where each layer can have neurons with differing operations, have received increasing attention [47]. While flexibility of operators allows ONNs to stay relatively shallow, it also results in a vast unstructured design space which is computationally costly to traverse.

Many recent works in architecture optimization utilize various techniques to reduce the computation needed, primarily by simplifying the evaluation procedure. SMASH [48] learns a hypernetwork that can predict weights for all the connections of an arbitrary deep network (given a specific representation), which reduces the need for training and makes random search a viable solution for discovering architectures. Progressive Neural Architecture Search (PNAS) [28] uses a separate recurrent network to approximately rank the candidate models without training them, allowing the search to focus only on more promising options. NASH [49] and LEMONADE [29] take advantage of network morphismsoperations that modify the structure of a trained network without affecting its output-to navigate the search space without training the models from scratch. Differentiable Architecture Search (DARTS) [30] provides a continuous relaxation of the NAS-Net cell structure problem and performs the search via gradient descent, iterating between the architecture and weight updates. While relatively more efficient in terms of computation, these methods do not address the issues of interpretability and semantic complexity.

There exists a number of works that model the network construction as a probabilistic process, sharing some similarities with the proposed approach. Methods based on reinforcement learning, such as NAS and its successors, use the probability of a given network to be produced from the current policy as a weight for the corresponding reward. InstaNAS [50] also has reinforcement learning at its core, but differs from other algorithms in this group, as it takes an instance-aware approach. Specifically, InstaNAS processes each data point by a separate network (a path within a large trained model), sampled from a parameterized distribution. NASBOT [51] models the architecture search as a Gaussian process. To facilitate this, the authors introduce a (pseudo)distance in the network design space and utilize an evolutionary algorithm as an optimizer. The most similar approach to ours is Probabilistic Neural Architecture Search (PARSEC) [52]. As in our work, PARSEC explicitly models a structural distribution of the neural networks, including the assumption of independence between individual operations. However, this distribution operates on a level of NAS-Net cell, while our method models the network as a whole without forced repeating patterns. Moreover, the search procedure is different: PARSEC uses Monte Carlo empirical Bayes to
iteratively update both the architectural priors and the tensor of shared weights, while we completely recompute the marginal probabilities over a subset of the samples and do not use weight sharing.

Our work draws inspiration from the estimation of distribution algorithms (EDAs) - the family of optimization methods originating from mid-1990s, which are closely related to genetic algorithms [53]. While most evolutionary algorithms maintain a candidate population, which implicitly defines the probability distribution of the solutions, EDAs define this distribution explicitly and tune its parameters throughout the optimization process. Our work mainly draws on two discrete univariate EDAs, Population-Based Incremental Learning (PBIL) [32] and Univariate Marginal Distribution Algorithm (UMDA) [33]. PBIL generates an intermediate population via sampling, applies a selection procedure, and updates the probabilistic model in the direction of selected samples, using a learning rate parameter. UMDA maintains the population of solutions, estimates a set of marginal probabilities from the best candidate(s), and uses them to produce the population of the next generation. For more information on EDAs, their applications and recent developments, we direct the reader to the survey by Hauschild and Pelikan [54]. To the best of our knowledge, ours is the first work to explicitly apply the EDA formulation to the deep neural network architecture search problem.

## III. METHODOLOGY

In this section, we describe and justify the proposed network representation, the design of the proposed algorithm Architecture Search by Estimation of network structure Distributions (ASED), as well as additional techniques to improve its capabilities.

## A. SEARCH SPACE AND NETWORK REPRESENTATION

The problem of optimizing the structure of a neural network is extremely high-dimensional. The choice of layer types (convolution, pooling) alone produces a combinatorial problem that grows exponentially with the increase in depth, and that is without taking into account layer hyperparameters (filter size, stride, channel count) and weights. Connectivity patterns add another dimension of complexity, as structures such as skip connections and parallel branches have been found beneficial in manually designed models [3]. For this reason many recent architecture optimization algorithms, starting with NAS-Net by Zoph et al. [26], utilize a constrained search space based on repeated structural motifs. Instead of searching for the architecture of the entire network, they instead work with cells, which are small subnetworks containing only a few layers. The target network is then constructed by repeating the cell a given number of times. This relaxation allows the cells to have almost arbitrary structures while maintaining the viability of the search. Another advantage is the directly controllable trade-off between the network power and complexity by varying the number of cell repetitions. It is common to speed up the search by using less cells and then increase their

number for the final evaluation of the discovered architecture. However, the unavoidable natural drawback of this approach is the fact that only a small subset of network design space is reachable with such constraints, and potentially better architectures may not be discoverable. Therefore, we opt for optimizing the whole network simultaneously.

We model a deep neural network as a multivariate random variable coming from a known probability distribution. For the sake of tractability, we consider only the choices of layer types for optimization, resulting in a discrete distribution, while other hyperparameters are not directly tuned by the search procedure. Specifically, we bind the values of filter sizes and strides with the layer type choices and set the channel count to an externally defined constant for all the layers. We denote the set of possible layer types as $L$ and call it the layer library. For the purpose of this work, we include the following ten common operations in the library (each with the corresponding shorthand notation):
id identity (output is equal to input),
c1 $1 \times 1$ convolution,
c3 $3 \times 3$ convolution,
c5 $5 \times 5$ convolution,
c7 $7 \times 7$ convolution,
d3 $3 \times 3$ dilated convolution (with dilation rate of 2),
d5 $5 \times 5$ dilated convolution (with dilation rate of 2),
m2 $2 \times 2$ max pooling,
m3 $3 \times 3$ max pooling (with stride 2),
a3 $3 \times 3$ average pooling (with stride 2 ).
We assume that the choice of each layer in a CNN is independently distributed. While this assumption is unlikely to hold in practice, it simplifies the formulation, and inter-layer interactions are implicitly taken into account during the search. Multivariate generalizations of the proposed method can potentially offer improvements and are a promising future work direction. Given our assumption, a discrete distribution of network structures can be represented as a matrix of probabilities $P$, where each row describes a layer and $P_{i j} \in\{0,1\}$ is the probability of $i$-th layer being the $j$-th layer type from $L$. Matrix $P$ is henceforth called prototype. The dimensions of $P$ are $N \times|L|$, where $N$ is the current number of layers in the network.

The probabilistic representation has a number of inherent advantages over maintaining the population of networks. Matrices have a wide range of available optimization approaches; many existing optimization heuristics outside of the scope of this work are straightforwardly applicable to the proposed representation. The prototype offers intuitive insight into the anytime state of the search, as the probability mass is always explicitly assigned for every point of the design space. The convergence of the search is easy to determine by how close the layer probabilities are to their extremes. Finally, the proposed representation can offer significant implementation advantages in a distributed setting, as only a small prototype matrix needs to be transferred between computational nodes, rather than full-scale models.

Evaluation of the prototype involves sampling network structures from it and measuring their performance (and potentially other metrics) on the given problem. By default, every sampled network has to be trained from scratch, which can result in fairly high computational costs, especially for deep prototypes (with large values of $N$ ). Weight sharing, when candidate networks reuse trained weights from the previous search iterations, is commonly used in other methods to reduce the overall training time. A form of weight sharing can be implemented in our framework by storing every newly encountered structure with its performance in a dictionary and retrieving these values instead of retraining, when such structure is sampled again. However, this solution has drawbacks of its own: as we continually deepen the prototype, the chances of repeatedly sampling the same structure are actually very low, while maintaining the dictionary would noticeably increase the memory requirements. In addition, weight sharing can give an unfair advantage to inferior structures, thus damaging the overall search [31]. For these reasons we opt to contain the computational costs by other means: specifically, limiting the value of $N$ and reducing the number of epochs for candidate training. We consider some other speedup options in the analysis section.

## B. SEARCH ALGORITHM

To construct an iterative architecture search algorithm with the above representation, three elements need to be defined - initialization, update and stopping condition. The proposed algorithm, denoted ASED, operates on a single prototype for the sake of simplicity. The depth of all networks on a given search step is the same due to the fixed prototype dimensions; to search across architectures of different sizes, we gradually increase the depth after each update step. While the prototype rows are never removed, the inclusion of identity in our layer library means that, in practice, networks with less than $N$ layers can be represented and discovered at any search stage.

The prototype $P$ is initialized as a $N_{\text {init }} \times|L|$ matrix with every element set to $1 /|L|$, where $N_{\text {init }}$ is a starting layer count. While a more specific prior can be given, the uniform initial distribution ensures that every reachable architecture is equally likely to be considered, which helps to emphasize early exploration. The choice of $N_{\text {init }}$ should be carefully considered, as a small value can result in premature convergence without sufficiently exploring the larger portion of the design space, but a large value can cause the search to be "lost" unless an impractically large number of samples is evaluated (due to the curse of dimensionality).

To update the prototype, sampling of $K$ candidate networks is performed first, with each layer independently selected from the discrete distribution given by the corresponding row of the prototype matrix. Each candidate model is then trained and evaluated on the target problem, and the temporary population is sorted by validation performance. While we evaluate and track the classification accuracy, we opt for another, additional measure to compute the candidate ranking. We adopt the multi-class Matthews coefficient [55], which is designed

to be robust to the class imbalance in the data, allowing the search to operate reliably in such cases. The formula of the multi-class Matthews coefficient is as follows:

$$
m=\frac{\sum_{k l m}\left(C_{k k} C_{l m}-C_{k l} C_{m k}\right)}{\sqrt{\sum_{k}\left(\sum_{l} C_{k l}\right)\left(\sum_{l^{\prime}} C_{k^{\prime} l^{\prime}}\right)} \sqrt{\sum_{k}\left(\sum_{l} C_{l k}\right)\left(\sum_{l^{\prime}} C_{l^{\prime} k^{\prime}}\right)}},}
$$

where $C$ is a confusion matrix. The value of the multi-class Matthews coefficient ranges between a data-dependent negative value $(\geq-1)$ and +1 . Whenever the denominator of the fraction in Equation 1 is zero, we set the output value to the lowest possible: -1 .

Given the ranking, the best $K_{s}<K$ models are then selected to directly induce the new prototype, which, due to the independence assumption, takes the following form:

$$
P_{i j}=\frac{1}{\left|K_{s}\right|} \sum_{k=1}^{K_{s}} x_{k i j}
$$

where $x_{k i j}$ is an indicator variable that is equal to 1 if $k$ selected candidate network has $j$ library item as $i$ layer, and 0 otherwise. This update step is equivalent to the one used in the UMDA algorithm [33]. Every update is followed by an addition of one or more rows to the prototype, according to the predefined schedule (denoted $n(t)$ ). These new layers are initialized with a uniform distribution. Note that we avoid explicitly preserving the best candidates between updates (the technique known as elitism). As our approach starts from the solution space of low dimensionality and gradually increases it, the bias towards early dominant solutions will result in premature convergence. The complete description of the ASED procedure is given in Algorithm 1. The search stops when the specified iteration limit $t_{\max }$ is reached or all the values of the prototype matrix become strictly 0 or 1 , which indicates complete convergence. A single network architecture with the highest probability is selected to be the final output.

## C. CONVERGENCE CONTROL TECHNIQUES

While the described search procedure navigates the search space by progressively narrowing down the region under consideration and should be capable of avoiding local optima, it can still get stuck in a local optimum and hence exhibit premature convergence. As the search progresses, individual layer type probabilities tend to approach either 0 or 1 regardless of their immediate impact on the network performance, as is established in the theory of EDAs [56], [57]. The proposed algorithm does not allow for any mechanisms to limit this; in fact, such an effect is desirable for the search convergence. Moreover, once a probability has achieved the value of exactly 0 or 1 , it becomes fixed and will not change thereafter, as all of the sampled networks will be the same with respect to the type of the corresponding layer. In case of 0 the corresponding operation is no longer considered, while in case of 1 the layer choice is made permanent, meaning that the dimensionality of the problem is essentially reduced from that

```
Algorithm 1 Architecture Search by Estimation of Network
Structure Distribution (ASED)
    Input: \(L, N_{\text {init }}, t_{\max }, K, K_{s}, n(t)\)
    \(N \leftarrow N_{\text {init }}\)
    for \(i \in\left\{1, \ldots, N_{\text {init }}\right\}, j \in\{1, \ldots,|L|\}\) do
        \(P_{i j} \leftarrow 1 /|L|\)
    end for
    for \(t \in\left\{1, \ldots, t_{\max }\right\}\) do
        Sample \(K\) candidate networks from \(P\)
        Train and evaluate candidate networks
        Sort candidate networks by validation performance
        \(S \leftarrow K_{s}\) best performing candidate networks
        Recompute \(P\) based on \(S\) (Eq. 2)
        Add \(n(t)\) new rows to \(P\)
        for \(i \in\{N+1, \ldots, N+n(t)\}, j \in\{1, \ldots,|L|\}\) do
            \(P_{i j} \leftarrow 1 /|L|\)
        end for
        \(N \leftarrow N+n(t)\)
        if \(\forall i, j P_{i j} \in\{0,1\}\) then
            break
        end if
    end for
    return \(P\)
```

point on. A subset of network structures becomes unreachable, which can be beneficial for navigating the design space, but can also mean the loss of potentially better solutions. We consider two different techniques to address this issue.

A common technique in EDAs involves capping the probabilities, such that extreme values are not achievable and each element instead spans the predefined range $\left[p_{\min }, p_{\max }\right]$, where $p_{\min }>0, p_{\max }<1$. In our setting, this means that there is always at least the probability of $p_{\min }$ for each layer type to be selected in any position, removing irreversible choices. Probability capping is implemented by simple row-wise proportional normalization of the prototype matrix after every prototype update step. We adopt the approach where the upper cap $p_{\max }$ is explicitly given as a parameter and the lower cap is then computed as

$$
p_{\min }=\frac{1-p_{\max }}{|L|-1}
$$

The normalization itself is then performed as follows:

$$
\begin{aligned}
& p_{i j}^{\prime}= \begin{cases}p_{\min } & \text { if } p_{i j} \leq p_{\min } \\
p_{i j} \cdot m_{i} & \text { if } p_{i j}>p_{\min }\end{cases} \\
& m_{i}=\frac{\sum B_{i}+\sum S_{i}-\left|S_{i}\right| \cdot p_{\min }}{\sum B_{i}}
\end{aligned}
$$

where $S_{i}=\left\{p_{i j} \mid p_{i j} \leq p_{\min }\right\}$ and $B_{i}=\left\{p_{i j} \mid p_{i j}>p_{\min }\right\}$ for $i \in$ $\{1, \ldots, N\}$. Note that $B_{i}$ cannot be 0 as the layer probabilities sum to 1 .

Another way to prevent the search from prematurely converging is to additionally modify the prototype between

iterations. One can, for instance, apply a small random perturbation, similar to how mutation is used in evolutionary algorithms. However, due to the search being driven by sampling, the effect of such mutation would be either insignificant or highly unpredictable. Instead, we opt for another operation, which we call prototype inversion, that replaces high probabilities with low values and vice versa. This prompts the search to explore exactly the previously discarded regions of the search space, while the currently dominant choices become extremely unlikely (the latter aspect evokes similarities to the well-known tabu search, which is an optimization technique that explicitly forbids the reuse of already seen solutions [58]). Naturally, such an inversion operation is highly destructive and can prevent the search from progressing, so it needs to be executed only at some iterations of the algorithm. Additionally, we save the current prototype just before inverting it to make sure the information is not lost, which essentially means the ASED algorithm can produce multiple solutions before the stopping condition is met.

To establish an inversion condition, we need to find the measure of convergence, as performing the inversion too early and/or too often can hinder the search process. $L_{2}$ textnobreakdash-norm of the probability vectors is suitable for this purpose, as it spans the interval $[1 / \sqrt{|L|}, 1]$. Here, the lower bound corresponds to the uniform distribution and the upper bound is achievable only when a single element (layer type) takes value 1 with every other being 0 . The $L_{2}$ textnobreakdash-norm of each prototype row is thus a measure of the certainty of the layer choice and increases as the search progresses. The condition for triggering the prototype inversion can then be a threshold on the $L_{2}$
textnobreakdash-norm of the prototype, averaged over all the rows (as they are assumed independent). If the inversion is used, this condition is checked at every iteration after the prototype is updated. The newly added uniformly distributed rows are ignored for the purposes of the mean norm calculation.

The inversion operation is implemented by subtracting each probability value from 1 (e.g., 0.85 becomes 0.15 ). For probabilities that have taken values of 0 or 1 this operation does not have the intended effect, as they still limit the exploration space. To suppress these extreme values, the inversion is followed by the probability capping normalization defined in Eq. (4), using the same parameters $p_{\max }$ and $p_{\min }$. As no probability goes to zero as a result of inversion, previously reachable solutions remain reachable, allowing for potential backtracking. We consider two types of inversion operation: the full inversion and the partial inversion. The former is applied row by row to the whole prototype. The latter is less destructive as it only applies to the subset of prototype rows which have the highest $L_{2}$-norm. The specific number of such rows is empirically set to $\lfloor\sqrt{N}\rfloor$. The partial inversion thus applies only to some of the most converged layers, preserving less confident choices as they are.

As both described techniques are simple mathematical operations on the prototype matrices, they do not incur
significant computational costs by themselves. However, as they influence the search behaviour of the algorithm towards slowing down the convergence, more iterations may be required to reach the solution of the same level of complexity as with the baseline variant. With respect to the given stopping condition, the proposed techniques can indirectly increase the overall running time of the algorithm, although the specific impact can be evaluated only empirically on a case-by-case basis.

## D. NON-LINEAR LAYER CONNECTIVITY

The default prototype specification supports only the simplest architectures, where the flow of the input data through the layers is strictly sequential. However, connections between non-adjacent layers, also known as shortcuts, play an important role in the powerful CNN models, such as ResNet [2] and DenseNet [3]. Shortcuts counteract the problem of vanishing gradients by ensuring the efficient flow of the backwards propagated signal, which allows models of much larger depth to be reliably trained. While we constrain layer counts during the architecture search due to computational limitations, the use of shortcuts still simplifies and speeds up the training of candidate models as the depth increases. By making deeper models more competitive against shallow ones already in the early training stages, shortcuts discourage premature convergence of the prototype. Thus, while implicit, they grant similar benefits as the techniques described in the previous subsection. Of course, our framework also benefits from shortcuts via increased expressiveness and generality.

There are several options for introducing shortcuts to the proposed formulation. One possibility is to define another prototype matrix, which would store the connectivity patterns, i.e., presence or absence of a connection, between layers. This additional prototype can be operated on with any procedure suitable for the original, such as the one given in Algorithm 1. The two matrices can be concatenated and optimized jointly, or they can be iterated between, introducing an additional step to the search procedure. It is worth noting, however, that such an approach significantly increases the problem complexity. Additional dimensions of the search space would require either exponentially more samples (in the joint case) or exponentially more iterations (in the iterative case) to ensure sufficient exploration. As a compromise between complexity and expressivity, we consider another approach — fixed shortcut patterns. We design simple shortcut-generating rules, inspired by existing deep CNN models, and apply them to any sampled candidates throughout the search, as well as to the ultimately discovered architectures (while ensuring the solutions remain valid). As the rule does not change over time, the search procedure can be expected to select structures which take the most advantage of the given shortcut pattern. This simple approach does not incur any additional computational costs. In fact, it can potentially speed up the training of the deeper networks, but this is not currently taken advantage of, as we use fixed training schedules for simplicity.

![img-0.jpeg](img-0.jpeg)

FIGURE 1. Illustrated shortcut patterns: a) both, $D=1$; b) residual, $D=2$; c) semi-dense, $D=2$; d) semi-dense, $D=3$.

We consider two types of shortcut patterns - the residual pattern and the semi-dense pattern. Both can be characterized by a single parameter $D \geq 1$. Residual pattern is inspired by the residual connections of ResNet [2]. In this case, the shortcut connects the output of the current layer to the output of the layer which is $D$ positions after (e.g., the following layer if $D=1$ ). Residual shortcuts cannot intersect or overlap, i.e., there can be no starting point between another starting point and its corresponding endpoint. In the case of the semi-dense pattern, every group of $D$ consequent layers have their outputs connected to the single endpoint at the output of the $(D+1)$ th layer. It is inspired by the DenseNet [3] with some simplification (the original pattern would have all the layers within the group connected). These shortcut patterns are illustrated in Fig. 1.

As in other deep models, the signal arriving via a shortcut is combined with the primary signal at the endpoint by simple addition. As in the early ResNet, we follow the post-activation pattern, i.e., the activation function is applied after signals are combined (but batch normalization, if applicable, precedes the signal combination). A notable issue for the proposed approach is the compatibility of the signal dimensions. In handcrafted architectures, shortcuts are typically applied within layer blocks that do not perform downsampling, and, therefore, the shapes of the primary and the shortcut signals always match. This does not necessarily hold within our framework: while convolutions are zero-padded to preserve dimensions between input and output, any layer can potentially be a pooling layer, breaking the compatibility. We resolve this issue by keeping track of the pooling operations that the primary signal is undergoing and applying them to the shortcut signal before adding the two together. Compatibility with regards to the channel number is guaranteed, since it remains constant throughout the network, with the exception of the original input, which is not allowed to be the starting point of a shortcut. It is also worth noting that, since our framework allows every layer to potentially represent an
identity function, the span of some of the shortcuts may in practice be less than $D$.

## IV. EXPERIMENTAL RESULTS AND ANALYSIS

In this section, we describe the experimental setting and obtained results and discuss their implications for this and future work.

## A. EXPERIMENTAL SETTING

Following the previous works of the field, the proposed ASED algorithm is validated in the image classification setting. We consider two datasets of differing difficulty and scale. As a simpler problem we utilize USPS dataset [59], which contains grayscale samples of handwritten digits of $16 \times 16$ pixels each. USPS has 10 classes, 7291 training examples, and 2007 test examples. The larger problem is CIFAR-100 dataset [60], which is one of the standard NAS benchmarks. It consists of 3-channel $32 \times 32$ RGB images. It contains 50 K training examples and 10 K test examples of 100 different classes. To allow for evaluation of candidate networks without leaking the test data, for both datasets we sample $20 \%$ of the original training images, maintaining class balance, to obtain validation sets. We use the original test sets only to report the performance of the final discovered model. Classification accuracy is used as the performance metric. We do not use any preprocessing for USPS. For CIFAR-100 the standard preprocessing procedure is applied: images are zero-padded by 4 pixels on each side, followed by random cropping down to $32 \times 32$ and horizontal flipping with probability 0.5 , as well as normalization to zero mean and unit variance. We do not employ other regularization techniques, such as cutout, drop-path, or shake-shake.

The algorithm parameters are set as follows. The search is initialized by sampling 10 K networks from the uniform 5-layer prototype, which covers $10 \%$ of the design space (as the current library permits $10^{5}$ possible 5-layer networks). At every iteration of the search, $K=1000$ networks are sampled, trained, and ranked, with the top $K_{s}=100$ forming the next prototype. As adding layers is initially affordable, but becomes more expensive later, the following growth schedule is adopted:

$$
n_{t}= \begin{cases}2, & \text { if } t \in\{1,2\} \\ 1, & \text { otherwise }\end{cases}
$$

For USPS dataset we additionally consider a modified set of settings to start from smaller networks, which allows for more gradual navigation of the search space in the early stages. With the modified search settings a 2-layer uniform prototype is used as a starting point (instead of a 5-layer one). The sampling between iterations is also reduced: 100 networks are generated and top 10 are selected to induce the next prototype. This results in the smaller number of candidates being evaluated, speeding up the search by approximately a factor of ten.

During the search, the channel count of all convolutional layers is set to 32 . To avoid the mismatch of signal

dimensions, every convolutional layer has its output padded to match the input; therefore, only the pooling layers can perform downsampling. We use PReLu as an activation function and apply the corresponding initialization policy of He et al. [61]. To make the discovered architectures output the class predictions, we perform global average pooling after the last sampled layer, followed by a fully connected layer of 100 PReLu-activated neurons, dropout with rate 0.5 , and a softmax layer. Batch normalization is not used within the candidate networks during the search; however, the final discovered architecture has batch normalization applied after every convolutional layer to maximize evaluation performance.

Training numerous deep networks from scratch incurs the majority of computational expenses of the search procedure. We therefore use a different, less intensive training regime, denoted as brief training, to produce metrics for candidate ranking during the search. Full training is reserved only for the evaluation of the final solution discovered by the architecture search. Both settings use stochastic gradient descent (SGD) with momentum of 0.9 to minimize the cross entropy loss. Brief training runs for 20 epochs, using the learning rate of $10^{-2}$ for the first 10 epochs and $10^{-3}$ thereafter. Full training runs for 200 epochs and uses the following schedule of learning rates: 0.01 for epochs 160, 0.02 for epochs $61-120,0.004$ for epochs $121-160$, and 0.001 for epochs 161-200. Full training also uses $L_{2}$-norm weight regularization with the coefficient of $10^{-4}$ (excluding PReLU weights, as recommended in [61]) and imposes the maximum $L_{2}$-norm constraint of 0.5 on weights. The batch size is set to 64 for USPS and 128 for CIFAR-100.

The settings for convergence control and non-linear connectivity variants are given as follows. For the variants that use probability capping we set $p_{\max }$ to 0.9 . The $L_{2}$-norm threshold for both full inversion and partial inversion is set to 0.65 , as that corresponds to the middle of the interval of the possible values. For the evaluation of shortcut-generating rules we consider $D=2$ and $D=3$, as $D=1$ has limited impact on the gradient flow and $D>3$ results in too few shortcuts created, given the network depth limitation. We do not consider the convergence control and the shortcut patterns jointly: since both of them act like regularizers on the search procedure, their significance is easier to analyze separately. The total number of iterations is $t_{\max }=9$ for the baseline and probability capped variants, which, under the adopted schedule, corresponds to the maximum layer count of 16 . However, for the inversion experiments the search is run further to allow for observing the results after multiple instances of inversion triggering. The achievable layer count is 22 for these variants.

All of our experiments are implemented in PyTorch and performed on a workstation with 4 GeForce 1080Ti GPU units. The running times are included with the reported results. The source code is available on GitHub. ${ }^{1}$

[^0]
## B. ARCHITECTURE SEARCH PERFORMANCE ON USPS

To the best of our knowledge, the current state of the art results on USPS are achieved by Oyedotun et al. [62]. Their solution is based on a well-known ResNet architecture, modified by a number of techniques: maxout activation function, elastic net regularization, and feature standardization. Furthermore, the final classification is obtained via an SVM that is trained on the features from the final convolutional layer. We compare the reported results from [62] with the solutions discovered by the baseline ASED in its default and modified configurations. The comparison between ResNet and both ASED setups is given in Table 1. The discovered architectures are shown in Fig. 2, using the notation defined in Section III.

The networks discovered by ASED clearly produce competitive results to modified S-ResNet. While the default search configuration shows a clear tradeoff between classification accuracy and the number of parameters, the modified search configuration can produce very competitive architectures with just 16 channels. The discovered architecture is significantly smaller than S-ResNet in both depth and overall memory requirement, while achieving a similar test error rate. ASED is, therefore, capable of discovering very efficient architectures in smaller problem domains. The discovered architecture notably differs from common patterns of handcrafted designs, lacking pronounced block-like repeating patterns and taking advantage of convolutions with larger kernels. This shows the potential of the neural architecture search to automatically discover novel and potentially superior network structures, which may lie outside the standard design spaces while not being discoverable by cell-based architecture search.

## C. ARCHITECTURE SEARCH PERFORMANCE ON CIFAR-100

We report the results of running the proposed ASED algorithm on CIFAR-100 with the default search configuration described above. In addition to the baseline given by Alg. 1, we also evaluate the proposed modifications of the search: probability capping (denoted ProbCap for clarity), the full inversion variant, the partial inversion variant, and the shortcut-generating rules of both types.

Fig. 3 shows the distributions of candidate models' performances on the validation set throughout the search iterations, under the brief training regime. The maximum and median accuracies (shown by the red dotted and green dashed lines respectively) show steady convergence patterns in most cases. The inversion variants are notable exceptions, as the inversion operations result in clearly noticeable, although temporary, accuracy drops. Probability capping stands out as an example of the premature convergence and stagnation, where the performance does not reach the levels of other ASED variants. It is also worth noting that the networks with semi-dense connection patterns appear to be more sensitive to the layer type changes, as throughout the search process a notable


[^0]:    ${ }^{1}$ https://github.com/anton-muravev/ased

![img-1.jpeg](img-1.jpeg)

FIGURE 2. Best discovered structures on USPS for a) default settings, b) modified settings. Shorthand notation from Section III. Pooling operations denoted by oval shapes.

TABLE 1. Comparison of the error rates (full training mode) on USPS dataset.

| Model | Test error (\%) | Parameter count | Model depth | Search time (GPU hours) |
| :--: | :--: | :--: | :--: | :--: |
| Maxout S-ResNet+ENR+SVM [62] | 2.34 | 169 K | 54 | N/A |
| Maxout S-ResNet+ENR+FS+SVM [62] | 2.19 | 169 K | 54 | N/A |
| ASED default, 16 channels | 2.49 | 77 K | 9 | 64.0 |
| ASED default, 32 channels | 2.25 | 305 K | 9 | 64.0 |
| ASED modified, 8 channels | 2.64 | 12 K | 10 | 6.4 |
| ASED modified, 16 channels | 2.25 | 46 K | 10 | 6.4 |
| ASED modified, 32 channels | 2.25 | 182 K | 10 | 6.4 |

portion of the sampled structures continues to exhibit poor performance.

While limiting the number of training epochs does allow for much faster evaluations during the search process, the performance estimates obtained this way (shown on Fig. 3) are pessimistic and not fully representative of the underlying models' capabilities. Therefore, we also conduct an extended evaluation of the best discovered networks by adding batch normalization modules and using the full training regime (see Section IV-A). In this comparison, we also include the best architecture from the initialization sample (generated from the 5-layer uniform prototype), as well as the best 16-layer architecture from 1000 networks generated from a uniformly random prototype. Every configuration is trained from scratch with the convolution channel counts of 32, 64,128 , and 256 . The results are reported in Table 2. The indicated layer count excludes any identity layers. The best-performing network structures from each algorithm variant are shown in Fig. 4.

Modified variants of ASED prove superior to the baseline version, with the notable exception of ProbCap, which appears to stagnate at an earlier point of the search, leaving it with the smallest depth. The shortcut-based variants overall outperform other solutions for lower channel counts, but the inversion-based variants are the most efficient for larger models (in terms of both accuracy and the number of parameters). This result shows that perturbing the prototype, even very aggressively, can lead to discovering better solutions with the same or smaller depth. The shortcuts are essential in reaching deeper models without premature convergence; such models are immediately more powerful and offer short-term performance advantages, but their larger sizes imply higher memory and computation requirements. Both types of the
proposed techniques also prove essential in increasing the advantage over the random search, which is otherwise quite competitive with the baseline variant of ASED.

The experiments also demonstrate the impact of brief training regime on the search process. Naturally, training a given model for less epochs usually results in lower performance; however, the algorithm design assumes that the relative ranking of candidates is still accurate, i.e., that the better structures outperform their inferiors. However, in practice this condition does not strictly hold, as can be seen by visually comparing the data from Fig. 3 and Table 2. The candidate ranking is partially distorted: while overall the prototype performance increases with time, validation results can be confusing for the search. For example, the baseline ASED variant has the highest validation performance with brief training regime, but ultimately proves to be the weakest. This leaves the possibility that some promising (or even superior) models can be mistakenly discarded during the search, which would be detrimental to the final result. While this problem could be reduced by extending the brief training regime, that would also have a dramatic effect on overall computational costs; alternative solutions should be investigated. One option is low-fidelity training with a different constraint: instead of limiting the epoch number one can limit the training data size or the number of channels. More sophisticated solutions include dynamic allocation of the training budget to prioritize more promising architectures (see e.g. [69]) or predicting the candidate performance from its properties without training it (similar to [28]). All of these options also have an additional benefit in speeding up the evaluation process, which is the slowest step in the ASED algorithm.

Another important observation can be made about the practical model depth achieved by different algorithm variants.

![img-2.jpeg](img-2.jpeg)

FIGURE 3. Distributions of validation accuracies (brief training mode) on CIFAR-100 across search iterations. The line plots show the maximum and median accuracies of the sampled networks.

If the evolution of the prototype reaches a point where the newly added layer assigns the largest probability to an identity operation, adding further layers is unlikely to introduce structural novelty, as they would also tend to converge to
identities. The search essentially stops at that point, as the depth does not increase further. All of the algorithm variants that do not use shortcuts demonstrate this behaviour, although the inversion variants are capable of avoiding it to an extent

TABLE 2. Comparison of best discovered architectures by their test accuracy (full training mode) on CIFAR-100 dataset.

| Model <br> Source | Layer <br> Count | 32 channels |  | 64 channels |  | 128 channels |  | 256 channels |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  |  | Acc. | Par. | Acc. | Par. | Acc. | Par. | Acc. | Par. |
| Initialization | 5 | 0.4898 | 131 K | 0.5835 | 513 K | 0.6419 | 2.0 M | 0.6846 | 8.1 M |
| Random uniform | 15 | 0.5794 | 223 K | 0.6621 | 879 K | 0.7200 | 3.5 M | 0.7499 | 13.9 M |
| ASED | 10 | 0.5659 | 224 K | 0.6582 | 886 K | 0.7102 | 3.5 M | 0.7483 | 14.1 M |
| ASED + Prob Cap | 8 | 0.5483 | 190 K | 0.6330 | 751 K | 0.6963 | 3.0 M | 0.7442 | 11.9 M |
| ASED + Full Inversion | 12 | 0.5827 | 268 K | 0.6728 | 1.1 M | 0.7297 | 4.2 M | 0.7729 | 16.9 M |
| ASED + Partial Inversion | 11 | 0.5748 | 236 K | 0.6641 | 932 K | 0.7249 | 3.7 M | 0.7652 | 14.8 M |
| ASED + Residual-2 | 16 | 0.6396 | 419 K | 0.6872 | 1.7 M | 0.7282 | 6.6 M | 0.7485 | 26.5 M |
| ASED + Residual-3 | 13 | 0.6194 | 367 K | 0.6689 | 1.5 M | 0.7068 | 5.8 M | 0.7315 | 23.2 M |
| ASED + Dense-2 | 15 | 0.6461 | 392 K | 0.6984 | 1.6 M | 0.7398 | 6.2 M | 0.7610 | 24.8 M |
| ASED + Dense-3 | 11 | 0.6092 | 251 K | 0.6678 | 1.0 M | 0.7084 | 3.9 M | 0.7348 | 15.8 M |

![img-3.jpeg](img-3.jpeg)

FIGURE 4. Best discovered structures for a) base procedure, b) probability capping, c) full inversion, d) partial inversion, e) residual-2 pattern, f) residual-3 pattern, g) semi-dense-2 pattern, h) semi-dense-3 pattern. Shorthand notation from Section III. Pooling operations denoted by oval shapes. Dotted lines indicate skip connections with pooling.
by resetting the probability of identity to a low value for previously discovered layers. This effect can once again be traced back to the brief training regime and the performance estimates it produces. Fig. 3 shows how narrow the range of the estimated accuracy is. The short training schedule biases the search towards architectures that show the fastest
improvement in early epochs. As a result, ASED exhibits low complexity bias. This can be advantageous in specific cases, but it makes the architectures of higher complexity, which are naturally slower to train, much less competitive and less likely to be retained. Shortcut-based ASED circumvents this problem by making the deeper models easier to train and,

TABLE 3. Test performance comparison of established and automatically discovered architectures on CIFAR-100 dataset.

| Method | Accuracy (\%) | Parameter count | Model depth | Search cost (GPU days) |
| :--: | :--: | :--: | :--: | :--: |
| FractalNet [63] | 76.7 | 38.6 M | 21 | N/A |
| Shake-Shake [64] | 84.2 | 26.2 M | 26 | N/A |
| Wide ResNet 28-10 [65] | 80.4 | 36.5 M | 28 | N/A |
| DenseNet-BC [3] | 82.8 | 25.6 M | 190 | N/A |
| Genetic CNN [20] | 70.9 | - | 17 | 17 |
| MetaQNN [66] | 72.9 | 11.2 M | 9 | 100 |
| Large Scale Evolution [21] | 77.0 | 40.4 M | $\geq 13$ | $\geq 2600$ |
| SMASH [48] | 79.4 | 16 M | 211 | 1.5 |
| Hill Climbing [49] | 76.6 | 22.3 M | 30 | 1 |
| NSGA-NET-128 [67] | 79.3 | 3.3 M | 21 | 8 |
| NSGA-NET-256 [67] | 80.2 | 11.6 M | 21 | 8 |
| PNAS [28] | 80.5 | 3.2 M | 15 | 225 |
| ENAS [27] | 80.6 | 4.6 M | 25 | 0.45 |
| DARTS [30] | 82.5 | 3.3 M | - | 4 |
| NAONet [68] | 84.3 | 10.8 M | 30 | 200 |
| ASED (best) | 77.3 | 16.9 M | 12 | 20 |

therefore, more competitive. The search can thus explore more complex solutions without the induced "limit" to the layer count. However, shortcut-based ASED is still to some extent affected by the bias towards simpler architectures: Table 2 shows that the superiority of the models with shortcuts is limited to the smaller channel counts, which are closer to the low-fidelity evaluation setting. While the low complexity bias limits the exploration of high-dimensional structures, it can be turned into an advantage in the appropriate problem setting, e.g., if the goal is to find specifically the simplest, fastest to train models.

The best discovered architectures (see Fig. 4) can be compared with common handcrafted designs, as well as with networks that are representable by cell-based search spaces. The solutions found by ASED have pooling layers distributed roughly regularly along the network depth. This draws parallels to other common designs, where pooling (more generally, downsampling) typically separates different network submodules/blocks. However, it is important to note that the architectures discovered by ASED overall do not contain repeating sequences of operations and, therefore, cannot be represented as sequences of identical cells. These final solutions lie outside of the search space of most other architecture search methods, which demonstrates the value of generality of the proposed representation. Another notable difference between the proposed and existing solutions is the tendency for the convolution kernel size to increase along the network depth. Handcrafted designs most commonly feature larger convolutions in the earlier stages, or simply use the same kernel size for all the layers. In the ASED-produced solutions, on the other hand, $7 \times 7$ convolutions and dilated $5 \times 5$ convolutions (with an effective receptive field of $9 \times 9$ ) are dominant in the later layers, which is most likely the result of the low complexity bias.

Table 3 presents the comparison between the ASED algorithm (the full inversion variant, due to the highest overall performance) and a variety of existing solutions with reported results on CIFAR-100. First four rows of the table are occupied by the handcrafted architectures, while the rest are the results of architecture search. The classification accuracy and other metrics are reproduced from the corresponding papers (an empty cell indicates that the value was not originally reported). PNAS, ENAS, DARTS, and NAONet are some of the most popular and high-performing methods, but their original papers did not report results on CIFAR-100, therefore we replicate the evaluations done by other authors: we borrow DARTS results from [70], while the rest are from [68]. PNAS, ENAS, DARTS, and NAONet all utilize cell-based search spaces, while the rest of the algorithms do not. It is important to note that the results from the table were acquired under a variety of non-matching environments and settings (note that this refers not to the search methods themselves, but only to the training of the final discovered architectures). These settings may include different learning algorithms, data preprocessing, regularization, early stopping, architecture transfer (commonly from CIFAR-10 dataset), and others. Our own reproduction of the results of other algorithms under a unified setting is not possible due to the time required, as well as the lack of public original codes. We choose the follow the basic standard procedure for CIFAR-100 to establish the clear baseline, but this incurs an inherent disadvantage in the numbers against more sophisticated training techniques used by some of the competing methods. It is therefore expected that the performance of ASED can be further improved just by extra tuning of the training setup for the output network, which we leave for the future work Increasing the maximal search depth is also promising and can be achieved, for instance, by combining the shortcuts with the inversion technique.

While this would lead to further increase in computation, it is worth emphasizing that ASED scales almost linearly in the parallel setting (due to the candidate models being sampled and processed independently from each other).

While ASED does not reach the state of the art in accuracy, it achieves a competitive performance overall, especially compared with the non-cell-based methods. This is noteworthy, given that the search space of ASED is relatively shallow, constantly growing in dimensionality and lacking the inherent repeating structure that greatly benefits the cell-based competitors. As previously mentioned, ASED also does not take advantage of the weight sharing or specifically tuned training procedure for its output. These facts, combined with the simple, parallelizable and extensible nature of ASED, make it a promising research direction. Given the multitude of ways to expand on the baseline solution, whether by modifying the representation or the search procedure, the proposed method is capable of discovering novel irregular architectures. The main limitation of ASED currently lies in scalability: further increasing the depth limit would result in longer candidate training, greatly increasing the overall runtime. Tackling this problem should be the priority in any future work.

## V. CONCLUSION

The automated neural architecture design is growing in importance as the application-driven demand outpaces the available expertise and resources. While there are many architecture search algorithms being proposed, the compromises they have to make for the sake of performance are often limiting the range of models that can be discovered. In this article, we proposed a novel probabilistic representation of the deep network structure and defined an architecture search algorithm, named ASED, that does not restrict the networks to repeating sequences of blocks and iteratively increases the search depth. This allows it to cover the regions of the design space which most existing approaches cannot reach, and the experiments indeed demonstrate that ASED can discover competitive non-regular architectures. However, in its current formulation ASED has difficulties when reaching larger depths, which limits its ability to construct models on the same scale as other methods. Still, the design of ASED is intuitive and extensible, which leaves many promising options for the future work and improvement. Specifically, the representation can be trivially extended by adding new layer operations, hyperparameter encodings and/or structural patterns. Computationally ASED benefits from the ease of parallel implementation, due to the independent processing of the candidate networks. The underlying search mechanism can incorporate many of the developments in the area of EDAs. Low complexity bias of ASED warrants investigation as a potentially beneficial effect, e.g., when optimizing under heavy computational constraints for execution. Finally, ASED also offers opportunities for studying the architecture search dynamics, as its internal state (prototype) is easily interpretable at any point of the search.

## ACKNOWLEDGMENT

This publication reflects the authors' views only. The European Commission is not responsible for any use that may be made of the information it contains.

## REFERENCES

[1] Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," Nature, vol. 521, no. 7553, pp. 436-444, May 2015.
[2] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 770-778.
[3] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, "Densely connected convolutional networks," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 2261-2269.
[4] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, "DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs," IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 4, pp. 834-848, Apr. 2018.
[5] S. Ren, K. He, R. Giesnick, and J. Sun, "Faster R-CNN: Towards real-time object detection with region proposal networks," in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 91-99.
[6] Z. C. Lipton, "The mythos of model interpretability," in Proc. ICML Workshop Human Interpretability Mach. Learn. (WHI), 2016, pp. 1-28.
[7] A. S. Charles, "Interpreting deep learning: The machine learning rorschach test?" 2018, arXiv:1806.00148. [Online]. Available: http://arxiv.org/abs/1806.00148
[8] G. Montavon, W. Samek, and K.-R. Müller, "Methods for interpreting and understanding deep neural networks," Digit. Signal Process., vol. 73, pp. 1-15, Feb. 2018.
[9] A. Nguyen, J. Yosinski, and J. Clune, "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 427-436.
[10] K. He, X. Zhang, S. Ren, and J. Sun, "Identity mappings in deep residual networks," in Proc. Eur. Conf. Comput. Vis. (ECCV), 2016, pp. 630-645.
[11] K. Weiss, T. M. Khoshgoftaar, and D. Wang, "A survey of transfer learning," J. Big Data, vol. 3, no. 1, p. 9, 2016.
[12] A. Muravev, J. Raitoharju, and M. Gabbouj, "On the layer selection in small-scale deep networks," in Proc. 7th Eur. Workshop Vis. Inf. Process. (EUVIP), Nov. 2018.
[13] T. Tommasi, N. Patricia, B. Caputo, and T. Tuytelaars, "A deeper look at dataset bias," in Domain Adaptation in Computer Vision Applications, G. Csurka, Ed. Cham, Switzerland: Springer, 2017, pp. 37-55, doi: 10.1007/978-3-319-58347-1_2
[14] D. Whitley, T. Starkweather, and C. Bogart, "Genetic algorithms and neural networks: Optimizing connections and connectivity," Parallel Comput., vol. 14, no. 3, pp. 347-361, Aug. 1990.
[15] X. Yao, "Evolving artificial neural networks," Proc. IEEE, vol. 87, no. 9, pp. 1423-1447, Sep. 1999.
[16] K. O. Stanley and R. Miikkulainen, "Evolving neural networks through augmenting topologies," Evol. Comput., vol. 10, no. 2, pp. 99-127, 2002.
[17] K. O. Stanley, D. B. D'Ambrosio, and J. Gauci, "A hypercube-based encoding for evolving large-scale neural networks," Artif. Life, vol. 15, no. 2, pp. 185-212, Apr. 2009.
[18] S. Kiranyaz, T. Ince, A. Yildirim, and M. Gabbouj, "Evolutionary artificial neural networks by multi-dimensional particle swarm optimization," Neural Netw., vol. 22, no. 10, pp. 1448-1462, Dec. 2009.
[19] D. Floreano, P. Dürr, and C. Mattiussi, "Neuroevolution: From architectures to learning," Evol. Intell., vol. 1, no. 1, pp. 47-62, Mar. 2008.
[20] L. Xie and A. Yuille, "Genetic CNN," in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 1388-1397.
[21] E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. Le, and A. Kurakin, "Large-scale evolution of image classifiers," in Proc. Int. Conf. Mach. Learn. (ICML), 2017, pp. 2902-2911.
[22] H. Zhang, S. Kiranyaz, and M. Gabbouj, "Finding better topologies for deep convolutional neural networks by evolution," 2018, arXiv:1809.03242. [Online]. Available: http://arxiv.org/abs/1809.03242
[23] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, "Regularized evolution for image classifier architecture search," Proc. AAAI Conf. Artif. Intell., vol. 33, 2019, pp. 4780-4789.

[24] Y. Sun, B. Xue, M. Zhang, and G. G. Yen, "Evolving deep convolutional neural networks for image classification," IEEE Trans. Evol. Comput., vol. 24, no. 2, pp. 394-407, Apr. 2020.
[25] B. Zoph and Q. V. Le, "Neural architecture search with reinforcement learning," in Int. Conf. Learn. Represent. (ICLR), 2017, pp. 1-16.
[26] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, "Learning transferable architectures for scalable image recognition," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 8697-8710.
[27] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, "Efficient neural architecture search via parameter sharing," in Proc. 35th Int. Conf. Mach. Learn. (PMLR), 2018, pp. 4095-4104.
[28] C. Liu, B. Zoph, M. Neumann, and J. Shlens, "Progressive neural architecture search," in Proc. Eur. Conf. Comput. Vis. (ECCV). Cham, Switzerland: Springer, 2018, pp. 19-35.
[29] T. Elsken, J. H. Metzen, and F. Hutter, "Efficient multi-objective neural architecture search via lamarckian evolution," in Proc. Int. Conf. Learn. Represent. (ICLR), 2019, pp. 1-23.
[30] H. Liu, K. Simonyan, and Y. Yang, "DARTS: Differentiable architecture search," in Int. Conf. Learn. Represent. (ICLR), 2019, pp. 1-13.
[31] K. Yu, C. Sciuto, M. Jaggi, C. Musat, and M. Salzmann, "Evaluating the search phase of neural architecture search," 2019, arXiv:1902.08142. [Online]. Available: http://arxiv.org/abs/1902.08142
[32] S. Baluja, Population-Based Incremental Learning: A Method for Integrating Genetic Search Based Function Optimization and Competitive Learning. Pittsburgh, PA, USA: Carnegie Mellon Univ., 1994.
[33] H. Mühlenbein, "The equation for response to selection and its use for prediction," Evol. Comput., vol. 5, no. 3, pp. 303-346, Sep. 1997, doi: 10.1162/eeco.1997.5.3.303.
[34] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Learning representations by back-propagating errors," in Neurocomputing: Foundations of Research, J. A. Anderson and E. Rosenfeld, Eds. Cambridge, MA, USA: MIT Press, 1988, pp. 696-699. [Online]. Available: http://dl.acm.org/citation.cfm?id=65669.104451
[35] T. Back, U. Hammel, and H.-P. Schwefel, "Evolutionary computation: Comments on the history and current state," IEEE Trans. Evol. Comput., vol. 1, no. 1, pp. 3-17, Apr. 1997.
[36] S. Luke, Essentials Metaheuristics, 2nd ed. Lulu, 2013. [Online]. Available: http://cs.gmu.edu/ sean/book/metaheuristics/
[37] F. Gomez and R. Miikkulainen, "Incremental evolution of complex general behavior," Adapt. Behav., vol. 5, nos. 3-4, pp. 317-342, Jan. 1997.
[38] X. Yao and Y. Liu, "A new evolutionary system for evolving artificial neural networks," IEEE Trans. Neural Netw., vol. 8, no. 3, pp. 694-713, May 1997.
[39] F. J. Gomez and R. Miikkulainen, "Solving non-Markovian control tasks with neuroevolution," in Proc. 16th Int. Joint Conf. Artif. Intell., vol. 2, 1999, pp. 1356-1361.
[40] K. O. Stanley, "Compositional pattern producing networks: A novel abstraction of development," Genetic Program. Evolvable Mach., vol. 8, no. 2, pp. 131-162, Jun. 2007.
[41] S. Risi and K. O. Stanley, "An enhanced hypercube-based encoding for evolving the placement, density, and connectivity of neurons," Artif. Life, vol. 18, no. 4, pp. 331-363, Oct. 2012.
[42] D. B. D’Ambrosio, J. Gauci, and K. O. Stanley, "HyperNEAT: The first five years," in Growing Adaptive Machines. Berlin, Germany: Springer, 2014, pp. 159-185.
[43] R. Miikkulainen, J. Liang, E. Meyerson, A. Rawal, D. Fink, O. Francon, B. Raju, H. Shahrzad, A. Navruzyan, N. Duffy, and B. Hodjat, "Evolving deep neural networks," in Artificial Intelligence in the Age of Neural Networks and Brain Computing. New York, NY, USA: Academic, 2019, pp. 293-312.
[44] Y. Sun, B. Xue, M. Zhang, and G. G. Yen, "Completely automated CNN architecture design based on blocks," IEEE Trans. Neural Netw. Learn. Syst., vol. 31, no. 4, pp. 1242-1254, Apr. 2020. [Online]. Available: https://ieeexplore.ieee.org/document/8742788/
[45] S. Kiranyaz, T. Ince, A. Iosifidis, and M. Gabbouj, "Progressive operational perceptions," Neurocomputing, vol. 224, pp. 142-154, Feb. 2017.
[46] S. Kiranyaz, T. Ince, A. Iosifidis, and M. Gabbouj, "Operational neural networks," 2019, arXiv:1902.11106. [Online]. Available: http://arxiv.org/abs/1902.11106
[47] D. T. Tran, S. Kiranyaz, M. Gabbouj, and A. Iosifidis, "Heterogeneous multilayer generalized operational perception," IEEE Trans. Neural Netw. Learn. Syst., vol. 31, no. 3, pp. 710-724, Mar. 2020. [Online]. Available: http://arxiv.org/abs/1804.05093
[48] A. Brock, T. Lim, J. M. Ritchie, and N. Weston, "Smash: One-shot model architecture search through hypernetworks," in Proc. Workshop MetaLearn. (MetaLearn), 2017, pp. 1-21.
[49] T. Elsken, J.-H. Metzen, and F. Hutter, "Simple and efficient architecture search for convolutional neural networks," in 6th Int. Conf. Learn. Represent. (ICLR), 2018, pp. 1-15.
[50] A.-C. Cheng, C. Hubert Lin, D.-C. Juan, W. Wei, and M. Sun, "InstaNAS: Instance-aware neural architecture search," 2018, arXiv:1811.10201. [Online]. Available: http://arxiv.org/abs/1811.10201
[51] K. Kandasamy, W. Neiswanger, J. Schneider, B. Poczos, and E. P. Xing, "Neural architecture search with Bayesian optimisation and optimal transport," in Proc. Adv. Neural Inf. Process. Syst., 2018, pp. 2016-2025. [Online]. Available: http://papers.nips.cc/paper/7472-neural-architecture-search-with-bayesian-optimisation-and-optimal-transport.pdf
[52] F. Paolo Casale, J. Gordon, and N. Fusi, "Probabilistic neural architecture search," 2019, arXiv:1902.05116. [Online]. Available: http://arxiv.org/abs/1902.05116
[53] M. Pelikan, D. E. Goldberg, and F. G. Lobo, "A survey of optimization by building and using probabilistic models," Comput. Optim. Appl., vol. 21, no. 1, pp. 5-20, 2002, doi: 10.1023/A:1013500812258.
[54] M. Hauschild and M. Pelikan, "An introduction and survey of estimation of distribution algorithms," Swarm Evol. Comput., vol. 1, no. 3, pp. 111-128, Sep. 2011. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2210650211000435
[55] J. Gorodkin, "Comparing two K-category assignments by a K-category correlation coefficient," Comput. Biol. Chem., vol. 28, nos. 5-6, pp. 367-374, Dec. 2004.
[56] Q. Zhang, "On stability of fixed points of limit models of univariate marginal distribution algorithm and factorized distribution algorithm," IEEE Trans. Evol. Comput., vol. 8, no. 1, pp. 80-93, Feb. 2004.
[57] T. Friedrich, T. Kötzing, and M. S. Krejca, "EDAs cannot be balanced and stable," in Proc. Genetic Evol. Comput. Conf., Jul. 2016, pp. 1139-1146, doi: $10.1145 / 2908812.2908895$.
[58] F. Glover and M. Laguna, "Tabu search," in Handbook of Combinatorial Optimization, D.-Z. Du and P. M. Pardalos, Eds. Boston, MA, USA: Springer, 1998, pp. 2093-2229, doi: 10.1007/978-1-4613-0303-9_33.
[59] J. J. Hull, "A database for handwritten text recognition research," IEEE Trans. Pattern Anal. Mach. Intell., vol. 16, no. 5, pp. 550-554, May 1994.
[60] A. Krizhevsky and G. Hinton, "Learning multiple layers of features from tiny images," Dept. Comput. Sci., Univ. Toronto, Toronto, ON, Canada, Tech. Rep. TR-2009, 2009.
[61] K. He, X. Zhang, S. Ren, and J. Sun, "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification," in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 1026-1034.
[62] O. K. Oyedotun, A. E. R. Shabayek, D. Aouada, and B. Ottersten, "Improving the capacity of very deep networks with maxout units," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Apr. 2018, pp. 2971-2975.
[63] G. Larsson, M. Maire, and G. Shakhnarovich, "FractalNet: Ultra-deep neural networks without residuals," in Proc. Int. Conf. Learn. Represent. (ICLR), 2017, pp. 1-11.
[64] X. Gastaldi, "Shake-shake regularization of 3-branch residual networks," in Int. Conf. Learn. Represent. (ICLR) Workshop, 2017, pp. 1-5.
[65] S. Zagoruyko and N. Komodakis, "Wide residual networks," in Proc. Brit. Mach. Vis. Conf., 2016, pp. 1-12.
[66] B. Baker, O. Gupta, N. Naik, and R. Raskar, "Designing neural network architectures using reinforcement learning," in Proc. 5th Int. Conf. Learn. Represent. (ICLR), 2017, pp. 1-18.
[67] Z. Lu, I. Whalen, V. Boddeti, Y. Dhebar, K. Deb, E. Goodman, and W. Banzhaf, "NSGA-NET: A multi-objective genetic algorithm for neural architecture search," in Proc. Genetic Evol. Comput. Conf. (GECCO), 2019, pp. 419-427.
[68] R. Luo, F. Tian, T. Qin, E. Chen, and T.-Y. Liu, "Neural architecture optimization," in Proc. Adv. Neural Inf. Process. Syst., 2018, pp. 7816-7827. [Online]. Available: http://papers.nips.cc/paper/8007-neural-architectureoptimization
[69] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar, "Hyperband: A novel bandit-based approach to hyperparameter optimization," J. Mach. Learn. Res., vol. 18, no. 185, pp. 1-52, 2018. [Online]. Available: http://jmlr.org/papers/v18/16-558.html
[70] H. Yu and H. Peng, "Cyclic differentiable architecture search," 2020, arXiv:2006.10724. [Online]. Available: http://arxiv.org/abs/2006. 10724

![img-4.jpeg](img-4.jpeg)

ANTON MURAVEV (Member, IEEE) received the B.Sc. and M.Sc. degrees in computer science from Tomsk Polytechnic University, Tomsk, Russia, in 2013 and 2015, respectively, and the M.Sc. degree in information technology from the Tampere University of Technology, in 2016. He is currently pursuing the Ph.D. degree with Tampere University, Tampere, Finland. His research interests include deep learning, pattern recognition, and evolutionary computation.
![img-5.jpeg](img-5.jpeg)

JENNI RAITOHARJU (Member, IEEE) received the Ph.D. degree from the Tampere University of Technology, Finland, in 2017. She currently works as the Senior Research Scientist of The Finnish Environment Institute, Jyväskylä. She has coauthored 23 international journal articles and 32 papers in international conferences. Her research interests include machine learning and pattern recognition methods along with applications in biomonitoring and autonomous systems. She leads two research projects funded by the Academy of Finland focusing on automatic taxa identification. She is also the Chair of Young Academy Finland, for the period 2019-2021.
![img-6.jpeg](img-6.jpeg)

MONCEF GABBOUI (Fellow, IEEE) received the M.S. and Ph.D. degrees in electrical engineering from Purdue University, in 1986 and 1989, respectively. He is currently a Professor of signal processing with the Department of Computing Sciences, Tampere University, Tampere, Finland. He was the Academy of Finland Professor from 2011 to 2015. His research interests include big data analytics, multimedia content-based analysis, indexing and retrieval, artificial intelligence, machine learning, pattern recognition, nonlinear signal and image processing and analysis, voice conversion, and video processing and coding. He is a member of the Academia Europaea and the Finnish Academy of Science and Letters. He has served as an Associate Editor and a Guest Editor for many IEEE and international journals.