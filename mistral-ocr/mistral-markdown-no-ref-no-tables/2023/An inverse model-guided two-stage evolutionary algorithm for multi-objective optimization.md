# An inverse model-guided two-stage evolutionary algorithm for multi-objective optimization 

Jiangtao Shen, Huachao Dong *, Peng Wang, Jinglu Li, Wenxin Wang

School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an 710072, China


#### Abstract

The estimation of distribution algorithm (EDA) is a kind of distinctive evolutionary algorithm that generates candidate solutions by directly sampling on distribution models. In this paper, we propose a distribution modelguided two-stage evolutionary algorithm for better solving multi-objective optimization problems (MOPs). To enhance modeling efficiency, the clustering method is employed to divide the population into multiple subpopulations. Then multivariate inverse models mapping from the objective space to the decision space are constructed by using a single decision variable and two objectives from each subpopulation. Then offspring are generated by randomly sampling the global and local objective space using the constructed inverse models. Moreover, a two-stage framework is proposed for better quality, i.e., convergence and diversity, of the solution set. In the first stage, exploration is mainly considered, during which the population converge rapidly. And exploitation is emphasized in the second stage, where the solution set is tuned by a replacement strategy. Experimental studies with several peer competitors on a set of widely-used benchmark MOPs as well as an engineering design MOP verify the competitiveness of the proposed method.


## 1. Introduction

Multi-objective optimization problems (MOPs) are common in engineering applications such as car design (Jain, \& Deb, 2013), aircraft design, water resource management (Kay, Tai, \& Seow, 2001), and autonomous underwater vehicles design (Dong, Wang, Fu, \& Song, 2021), etc. Unlike a single solution to the single objective optimization problem (SOP), the optimum of an MOP is a set of solutions due to the conflicting nature of objectives, which are called the Pareto optimal set (PS) of the decision space corresponding to the Pareto optimal front (PF) of the objective space. Specifically, an MOP can be formulated as

$$
\min \quad \boldsymbol{f}(\boldsymbol{x})=\left(f_{1}(\boldsymbol{x}), f_{2}(\boldsymbol{x}), \ldots, f_{M}(\boldsymbol{x})\right)
$$

s.t. $x \in \Omega$
where $M$ is the number of conflicting objectives, $\boldsymbol{x}=\left(x_{1}, x_{2}, \ldots, x_{d}\right)$ denotes the decision vector with $d$ decision variables, and $\Omega \subseteq \mathbb{R}^{d}$ represents the decision space. When $M>3$, it is termed a many-objective optimization problem (MaOP). $\boldsymbol{p}$ is said to dominate $\boldsymbol{q}$ (denoted as $\boldsymbol{p}<$ $\boldsymbol{q})$ if and only if $f_{i}(\boldsymbol{p}) \leq f_{i}(\boldsymbol{q})$ for every $i \in\{1,2, \ldots, M\}$ and $f_{i}(\boldsymbol{p})<f_{i}(\boldsymbol{q})$ for at least one index $j \in\{1,2, \ldots, M\}$. If neither $\boldsymbol{p}$ dominates $\boldsymbol{q}$ nor $\boldsymbol{q}$
dominates $\boldsymbol{p}$, the two solutions are said to be nondominated to each other.

For an MOP, it is desired that the attainable solution set can represent the PF in the objective space effectively. Evolutionary algorithms (EAs) are considered an efficient method for solving MOPs since they are population-based, which means a set of solutions can be obtained in a single execution. This property advances the vigorous development of multi-objective evolutionary algorithms (MOEAs). During the past two decades, numerous MOEAs have been proposed for dealing with MOPs, which are basically divided into three categories: dominance-based MOEAs (Deb, Pratap, Agarwal, \& Meyarivan, 2002), decompositionbased MOEAs (Zhang, \& Li, 2007), and indicator-based MOEAs (Zitzler, \& Künzli, 2004). The elitist nondominated sorting genetic algorithm II (NSGA-II) (Deb, Pratap, Agarwal, \& Meyarivan, 2002) is a representative MOEA based on dominance relation. It combines crowding distance to select candidate solutions to improve the convergence and diversity of the population. Others like the strength Pareto EA (SPEA and SPEA2) (Zitzler, \& Thiele, 1999; Zitzler, Laumanns, \& Thiele, 2001), the Pareto envelope-based selection algorithm (PSEA and PSEAII) (Corne, Knowles, \& Oates, 2000; Corne, Jerram, \& Knowles, 2001),

[^0]
[^0]:    * Corresponding author.

    E-mail addresses: shen_jiang_tao@163.com (J. Shen), hdong@nwpu.edu.cn (H. Dong), wangpeng305@nwpu.edu.cn (P. Wang), lijinglu@mail.nwpu.edu.cn (J. Li), wzwang@mail.nwpu.edu.cn (W. Wang).

which exhibit high effectiveness for solving 2-objective and 3-objective MOPs. The decomposition-based MOEAs decompose an MOP into a set of SOPs and optimize them simultaneously. They usually apply a set of evenly distributed reference vectors/points in the objective space for better diversity. Several representative MOEAs of this category are the MOEA based on decomposition (MOEA/D) (Zhang, \& Li, 2007), the nondominated sorting algorithm III (NSGA-III) (Deb, \& Jain, 2013), and the reference vector guided EA (RVEA) (Cheng, Jin, Olhofer, \& Sendhoff, 2016), etc. The indicator-based MOEAs use an indicator value to represent the quality of a multi-objective solution. Indicator-based EA (IBEA) (Zitzler, \& Klutel, 2004) performs well on convergence. It employs a predefined fitness rule to measure the contribution of each individual to the population. Hypervolume (HV) is used in MOEAs (Bader, \& Zitzler, 2011) extensively, which has good consistency with the convergence and diversity of the solution set. Nevertheless, as the number of objectives increases, the computational cost for the calculation of HV becomes prohibitively expensive.

Reproduction is a crucial part of MOEA, which determines if promising candidate solutions can be generated for evolution. Simulated binary crossover, as a kind of crossover operator (Deb, Sindhya, \& Okabe, 2007; Deb, \& Goyal, 1996; Davis, 1985), is commonly used by MOEAs for reproduction, i.e., generating offspring. However, this operator shows strong randomness which might generate solutions that deviate from the PS, thus misleading the search direction and degenerating the population (Cheng, Jin, Narukawa, \& Sendhoff, 2015). The estimation of distribution algorithm (EDA) is a new type of evolutionary algorithm, which is able to learn problem structures and generate offspring by directly sampling in the objective space (Cheng, Jin, \& Narukawa, 2015).

From the Karush-Kuhn-Tucker condition, it can be deduced that the PS of a continuous MOP is a piecewise continuous manifold with $M-1$ dimension (Dellnitz, Schütze, \& Hestermeyer, 2005), $M$ denotes the objective number. For a three-objective continuous MOP, its PS is a piecewise continuous surface in the decision space, and the PS of a biobjective continuous MOP is a piecewise continuous curve. Actually, the PS of the widely used ZDT (Deb, 1999) test problems are line segments in the decision space. Based on the regularity property, Zhang et al. developed a regularity model-based multi-objective EDA (RMMEDA) (Zhang, Zhou, \& Jin, 2008). Offspring are generated by sampling from the probability distribution of a piecewise continuous manifold, and the distribution model is constructed by utilizing the local principal component analysis (PCA) algorithm. Meanwhile, the environmental selection based on NSGA-II is applied to screen out the parent population for the next generation. Experimental studies on a set of benchmark problems demonstrate that RM-MEDA is superior to its peer competitors, namely PCX-NSGA-II (Deb, Sinha, \& Kukkonen, 2006), GDE3 (Kukkonen, \& Lampinen, 2006) and MIDEA (Bosman, \& Thierens, 2005). Based on the Gaussian process, Cheng et al. proposed an inversemodel-assisted evolutionary algorithm (IM-MOEA) (Cheng, Jin, Narukawa, \& Sendhoff, 2015). In IM-MOEA, a set of predefined reference vectors partition the objective space into several subregions, then univariate inverse models based on the Gaussian process that map a single objective to a single decision variable are trained. To reduce the number of distribution models, instead of using all the decision variables, much fewer decision variables than dimensions of MOP are randomly selected for modeling. After offspring are produced by sampling with the inverse models, new subpopulations of each subregion are selected by the environmental selection based on NSGA-II. IM-MOEA performs best on a set of benchmark problems when compared with RM-MEDA, IRM-MEDA (Wang, Xiang, \& Cai, 2012), MOEA/D-DE (Li, \& Zhang, 2008), and NSGA-II. Since the evenly distributed reference vectors may not be effective for MOPs with irregular PF, i.e., degenerated or disconnected PF, IM-MOEA with adaptive reference vectors (A-IM-MOEA) (Cheng, Jin, \& Narukawa, 2015) is proposed to address this issue, which adaptively adjusts the reference vectors according to the population distribution in the objective space. Zhang et al. proposed an enhanced version
of IM-MOEA, termed AN-IMMOEA (Zhang, Liu, Gao, Xu, \& Zhu, 2020), which employs a nonrandom grouping strategy to facilitate modeling. In addition, AN-IMMOEA uses an adaptive reference vector strategy to improve its performance on a wide range of MOPs with irregular and regular PFs. Experimental results on benchmark problems demonstrate that AN-IMMOEA behaves better than IM-MOEA and A-IM-MOEA.IMMOEA/D (Farias \& Araújo, 2021) is a recently proposed MOEA based on MOEA/D and inverse models, which uses the $k$-means clustering method to partition the population and decomposition-based strategy for selection. It shows better performance on large-scale MOPs and manyobjective optimization problems when compared with IM-MOEA.

Many existing MOEAs adopt the multistage framework during the optimization process. Qi et al. proposed a variant of MOEA/D with two stages, named MOEA/D-AWA (Qi et al., 2014). In the first stage, the weight vectors are fixed to guarantee convergence, then at the second stage, the weight vectors are adaptively adjusted with population distribution to improve diversity. In the multistage EA (MSEA) (Tian, He, Cheng, \& Zhang, 2019), the optimization process is divided into three stages according to the current population information, which are used for convergence, diversity, and fine-tuning of the population, respectively. Some many-objective EAs (MaOEAs) also adopt the multistage strategy, such as MaOEA based on objective space reduction and diversity improvement (MaOEA-R\&D) (He, \& Yen, 2015), MaOEA based on two stages (MaOEA-IT) (Sun, Xue, Zhang, \& Yen, 2018), two-stage evolutionary algorithm (TSEA) for many-objective optimization (Chen, Cheng, Pedrycz, \& Jin, 2019), and MaOEA based on multistage (MaOEAMS) (Shen, Wang, Dong, Li, \& Wang, 2022). These algorithms consider convergence and diversity in different stages to improve their performance on many-objective optimization problems with large objective space. Consequently, employing different selection strategies at different stages facilitates MOEAs to obtain better solution set in terms of convergence and diversity.

Although IM-MOEA and their variants perform well on some MOPs, they still have room for improvement. For example, they use a set of evenly distributed vectors to assist the optimization process, which leads to poor performance on MOPs with irregular PFs. Moreover, they apply univariate models considering a single decision variable and a single objective, which ignores the coupling relations of multiple objectives and may not be tailored for multi-objective optimization. To address the above issues and employ inverse models more efficiently for solving MOPs, an inverse model-guided two-stage evolutionary algorithm termed IMTSEA is proposed in this paper. The contributions of this paper are as follows.
(1) To improve search efficiency, different from the traditional reproduction strategy such as crossover and mutation, a novel reproduction strategy based on multivariate inverse models is proposed to generate offspring. To facilitate modeling, the population is partitioned into multiple subpopulations by the $k$-means clustering method. Bivariate inverse quadratic regression models are constructed according to the subpopulations, and offspring are generated by sampling the global and local regions of the inverse models.
(2) A two-stage framework is proposed for a better balance between exploration and exploitation. In the first stage, environmental selection based on batch updating is applied to highlight exploration, during which the population converge to the PF rapidly. In the second stage, a one-by-one selection strategy is applied to avoid the negative effect of batch updating and achieve better exploitation.
(3) The proposed method is compared with several state-of-the-art MOEAs on a set of widely-used benchmark MOPs. Experimental results demonstrate the competitiveness of IMTSEA, including the effectiveness of the reproduction strategy and two-stage strategy. Furthermore, IMTSEA is applied to the shape design of BWBUG, and the lift-drag ratio of BWBUG is improved apparently after optimization, which demonstrates the proposed method is able to obtain a set of satisfactory solutions on practical MOPs.

The remainder of this article is organized as follows. Section 2

introduces IM-MOEA. Section 3 details the proposed IMTSEA. Experimental studies on benchmark MOPs and practical engineering MOP are presented in Sections 4, and 5, respectively. Lastly, Section 6 gives conclusions and future works.

## 2. Im-MOEA

IM-MOEA, as one of the representative MOEAs based on inverse models, is closely related to our study, and will be introduced in this section.

Generally, EDAs approximate the distribution of the candidate solutions of the decision space, while IM-MOEA builds inverse models that map from the objective space to the decision space. Fig. 1 presents the schematic of the core idea of using inverse models, where $\boldsymbol{f}$ is the objective functions, and $\boldsymbol{X}^{p}$ and $\boldsymbol{Y}^{p}$ denote the decision and objective vectors of the parent population, respectively. To generate offspring, inverse models are constructed by mapping $\boldsymbol{Y}^{\boldsymbol{P}}$ to $\boldsymbol{X}^{\boldsymbol{P}}$, new objective vector $\boldsymbol{Y}^{\boldsymbol{P}}$ are evenly generated, then new decision variables $\boldsymbol{X}^{\boldsymbol{P}}$ in the decision space that corresponds to $\boldsymbol{Y}^{\boldsymbol{P}}$ are approximated by inverse models. After that, offspring are obtained by evaluating $\boldsymbol{X}^{\boldsymbol{P}}$ with the objective functions.

To facilitate inverse modeling, IM-MOEA divides the objective space into different subregions through a set of evenly distributed reference vectors. Specifically, $K$ reference vectors partition the population into $K$ subpopulations by associating each individual to the reference vector that has the minimum angle between them. The associating process is illustrated in Fig. 2, where solution $s$ is associated with $\boldsymbol{v}_{2}$ since $\theta_{1}<\theta_{2}$. Since it is difficult to train an $M$-input-and-d-output approximation model, where $d$ denotes the number of decision variables. IM-MOEA decomposes the multivariate models into $M \times L$ univariate models, where $L$ decision variables are randomly selected from $d$ decision variables to train inverse models and $L \ll d$. Take a 2-objective problem with 8 decision variables as an example. If $L=2$, after random assignment, $x_{2}$ and $x_{4}$ are assigned to the $f_{1}$, and $x_{3}$ and $x_{6}$ are assigned to the $f_{2}$. Then the first group contains the following two inverse models $I M\left(x_{2} \mid f_{1}\right), I M$ $\left(x_{4} \mid f_{1}\right)$, and the second group contains $I M\left(x_{3} \mid f_{2}\right), I M\left(x_{6} \mid f_{2}\right)$. As a consequence, the four decision variables $x_{2}, x_{3}, x_{4}$, and $x_{6}$ will be replaced by the newly generated values of the inverse models, and the other four decision variables i.e., $x_{1}, x_{3}, x_{7}$, and $x_{8}$ will remain unchanged. Consider the inverse model $I M\left(x_{i} \mid f_{j}\right)$ trained according to a subpopulation with $N t$ solutions, which can be stated as follows.
$\boldsymbol{x}_{i}=g\left(\boldsymbol{f}_{j}\right)+\boldsymbol{\varepsilon}$
where $\boldsymbol{x}_{i}=\left(\boldsymbol{x}_{i}^{1}, \boldsymbol{x}_{i}^{2}, \ldots, \boldsymbol{x}_{i}^{\mathrm{Nt}}\right)^{T}, \boldsymbol{f}_{j}=\left\langle f_{j}^{1}, f_{j}^{2}, \ldots, f_{j}^{\mathrm{Nt}}\right)^{T}, \boldsymbol{\varepsilon}-\boldsymbol{N}\left(\boldsymbol{0},\left(\boldsymbol{\sigma}_{\mathrm{Nt}}\right)^{T} \boldsymbol{f}\right)$ is an error term of the Gaussian process (Seeger, 2004), and $\boldsymbol{f}$ is the identity matrix. For a test input $f_{i}$, the predicted output $x_{i}$ can be obtained by a
![img-0.jpeg](img-0.jpeg)

Fig. 1. Schematic of the core idea of using inverse models.
![img-1.jpeg](img-1.jpeg)

Fig. 2. Illustration of associating a candidate solution with its closest reference vectors according to angles.
normal distribution, and its mean and variance can be calculated by
$\mu_{i, i}=\boldsymbol{C}_{i}^{T}\left(\boldsymbol{C}+\left(\sigma_{\mathrm{Nt}}\right)^{T} \boldsymbol{f}\right)^{-1} x_{i}$
$\left(\sigma_{i, i}\right)^{2}=\boldsymbol{C} \cdot-\boldsymbol{C}_{i}^{T}\left(\boldsymbol{C}+\left(\sigma_{\mathrm{Nt}}\right)^{T} \boldsymbol{f}\right)^{-1} \boldsymbol{C} \cdot$
where $\boldsymbol{C}=\left[c\left(f_{j}^{1}, f_{j}^{1}\right), \ldots, c\left(f_{j}^{\mathrm{Nt}}, f_{j}^{\mathrm{Nt}}\right)\right]$ is a matrix of covariance parameters between each element in the training data $\boldsymbol{f}_{j}, \boldsymbol{C}^{n}=\left[c\left(f_{j}^{1}, f_{j}^{1}\right), \ldots, c\left(f_{t}\right.\right.$, $\left.\left.f_{j}^{\mathrm{Nt}}\right)\right]$ is a vector of covariance parameters between the test input $f_{i}$ and each element in the training data $\boldsymbol{f}_{j}$, and $\boldsymbol{C} \cdot \cdot$ is the covariance parameters of test data $f_{i}$. Considering the computational efficiency, the following linear covariance function is adopted
$c\left(f_{j}^{n}, f_{j}^{n}\right)=f_{j}^{n T} f_{j}^{n}$
And the input decision variables are evenly generated within an interval as
$f_{i}=\left\{\frac{\left|f_{i}^{\min }-0.5 \boldsymbol{f}_{i}, f_{i}^{\max }+0.5 \boldsymbol{f}_{i}\right|}{N s}\right\}$
where $\boldsymbol{f}_{i}=f_{j}^{\max }-f_{j}^{\min }$, and $N s$ denotes the sample size.
After the offspring of each subpopulation are generated, they are merged with their corresponding parent population for environmental selection of the next generation. The environmental selection strategy of IM-MOEA is the same as that of NSGA-II. The framework of IM-MOEA is given in Fig. 3. IM-MOEA performs well on some MOPs, but its performance on MOPs with irregular PFs degenerates since it applies a set of evenly distributed reference vectors to assist the optimization. Moreover, univariate inverse models may not tailor for MOPs, which ignores the coupling relations of multiple objectives. Therefore, this paper dedicates to using inverse models more efficiently for solving MOPs.

## 3. The proposed IMTSEA

To utilize inverse models efficiently and make a better exploration and exploitation in the decision space, an inverse model-guided twostage evolutionary algorithm (IMTSEA) is proposed in this paper. Fig. 4

![img-2.jpeg](img-2.jpeg)

Fig. 3. Framework of IM-MOEA.
![img-3.jpeg](img-3.jpeg)

Fig. 4. Framework of IMTSEA.
presents the framework of IMTSEA, whose pseudo-code is given in Algorithm 1. First, the reproduction process of IMTSEA is different from that of IM-MOEA, which is based on multivariate inverse models. In addition, IMTSEA has two types of environmental selection strategies, namely Environmental selection 1 and Environmental selection 2 corresponding to the first and the second stage, respectively. If $F E /$ $F E_{\max }<T$, Environmental selection 1 is applied, otherwise Environmental selection 2 is applied, where FE represents the number of current fitness evaluations, $F E_{\max }$ is the maximum number of fitness evaluations, and $T$ is a prefixed parameter for adjusting the ratio of two stages. Furthermore, IM-MOEA uses subpopulations for environmental selection, while IMTSEA's selection mechanism applies to the entire combined population, thus the population size could be fixed. In the remaining part of the present section, the multivariate inverse modeling and two-stage strategies will be elaborated.

## Algorithm 1: IMTSEA

Input: Population size $N$; threshold $T$; number of clusters $K$, number of models for each combination of objectives $L$; maximum number of fitness evaluations $F E_{\max }$;
Output: Final population $P$
$01 \quad P \leftarrow$ Initialization $(N)$;
$02 \quad F E \leftarrow N$;
03 while $F E \leq F E_{\max }$ do
$03\left[P^{1}, P^{2}, \ldots, P^{K}\right] \leftarrow$ Clustering $(P, K)$;
$04\left[\begin{array}{lll}O^{1}, O^{2}, \ldots, O^{K}\end{array}\right]$ - Reproduction by multivariate inverse models $\left(\left[P^{1}, P^{2}, \ldots\right.\right.$, $\left.\left.P^{K}\right], L\right)$;
$05 \quad O \leftarrow O^{1} \cup O^{2} \cup \ldots \cup O^{K} ;$
$F E \leftarrow F E \div|O| ;$
06 if $F E / F E_{\max }<T$
$07 \quad P \leftarrow$ Environmental selection $1(P, O, N)$;
08 else
$09 \quad P \leftarrow$ Environmental selection $2(P, O)$;
(continued on next column)
(continued)

## Algorithm 1: IMTSEA

10 end if
11 end while

### 3.1. Reproduction based on multivariate inverse models

Algorithm 2: Reproduction by multivariate inverse models
Input: Subpopulation $P^{1}, P^{2}, \ldots P^{K}$; number of models for each combination of objectives $L$;
Output: Offspring $O^{1}, O^{2}, \ldots O^{K}$;
01 for $k \leftarrow 1: K$
$02 \quad D e c^{k} \leftarrow$ Decision matrix of $P^{k}$;
$03\left[\left(f_{1}, f_{2}\right),\left(f_{3}, f_{4}\right), \ldots\right] \leftarrow$ All combinations of two objectives $(M)$;
04 for each $\left(f_{i 1}, f_{i 2}\right) \in\left\{\left(f_{1}, f_{2}\right),\left(f_{3}, f_{4}\right), \ldots\right\}$
$05\left[x_{i 1}, x_{i 2}, \ldots, x_{i k}\right]$ - Randomly select $L$ decision variables $(d, L)$;
06 for $l \leftarrow 1: L$
$07 \quad I M\left(x_{i l} \mid f_{j 1}, f_{j 2}\right) \leftarrow$ Inverse Modeling $\left(x_{i l}, f_{j 1}, f_{j 2}\right)$
$08\left[x_{\text {new1 }}, x_{\text {new2 }}, \ldots\right]$ - Reproduce by inverse models;
09 Replace the corresponding decision variables of $D e c^{k}$ by $\left[x_{\text {new } 1}, x_{\text {new2 }}, \ldots\right]$;
10 end for
11 end for
$12 D e c^{k} \leftarrow$ Repair the invalid candidate solutions $\left(\right.$ Dec $\left.^{k}\right)$;
$13 O b f^{k} \leftarrow$ Evaluate $D e c^{k}$ by fitness functions;
$14 \quad O^{k} \leftarrow\left[\operatorname{Dec}^{k} \mid O b f^{k}\right]$;
15 end for

To facilitate the modeling process and improve the performance on a wide range of MOPs with normalized and badly scaled PFs, instead of using evenly distributed reference vectors, the $k$-means cluster method is applied in IMTSEA to partition the population into $K$ subpopulations $P^{1}$,

$P^{2}, \ldots, P^{K}$. Note that the partition process is implemented according to the objective values of the population in objective space. After that, $K$ subpopulations are used for generating offspring $O^{1}, O^{2}, \ldots, O^{K}$, the pseudo-code of reproduction by multivariate inverse models is given in Algorithm 2. For each combination of objective $f_{j 1}$ and $f_{j 2}$, multivariate inverse models, $I M\left(x_{i} \mid f_{j 1}, f_{j 2}\right), 1 \leq i \leq d, 1 \leq j 1, j 2 \leq M, j 1 \neq j 2$, are constructed according to each subpopulation. Theoretically, $d$ inverse models need to be built for each objective combination, $M c \times d$ in total, where $M c$ is the combinatorial number that selects two objectives from $M$ objectives without repetition, i.e., $M c=C_{M}^{1}$. To alleviate the computation budget by reducing the number of inverse models, the random grouping technique is applied. $L$ decision variables are randomly selected from $d$ decision variables to train inverse models, where $L \ll d$. For example, for a 3-objective problem with ten decision variables, three groups of models will be trained. To assume the situation that $L=2, x_{2}$, $x_{4}$ are assigned to $f_{1}$ and $f_{2}, x_{1}, x_{6}$ to $f_{1}$ and $f_{3}$, and $x_{5}, x_{8}$ to $f_{2}$ and $f_{3}$. Therefore, the first group contains the following two inverse models: $I M$ ( $x_{2} \mid f_{1}, f_{2}$ ) and $I M\left(x_{4} \mid f_{1}, f_{2}\right)$, the second group has $I M\left(x_{5} \mid f_{1}, f_{3}\right)$ and $I M$ $\left(x_{6} \mid f_{1}, f_{3}\right)$ and the third group has $I M\left(x_{5} \mid f_{2}, f_{3}\right)$ and $I M\left(x_{8} \mid f_{2}, f_{3}\right)$. Then, $x_{1}, x_{2}, x_{4}, x_{5}, x_{6}$, and $x_{8}$ will be replaced by newly generated decision variables by sampling on inverse models, and other decision variables, i. e., $x_{3}, x_{7}, x_{9}$, and $x_{10}$, will remain unchanged.

Due to the high efficiency of the quadratic regression model on small samples (Box, \& Draper, 1987), it is selected as the paradigm of inverse models in this study. Certainly, other types of surrogate models, such as the Gaussian process, radial basis functions (Broomhead, \& Lowe, 1988), support vector machines (Kong, Chai, Yang, \& Ding, 2013), etc., could be considered too. Concretely, an inverse model $I M\left(x_{i} \mid f_{j 1}, f_{j 2}\right)$ trained according to a subpopulation with $N t$ solutions, which can be formulated as follows
$x_{i}=r\left(f_{j 1}, f_{j 2}\right)+e$
where $x_{i}=\left(x_{i}^{1}, x_{i}^{2}, \ldots, x_{i}^{\mathrm{tn}}\right)^{T}, f_{j 1}=\left(f_{j 1}^{1}, f_{j 1}^{2}, \ldots, f_{j}^{\mathrm{tn}}\right)^{T}, f_{j 2}=$ $\left(f_{j 2}^{1}, f_{j 2}^{2}, \ldots, f_{j 2}^{\mathrm{tn}}\right)^{T}$, $\varepsilon$ is an error term of regression model. Given a test input $\left(f_{i 1}, f_{i 2}\right)$, the predicted output $x_{i}$ can be obtained by
$x_{c}=w_{0}+w_{1} f_{o}+w_{2} f_{c 1}+w_{3} f_{o 1} f_{c 2}+w_{4} f_{c 1}^{2}+w_{5} f_{c 2}^{2}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\psi}\left(f_{o 1}, f_{c 2}\right)$
where $\boldsymbol{w}=\left(\begin{array}{llll}w_{0}, & w_{1}, & w_{2}, & w_{3}, & w_{4}, & w_{5}\end{array}\right)^{T}, \boldsymbol{\psi}\left(f_{i 1}, f_{i 2}\right)=$ $\left(1, f_{i 1}, f_{i 2}, f_{i 1} f_{i 2}, f_{i 1}^{2}, f_{i 2}^{2}\right)^{T}$, and the maximum likelihood estimate of $\boldsymbol{w}$ can be obtained by
$\boldsymbol{w}=\left(\boldsymbol{F}^{\mathrm{T}} \boldsymbol{F}\right)^{-1} \boldsymbol{F}^{\mathrm{T}} \boldsymbol{x}$
where $\boldsymbol{F}$ is an $N t \times 6$ matrix
$\boldsymbol{F}=\left[\boldsymbol{\psi}\left(f_{j 1}^{1}, f_{j 2}^{1}\right), \boldsymbol{\psi}\left(f_{j 1}^{2}, f_{j 2}^{2}\right), \ldots, \boldsymbol{\psi}\left(f_{j 1}^{\mathrm{tn}}, f_{j 2}^{\mathrm{tn}}\right)\right]^{\mathrm{T}}$
i.e.,
$\boldsymbol{F}=\left[\begin{array}{cccccc}1 & f_{j 1}^{1} & f_{j 2}^{1} & f_{j 1}^{2} f_{j 2}^{1} & \left(f_{j 1}^{3}\right)^{2} & \left(f_{j 2}^{1}\right)^{2} \\ 1 & f_{j 1}^{2} & f_{j 2}^{2} & f_{j 2}^{2} f_{j 2}^{2} & \left(f_{j 1}^{3}\right)^{2} & \left(f_{j 2}^{2}\right)^{2} \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\ 1 & f_{j 1}^{\mathrm{tn}} & f_{j 2}^{\mathrm{tn}} & f_{j 1}^{\mathrm{tn}} f_{j 2}^{\mathrm{tn}} & \left(f_{j 1}^{\mathrm{tn}}\right)^{2} & \left(f_{j 2}^{\mathrm{tn}}\right)^{2}\end{array}\right]$
If the population size of a cluster is small, quadratic regression models show high efficiency for modeling and predicting according to Eq. (8) and Eq. (9). Once $\boldsymbol{w}$ is obtained, a new decision variable in the decision space corresponding to a 2-objective vector in the objective space could be predicted. Theoretically, decision-makers can integrate their preferences in generating samples. In this paper, without loss of generality, new samples are randomly generated within a 2-objective space as
$\left|f_{j 1}, f_{j 2}\right| \in\left\{\begin{array}{l}\left|z_{c}^{\max }-0.5 \xi, z_{c}^{\max }+0.5 \xi_{c}\right|^{\mathrm{T}}, r<0.5 \\ \left|z^{\min }-0.5 \xi, z^{\max }+0.5 \xi\right|^{\mathrm{T}}, r \geqslant 0.5\end{array}\right.$
where $z_{c}^{\min }=\left|f_{j 1}^{\min }, f_{j 2}^{\min }\right|$ and $z_{c}^{\max }=\left|f_{j 1}^{\max }, f_{j 2}^{\max }\right|$, which are vectors consisting of the minimum and maximum values of objective $j 1$ and $j 2$ in the subpopulation, respectively; $\boldsymbol{z}^{\min }$ and $\boldsymbol{z}^{\max }$ are vectors consisting of the minimum and maximum values of $j 1$ and $j 2$ objective of the whole current population, respectively; $\xi_{c}=z_{c}^{\max }-z_{c}^{\min }, \xi=z^{\max }-z^{\min }$, and $r$ is randomly generated from $[0,1]$. For simplicity, the size of offspring is the same as the corresponding subpopulation.

Fig. 5 displays the reproduction mechanism of the proposed IMTSEA, where the quadratic regression model is built based on a subpopulation with six individuals. Different from IM-MOEA which samples the inverse models that are built on a subregion, the proposed method randomly samples on regions extended from the current population or the subpopulation, i.e., region 1 or region 2. Each time before sampling the inverse models, one region between the two regions is randomly selected. By doing this, the population can explore the decision space efficiently and converge to the PF rapidly on some problems that inverse models estimate well.

### 3.2. Two-stage strategy

After the offspring of each subpopulation is obtained by multivariate inverse models, they are merged into one offspring group for
![img-4.jpeg](img-4.jpeg)

Fig. 5. Illustration of the reproduction based on multivariate inverse models, where $\boldsymbol{\xi}(i)$ and $\boldsymbol{\xi}_{c}(i)$ represent the $i$ th element of $\boldsymbol{\xi}$ and $\boldsymbol{\xi}_{c}$, respectively. Region 1 and region 2 are extended from the current population and a subpopulation, respectively.

![img-5.jpeg](img-5.jpeg)

Fig. 6. An example to illustrate the two stages of IMTSEA.
environmental selection. In order to improve the quality of the solution set, a two-stage strategy is proposed in this paper, which includes the exploration stage (the first stage), and the exploitation stage (the second stage) that correspond to Environmental selection 1 and Environmental selection 2, respectively. In the first stage, the environmental selection based on NSGA-II is applied, during which the population is updated in batches and converges to the PF rapidly. However, after the population are converged, the environmental selection based on batch update may make the population degenerate to some degree. To avoid this phenomenon and better exploit the search space, the other environmental selection based on a one-by-one mechanism is employed in the second stage. Fig. 6 illustrates the two-stage strategy by a 2-objective problem with five candidate solutions. According to the figure, solutions approach the PF from the initial area in the first stage and show better convergence and diversity after tuning in the second stage.

Algorithm 3 presents the details of Environmental selection 1, which is the same as that of NSGA-II. After offspring O is obtained, the parent population with N members is screened out from the union set of P and O according to the nondominated levels and crowding distances (Deb, Pratap, Agarwal, \& Meyarivan, 2002). At the initial stage of optimization, the strategy shows high efficiency and performs well on exploration, which can make the population converge to the vicinity of PF rapidly. However, this kind of environmental selection based on batch update may lead to the degeneration of the population, i.e., over exploration. For example, some solutions may be important for local diversity, but they are still deleted since they are not in the first nondominated levels. Fig. 7 presents the iterative IGD values of NSGA-II on
![img-6.jpeg](img-6.jpeg)

Fig. 7. Iterative IGD values of NSGA-II on 3-objective DTLZ2, where the population size and $F E_{\max }$ are set to 100 and 20000 , respectively.

3-objective DTLZ2 (Deb, Thiele, Laumanns, \& Zitzler, 2005), where population size and $\mathrm{FE}_{\text {max }}$ are set to 100 and 20,000 , respectively. As is shown in the figure, the population converges rapidly at the preliminary stage but degenerates to some extent in the subsequent optimization process.

```
Algorithm 3: Environmental Selection 1
    Input: Current population \(P\), offspring \(O\), population size \(N\);
Output: New parent population \(P\);
    \(01 \quad P \leftarrow P \cup O\);
    \(02 \quad\left[F_{1}, F_{2}, \ldots\right] \leftarrow\) Nondominated sorting \((P)\);
    \(03 \quad i \leftarrow\) Minimum value s.t. \(\left[F_{1} \cup \ldots \cup F_{i}\right] \geq N\)
    \(04 \quad \text { if }\left[F_{1} \cup \ldots \cup F_{i}\right]>N\)
    \(05 \quad\) Delete \(\left[F_{1} \cup \ldots \cup F_{i}\right]-N\) solutions from \(F_{i}\) with the worst crowding distance
    values;
    \(06 \quad\) end if
    \(07 \quad P \leftarrow P_{1} \cup P_{2} \cup P_{0}\)
```

To address the above issue, a one-by-one selection strategy is applied in the second stage to guarantee the quality of the population is improved step-by-step, the process is presented in Algorithm 4. First, the solution that is nearest to the $i$-th offspring $O_{i}$ in $P_{i}$ termed $P_{i c}$ is found. Then the convergence and diversity degrees of the two solutions are calculated. Lastly, $O_{i}$ replaces $P_{i}$ only if both the convergence and diversity degrees of $O_{i}$ are better than that of $P_{i}$. Fig. 8. uses an example to illustrate this process, where the solution $\boldsymbol{p}$ of the current population is replaced by an offspring $\boldsymbol{q}$ since the $\boldsymbol{q}$ shows better convergence and diversity.

The convergence degree of a solution $\boldsymbol{x}$, termed $\operatorname{Con}(\boldsymbol{x})$, is formulated as
![img-7.jpeg](img-7.jpeg)

Fig. 8. An illustration of the Environmental selection 2. In this example, the solution $\boldsymbol{p}$ of the current population is replaced by an offspring $\boldsymbol{q}$ since the $\boldsymbol{q}$ has better convergence and diversity degrees.

$\operatorname{Con}(\boldsymbol{x})=\sum_{i=1}^{M} f_{i}(\boldsymbol{x})$
For minimization MOPs, a smaller value of convergence degree means better convergence. And the diversity degree of a solution $\boldsymbol{x}$, termed $\operatorname{Div}(\boldsymbol{x})$, is defined as (Shen, Wang, Dong, Li, \& Wang, 2022)
$\operatorname{Div}(\boldsymbol{x})=\|\boldsymbol{f}(\boldsymbol{x})-\boldsymbol{f}\left(\boldsymbol{x}_{s}\right)\|+s\|\boldsymbol{f}(\boldsymbol{x})-\boldsymbol{f}\left(\boldsymbol{x}_{s}\right)\|$
where $\boldsymbol{x}_{s t}$ and $\boldsymbol{x}_{t 0}$ are two individuals that are closest to $\boldsymbol{x}$ and second closest to $\boldsymbol{x}$ in the objective space, respectively. $\|\boldsymbol{a}-\boldsymbol{b}\|$ represents the Euclidean distance of the two points $\boldsymbol{a}$ and $\boldsymbol{b}, s$ is a small value (e.g., $10^{-6}$ ) (Zitzler, Laumanns, \& Thiele, 2001). And a larger value of a diversity degree means better diversity. Before Eq. (13) and Eq. (14) are applied, the objective values should be normalized using the ideal point and nadir point of the solution set (Elarbi, Bechikh, Gupta, Said, \& Ong, 2017) as follows
$\boldsymbol{f}^{\prime}(\boldsymbol{x})=\frac{\boldsymbol{f}(\boldsymbol{x})-\boldsymbol{z}_{\text {nadir }}}{\boldsymbol{z}_{\text {ideal }}-\boldsymbol{z}_{\text {nadir }}}$
where $\boldsymbol{z}_{\text {ideal }}$ and $\boldsymbol{z}_{\text {nadir }}$ represent the objective vectors that consist of the minimum values of each objective from the solution set and the maximum values of each objective from the PF, respectively, $\boldsymbol{f}(\boldsymbol{x})$ denotes the objective vector of solution $\boldsymbol{x}$, and $\boldsymbol{f}^{\prime}(\boldsymbol{x})$ is the normalized objective vector.
Algorithm 4: Environmental selection 2
Input: Current population $P$, offspring $O$, population size $N$;
Output: New parent population $P$;
01 for $(=1:|O|)$
$P_{i} \leftarrow$ Find the solution that is nearest to $O_{i}$ in the objective space;
$C o n\left(P_{i}\right), C o m\left(O_{i}\right) \leftarrow$ Calculate convergence degrees $\left(P_{i}, O_{i}\right)$;
$D i r\left(P_{i}\right), D i r\left(O_{i}\right) \leftarrow$ Calculate diversity degrees $\left(P_{i}, O_{i}\right)$;
if $\operatorname{Con}\left(O_{i}\right) \leq \operatorname{Con}\left(P_{i}\right) \& \operatorname{Div}\left(O_{i}\right) \geq \operatorname{Div}\left(P_{i}\right)$
$P \leftarrow P \backslash P_{i i}$
$P \leftarrow P \backslash O_{i i}$
end if
end for

### 3.3. Constraint handling

In this paper, a widely-used constraint violation function is applied to evaluate the degree of violation of candidate solutions (Jain, \& Deb, 2013). Assume there are $J$ inequality constraints and $K$ equality constraints, for a solution $\boldsymbol{x}$, its degree of constraint violation $C V(\boldsymbol{x})$ can be stated as
$C V(\boldsymbol{x})=\sum_{j=1}^{K}\left\langle z_{j}(\boldsymbol{x})\right\rangle+\sum_{k=1}^{K}\left|h_{k}(\boldsymbol{x})\right|$
where $\boldsymbol{z}_{k}(\boldsymbol{x}) \geq 0(j=1,2, \ldots, J)$ represents the inequality constraints, $h_{k}(\boldsymbol{x})=0(k=1,2, \ldots, K)$ represents the equality constraints (Cuate, Uribe, Lara, \& Schütze, 2020), $\left\lvert\, \boldsymbol{g}(\boldsymbol{x})\right.$ returns $\left.-\boldsymbol{g}_{k}(\boldsymbol{x})\right.$ if $\boldsymbol{g}_{k}(\boldsymbol{x})<0$ and returns 0 otherwise, $\left|h_{k}(\boldsymbol{x})\right|$ returns the absolute value of $h_{k}(\boldsymbol{x})$. Then, the objective values of each solution are adjusted by adding its $C V$ value as follows for environmental selection.
$\boldsymbol{f}_{s, i}(\boldsymbol{x})=\left(f_{i}(\boldsymbol{x})+C V(\boldsymbol{x}), f_{i}(\boldsymbol{x})+C V(\boldsymbol{x}), \ldots, f_{i k}(\boldsymbol{x})+C V(\boldsymbol{x})\right)$
where $\boldsymbol{f}_{s}(\boldsymbol{x})$ represents the adjusted objective vector considering the constraints violation. In this paper, $C V$ value of a solution $\boldsymbol{x}$ is normalized using the method suggested in NSGA-III (Jain \& Deb, 2013) before use. Specifically, for the second stage of IMTSEA, the offspring with a better convergence degree, diversity degree, and small value of $C V$ than its nearest solution in the parent population will replace the latter.

### 3.4. Computational complexity analysis

This section briefly analyzes the computational complexity of IMTSEA. Assume the objective number is $M$, the population size is $N$, the number of clusters of population $K$, and the number of models for each group $L$. The expected value of the data size for training the inverse models is $N / K$. Since the computational complexity of training a quadratic regression model is $O\left((N / K)^{2}\right)$, the computational complexity of the reproduction strategy based on multivariate inverse models is $O$ $\left(M \times L \times K \times\left(N / K\right)^{2}\right)=O\left(M L N^{2} / K\right) \approx O\left(M N^{2}\right)$. The computational complexity of Environmental selection 1 is the same as NSGA-II, which is $O\left(M N^{2}\right)$. In the second stage, the computational complexity of finding the solution that nearest an offspring is $O(M N)$, calculating the convergence and diversity degrees of an offspring are $O(M)$ and $O(M N)$, respectively, and calculating the convergence and diversity degrees of the parent population are $O(M N)$ and $O\left(M N^{2}\right)$, respectively. The total computational complexity of Environmental selection 2 is $O$ $\left(M N^{2}+M N \times N+M \times N+M N \times N\right) \approx O\left(M N^{2}\right)$. Therefore, the total computational complexity of IMTSEA in one generation is $O\left(M N^{2}\right)$.

## 4. Experimental studies on benchmark MOPs

In this section, we demonstrate the effectiveness of IMTSEA by comparing it with several state-of-the-art MOEAs, i.e., IBEA (Zitzler, \& Künzli, 2004), NSGA-III (Deb, \& Jain, 2013), IM-MOEA (Cheng, Jin, Narukawa, \& Sendhoff, 2015), MaOEA-IT (Sun, Xue, Zhang, \& Yen, 2018), and OPE-MOEA (Chen, et al., 2019). The reasons for choosing them as peer competitors are as follows.

IBEA is a representative indicator-based MOEA, which is employed frequently for comparison of the performance of MOEAs. It selects offspring according to their indicator values and shows high performance on MOPs and MaOPs, especially in convergence (Wang, Jiao, \& Yao, 2014).

NSGA-III is one of the most popular decomposition-based MOEAs in the multi-objective optimization community. It has a similar framework to NSGA-II but selects offspring based on dominance relation and weight vectors. The superiority of NSGA-III has been demonstrated on a variety of MOPs and MaOPs, including practical MOPs.

IM-MOEA is a representative MOEA based on inverse models. It constructs Gaussian process-based inverse models mapping solutions from the objective space to the decision space, and offspring are generated by sampling on the inverse models. Experimental results demonstrate that IM-MOEA shows robust performance on a variety of test problems.

MaOEA-IT is a recently proposed MOEA based on two stages. In the first stage, a nondominated dynamic weight aggregation method is applied to find the Pareto-optimal solutions, and the solutions are used to learn the Pareto-optimal subspace for convergence. Then diversity is addressed by solving a set of single-objective optimization problems within the learned Pareto-optimal subspace. MaOEA-IT performs well on a variety of MOPs and MaOPs.

OPE-MOEA is a recently proposed decomposition-based MOEA that uses an adaptive allocation strategy to enhance population convergence. It develops a novel metric to measure the contributions of subspaces to population convergence and adaptively allocates computational resources to each subspace according to their contributions to the population. OPE-MOEA outperforms several state-of-the-art MOEAs, including NSGA-III on a set of widely-used benchmark problems.

The experiments are conducted on several groups of common benchmark MOPs, i.e., DTLZ1-7 (Deb, Thiele, Laumanns, \& Zitzler, 2005), WFG1-9 (Huband, Hingston, Barone, \& While, 2006), ZDT1-4, 6 (Deb, 1999), and MaF1-10 (Cheng, Li, \& Tian, 2017). The maximum number of FEs ( $F E_{\max }$ ) is set to 10,000 for all the instances in absence of exceptional circumstances. The times of independent runs on each instance are set to 20 . The Wilcoxon rank sum test is adopted in each test instance at a significance level of $0.05,{ }^{\prime}+{ }^{\prime}, \because \because$, and ' $=$ ' indicate that the

result of other compared algorithms is significantly better, significantly worse, and statistically similar to that of the proposed method. All experiments in the present section are executed on the PlatEMO (Tian, Cheng, Zhang, \& Jin, 2017).

### 4.1. Performance metric

We employ the inverted generational distance (IGD) (Bezerra, LópezIbánez, \& Stützle, 2017), the hypervolume (HV) (Bader, \& Zitzler, 2011), and the $\Delta_{p}$ metric (Schütze, Esquivel, Lara, \& Coello, 2012) to evaluate the performance of the compared algorithms, which are three widely-used indicators in the multi-objective optimization community.

The IGD metric can be formulated as
$\operatorname{IGD}(S, R)=\frac{1}{|R|} \sum_{i=1}^{|R|} \min _{1 \leqslant r \mid S i} d i s\left(\boldsymbol{s}_{i}, \boldsymbol{r}_{i}\right)$
where $S$ denotes the nondominated solution set obtained by the optimizer, $R$ is a set of evenly distributed reference points on the PF, $\operatorname{dis}\left(\boldsymbol{s}_{0}, \boldsymbol{r}_{i}\right)$ denotes the Euclidean distance between $s_{i}$ in $S$ and $\boldsymbol{r}_{i}$ in $R$. $|\boldsymbol{R}|$ is the number of reference points in $R$, which is set as 10,000 for all instances. Generally, a smaller IGD value means better performance.

The HV value of a solution set $S$ is formulated as
$\operatorname{HV}(S, \boldsymbol{r})=\sum_{i=1}^{|S|} V\left(\boldsymbol{s}_{i}, \boldsymbol{r}\right)$
where $\boldsymbol{r}$ is a reference point, $V\left(\boldsymbol{s}_{i}, \boldsymbol{r}\right)$ denotes the hypercube bounded by the $i$-th solution $s_{i}$ and $\boldsymbol{r}$. Before HV values are calculated, objective values of all the solutions should be normalized by Eq. (15), $\boldsymbol{r}$ is set as $(1.1, \ldots, 1.1)$, which has the same vector length as the objective number. A larger HV value means better performance.

The $\Delta_{p}$ value of a solution set $S$ can be formulated as
$\Delta_{p}(S, R)=\max \left\{\operatorname{GD}_{p}(S, R), \operatorname{IGD}_{p}(S, R)\right\}$
where
$\operatorname{GD}_{p}(S, R)=\left(\frac{1}{|S|} \sum_{i=1}^{|S|} \min _{1 \leqslant r \mid S i} d i s\left(\boldsymbol{s}_{i}, \boldsymbol{r}_{i}\right)^{p}\right)^{1 / p}$
$\operatorname{IGD}_{p}(S, R)=\left(\frac{1}{|R|} \sum_{i=1}^{|R|} \min _{1 \leqslant r \mid S i} d i s\left(\boldsymbol{s}_{i}, \boldsymbol{r}_{i}\right)^{p}\right)^{1 / p}$
where $S, R$, and $d i s\left(\boldsymbol{s}_{i}, \boldsymbol{r}_{i}\right)$ are the same to those in Eq. (18). The parameter $p$ is set to 1 in this study, and a smaller $\Delta_{p}$ value means better performance.

### 4.2. Experimental settings

1) Reproduction operators: For IBEA, NSGA-III, and MaOEA-IT, the simulated binary crossover (SBX) and the polynomial mutation (PM) (Deb, Pratap, Agarwal, \& Meyarivan, 2002) are employed to generate offspring, where the distribution index is set to 20 , the crossover probability is set to 1 , and the mutation probability is set to $1 / d, d$ denotes the number of decision variables. For OPE-MOEA, the DE operator (Price, 2013) and PM are applied for reproduction, where the differential factor $F$ is set as 0.5 , the crossover ratio $C R$ was set as 1 , and the parameters of PM are the same as that in MaOEA-IT. While for IM-MOEA and IM-MOEA/D offspring are generated by estimation models.
2) Population size: The population size is set to 100 for IBEA and IMTSEA. For the other four algorithms using weight vectors, the population size is set to $100,91,105$, and 110 on $2,3,5$, and 10 -objective problems, respectively.
3) Parameters in each algorithm: The parameters of all the compared algorithms are listed in Table 1. For a fair comparison, parameter settings are the same as that in the original literature.

### 4.3. Comparison with state-of-the-art MOEAs

The statistical results for IGD values of IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, and IMTSEA on the test suite are listed in Table 2, where the best result of each instance is highlighted. From the 21 instances, IMTSEA performs best on 9 instances for IGD values, IBEA, NSGA-III, IM-MOEA, MaOEA-IT, and OPE-MOEA behave best on 4, 3, 1, 0 , and 4 instances, respectively. Obviously, IMTSEA and OPE-MOEA achieve the top two performance among the compared algorithms.

DTLZ1, DTLZ3, and ZDT4 are multimodal problems, which cause difficulties in the modeling of IM-MOEA and IMTSEA. Therefore, four MOEAs based on SBX or DE, and PM, i.e., IBEA, NSGA-III, MaOEA-IT, and OPE-MOEA, perform better than the two based on inverse models on these problems. Similarly, since WFG1 has a number of transformation functions, which degenerates the accuracy of estimation models, and leads to poor performances IMTSEA.

For DTLZ2, 4, ZDT1, 2, 6, and WFG4-9 with regular PFs, IMTSEA significantly outperforms the compared algorithms on most of the instances, except DTLZ4, WFG4, and WFG8, on which IMTSEA are outperformed by OPE-MOEA or NSGA-III. Figs. 9, 13, and 14 plot the nondominated solutions of the compared algorithms on DTLZ2, ZDT6, and WFG9, respectively. According to the figures, IMTSEA, NSGA-III, and OPE-MOEA achieve the top three performance on DTLZ2 and WFG9, and IMTSEA significantly outperforms its peer competitors on ZDT6 in terms of convergence and diversity, except OPE-MOEA.

DTLZ5, 6, and WFG3 are MOPs with degenerated PFs. For DTLZ5 and DTLZ6, IMTSEA significantly outperforms the compared algorithms on all the instances. Nondominated solutions obtained by the six MOEAs on DTLZ6 are presented in Fig. 10. From the figure, for DTLZ6, IMTSEA ranks first among the compared MOEAs, IBEA, NSGA-III, MaOEA-IT, and OPE-MOEA show good convergence but poor diversity, the other algorithm, i.e., IM-MOEA, fail to approximate the PF of DTLZ6. For WFG3, IBEA shows the best performance and IMTSEA outperforms IM-MOEA and MaOEA-IT.

DTLZ7, ZDT3, and WFG2 are MOPs with disconnected PFs, IMTSEA behaves best on all the instances. The nondominated solutions of DTLZ7 and ZDT3 are displayed in Fig. 11 and Fig. 12, respectively. For DTLZ7 and ZDT3, only IBEA and IMTSEA obtain an approximate PF of high quality in terms of convergence and diversity. Compared with IM-MOEA that applies a set of evenly distributed reference vectors to assist the optimization, the proposed IMTSEA uses a two-stage framework based on the Pareto dominance relation (first stage) and one-by-one mechanism (second stage). Therefore, the proposed method achieves better performance than IM-MOEA on MOPs with irregular PFs, i.e., DTLZ5, 6, 7, ZDT3, WFG2, and WFG3.

The statistical results for HV values of IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, and IMTSEA on the test suite are listed in Table 3, where the best result of each instance is highlighted. Note that ' 0 ' means the corresponding solution set is far away from the PS and fails

Table 1
The parameter settings of IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, and IMTSEA.

Table 2
The mean IGD values and standard deviations obtained by IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, and IMTSEA, the best result of each row is highlighted.

![img-8.jpeg](img-8.jpeg)

Fig. 9. Nondominated solutions obtained by the six compared algorithms on DTLZ2.
![img-9.jpeg](img-9.jpeg)

Fig. 10. Nondominated solutions obtained by the six compared algorithms on DTLZ6.
to approximate the PF. From the 21 instances, IMTSEA performs best on 12 instances for HV values, IBEA, NSGA-III, IM-MOEA, MaOEA-IT, and OPE-MOEA behave best on $5,2,0,0$, and 2 instances, respectively. From the HV metric, IMTSEA achieves the best performance among the compared algorithms. According to the table, IMTSEA significantly outperforms IBEA, NSGA-III, IM-MOEA, and MaOEA-IT on most of the
test instances. OPE-MOEA allocates computational resources to each subspace adaptively, which balances convergence and diversity well and shows competitive performance with IMTSEA under the IGD metric. However, when the HV metric is applied, IMTSEA shows some advantage over OPE-MOEA. Concretely, IMTSEA outperforms OPE-MOEA on 12 instances and performs worse than OPE-MOEA on 7 instances. The

![img-10.jpeg](img-10.jpeg)

Fig. 11. Nondominated solutions obtained by the six compared algorithms on DTLZ7.
![img-11.jpeg](img-11.jpeg)

Fig. 12. Nondominated solutions obtained by the six compared algorithms on ZDT3.
results validate the effectiveness of the proposed IMTSEA.
To further prove the effectiveness of the proposed IMTSEA, the $\Delta_{p}$ metric is also applied. The statistical results for $\Delta_{p}$ values of IBEA, NSGAIII, IM-MOEA, MaOEA-IT, OPE-MOEA, and IMTSEA on the test suite are listed in Table 3, where the best result of each instance is highlighted. From the 21 instances, IMTSEA performs best on 12 instances for $\Delta_{p}$ values, IBEA, NSGA-III, IM-MOEA, MaOEA-IT, and OPE-MOEA behave best on $3,2,1,0$, and 3 instances, respectively. Obviously, the proposed IMTSEA shows the best overall performance among the compared algorithms when $\Delta_{p}$ values are considered. In summary, from the three tables (Table 2, Table 3, and Table 4) and figures, the reproduction strategy based on multivariate inverse models and two-stage strategy contribute to the strong competitiveness of IMTSEA when compared with state-of-the-art MOEAs.

### 4.4. Effectiveness of the reproduction strategy: as a whole

A novel reproduction strategy is proposed in this paper, which randomly samples two types of objective spaces based on inverse quadratic regression models. To demonstrate the whole effectiveness of the reproduction strategy, IMTSEA is compared with its two variants, namely, IMTSEA-GA and IMTSEA-IM, where the former uses SBX, to replace the reproduction strategy of IMTSEA, and the latter uses the reproduction strategy of IM-MOEA to replace that of IMTSEA.

Table 5 presents the mean IGD values and standard deviations obtained by the three algorithms. From the table, IMTSEA outperforms IMTSEA-GA and IMTSEA-IM under overall consideration. Meanwhile, it shows the best performance on most instances. IMTSEA-GA ranks second and behaves better than IMTSEA on some instances, i.e., DLTZ1, 3, 5, ZDT4, and WFG1, 3, 5, 8. IMTSEA-IM ranks last and shows worse performance than IMTSEA on most instances. Although the multimodal MOPs like DTLZ1, 3 and deceptive MOPs like WFG5, degenerate the accuracy of inverse models, which leads to poor performance of IMTSEA. IMTSEA shows superiority on more instances like DTLZ6, ZDT1-3, and 6 , where multivariate inverse models probably work well. The iterative IGD values of the three algorithms on DTLZ6 and ZDT6 are presented in Fig. 15. As can be observed from the figure, the
reproduction strategy of IMTSEA shows high convergence speed among the three compared algorithms on the two instances. Comprehensively, the proposed reproduction strategy shows strong competitiveness when compared with SBX and the reproduction strategy of IM-MOEA.

### 4.5. Effectiveness of the reproduction strategy: components analysis

The effectiveness of the three main components (i.e., use of the clustering method, multivariate inverse models mapping, sampling of the global and local objective space) of the proposed reproduction strategy is analyzed in this section, where IMTSEA is compared with its four variants named Variant-1, Variant-2, Variant-3, and Variant-4. In Variant-1, instead of using the clustering method, population are partitioned by reference vectors as that in IM-MOEA, which is used to validate the effects of the clustering method. Variant-2 employs univariate inverse models to replace the multivariate inverse models of IMTSEA, which can manifest the promotion brought by multivariate inverse models. Variant-3 and Variant-4 only sample on the global and local objective space, respectively, which are applied to demonstrate the effectiveness of the sampling strategy of IMTSEA. Table 6 lists the statistic IGD results obtained by IMTSEA and its four variants, and the best result of each row is highlighted.

Since IMTSEA does not rely on uniform reference vectors to partition the population, it performs better than Variant-1 on most of the WFG test problems that are badly scaled. From 22 instances, IMTSEA significantly outperforms Variant-1 on 11 and performs worse only on 4, which validates the effectiveness of using the k-means cluster method.

From 22 instances, IMTSEA significantly outperforms Variant-2 on 10 and performs worse only on 3 . It can be concluded from the results that bivariate inverse models show better prediction accuracy than univariate inverse models from an overall perspective.

Since Variant-3 samples only on the global objective space, which means exploration is highlighted, it performs worse than IMTSEA on most instances. Similarly, Variant-4 samples only on the local region of the objective space, which leads to much worse performance on the instances that have demand for exploration, such as DTLZ6, 7, and ZDT1, 2, 3, 6. The results of Variant-3 and Variant-4 validate the
![img-12.jpeg](img-12.jpeg)

Fig. 13. Nondominated solutions obtained by the six compared algorithms on ZDT6.

![img-13.jpeg](img-13.jpeg)

Fig. 14. Nondominated solutions obtained by the six compared algorithms on WFG9.
importance of the balance between exploration and exploitation, and the effectiveness of sampling of the global and local objective space is demonstrated.

### 4.6. Effectiveness of the two stages

In IMTSEA, for better exploration and exploitation, two stages based on two types of environmental selection are applied. We use a threshold $T$ to control the proportion of the first stage, which is important for the performance of the proposed algorithm. If $T$ is too small, IMTSEA enters into the second stage early, which may lead to an incomplete process of exploration and poor convergence or diversity of solution set. On the contrary, if $T$ is excessively large, only exploration is highlighted, and exploitation of local regions is neglected, which may lead to poor performance similarly. To demonstrate the effectiveness of the two stages and study the influence of parameter $T$, we conduct IMTSEA on the test suite with $T$ varying from 0 to 1 , the results are presented in Table 7. As
is shown from the table, IMTSEA with $T=0.6$ performs best on most test instances and ranks overall first, which verified the effects of the twostage strategy of IMTSEA. Based on the results above, $T=0.6$ is recommended in this paper.

For better visualization, the nondominated solutions obtained by IMTSEA when $T=0,0.6$, and 1 on DTLZ2 are plotted in Fig. 16. From the figure, when $T=0$, only Environmental selection 2 is applied, placing too much emphasis on exploitation results in poor convergence of population. When $T=1$, only Environmental selection 1 is applied, and although population converge well, they show poor diversity and uniformity. While when $T=0.6$, exploration and exploitation are balanced well, the solution set is good in terms of convergence and diversity. And the effectiveness of the two-stage framework is demonstrated by the way.

Table 3
The mean HV values and standard deviations obtained by IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, and IMTSEA, the best result of each row is highlighted.

Table 4
The mean $\Delta_{\mathrm{p}}$ values and standard deviations obtained by IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, and IMTSEA, the best result of each row is highlighted.

### 4.7. Sensitivity analysis for parameters $K$ and $L$

In addition to parameter $T$ studied before, the other two parameters require to be analyzed, i.e., the number of clusters of population $K$ and the number of models for each combination of objectives $L$.

The number of clusters $K$ determines the size of subpopulations. If $K$ is too large, data of some subpopulations may not be sufficient for training accurate inverse models. While a too small $K$ may lead to high complexity and redundancy. Given a population size $N=100$, we conduct IMTSEA with $K=5,10,15,20,25$, and 30 when $L=5$ on the test suite, and plot the average ranks according to the IGD values in Fig. 17 (left). From the figure, IMTSEA with $K=20$ ranks first, which validates our analysis. Therefore, $K=20$ is recommended in this paper. To reduce the total number of inverse models, parameter $L \ll d$, where $d$ denotes the dimensions of decision space. A too small $L$ may weaken the effectiveness of reproduction since only a few decision variables are updated each time. And a too large $L$ is good for exploration and harmful to exploitation since many decision variables are updated each time. We investigate the performance of IMTSEA with $L=1,3,5,7$, and 9 when $K=20$ on the test suite, the average ranks of IGD values are displayed in Fig. 17 (right). From the figure, IMTSEA with $L=5$ shows the best overall performance. Accordingly, $L=5$ is recommended in this paper. It should be noted that the best parameter settings, i.e., $T=0.6, K=20$, and $L=5$, are obtained by several iterations.

### 4.8. Limitation on MOPs with complex PSs and a remedy

Although IMTSEA performs well on a wide range of test MOPs, it has limitations, like most other algorithms. One main limitation is that IMTSEA shows poor performance on problems with complex PSs. For some decomposition-based MOEAs, such as MOEA/D-M2M, OTOEM, and OPE-MOEA which emphasize that diversity based on space partition is first, and always allocate a number of population to each subproblem/subspace, they perform well on MOPs with complex PS since only a small amount of population can be trapped in the deceptive areas before the algorithm is converged. However, for MOEAs that adopt the convergence first and diversity second selection strategy, such as NSGAII, MaOEA-IT, and the proposed IMTSEA, the population of them can be easily trapped in the deceptive areas (Liu, et al., 2013), thus resulting in poor performance on MOPs with complex PS.

In this paper, we use the environmental selection strategy based on NSGA-II (Environmental Selection 1) to achieve satisfying performance on most MOPs whose shapes of PSs are relatively smooth. But this selection strategy is not able to find the global PF of problems with complex PSs, such as the MOP series (Liu, Gu, \& Zhang, 2013). A remedy is using an environmental selection strategy based on decomposition to select population by diversity first. Here, we employ the selection strategy based on the Tchebycheff decomposition approach (Zhang, \& Li, 2007), named Environmental selection*, to improve the performance of IMTSEA on MOPs with complex PSs, the details are presented in Algorithm 5. The Tchebycheff decomposition approach can be formulated

Table 5
The mean IGD values including standard deviations obtained by IMTSEA-GA, IMTSEA-IM, and IMTSEA, the best result of each row is highlighted.
as
$\operatorname{ming}^{T C H}\left(x \mid r, z^{*}\right)=\max _{r \leq z \leq N}\left(r, \mid f_{r}(x)-z_{r}^{*}\right)$
Index of the reference vector $\boldsymbol{r}_{i}$ that is associated to a solution $\boldsymbol{o}$ is defined as follows
$i=\arg \min _{r \leq z \leq N}\left\{g^{T C H}\left(o \mid r_{i}, z^{*}\right)\right\}$
where $N V$ denotes the number of reference vectors.
Algorithm 5: Environmental selection*

Input: Current population $P_{i}$ offspring $O$, a set of reference vector $R=\left(r_{1}, r_{2}, \ldots r_{i}\right)$, number of neighbors of each reference vector $T$;
Output: New parent population $P_{i}$
61 Find the $T$ neighbors $R(i)$ of each $\boldsymbol{r}_{i} \in R$;
62 Find the ideal point $\boldsymbol{x}^{*}$ of $P: \cup$
63 for each $o \in O$
64 $\boldsymbol{o} \leftarrow$ Find the associating reference vector $\boldsymbol{r}_{i}$ for $\boldsymbol{o}$ using Eq. (23);
65 for each $j \in R(i)$
$66 \quad S_{j} \leftarrow$ Solutions associated to reference vector $r_{j}$ from $P_{i}$
67 if $S_{j}==\varnothing$
$68 \quad P \leftarrow P: \cup \boldsymbol{r}_{i}$
69 else
$10 \quad$ for $s \in S_{j}$
$11 \quad$ if $g^{\text {TCH }}\left(\boldsymbol{o} \mid r_{j}, \boldsymbol{x}^{*}\right) \leq g^{\text {TCH }}\left(\boldsymbol{s} \mid r_{j}, \boldsymbol{x}^{*}\right)$
$12 \quad s \leftarrow \boldsymbol{o}$
13 end if
14 end for
15 end if
16 end for
17 end for

Six algorithms including IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPEMOEA, and IMTSEA, are compared with IMTSEA* that takes Environmental selection* as the selection strategy on MOP1-5, where population size is set to 200 , the $F E_{\text {max }}$ is set to 60,000 , and the number of reference vectors is set to 200 . The statistic results with the IGD metric on MOP1-5 of the seven compared algorithms are presented in Table 8. The best result of each row is highlighted. The Nondominated solutions obtained by the compared algorithms on MOP3 are plotted in Fig. 18. As can be observed from the table and figures, IMTSEA* embedded selection strategy based on decomposition significantly outperforms IMTSEA and the other five compared algorithms on nearly all the test instances with complex PSs, which validates the effectiveness of the remedy. OPEMOEA and IM-MOEA stand behind IMTSEA* since they emphasize that diversity based on space partition is first, while IBEA, NSGA-III, MaOEAIT, and IMTSEA fail to approximate the PFs of problems with complex PSs since they give priority to convergence.

### 4.9. Comparison on MaOPs

In IMTSEA, the environmental selection is based on the Pareto dominance relation, which cannot solve MaOPs since the loss of selection pressure in high dimensional objective space. However, IMTSEA can be easily modified to handle MaOPs by replacing the nondominated sorting approach based on the Pareto dominance relation with that based on the Strengthened dominance relation (SDR) (Tian, Cheng, Zhang, Su, \& Jin, 2018) in environmental selection (Algorithm 3, line 2), which is termed IMTSEA**. To investigate the performance of the proposed method on MaOPs, IMTSEA** is compared with IMTSEA, IMTSEA*, IBEA, NSGA-III, IM-MOEA, MaOEA-IT, and OPE-MOEA on 5 and
![img-14.jpeg](img-14.jpeg)

Fig. 15. Iterative IGD values of IMTSEA-GA, IMTSEA-IM, and IMTSEA on DTLZ6 (left) and ZDT6 (right).

Table 6
The mean IGD values including standard deviations obtained by IMTSEA and its four variants, the best result of each row is highlighted.

Table 7
The mean IGD values obtained by IMTSEA when $T$ varies from 0 to 1 , the best result of each row is highlighted.

![img-15.jpeg](img-15.jpeg)

Fig. 16. Nondominated solutions obtained by IMTSEA when $T=0,0.6$, and 1 on DTLZ2.

10-objective $\mathrm{MaF} 1-10$, where population size is set to 100 , the $\mathrm{FE}_{\text {max }}$ is set to 20000 . $\mathrm{MaF}$ is a recently proposed benchmark test suite, which constitutes a set of test MOPs with diverse properties and is widely used
to test the performance of MOEAs on MaOPs. The statistic results with the IGD metric on $\mathrm{MaF} 1-10$ of the eight compared algorithms are presented in Table 9. The best result of each row is highlighted.

![img-16.jpeg](img-16.jpeg)

Fig. 17. Average ranks of IGD values of IMTSEA on the test suite with $K=5,10,15,20,25$, and $30(L=5)$, and $L=1,3,5$, and $9(K=20)$, respectively.

Table 8
The mean IGD values and standard deviations obtained by IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, IMTSEA, and IMTSEA* on MOP1-5, the best result of each row is highlighted.

![img-17.jpeg](img-17.jpeg)

Fig. 18. Nondominated solutions obtained by the seven compared algorithms on MOP3.

From 20 instances, IMTSEA** shows the best performance on 13 instances, and IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, IMTSEA, and IMTSEA* perform best on $5,2,0,0,0,0$, and 0 instances, respectively. IMTSEA** outperforms other compared algorithms on nearly all the test instances on MaF1-2, and MaF5-9. It is obvious that IMTSEA** shows the best performance on the test suite of MaOPs under overall consideration. Fig. 19 displays the nondominated solutions obtained by the eight compared algorithms on 5-objective MaF1. From the figure, IMTSEA** and IBEA achieve the top two performance with respect to both convergence and diversity, solution sets of NSGA-III and IMTSEA show good diversity but poor convergence.

MaF3 and MaF4 are modified by DTLZ3, which has a great number of local optimums and sharply reduces the accuracy of inverse models. Therefore, IMTSEA and its two variants and IM-MOEA show poor performance on MaF3-4, IBEA achieves good convergence and performs best on them. IMTSEA and IM-MOEA behave badly on the test suite since they employ environmental selection based on the Pareto dominance relation, which shows poor convergence on MaOPs. NSGA-III, MaOEAIT, OPE-MOEA, and IMTSEA* use a set of uniformly distributed weight vectors for selection, which perform well on problems with regular PFs. However, their effectiveness is degenerated on the MaF test suite with various irregular PFs, such as inverted PF, badly scaled PF, disconnected PF, and degenerated PF.

All in all, the performance of IMTSEA** on the MaF test suite
validates the robustness of the proposed framework of IMTSEA and the effectiveness of utilizing multivariate inverse models and a two-stage strategy for many-objective optimization.

## 5. Practical applications

In this section, the effectiveness of the proposed IMTSEA and its two variants on practical engineering MOPs is investigated by comparing it with IBEA, NSGA-III, and IM-MOEA. First, three widely used benchmark engineering MOPs are applied for the experiment, including a 3-objective car side impact problem (CSI) (Jain \& Deb, 2013), a 5-objective water resource management problem (WRM) (Jain \& Deb, 2013), and a 10-objective general aviation aircraft design problem (GAA) (Habib, Singh, Chugh, Ray, \& Miettinen, 2019). Second, the comparison is conducted on a 2-objective shape design problem of BWBUG, where we introduce the problem in detail. The parameter settings are the same as that in Section 4.

### 5.1. Investigation on benchmark engineering MOPs

The mean IGD and HV values and standard deviations obtained by IBEA, NSGA-III, IM-MOEA, IMTSEA, IMTSEA*, and IMTSEA** on the three benchmark engineering MOPs are listed in Table 10. For the 3objective CSI, IMTSEA performs best for both IGD and HV metrics.

Table 9
The mean IGD values and standard deviations obtained by ISEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, IMTSEA, IMTSEA* and IMTSEA** on MaF1-10, the best result of each row is highlighted.

![img-18.jpeg](img-18.jpeg)

Fig. 19. Nondominated solutions obtained by the eight compared algorithms on 5-objective MaF1.

Table 10
The mean IGD and HV values and standard deviations obtained by ISEA, NSGA-III, IM-MOEA, IMTSEA, IMTSEA*, and IMTSEA** on CSI, WRM, and GAA, the best result of each row is highlighted.

Nondominated solutions obtained by the six compared algorithms on CSI are plotted in Fig. 20, where the solutions are marked as red points and PF is approximated by grey points. From the figure, IMTSEA and NSGA-III achieve the top two performance on the problem, while the solution set of IMTSEA shows better diversity than that of NSGA-III, and the other four algorithms fail to reach the PF. For the other two MaOPs, i.e., WRM and GAA, IMTSEA** achieves the best performance, while IMTSEA behaves poorly since it has a defect on convergence in the highdimensional objective space.

From the results, IMTSEA shows good convergence and diversity in solving practical MOPs with 2 or 3 objectives, while IMTSEA** shows competitive performance on solving practical MaOPs when comparing with NSGA-III, and although IMTSEA* performs well on MOPs with complex PSs, it fails to solve practical MOPs with badly scaled objectives due to its over-reliance on a set of uniformly distributed weight vectors. In a word, the competitiveness of the proposed IMTSEA and its variant IMTSEA** is verified according to their optimization results on the three benchmark engineering MOPs.

### 5.2. Application of shape design of BWBUG

1) Problem introduction

In this subsection, IMTSEA is applied for the shape optimization of BWBUG. Fig. 21 illustrates the MOPs, where 7 plane variables $\left(d_{1}, d_{2}, d_{3}\right.$, $\left.d_{4}, l_{1}, l_{2}, l_{3}\right)$ are used to control the plane shape of BWBUG and 5 section variables ( $c_{1}, c_{2}, c_{3}, c_{4}, c_{5}$ ) base on CST method (Kulfan, 2008) are used to control the section shape of hydrofoil (Dong, Wang, Fu, \& Song, 2021), where the shape range of the section is from NACA0012 to NACA0022. For better dynamic performance, two optimization objectives are maximizing the lift coefficient $\left(C_{l}\right)$ and minimizing the drag coefficient $\left(C_{d}\right)$. For convenience, the maximizing objective $C_{l}$ is reformulated to be $-C_{l}$ to convert the above problem into a 2-objective minimization optimization problem. Specifically, the optimization formula is summarized below

$$
\begin{gathered}
\min f(x)=\left(-C_{l}, C_{d}\right) \\
\text { s.t. } \boldsymbol{x}=\left[c_{1}, c_{2}, c_{3}, c_{4}, c_{5}, d_{1}, d_{2}, d_{3}, d_{4}, l_{1}, l_{2}, l_{3}\right] \in\left[\boldsymbol{x}_{\text {linear }}, \boldsymbol{x}_{\text {upper }}\right]^{\mathrm{T}}
\end{gathered}
$$

where $\boldsymbol{x}_{\text {linear }}=[-0.0573,-0.0494,-0.0557,-0.0383,-0.0574,300$, $150,400,860,450,300,100)^{\mathrm{T}}, \boldsymbol{x}_{\text {upper }}=[0.0573,0.0494,0.0557$, $0.0383,0.0574,400,300,500,1000,600,440,250]^{\mathrm{T}}$, the unit of the seven plane variables is mm . A half of the wing span $(L)$ and the chord length $(D)$ of the BWBUG are fixed to 1500 mm and 1000 mm , respectively.

The calculation of $C_{l}$ and $C_{d}$ is time-consuming, and only hundreds of Computational Fluid Dynamic (CFD) simulations can be afforded. Hence, we use the pattern of offline optimization to tackle this problem. First, $11 d-1$ individuals are generated using the Latin hypercube sampling (LHS) method (McKay, Beckman, \& Conover, 2000), and evaluated by CFD simulations. Then, two quadratic regression models that are used to replace the two objective functions, i.e., $C_{l}$ and $C_{d}$, are constructed by the simulation results. The mathematical formulas of the two objectives are given in the appendix, whose $\mathrm{R}^{2}$ values are 0.9943 and 0.9905 , respectively. $\mathrm{R}^{2}$ is used for estimating the accuracy degree of the regression models, which is formulated as
$\mathrm{R}^{2}=1-\frac{\sum_{i=1}^{N}\left(f_{i}-\widehat{f}_{i}\right)^{2}}{\sum_{i=1}^{N}\left(f_{i}-\widehat{f}\right)^{2}}$
where $f_{i}$ is the real function value of the $i$-th sample, $\widehat{f}_{i}$ is the estimated value of the $i$-the sample, $\widehat{f}$ is the mean objective value of the training set, and $N t$ is the size of training data. $\mathrm{R}^{2}$ values are very close to 1 , which denotes the regression models approximate the real objective functions well.
2) Results discussions

The nondominated solutions of the six algorithms are presented in Fig. 22. From the figure, IMTSEA, and NSGA-III achieve the top two performance on this problem, the diversity of other compared algorithms is worse than that of IMTSEA, and IM-MOEA shows the worst performance among the six algorithms. In region A, the convergence of IMTSEA is not as good as that of IBEA, NSGA-III, and IMTSEA**, but it shows good diversity. In region B, IMTSEA outperforms the other five algorithms in both convergence and diversity.

The mean IGD and HV values obtained by the six MOEAs are listed in Table 11. The reference set of IGD values is obtained by combining all nondominated solutions of the six compared algorithms in the run associated with the best HV value. And the reference point for calculating HV values is (1.1, 1.1). Before HV values are calculated, all the solutions are normalized by Eq. (15), where $\boldsymbol{z}_{\text {filted }}$ and $\boldsymbol{z}_{\text {nadir }}$ are set to $(-0.6,0.02)$ and $(-0.3,0.04)$, respectively. In addition, the iterative mean IGD and HV values of the compared algorithms over 20 independent runs are plotted in Fig. 23. From the table and figure, IMTSEA achieves the best performance on this problem in terms of IGD and HV values.

For further analysis, four representative solutions from the bestnondominated solution set of IMTSEA, i.e., A, B, C, D, and the baseline solution in the center of the decision space are presented in Fig. 24, including their plane and section shapes. As can be observed from the figure, a series of shapes of BWBUGs with different dynamic properties are obtained. Solution A has the largest $C_{l}$, solution D has the smallest $C_{d}$, and solutions B and C have better comprehensive properties when both $C_{l}$ and $C_{d}$ are considered. Moreover, the $C_{l}, C_{d}$, and $C_{l} / C_{d}$ of the five solutions by CFD simulations are listed in Table 12, where the improvement of $C_{l} / C_{d}$ of each row is highlighted. From the table, $C_{l} / C_{d}$ of all the representative solutions are improved when compared with the baseline solution, and the value of solution C is 16.6136 , which is increased by $18.83 \%$. The optimization results demonstrate the effectiveness of the proposed IMTSEA in solving real-world MOPs.

## 6. Conclusions

In this work, an inverse model-guided two-stage evolutionary algorithm namely IMTSEA is proposed for better solving MOPs. Different from the reproduction strategy of IM-MOEA, a novel reproduction strategy based on the clustering method and multivariate inverse models is proposed to generate offspring, which is more suitable for solving a wide range of MOPs with regular and irregular PFs. Moreover, for a better balance between exploration and exploitation, a two-stage framework is proposed, where exploration and exploitation are considered in the first and second stages, respectively.
![img-19.jpeg](img-19.jpeg)

Fig. 20. Nondominated solutions obtained by the six compared algorithms on CSI, where the solutions are marked as red points and PF is approximated by grey points. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

![img-20.jpeg](img-20.jpeg)

Fig. 21. Illustration of the shape design problem of BWBUG, where 12 decision variables including 7 plane variables and 5 section variables are considered.
![img-21.jpeg](img-21.jpeg)

Fig. 22. Nondominated solutions obtained by IBEA, NSGA-III, IM-MOEA, IMTSEA, IMTSEA*, and IMTSEA** on the shape design problem of BWBUG.

Table 11
The mean IGD values and HV values obtained by IBEA, NSGA-III, IM-MOEA, IMTSEA, IMTSEA*, and IMTSEA** on the shape design problem of BWBUG, the best result of each row is highlighted.

The proposed method is compared with several state-of-the-art MOEAs on a set of widely-used benchmark MOPs. Experimental results demonstrate the competitiveness of IMTSEA, including the effectiveness of the reproduction strategy and two-stage strategy. The proposed IMTSEA is also modified to handle MOPs with complex PSs and MaOPs. Furthermore, IMTSEA is applied to the shape design of BWBUG, and the lift-drag ratio of BWBUG is improved apparently after optimization, which demonstrates the proposed method is able to obtain a set of satisfactory solutions on practical MOPs. However, IMTSEA still has much room for improvement. For example, the two-stage strategy is simply based on the fixed parameter $T$, the information of the current population and the properties of problems are not taken into account. Besides, offspring are generated by sampling on the inverse models randomly, which may decrease the convergence velocity of the population.

In the future, enhancing the performance of the proposed method further by reducing the limitations mentioned above is a promising
direction. Extending the IMTSEA to solve MOPs with many objectives/ constraints is another direction. In addition, we plan to extend the proposed IMTSEA to solve expensive engineering MOPs which involve time-consuming online sampling.

# CRediT authorship contribution statement 

Jiangtao Shen: Methodology, Writing - original draft. Huachao Dong: Methodology. Peng Wang: Investigation, Supervision. Jinglu Li: Writing - review \& editing. Wenxin Wang: Writing - review \& editing.

## Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

![img-22.jpeg](img-22.jpeg)

Fig. 23. Iterative mean IGD and HV values of the compared algorithms on the shape design problem of BWBUG over 20 independent runs.
![img-23.jpeg](img-23.jpeg)

Fig. 24. The baseline solution and four representative solutions, i.e., A, B, C, and D obtained by IMTSEA, including their plane shapes and section shapes.

Table 12
$C_{i}, C_{d}$, and $C_{i} / C_{d}$ of the baseline solution and solutions $\mathrm{A}, \mathrm{B}, \mathrm{C}$, and D calculated by CFD simulations, where improvement of $C_{i} / C_{d}$ of each representative solution is highlighted.
## Data availability

The authors do not have permission to share data.

## Acknowledgments

This work is partially sponsored by the National Natural Science Foundation of China (Grant No. 52175251, 52205268), the National Basic Scientific Research Program of China (Grant No. JCKY2021206B005), and the Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University (Grant No. CX2022007).

## Appendix

Mathematical formulas of the shape design MOP of BWBUG are given below, where $C_{i}$ is to be maximized and $C_{d}$ is to be minimized.
$C_{i}=0.378688247-0.832080301 c_{1}-1.213195305 c_{2}-0.81098083 c_{3}$ $-0.576406834 c_{4}+0.415971574 c_{5}+0.001410837 d_{1}+0.000887338 d_{2}$ $-5.76 \mathrm{e}-05 d_{3}-0.001031134 d_{4}+0.000330613 l_{1}-0.000407656 l_{2}-$ $0.000210991 l_{3}-0.290477621 c_{2} 1+0.271232079 c_{1} c_{2}-0.084$ $654559 c_{1} c_{3}+0.708125199 c_{1} c_{4}-0.075673762 c_{1} c_{5}+0.001074831 c_{1} d_{1}$ $-0.00017815 c_{1} d_{2}+0.000223705 c_{1} d_{3}+0.000285734 c_{1} d_{4}+4.95 \mathrm{e}-$ $05 c_{1} l_{1}-3.47 \mathrm{e}-05 c_{1} l_{2}+0.000483043 c_{1} l_{3}-1.167559952 c 2$ $2-0.051531509 c_{2} c_{3}-0.242312457 c_{2} c_{4}+0.055686856 c_{2} c_{5}+0.00$ $11451 c_{2} d_{1}-0.000307892 c_{2} d_{2}+0.000186094 c_{2} d_{3}+0.00081559$ $1 c_{2} d_{4}+0.000195101 c_{2} l_{1}-0.000338903 c_{2} l_{2}-3.53 \mathrm{E}-05 c_{2} l_{3}-$ $0.020613323 c 2 \quad 3-1.130342433 c_{3} c_{4}-0.139873477 c_{3} c_{5}+0.0$ $00904447 c_{3} d_{1}+9.50 \mathrm{e}-05 c_{3} d_{2}-0.0008219 c_{3} d_{3}+0.000460848 c_{3} d_{4}-$ $3.63 \mathrm{e}-05 c_{3} l_{1}+0.000493457 c_{3} l_{2}+0.000208254 c_{3} l_{3}+0.702564133 c 2$ $4-0.915137354 c_{4} c_{5}-0.00468404 c_{4} d_{1}-0.001671165 c_{4} d_{2}-$ $0.000516777 c_{4} d_{3}+0.000979989 c_{4} d_{4}+2.04 \mathrm{e}-05 c_{4} l_{1}+9.65 \mathrm{e}-07 c_{4} l_{2}-$ $9.34 \mathrm{e}-05 c_{4} l_{3}-0.981985148 c 2 \quad 5-0.000498347 c_{5} d_{1}-0.001133844 c_{5} d_{2}$ $-0.000530027 c_{5} d_{3}-0.000193207 c_{5} d_{4}-0.000358434$ $c_{5} l_{1}+0.000302904 c_{5} l_{2}-4.04 \mathrm{e}-05 c_{5} l_{3}-7.26 \mathrm{e}-07 \mathrm{~d} 21+2.47 \mathrm{e}-07 \mathrm{~d}_{1} \mathrm{~d}_{2}-$ $1.52 \mathrm{e}-07 \mathrm{~d}_{1} \mathrm{~d}_{3}+7.04 \mathrm{e}-08 \mathrm{~d}_{1} \mathrm{~d}_{4}-1.17 \mathrm{e}-07 \mathrm{~d}_{1} l_{1}-9.60 \mathrm{e}-07 \mathrm{~d}_{1} l_{2}+7.44 \mathrm{e}-$ $07 \mathrm{~d}_{1} l_{3}-3.36 \mathrm{e}-08 \mathrm{~d} 22-1.80 \mathrm{e}-07 \mathrm{~d}_{2} \mathrm{~d}_{3}-3.65 \mathrm{e}-07 \mathrm{~d}_{2} \mathrm{~d}_{4}-3.39 \mathrm{e}-07 \mathrm{~d}_{2} l_{1}-$ $2.65 \mathrm{e}-07 \mathrm{~d}_{2} l_{2}-7.50 \mathrm{e}-09 \mathrm{~d}_{2} l_{3}-6.04 \mathrm{e}-07 \mathrm{~d} 23+3.39 \mathrm{e}-07 \mathrm{~d}_{2} d_{4}-1.22 \mathrm{e}-$ $07 \mathrm{~d}_{3} l_{1}+1.25 \mathrm{e}-06 \mathrm{~d}_{3} l_{2}+1.79 \mathrm{e}-07 \mathrm{~d}_{3} l_{3}+5.65 \mathrm{e}-07 \mathrm{~d} 2 \quad 4-2.38 \mathrm{e}-07 \mathrm{~d}_{4} l_{1}-$ $7.17 \mathrm{e}-08 \mathrm{~d}_{4} l_{2}-8.38 \mathrm{e}-08 \mathrm{~d}_{4} l_{3}+3.66 \mathrm{e}-07 \mathrm{I} 2 \quad 1-2.25 \mathrm{e}-07 \mathrm{l}_{1} l_{2}-3.18 \mathrm{e}-$ $08 l_{1} l_{3}+6.43 \mathrm{e}-07 \mathrm{I} 22-7.11 \mathrm{e}-08 l_{2} l_{3}+8.65 \mathrm{e}-08 \mathrm{I} 23$.
$C_{d}=0.054475979-0.054969362 c_{1}-0.110855686 c_{2}-0.048233$ $374 c_{3}-0.010671046 c_{4}+0.044560805 c_{5}+7.80 \mathrm{e}-05 d_{1}+5.53 \mathrm{e}-05 d_{2}-$ $1.98 \mathrm{e}-06 d_{3}-0.000120808 d_{4}+1.81 \mathrm{e}-05 l_{1}-3.84 \mathrm{e}-05 l_{2}-2.74 \mathrm{e}-$ $061 l_{3}+0.052193437 c 2 \quad 1+0.091083518 c_{1} c_{2}+0.002970738 c_{1} c_{3}+0$ $.041981215 c_{1} c_{4}-0.034664934 c_{1} c_{5}+0.000113405 c_{1} d_{1}+1.38 \mathrm{e}-$ $05 c_{1} d_{2}+3.56 \mathrm{e}-05 c_{1} d_{3}+1.34 \mathrm{e}-054 c_{1} d_{4}-1.06 \mathrm{e}-05 c_{1} l_{1}+2.32 \mathrm{e}-06$ $c_{1} l_{2}+5.20 \mathrm{e}-05 c_{1} l_{3}+0.010894935 c 2 \quad 2+0.13849716 c_{2} c_{3}-0.01422$ $7861 c_{2} c_{4}-0.063279033 c_{2} c_{5}+9.98 \mathrm{e}-05 c_{2} d_{1}-1.01 \mathrm{e}-05 c_{2} d_{2}+3.89 \mathrm{e}-$ $05 c_{2} d_{3}+7.84 \mathrm{e}-05 c_{2} d_{4}+2.20 \mathrm{e}-05 c_{2} l_{1}-3.15 \mathrm{E}-05 c_{2} l_{2}+8.95 \mathrm{e}-06 c_{2} l_{3}+0$ $.147836054 c 2 \quad 3+0.055947404 c_{2} c_{4}-0.028441475 c_{3} c_{5}+2.97 \mathrm{e}-$ $05 c_{3} d_{1}+1.01 \mathrm{e}-05 c_{3} d_{2}-7.11 \mathrm{e}-05 c_{3} d_{3}+7.03 \mathrm{e}-05 c_{3} d_{4}-1.19 \mathrm{e}-$ $05 c_{3} l_{1}+4.26 \mathrm{e}-05 c_{3} l_{2}+2.28 \mathrm{e}-05 c_{3} l_{3}+0.183418579 c 24+0.01093320$ $2 c_{4} c_{5}-6.18 \mathrm{e}-05 c_{4} d_{1}-0.000144918 c_{4} d_{2}-5.91 \mathrm{e}-05$ $c_{4} d_{3}+0.000106982 c_{4} d_{4}-7.62 \mathrm{e}-06 c_{4} l_{1}-3.57 \mathrm{e}-06 c_{4} l_{2}-1.39 \mathrm{e}-05 c_{4} l_{3}$ $-0.037922322 c 25+4.93 \mathrm{e}-06 c_{2} d_{1}-9.20 \mathrm{e}-05 c_{2} d_{2}-2.58 \mathrm{e}-05 c_{3} d_{3}-$ $2.49 \mathrm{e}-05 c_{3} d_{4}-3.37 \mathrm{e}-05 c_{3} l_{1}+3.17 \mathrm{e}-05 c_{3} l_{2}-6.60 \mathrm{e}-06 c_{3} l_{3}-8.15 \mathrm{e}-$ $09 \mathrm{~d} 21+3.25 \mathrm{e}-08 d_{1} d_{2}+1.16 \mathrm{e}-08 d_{1} d_{3}-2.03 \mathrm{e}-08 d_{1} d_{4}-1.51 \mathrm{e}-08 d_{1} l_{1}$ $-8.50 \mathrm{e}-08 d_{1} l_{2}+6.72 \mathrm{e}-08 d_{1} l_{3}+3.11 \mathrm{e}-08 \mathrm{~d} 22-2.75 \mathrm{e}-09 d_{2} d_{3}-2.91 \mathrm{e}-$ $08 d_{2} d_{4}-3.16 \mathrm{e}-08 d_{2} l_{1}-2.07 \mathrm{e}-08 d_{2} l_{2}+1.74 \mathrm{e}-09 d_{2} l_{3}-4.04 \mathrm{e}-08 d 2$

$3+1.82 \mathrm{e}-08 \mathrm{~d}_{2} \mathrm{~d}_{4}-3.08 \mathrm{e}-08 \mathrm{~d}_{1}+1.05 \mathrm{e}-07 \mathrm{~d}_{2} \mathrm{~d}_{2}-1.03 \mathrm{e}-08 \mathrm{~d}_{3} \mathrm{~d}_{3}+7.3$ $8 \mathrm{e}-08 \mathrm{~d} 24-1.47 \mathrm{e}-08 \mathrm{~d}_{4} \mathrm{l}_{1}-4.72 \mathrm{e}-09 \mathrm{~d}_{4} \mathrm{l}_{2}-1.03 \mathrm{e}-08 \mathrm{~d}_{4} \mathrm{l}_{3}+4.24 \mathrm{e}-08 \mathrm{l}_{2}$ $1-1.71 \mathrm{e}-08 \mathrm{l}_{1} \mathrm{l}_{2}-6.13 \mathrm{e}-09 \mathrm{l}_{1} \mathrm{l}_{3}+5.98 \mathrm{e}-08 \mathrm{l}_{2} 2-1.29 \mathrm{e}-08 \mathrm{l}_{2} \mathrm{l}_{3}+1.46 \mathrm{e}-$ 08 l 2 3.
