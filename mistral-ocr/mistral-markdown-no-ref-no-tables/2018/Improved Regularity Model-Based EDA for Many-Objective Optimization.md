# Improved Regularity Model-based EDA for Many-objective Optimization 

Yanan Sun, Member, IEEE, Gary G. Yen, Fellow, IEEE, and Zhang Yi, Fellow, IEEE


#### Abstract

The performance of multi-objective evolutionary algorithms deteriorates appreciably in solving many-objective optimization problems which encompass more than three objectives. One of the known rationales is the loss of selection pressure which leads to the selected parents not generating promising offspring towards Pareto-optimal front with diversity. Estimation of distribution algorithms sample new solutions with a probabilistic model built from the statistics extracting over the existing solutions so as to mitigate the adverse impact of genetic operators. In this paper, an improved regularity-based estimation of distribution algorithm is proposed to effectively tackle unconstrained many-objective optimization problems. In the proposed algorithm, diversity repairing mechanism is utilized to mend the areas where need non-dominated solutions with a closer proximity to the Pareto-optimal front. Then favorable solutions are generated by the model built from the regularity of the solutions surrounding a group of representatives. These two steps collectively enhance the selection pressure which gives rise to the superior convergence of the proposed algorithm. In addition, dimension reduction technique is employed in the decision space to speed up the estimation search of the proposed algorithm. Finally, by assigning the Pareto-optimal solutions to the uniformly distributed reference vectors, a set of solutions with excellent diversity and convergence is obtained. To measure the performance, NSGA-III, GrEA, MOEA/D, HypE, MBN-EDA, and RM-MEDA are selected to perform comparison experiments over DTLZ and DTLZ* test suites with $3-, 5-, 8-, 10$-, and 15-objective. Experimental results quantified by the selected performance metrics reveal that the proposed algorithm shows considerable competitiveness in addressing unconstrained manyobjective optimization problems.


Index Terms-Estimation distribution algorithm (EDA), manyobjective evolutionary algorithm (MaOEA), regularity-based EDA, diversity repairing, decision space dimension reduction.

## I. INTRODUCTION

MANY-OBJECTIVE optimization problems (MaOPs) concern solving $M$ conflicting objectives simultane-

This work is supported in part by the scholarship from China Scholarship Council; in part by the Miaozi Project in Science and Technology Innovation Program of Sichuan Province, China; and in part by the National Natural Science Foundation of China under Grant 61432012 and Grant U1435213. (Corresponding author: Gary G. Yen.)

Yanan Sun is with the College of Computer Science, Sichuan University, Chengdu 610065 CHINA and with the School of Engineering and Computer Science, Victoria University of Wellington, Wellington 6140 NEW ZEALAND (e-mail:yanan.sun@ecs.vuw.ac.nz).

Gary G. Yen is with the School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK 74078 USA (e-mail:gyen@okstate.edu).

Zhang Yi is with the College of Computer Science, Sichuan University, Chengdu 610065 CHINA (e-mail:zhangyi@scu.edu.cn).
ously where $M$ is greater than three [1]. Generally, an MaOP has the following formulation described by Equation (1)

$$
\left\{\begin{array}{c}
f(x)=\left[f_{1}(x), \cdots, f_{M}(x)\right] \\
\text { s.t. } \quad x \in \Lambda
\end{array}\right.
$$

where $\Lambda \in \mathbb{R}^{n}$ is the decision space, $f: \Lambda \rightarrow \Omega \in \mathbb{R}^{M}$ is the objective space. Without loss of generality, it is assumed that $f(x)$ is a minimization problem in which $f_{1}(x), \cdots, f_{M}(x)$ are to be minimized. Because of MaOPs widely existing in many real-world applications, such as management in land exploitation with 14-objective [2], calibration problems of automotive engine with 10-objective [3], to name a few, there is a strong incentive for efficiently and effectively solving MaOPs.

In MaOPs, there is no single perfect solution that optimizes all of the objectives at the same time but a set of Pareto-optimal solutions in which each individual is non-dominated with respect to each other. In addition, all the Pareto-optimal solutions constitute the Pareto-optimal set (PS) in the decision space while the image of PS produces a Pareto-optimal front (PF) in the objective space. Commonly, the goal in solving MaOPs is to obtain a limit number of Pareto-optimal solutions, which are uniformly distributed in PF, where a decision-maker can delegate a solution based on his or her preference. Among all the approaches for handing MaOPs, evolutionary algorithms are considered preferable because of the searching power exerted in these population-based meta-heuristics. During the past several decades, numerous multi-objective evolutionary algorithms (MOEAs), such as elitist non-dominated sorting genetic algorithm (NSGA-II) [4], advanced version of strength Pareto evolutionary algorithm (SPEA2) [5], have been developed for dealing with multi-objective optimization problems (MOPs) in which at most three objectives are to be optimized simultaneously. However, their performance degraded drastically in addressing MaOPs [6]. The main reason is the loss of selection pressure which is caused by the dominance resistance (DR) [7] and the curse of dimensionality [8] phenomena. To be specific, DR refers to a large proportion of solutions in which individuals are the best in one or very few objectives but far worse in others, and these solutions cannot be discriminated by the original Pareto domination principle. Then densitybased secondary measurement is activated to decide which solutions are allowed to survive in the next generation [9]. Because of the behavior influenced by DR pointed out in [10], the selected solutions do not necessarily converge to the PF [8]. To this end, various many-objective evolutionary algorithms

(MaOEAs) for tackling MaOPs have been developed ${ }^{1}$, such as multi-objective evolutionary algorithm based on decomposition (MOEA/D) [11], hypervolume-based many-objective optimization algorithm (HypE) [12], grid-based evolutionary algorithm for many-objective optimization (GrEA) [13], manyobjective optimization algorithm using reference point based non-dominated sorting (NSGA-III) [14], and etc. [9], [15][17] ${ }^{2}$. More precisely, MOEA/D employs the decompositionbased approach to construct a set of single objective problems by aggregating objectives considered in the original MaOP with different predefined weight vectors. New solutions are generated within a sub-region and diversity is maintained by the uniformly distributed weight vectors. The promising solutions are selected in HypE based on their fitness which is assigned by the corresponding contribution in hypervolume measure. As the computation of exact hypervolume is prohibitive, Monte Carlo simulation is employed to address this limitation. GrEA utilizes grid-based approach to show better performance in solving MaOPs by introducing the gridbased fitness comparison to relax the Pareto-based dominance relationship and grid-metrics to improve the diversity. Compared to NSGA-II, the improvement of NSGA-III is in the diversity mechanism by assigning the solutions to a set of uniformly distributed reference vectors. In summary, these state-of-the-art MaOEAs mentioned above mainly contemplate on two distinct issues: 1) reform the comparison manner in the traditional dominance relationship, such as HypE and GrEA and 2) apply new designs to reinforce the diversity, such as MOEA/D and NSGA-III.

```
Algorithm 1: Framework of An EDA
    \(t \leftarrow 0\);
    \(2 P_{t} \leftarrow\) Randomly initialize the population;
    while termination is not satisfied do
        \(M \leftarrow\) Built probabilistic models from \(P_{t}\);
        \(t \leftarrow t+1\);
        \(U_{t} \leftarrow\) Generate offspring from \(M\);
        \(P_{t} \leftarrow\) Select promising solutions from \(U_{t} \cup P_{t-1}\);
    end
    9 Return \(P_{t}\).
```

It is highly expected that individuals generated by selected parents with the crossover and mutation operators would march towards the PF in a MOP. However, this will not be the case in MaOPs due to the DR phenomenon. Specifically, if the parents are neighbors of DR solutions their offspring are also DR solutions. Otherwise, the newly generated solutions are not necessarily better than their parents who stand at a large space of many-objectives with a remote distance. This can be seen as the inefficiency of existing genetic operators for MaOPs [8]. Moreover, Deb et al. in [18] concluded that the

[^0]performances of MOEAs are significantly influenced by the genetic operators which cannot ensure to generate promising offspring. Furthermore, the parameters in genetic operators need empirically configured. For example, the distribution index of SBX in NSGA-III needs to be set at a larger number. For this purpose, researchers have developed estimation of distribution algorithms (EDAs) to tackle optimization problems [19]-[21] by generating new solutions without involving the traditional genetic operators, but probabilistic models which are built based on the statistics of the visited solutions. A general framework of EDAs is illustrated in Algorithm 1. Typically, the EDAs-based MOEAs are broadly classified into two categories based on their estimation models.

The first category covers the Bayesian network-based EDAs. For example, multi-objective Bayesian optimization algorithm [22] utilized the Bayesian optimization algorithm (BOA) to build a Bayesian network as its model for generating offspring. A related work was investigated in [23] to predict the model by strengthening Pareto ranking approach [24] and BOA. Furthermore, Laumanns et al. in [25] proposed a Bayesian multi-objective optimization algorithm whose model is built over the solutions selected by $\epsilon$-Pareto ranking method [26]. In addition, an improved non-dominated sorting approach was employed by decision tree-based multiobjective EDA [27] to select a subset of solutions serving for a regression decision tree to learn the model. Recently, the multidimensional Bayesian network (MBN-EDA) was proposed in [21] specifically for addressing MaOPs.

The other category is often known as the mixture probability model-based EDAs. Examples include the multi-objective mixture-based iterated density estimation evolutionary algorithm [28] employing the mixed probability distributions to sample well-distributed solutions; and the multi-objective Parzen-based EDA [29] learning from the Gaussian and Cauchy kernels to build its models. In [30], the multi-objective hierarchical Bayesian optimization algorithm was designed by the mixture Bayesian network-based probabilistic model for discrete multi-objective optimization problems. In addition, the multi-objective extended compact genetic algorithm [31] took a marginal product model as the mixture of probability model. Furthermore, a regularity-based model EDA (RM-MEDA) was proposed in [20] in which the model is built based on the mixture normal distribution over the regularity. Zhou et al. [32] proposed a regularity-based method for solving the MOPs requiring the objective spaces to be $(m-1)$ dimensions.

It is believed that EDAs are capable of solving MaOPs without suffering the disadvantages of MOEAs with traditional genetic operators. Although, MBN-EDA has shown the promise in solving MaOPs, the development of many-objective optimization EDAs (MaOEDA) is still in infancy. Especially, probability models based on regularity have been extensively investigated in the discipline of statistical learning [33], [34], and regularity-based model are easier to build, and fairly effective. Based on our recent research achievements on this topic [35], [36] and motivated by the success of regularitybased EDAs for MOPs [20], [37], [38], an improved regularitybased EDA for MaOPs, in short named MaOEDA-IR, is proposed in this paper. To be specific, models employed to


[^0]:    ${ }^{1}$ The algorithms which was designed originally for MOPs while is extended for MaOPs are also categorized to MaOEAs in this paper.
    ${ }^{2}$ Typically, these MaOEAs can be classified into three basic categories: Dominance-based, Decomposition-based and Hypervolume-based. MOEA/D and HypE are from the second and third categories, respectively. NSGA-III and GrEA are the hybridization of the first and the second categories.

generate new solutions in the proposed algorithm are built based on a group of neighbors selected by a uniformly distributed reference vectors. In order to improve the selection pressure, diversity repairing mechanism is developed to prevent the adverse DR phenomenon in each generation and push the new solutions toward having a closer proximity to the PF. Furthermore, dimension reduction technique is employed priori to the evolution to reduce the cost for exploration search. Specifically, convergence in the proposed algorithm is guaranteed by repairing diversity and sampling solutions based on the reference vectors, while diversity is facilitated by selecting solutions with the nearest perpendicular distances to the reference vectors. Compared to traditional MaOEAs and EDAs, the contributions of the proposed algorithm are summarized as follows:

1) Extend the uses of regularity model-based EDAs to MaOPs. In addition, reference vectors-based diversity mechanism is incorporated into the proposed algorithm to enhance the selection pressure.
2) Large search space poses a challenge for regularity model-based EDAs as do to all MaOEAs. To this end, dimension reduction technique is utilized in the decision space to speed up the exploration search for sampling promising solutions.
3) Convergence and diversity are considered equally important in the design of a quality MOEA or MaOEA. In the proposed algorithm, convergence is mainly treated in the first stage (the phase of dimension reduction), while diversity is focused in the second stage.
The remainder of this paper is organized as follows. Related reference vectors-based MaOEA, evolutionary algorithms based on dimension reduction, and the seminal work of regularity-based EDA are reviewed in Section II, respectively. In Section III, the framework of the proposed algorithm is outlined, and the respective steps are detailed. In addition, the complexity of the proposed algorithm are analyzed, and two crucial sub-components of the proposed algorithm as well as the principles for selecting neighbor solutions for building the model are discussed. In Section IV, a series of experiments are performed over widely used test suites against chosen peer competitors and the results measured by the selected performance metrics are statistically compared, in addition to experiments on investigating the effect of neighbor size, diversity repairing, and dimension reduction. Finally, conclusion and future work are given in Section V.

## II. Related Works

Because the proposed algorithm are mainly concerned on the reference vectors, dimension reduction, and regularitybased EDA model, works related to these aspects are discussed. To be specific, state-of-the-art MaOEA, NSGA-III which is based on the reference vectors, is introduced first. Then the evolutionary algorithms employing dimension reduction are reviewed. Next, RM-MEDA which is a regularity-based EDA is analyzed. Finally, the disadvantages of RM-MEDA for solving MaOPs, and the differences to the proposed algorithm are highlighted.

## A. NSGA-III

The major difference of NSGA-III compared to its predecessor is the diversity improving mechanism which is performed by the reference vectors, and it begins to take effect when $j$ solutions need to be chosen from the non-dominated sorting front $F_{i}$ where $i>0,0<j<\left|F_{i}\right|$, and $|\cdot|$ is a cardinality operator. To be specific, each reference vector is assigned to solutions in $F_{k}$ where $0<k<i-1$ by calculating which solution has the nearest perpendicular distance to it. Then the reference vector $v$ who is assigned the smallest number of solutions is marked and the solution $p$ in $F_{i}$ that has the smallest perpendicular distance to $v$ is selected. Next, solution $p$ is removed from $F_{i}$ and the assigned number of $v$ is increased by 1 . These steps are iterated until $j$ solutions are selected from $F_{i}$. Because these reference vectors are uniformly distributed, the selected solutions are hopefully evenly distributed in the objective space. Specifically, the reference vectors are uniformly generated in the $R_{n}^{M}$ space, and the sum of elements in one reference vector is equal to 1. However, the problem to be optimized is not necessary in this unit hyperplane. For this purpose, all the objectives are to be normalized priori to calculating the perpendicular distances. This mechanism of uniformly distributed reference vector assisting to select solutions is more and more preferred by MaOEAs due to its explicitly diversity preserving nature.

## B. Evolutionary Algorithms Based on Dimension Reduction

The notion that state-of-the-art MOEAs are capable of solving the MOPs naturally leads to an intent of reducing the number of objective in a MaOP (i.e., dimension reduction in the objective space), and then applying these powerful MOEAs in solving them. Specifically, dimension reduction in the objective space refers to removing the redundant objectives, while the same solutions are obtained as with all objectives involved [39]. With the utilization of dimension reduction, the computational complexity is reduced due to a smaller number of objectives. In summary, the dimension reduction schemes considered in literatures can be sorted into two categories. The first category is often known as the correlation-based methods, such as the works in [40]-[42]. In addition, the correntropy principal component analysis (C-PCA), maximum variance unfolding PCA (MVU-PCA) [43], and PCA-based algorithm [44] were proposed to analyze the correlation between the solutions generated in each generation, while Saxena et al. [45] proposed the linear PCA and nonlinear MVC over a set of Paretooptimal solutions to check the correlation. In addition, Guo et al. [46] employed the interdependence factors to identify the correlation for dimension reduction. Recently, Wang and Yao proposed a novel approach to reduce the objective dimension by measuring the linear and nonlinear correlation between objectives using nonlinear correlation information entropy [47]. The second category covers the algorithms which employ the dimension reduction based on dominance structure, such as the work [41] in which the $\epsilon$-dominance was employed to identify the redundant objectives. In addition, the Pareto Corner Search Evolutionary Algorithm [48] utilized the corner sorting technique to find the corner solutions in which the di-

mension reduction was performed. In summary, the dimension reduction techniques in these algorithms are performed in the objective space, while the proposed improved regularity-based MaOEDA builds its model in the decision space. Moreover, it is common that the number of decision variables is much greater than that of the objectives. As a consequence, it is reasonable to reduce the dimension of the decision space for the purpose of computational efficiency and effectiveness.

## C. RM-MEDA

The probability model of RM-MEDA is built based on the regularity of decision space. To be specific, all the solutions are clustered into multiple disjoint groups by local PCA [49] first, and then models are constructed to generate new solutions for each group. Specifically, local PCA is employed for manifold dimension reduction by performing multiple PCA operations in each piecewise linear segment over the entire given data. Compared to PCA, local PCA is considered better to collectively capture the global structure. For intuitively comparing the effects of local PCA and PCA, an example is plotted in Fig. 1 in which it is clearly shown that the local PCA works better in estimating the entire structure of the data. Supposed that $\lambda_{1}, \cdots, \lambda_{N}$ are the eigenvalues of the covariance matrix of one group, $\lambda_{1} \geq \cdots \geq \lambda_{N}$, and the corresponding eigenvectors are $v_{1}, \cdots, v_{N}$. Then the model is formulated with the assumption that the PS is a $(M-1)$ dimensional piecewise manifold in a continuous problem. In further, solutions are sampled from this model with the number which is proportional to the volume of the model to that of all the models. Combined with non-dominated sorting, a set of solutions with diversity approximating the PF are generated.

EDAs generate new solutions with probabilistic model to reject the detrimental consequence lead by genetic operator. Especially, regularity-based model is much preferable than most Bayesian network-based models in EDAs because of the simplicity yet effectiveness [50]. For example, EDAs based on Bayesian network need the procedure of training while regularity-based methods are analytic. However, because RMMEDA is originally developed for solving MOPs with variables linkage [20], it may not be suitable for solving MaOPs. For example, the diversity in RM-MEDA is maintained by uniformly sampling new solutions in the decision space and this cannot give rise to the corresponding diversity in the objective space especially in MaOPs such as all the test problems in DTLZ [51]. Furthermore, local PCA makes sense when the PS is in full rank in the decision space, which is not necessary the case in MaOPs. To this end, an improved regularity-based EDA for effectively addressing MaOPs is proposed in this paper. In addition, a new diversity facilitating mechanism which employs the uniformly distributed reference vectors is also incorporated to improve the selection pressure. Furthermore, the dimension reduction technique based on the correlation scheme is utilized in the decision space over a set of Pareto-optimal solutions to save the computational complexity. Compared to RM-MEDA, the main contributions of the proposed algorithm are listed as follows.

1) The diversity improving mechanism of the proposed algorithm is compatible to most problems while RM-

MEDA is only suitable to the problems in which the PS has the same image of PF.
2) The PS in RM-MEDA is not allowed to lie on any subspaces of the decision space, while the proposed algorithm has no such a requirement of it.
3) RM-MEDA is implemented with the manifold assumption that the PS is a piece-wise manifold with $(M-1)$ dimension. In this paper, the proposed algorithm is developed without such an assumption.

## III. PROPOSED Algorithm

In this section, the framework of the proposed algorithm, i.e., improved regularity model-based EDA for many-objective optimization (MaOEDA-IR), is given first. Then the details of each step in the framework are presented. This is followed by the analysis of the computational complexity. Finally, significant sub-components of the proposed algorithm, and the principles of selecting neighbor solutions are discussed, respectively. It is noted here that the proposed algorithm is given in the context of problem described by Equation (1).

## A. Framework of the Proposed Algorithm

The proposed algorithm begins with reducing the dimension of the decision space (Subsection III-B). Then new promising solutions are generated in the reduced decision space to speed up the evolutionary progress and lower the computational cost of exploitation search in the proposed algorithm, and their fitness are evaluated (Subsection III-C). Next, a set of generated reference vectors in $R_{+}^{M}$ (it refers to the sub-part of $R^{M}$, where all points in this sub-part are with element values no less than 0 ) is mapped (Subsection III-D), and regularity-based model is built (Subsection III-E) for repairing the diversity of the proposed algorithm (Subsection III-F) in which a set of solutions $R_{t}$ are generated. Based on $R_{t}$, new offspring are generated (Subsection III-G). With the help of environmental selection operator (Subsection III-H), a set of solutions with a better quality in convergence and diversity are obtained. Repairing the diversity, model updating, sampling new solutions, and environmental selection are performed one by one in a limit number of generations. At last, final selection is utilized to choose the representative solutions for the available slots (Subsection III-H). In addition, maximum generation number, a set of uniformly distributed reference vectors, neighbor size, and respective threshold for dimension reduction and model building need to be made available prior to the proposed algorithm running. In summary, the framework of the proposed algorithm is listed in Algorithm 2.

## B. Reducing the Dimension of Decision Space

Dimension reduction technique is used to reduce the volume of exploration space to speed up the search of sampling new solutions. Ideally, the subspace of PS is desirable. To this end, a set of Pareto-optimal solutions is suitable to be the training data and then exploitation is performed in the subspace. Noted here that, the training data only require the solutions with convergence and the diversity is not necessary. Consequently,

![img-0.jpeg](img-0.jpeg)
(a)
![img-1.jpeg](img-1.jpeg)
(b)

Fig. 1. An example comparison of local PCA and PCA on the same data. Specifically, Fig. 1a denotes the utilization of local PCA, while Fig. 1b denotes that of PCA. The solid lines on both figures denote the main directions of the principal components. Obviously, local PCA is better to capture the global structure of the give data.

## Algorithm 2: Main Framework of MaOEDA-IR

Input: the maximum number of generations $t_{\max }$, a set of unit reference vectors $\mathbf{r}_{0}=\left\{r_{0,1}, \cdots, r_{0, N}\right\}$, neighbor size $T$, dimension reduction threshold $\alpha$, regularity-based model threshold $\beta$;
Output: final population $P$;
1 Reduce the dimension of decision space from $R^{n}$ to $R^{k}$;
2 Create population $P_{0}$ with $T N$ individuals from $R^{k}$;
3 Evaluate the fitness of $P_{0}$;
4 Map reference vectors $\mathbf{r}_{0}$ to $\mathbf{v}_{0}=\left\{v_{0,1}, \cdots, v_{0, N}\right\}$;
5 Build the regularity-based model $\Phi$;
$6 t \leftarrow 0$;
7 while $t<t_{\max }$ do
$R_{t} \leftarrow$ Repair the diversity;
$S \leftarrow$ Non-dominated selection from $P_{t} \cup R_{t}$;
Map reference vectors $\mathbf{v}_{t-1}$ to $\mathbf{v}_{t}=\left\{v_{t, 1} \cdots, v_{t, N}\right\}$;
$Q_{t} \leftarrow$ Generate offspring;
$S \leftarrow$ Non-dominated selection from $P_{t} \cup R_{t} \cup Q_{t}$;
Update reference vectors $\mathbf{v}_{t}=\left\{v_{t, 1} \cdots, v_{t, N}\right\}$;
$P_{t+1} \leftarrow$ Environmental selection from $Q_{t} \cup R_{t} \cup P_{t}$;
$t \leftarrow t+1$;
8 end
$P \leftarrow$ Final selection from $P_{t_{\max }}$;
18 Return $P$.
algorithms such as the conventional weighted aggregation method [52] for problems with convex PF, and the evolutionary dynamic weighted aggregation methods [53] for problems with non-convex PF are suitable for this. In this paper, the Pareto Corner Search Evolutionary Algorithm (PCSEA) [48] is employed because 1) a set of corner Pareto-optimal solutions which can be used as the training data as well as the extreme points (extreme points are employed for building the regularitybased model in Subsection III-E) are obtained simultaneously when the PCSEA completes, 2) the source code is available, and 3) the computational cost is promising compared to the algorithms selected for generating the extreme points and the training data. Moreover, the PCSEA has been successfully employed to generate solutions for dimension reduction in the objective space in its seminal paper. In each generation of PCSEA, $2 M$ lists are increasingly ordered, where $M$ is the number of objectives. Specifically, the first $M$ lists are about the $M$ objectives, while the last $M$ lists are with the exclusive $L_{2}$ norm square. Especially, the exclusive $L_{2}$ norm square of
the $i$-th objective is with the form $\sum_{j=1, j \neq i}^{M} f_{i}^{2}$. By assigning the solutions in the top of the lists with a smaller rank value, the corner solutions are highlighted (more details can be found in [48]). When the evolution process of PCSEA is completed, the Pareto-optimal solutions, which are denoted by $S_{p}$, are selected from the population.

For convenience of the development, a matrix $X$ is used to represent $S_{p}$. Specifically, each row in $X$ denotes one solution while the columns refer to the different dimension of decision variables. Then the process of dimension reduction is illustrated in Algorithm 3 in which it can be divided into two parts. The first part is to employ the principal component analysis (PCA) to find the projected space in which $X$ keeps its $(\alpha \times 100 \%)$ information where $\alpha$ is a predefined threshold (lines 1-3). After the transformed data is projected back to its original space (in line 4), the values at reduced dimensions become 0 . Thereafter, the index of the reduced dimensions $I$ are selected, which are implemented in lines 5-11. Then, $I$ is saved together with the mean value $\mu$ of $X$ for evaluating the fitness of solutions generated in the reduced decision variable space.

```
Algorithm 3: Dimension Reduction
    Input: the matrix \(X\) representing \(S_{p}\), threshold \(\alpha\);
    Output: the index of the removed dimension, the mean
        value of \(X\);
    \(\mu \leftarrow\) Compute the column mean value of \(X\);
    \(X^{\prime} \leftarrow\) Subtract \(\mu\) from \(X\);
    \(U \leftarrow\) Select principal components with threshold \(\alpha\);
    \(X \leftarrow U^{\prime} U X^{\prime} ;\)
    \(I \leftarrow \emptyset ; V \leftarrow \emptyset ;\)
    for \(j \leftarrow 1\) to \(n\) do
        \(s \leftarrow\) compute the \(j\)-th column sum of \(\tilde{X}\);
        if \(s==0\) then
            \(I \leftarrow I \cup j ;\)
        end
    end
12}\mathrm{ Return \(I, \mu\).
```

In particular, the main reason for not using $U$ but the original space is for reducing the computational complexity. Specifically, $U$ is not the original decision space, and the solutions sampled from $U$ cannot be directly used for fitness evaluation. There are two ways to solve this problem. The first one is to sample solutions from $U$ and then transform

solutions into the original space when they are evaluated for the fitness. The other one is to transform $U$ to the original space in advance, and then sample solutions from the original space. If we use the first method, the transformation operation needs to be performed for each solution in each generation. However, if we use the second method, we only need to do this transformation once. Obviously, the second method is with less computational complexity. Next, we will explain the mechanism of lines 5-11 in Algorithm 3.
By using PCA, the generated principal space $U$ is with less number of dimensions compared to that of the original space. When $U$ is transformed to the original space, the values of the dimensions reduced by PCA are all zeros. Therefore, we use lines 5-11 in Algorithm 3 to find these dimensions by checking where their values are zeros. Once we find these dimensions, we remove them and store their indexes. In Algorithm 3 we use $I$ to denote the space without the reduced dimensions, and sample solutions from $I$. When these solutions are used for fitness evaluation, just padding their corresponding mean value (stored in $\mu$ ) to the corresponding dimension based on the stored index, and using them to do fitness evaluation.
In most PCA-based methods, none of solutions sampled from the reduced space needs to be operated in the original space. However, in the proposed design, the solutions must be transformed back to the original space for fitness evaluation. Naturally, the dimension reduction technique used here is quite different from a general PCA-based method.

## C. Creating Population and Evaluating Fitness

Assuming the dimension of the reduced decision variable space is $k$ (obviously $k=n-|I|$ ). Based on the design principles of the proposed algorithm, each final solution of the proposed algorithm is desirable to be associated with one reference vector, and the model is built on the neighbor solutions of one particular reference vector. Therefore, the population is randomly initialized in $R^{k}$ with size $T N$. In order to evaluate the fitness, the created population needs to be translated back to $R^{n}$, which is demonstrated by Algorithm 4. Specifically, the translated population $P_{0}^{\prime}$ has the same number to that of the initialized population $P_{0}$, and each individual in $P_{0}^{\prime}$ is with $n$-dimension. Furthermore, the values in reduced dimension are equal to that of the elements in the mean value vector $\mu$. To this end, $P_{0}^{\prime}$ is initialized in $R^{T N \times n}$ (line 1) with zeros first. Then, the element values in each row of $P_{0}^{\prime}$ is set to be as $\mu$ (line 2). Finally, each column of $P_{0}$ is added to the corresponding column of $P_{0}^{\prime}$, which is implemented by lines 4 7. Once the translation is performed, the fitness is evaluated by introducing the population to the problem to be optimized.

## D. Mapping Reference Vectors

Conventionally, Das and Dennis's method [54] is employed to generate the uniformly distributed reference vectors $\mathbf{r}_{0}$ which is constructed in $R_{+}^{M}$. However, the PF of the problem to be optimized does not necessary span the entire $R_{+}^{M}$ space. In order to keep the same number of reference vectors in the PF, $\mathbf{r}_{0}$ needs to be mapped. For illustrating this motivation,

## Algorithm 4: Translate the Population

Input: population $P_{0} \in R^{k}$, index of reduced dimension $I$, mean value $\mu$;
Output: population $P_{0}^{\prime} \in R^{n}$;
1 Initialize a matrix $P_{0}^{\prime} \in R^{T N \times n}$ with zeros;
2 Copy $\mu$ to each row of $P_{0}^{\prime}$;
$3 l \leftarrow 1$;
4 for $i \leftarrow 1$ to $n$ and $i$ not in $I$ do
5 Add the $l$-th column of $P_{0}$ to the $i$-th column of $P_{0}^{\prime}$;
$6 \quad l \leftarrow l+1$;
7 end
8 Return $P_{0}^{\prime}$.

## Algorithm 5: Mapping Reference Vectors

Input: reference vectors $\mathbf{r}_{0}$, data $S_{p}$ from Algorithm 3;
Output: mapped reference vector $\mathbf{v}_{0}$;
$1 F \leftarrow$ Calculate the objective values of $S_{p}$;
$2 \mathbf{z}^{\mathbf{u}} \leftarrow \emptyset$;
for $i \leftarrow 1$ to $M$ do
$4 \quad \mathbf{v} \leftarrow[0, \cdots, 0]$;
for each $f$ in $F$ do
if $f_{i}>v_{i}$ then
$v \leftarrow f$;
end
end
$\mathbf{z}^{\mathbf{u}} \leftarrow \mathbf{z}^{\mathbf{u}} \cup \mathbf{v}$;
end
$12 \mathbf{z}^{*} \leftarrow \emptyset ;$
for each $f$ in $F$ do
$14 \quad v \leftarrow+\infty$;
for $i \leftarrow 1$ to $M$ do
if $f_{i}<v$ then
$v \leftarrow f_{i}$;
end
end
end
21 end
22 Update each $z$ in $\mathbf{z}^{\mathbf{u}}$ by $z \leftarrow z-\mathbf{z}^{*}$;
$23 \mathbf{a}=\left[a_{1} \cdots, a_{M}\right] \leftarrow$ Find the intercepts of the hyperplane
constructed by $\mathbf{z}^{\mathbf{u}}$;
for $i \leftarrow 1$ to $N$ do
$25 \quad \begin{aligned} & v_{0, i} \leftarrow=r_{0, i} \times \mathbf{a}+\mathbf{z}^{*} \\ & \text { end }\end{aligned}$
27 Return $\mathbf{v}_{0}=\left\{v_{0,1}, \cdots, v_{0, N}\right\}$.
an example in the 2-dimensional space is plotted in Fig. 2 in which nine blue lines denote the reference vectors and line $A B$ denotes the PF. Specifically, Fig. 2a describes the reference vectors generated by Das and Dennis's method and only four reference vectors intersect $A B$, while Fig. 2b depicts the nine reference vectors intersect $A B$ after they are mapped. Because the final solutions in the proposed algorithm are selected by their corresponding reference vector, it is obvious that the design in Fig. 2b is preferred. In addition, Algorithm 5 presents the details of mapping the reference vectors which

can be divided into four steps. First, the objective values of the training data for dimension reduction in Algorithm 3 and the extreme points (denoted by $\mathbf{z}^{\mathbf{n}}$ ) are calculated (lines 1-11). Second, the ideal point is derived by selecting the minimum values in all objectives, which are implemented in lines 1221. Noted here that, this approach to calculate the ideal point is also utilized in [55]-[59]. Third, the extreme points are updated by subtracting the ideal point and the intercepts are calculated (lines 22-23) by solving the equation ${ }^{3}$

## E. Building the Regularity-based Model

Generally, the regularity-based model is composed of multiple sub-models due to the complexity of regularity on which a unified model is difficult to exactly capture the global intrinsic relation [20], [61]. In this proposed algorithm, each submodel is built based on one reference vector with its neighbor solutions and Algorithm 6 presents the details.

```
Algorithm 6: Build the \(j\)-th Regularity-based Sub-model
    Input: neighbor solutions \(S_{n}\) of the \(j\)-th reference vector,
        threshold \(\beta\), enlargement factor \(\gamma\);
    Output: model \(\Phi\);
    1 Let matrix \(X\) denote \(S_{n}\);
    \(2 \mu \leftarrow\) Compute the column mean value of \(X\);
    \(3 X^{\prime} \leftarrow\) Subtract \(\mu\) from each rom in \(X\);
    \(4\left[\lambda_{1}, \cdots, \lambda_{M}\right],\left[v_{1}, \cdots, v_{M}\right] \leftarrow\) Eigen-factorize the
        covariance matrix of \(X\) and descend the eigenvalues
        and eigenvectors;
    \(5 i \leftarrow 0\)
    6 while \(\left(\lambda_{1}+\cdots+\lambda_{i}\right) /\left(\lambda_{1}+\cdots+\lambda_{M}\right)<\beta\) do
    \(7 \quad i \leftarrow i+1\)
    8 end
    \(9 y \leftarrow X^{\prime} \times\left[v_{1}, \cdots, v_{i}\right]\)
    \(10 l, u \leftarrow\) Find the minimum and the maximum values in
        each row of \(y\)
    \(11 \Omega \leftarrow\left\{\mu+\sum_{j=1}^{i} \tau v_{i}, l-\gamma(u-l) \leq \tau \leq u+\gamma(u-l)\right\}\)
    \(12 \epsilon \leftarrow \frac{1}{M-i+1} \sum_{j=i}^{M} \lambda_{j}\)
    13 Return \(\Phi=\Omega+\epsilon\)
```

To be specific, given the neighbor solutions $S_{n}$ of the $j$-th reference vector, the threshold $\beta$, and the enlargement factor $\gamma$. The steps of building the $j$-th sub-model are illustrated as follows. First, $S_{n}$ is represented by the matrix $X$ and centered (lines 1-3). Then the eigenvalues as well as the eigenvectors of $X$ are obtained and ordered based on descending the eigenvalues (line 4). Lines 5-8 demonstrates the search of principal components on which the centered data is projected (line 9). Next, the projected space is constrained by find the minimum and the maximum values of projection (line 10),

[^0]and the latent space for generating new offspring is obtained with the enlargement factor and the principle components (line 11). Finally, the noise for the latent space is computed from the mean values of the eigenvalues regarding the nonprinciple components (line 12), and the regularity-based model is obtained.

## F. Repairing the Diversity

Diversity repairing is employed by sampling new solutions to mitigate the adverse of the phenomenon that reference vectors lack associated solutions. To this end, all the nondominated individuals in the current population are enumerated to find their respective nearest reference vectors first, which is implemented by lines 2-5. Then lines 8-10 demonstrate the selection of neighbor solutions. In addition, the model building and new solution sampling are presented in line 11 and lines 12-14, respectively. Noted in the phase of selecting neighbor solutions for reference vector $v_{t, i}$, its neighbor solutions are from the current population $S_{t}$ and the non-dominated solutions in $S_{t}$, the motivation of which is discussed in Subsection III-J. In addition, it is obvious that the size of the neighbor solutions is not necessary with $T$. I.e., it is with $T+1$ when the selected non-dominated neighbor solution has been included in line 8 . The reasons that the neighbor solution size are not strictly kept with $T$ are that 1) it does not affect the built model and 2) removing the extra solution gives more computational cost.

```
Algorithm 7: Repair the Diversity
    Input: current population \(P_{t}\), reference vectors \(\mathbf{v}_{t}\),
        neighbour size \(T\);
    Output: new solutions \(R_{t}\);
    \(1 S=\left\{s_{1}, \cdots, s_{k}\right\} \leftarrow\) Non-dominated selection from \(P_{t}\);
    \(2 I \leftarrow \emptyset\);
    3 for each \(s\) in \(S\) do
    \(4 \quad I \leftarrow I \cup \arg \min _{i}\left\|s-v_{t, i} s v_{t, i}^{T} /\left(v_{t, i} v_{t, i}^{T}\right)\right\|\)
    5 end
    \(6 R_{t} \leftarrow \emptyset\);
    7 for \(i \leftarrow 1\) to \(N\) and \(i\) not in \(I\) do
        neighbour \((i) \leftarrow\) Select \(T\) solutions from current
        population who have the smallest perpendicular
        distances to \(v_{t, i}\);
        \(j \leftarrow \arg \min _{j}\left\|s_{j}-v_{t, i} s_{j} v_{t, i}^{T} /\left(v_{t, i} v_{t, i}^{T}\right)\right\|\);
        neighbour \((i) \leftarrow \operatorname{neighbour}(i) \cup s_{j}\);
        Build model \(\Phi_{i}=\Omega_{i}+\epsilon_{i}\) with neighbour \((i)\);
        \(A \leftarrow\) Uniformly sample \(T\) points in
        \([l-\gamma(u-l), u+\gamma(u-l)]\) from \(\Omega_{i}\);
        \(B \leftarrow\) Sample points from the normal distribution
        with mean 0 and standard derivation \(\sqrt{\epsilon_{i}}\);
        \(R \leftarrow(A+B)\);
        \(R_{t} \leftarrow R_{t} \cup R\)
    16 end
    17 Return \(R_{t}\).
```


[^0]:    ${ }^{3}$ Repetitive points existing in $\mathbf{z}^{\mathbf{n}}$ will lead multiple solutions to this equation. To avoid this, the trick introduced in [60] is employed. $\mathbf{z}^{\mathbf{n}} \times(1 / a)=I$ where $I$ is an identity vector. Fourth, the reference vectors uniformly generated in $R_{t, i}^{M}$ are mapped by lines $24-26$ as mapped reference vectors $\mathbf{v}_{0}$. Noting that, the last two steps are necessary. Otherwise, only the origin of the coordinate system has been moved to the idea point, and there is still no sufficient numbers of reference vectors intersecting with line $A B$ (see Fig. 2c).

![img-2.jpeg](img-2.jpeg)

Fig. 2. A 2-dimensional example to illustrate the motivation of mapping the reference vectors. Specifically, there are nine reference vectors (blue lines) generated by the Das and Dennis's method, and line $A B$ denotes the Pareto front. The reference vectors without mapping are plotted in Fig. 2a in which only four reference vectors intersect $A B$, while the reference vectors which have been mapped are plotted in Fig. 2b in which all the sampled reference vectors intersect $A B$ (i.e., the situation that Algorithm 5 have been performed). Fig. 2c denotes the situation that only lines 1-22 in Algorithm 5 have been performed.

## G. Generating Offspring

After repairing the diversity, the solution set $R_{t}$ is generated. Then, non-dominated solutions in $S$ are selected from $R_{t}$ and the current population $P_{t}$ (line 9 of Algorithm 2). Next, reference vectors $\mathbf{v}_{t}=\left\{v_{t, 1}, \cdots, v_{t, N}\right\}$ are mapped (line 10 of Algorithm 2), which is performed by Algorithm 5 with the input parameters $\mathbf{v}_{t-1}$ and $S$. Finally, offspring $Q_{t}$ are generated by Algorithm 8. In summary, generating offspring can be viewed as the diversity repairing in each reference vector, which could be investigated through the analogy between lines 4-10 of Algorithm 8 and lines 8-14 of Algorithm 7. However, the motivation of generating offspring is different to that of diversity repairing, which will be discussed in Subsection III-J. In addition, updating the reference vectors is motivated by achieving a better performance of the proposed algorithm, although it has been reported that PCSEA is capable of finding the approximated corner solutions from which the extreme points are derived [48].

```
Algorithm 8: Generate Offspring
    Input: non-dominated solutions \(S=\left\{s_{1}, \cdots, s_{k}\right\}\),
    1 neighbour size \(T\), reference vectors \(\mathbf{v}_{t}\);
        Output: offspring \(Q_{t}\);
    \(Q_{t} \leftarrow \emptyset\);
for \(i \leftarrow 1\) to \(N\) do
    4}\left|\begin{array}{l}\text { neighbour }(i) \leftarrow \text { Select } T \text { solutions from } S \text { who have } \\ \text { the smallest perpendicular distances to } v_{t, i} ; \\ j \leftarrow \arg \min _{j}\left|\left|s_{j}-v_{t, i} s_{j} v_{t, i}^{T} /\left(v_{t, i} v_{t, i}^{T}\right)\right|\right| ; \\ \text { neighbour }(i) \leftarrow \text { neighbour }(i) \cup s_{j} ; \\ \text { Build model } \Phi_{i}=\Omega_{i}+\epsilon_{i} \text { with neighbour }(i) ; \\ A \leftarrow \text { Uniformly sample } T \text { points in } \\ {[l-\gamma(u-l), u+\gamma(u-l)] \text { from } \Omega_{i} ;} \\ B \leftarrow \text { Sample points from the normal distribution } \\ \text { with mean } 0 \text { and standard derivation } \sqrt{\epsilon_{i}} ; \\ Q \leftarrow(A+B) ; \\ Q_{t} \leftarrow Q_{t} \cup Q ;\end{array}\right. \\ \text { end } \\
\hline 13 Return \(Q_{t}\).
```


## H. Environmental Selection and Final Selection

After offspring $Q_{t}$ are generated, non-dominated solutions $S$ are selected from the current population, i.e., $P_{t} \cup R_{t} \cup Q_{t}$ (line 12 of Algorithm 2). Then, reference vectors $\mathbf{v}_{t}=$ $\left\{v_{t, 1}, \cdots, v_{t, N}\right\}$ are updated (line 13 of Algorithm 2) by performing Algorithm 5 with the input parameters $S$ and itself. Next, the environmental selection is performed. Specifically, the environmental selection aims at removing ill-fit solutions from the current population and maintaining a limit size of representative individuals to reduce the cost of computation in each generation. The final selection is to choose the best-fit solutions. Moreover, both selections are dependent in the proposed algorithm. In the framework of the proposed algorithm, it is assumed that $N$ solutions are needed by the decision-maker when the algorithm is finished. Because the proposed algorithm is based on the statistics of regularity of the $T$ neighbors regarding each solution in $N$. As a consequence, the number of solutions for building the model should be $T N$. In addition, extra solutions are incorporated from the phases of diversity repairing and offspring generating in each generation. To this end, the purpose of environmental selection is for maintaining a size of population with the same number of initialized population while the final selection is for selecting $N$ solutions. It is hopeful that each reference point has $T$ neighbor solutions after the environmental selection which is described by Algorithm 9. Specifically, final selection is implemented if the number $T$ is replaced by 1 in the environmental selection.

In summary, the environmental selection covers two steps. The first step is to assign the reference vectors by selecting the non-dominated solutions which have smallest perpendicular distances to them (lines 2-6 of Algorithm 9). The other is to set $T$ solutions for each reference vector. Specifically, the nondominated sorting and truncated selection is employed when more than $T$ solutions are assigned to one reference vector (lines 7-20 of Algorithm 9), otherwise necessary solutions are selected from the current population based on the smallest perpendicular distances (lines 22-30 of Algorithm 9). Noted that, selected solutions are removed from the current popula-

## Algorithm 9: Environmental Selection

Input: non-dominated solutions $S=\left\{s_{1}, \cdots, s_{k}\right\}$,
1 neighbour size $T$, reference vectors $\mathbf{v}_{t}$; Output: $P_{t+1}$;
$2 L_{1}, \cdots, L_{N} \leftarrow \emptyset$;
for each $s$ in $S$ do
$i \leftarrow \arg \min _{i}\left\|s-v_{t, i} s v_{t, i}^{T} /\left(v_{t, i} v_{t, i}^{T}\right)\right\|$;
$L_{i} \leftarrow L_{i} \cup s$;
end
for $i \leftarrow 1$ to $N$ do
if $\left|L_{i}\right|>T$ then
$\left\{F_{1}, \cdots, F_{l}\right\} \leftarrow$ Non-dominated sorting solutions in $L_{i}$;
$k \leftarrow 1, L_{i} \leftarrow \emptyset$;
while $\left|L_{i}\right|+\left|F_{k}\right| \leq T$ do
$L_{i} \leftarrow L_{i} \cup F_{k}$
$k \leftarrow k+1$;
end
if $\left|L_{i}\right|<T$ then
$D \leftarrow$ Select $T-\left|L_{i}\right|$ solutions from $F_{k}$ who have the smallest perpendicular distances to $v_{t, i}$
$L_{i} \leftarrow L_{i} \cup D$
end
end
end
From current population removing solutions in $L_{i} \cup \cdots \cup L_{N}$
for $i \leftarrow 1$ to $N$ do
if $\left|L_{i}\right|<T$ then
$D \leftarrow$ Select $T-\left|L_{i}\right|$ solutions from current population who have the smallest perpendicular distances to $v_{t, i}$
$L_{i} \leftarrow L_{i} \cup D$
else
$D \leftarrow$ Select $T$ solutions from $L_{i}$ who have the smallest perpendicular distances to $v_{t, i}$;
end
From current population removing solutions in $D$;
end
Return $Q_{t}=L_{i} \cup \cdots \cup L_{N}$.
tion to avoid repetitions being re-selected (lines 21 and 29 of Algorithm 9).

## I. Computational Complexity

Complexity of the proposed algorithm is analyzed with the context of problem defined by Equation (1). For convenience, the number of solutions for PCSEA to generate the training data for dimension reduction is set to be same to that in the final selection of the proposed algorithm. Consequently, the computational times of PCSEA is $O(M N \log (N))$. For the dimension reduction, the most cost is for computing the eigenvalues and eigenvectors which has $O\left((T N)^{3}\right)$ computations. In summary, the computational times of dimension reduction is $O\left((T N)^{3}\right)$. I addition, the creating population and fitness evaluation need $O(T N k)$ and $O(T N M)$ computation
times, respectively. The computational complexity of mapping reference vectors is $O(T N M)$. Moreover, the complexity of building the model is mainly contributed by the matrix factorization whose complexity is $O\left(T^{3}\right)$. Briefly, lines 1-5 in Algorithm 2 takes $O\left((T N)^{3}\right)$ computational times. Furthermore, the model building, non-dominated selection from $P_{t} \cup R_{t}$, non-dominated selection from $P_{t} \cup R_{t} \cup Q_{t}$ dominate the computational complexity of the diversity repairing, generating offspring, and environmental selection. As a consequence, the computational complexity of lines 8-14 is $O\left(M N^{2}\right)$ or $O\left(T^{3}\right)$ which is greater. In addition, the final selection takes $O\left(M N^{2}\right)$ computational times. Generally, the neighbor size $T$ is generally set to be a number with order of magnitude 1 , and the maximum generation is set to that of magnitude 1. In summary, the computational complexity of the proposed algorithm is $O\left((T N)^{3}\right)$, where $T$ is the neighbor size, and $N$ is the number of solutions the decision-makers require.

## J. Discussion

In the proposed algorithm, two sub-components, dimension reduction, and diversity repairing are well-designed to guarantee the performance. In this section, their design motivations are discussed first, and then the experimental verification are presented in Sections IV-F and IV-G.

First, redundancy exists in the high dimensional data, and data from a small part of the dimensions is sufficient to represent these high dimensional data, such as in the feature selection discipline. In order to obtain these low dimensional data, dimension reduction technique is utilized. Employing these low dimensional data against the original high dimensional data can significantly benefit the utilization of data, such as lowering the computational cost, improving the precision by removing the interference from the elements in the redundant dimensions, and so on. Furthermore, a series of literatures [40]-[46], [48] have been proposed to reduce the objective number in MaOPs, thereafter state-of-the-art MOEAs can be utilized to solve them. Generally, the numbers of decision variables are greater than that of objectives in MaOPs, such as the $M$-dimensional DTLZ7 [51] with $19+M$ decision variables, and as well in MOPs, for example, 2-dimensional ZDT1 problem [62] with 30 decision variables. Actually, it is no surprise that problems are with the number of decision variables greater than that of objectives because it is difficult to determine which particular factors affect the response, and a better way for modeling is to select all the observed factors, which is always with a larger size. As a result, dimension reduction in the decision space is appropriate and well justified, specifically for the proposed algorithm which is based on the regularity of the decision space. Moreover, the Pareto-optimal solutions can be viewed as the features of the decision space in solving MaOPs, and if we obtain the subspace of the PS, subsequent operations can be constrained in this subspace to reduce the cost of exploration. Basically, this subspace is obtained by reducing the dimension on the Pareto-optimal solutions. Ideally, only the diversity is concerned if the exact subspace of the PS is obtained, which is not true in implementation. To this end, extra components are incorporated in

the proposed algorithm for improving the convergence, such as the reference vector updating, and non-dominated selection for building model, diversity repairing, generating offspring, environmental selection, and final selection.
To intuitively understand the diversity repairing mechanism in the proposed algorithm, an example of 3-objective DTLZ1 problem [51] is illustrated in Fig. 3. To be specific, twelve different markers in red are shown in Fig. 3a) identifying a set of uniformly distributed reference vectors in the objective space. The corresponding solutions (given the same markers) in the decision space with the reduced dimensions are shown in Fig. 3b) which are not necessarily uniformly distributed. For each given solution in the decision space, nine different neighbors are chosen and displayed in the same marker as shown in Fig. 3c). However, all the reference vectors are not necessarily assigned by the solutions in one generation due to the heuristic nature of evolution. This can be seen from Fig. 3d) in which blue markers denote all the solutions in this generation and the area constrained by the ellipse circle implies that there is no solution for assigning to the corresponding reference vector. To this end, diversity repairing mechanism is activated by using the neighbor solutions. The "cross" markers in Fig. 3e) are the solutions selected for generating new solutions. These solutions are then used for diversity repairing and generate solutions shown in "hollow circle" markers in Fig. 3f). Intuitively, diversity repairing is employed for improving the diversity by assigning the corresponding solution. In fact, the convergence is strengthened in the phase of diversity repairing by generating new solutions from which dominated solution is assigned to the corresponding reference vector which is short of diversity previously. In summary, both convergence and diversity are promoted by diversity repairing mechanism.
Normally, the neighbor solutions for one particular reference vector consist of one non-dominated solution and other nearest solutions to this reference vector from the current population. With this neighbor solution assignment, it is hopeful that solutions with convergence and diversity are sampled from the built model. Moreover, Fig. 4 highlights our motivation on this design. Especially, the blue line denotes the reference vector, black circles, $A, B, C, D$ and $E$ denotes the current population, rectangle area denotes the built model, and red circles denoted the sampled offspring. Fig. 4a plots the solutions which have the nearest perpendicular distances to the reference vectors, while Fig. 4b depicts the non-dominated solution included into the neighbor solutions. It is obviously that solutions with good diversity and convergence are generated in Fig. 4b.

## IV. EXPERIMENTS

To demonstrate the quality of the proposed algorithm, a series of experiments are well-designed and performed on 8 test problems, which are from two benchmark test suits, DTLZ [51] and DTLZ ${ }^{-1}$ [63], with $3-, 5-, 8-, 10-$, and 15 -objective. Since the proposed MaOEDA-IR is an EDAbased algorithm for solving MaOPs, state-of-the-art algorithms covering two categories 1) traditional MaOEAs (NSGAIII [14], MOEA/D [11], GrEA [13], and HypE [12]) and
![img-3.jpeg](img-3.jpeg)

Fig. 3. A schematic diagram of the diversity repairing mechanism. In Figs. 3b)-3c), the blue markers denote the uniformly distributed solutions in the decision variable space with the reduced dimension, and their corresponding objectives are plotted with the same color in Fig. 3a). A set of uniformly distributed reference vectors is plotted in Fig. 3a) in red color and their corresponding solutions and neighbors are plotted with the same shape in Figs. 3b) and 3c), respectively. The blue markers in Figs. 3d)-3f) denote all the solutions in the reduced dimension decision variable space in one generation. Especially, the area constrained by the ellipse circle denotes there is no solution for assigning to the corresponding reference vector. The cross markers in Fig. 3e) are the solutions selected for generating new solutions which are plotted in Fig. 3f) with solid markers to repair the diversity of the corresponding reference vector.
2) EDA-based evolutionary algorithm (MBN-EDA [21], and RM-MEDA [20]) are considered as the peer competitors to compare the performance against the proposed MaOEDA-IR.

In the following subsections, the selected benchmark test problems are introduced first. Then, the performance indicators chosen to measure the results generated by these compared algorithms are documented. Next, the parameter settings utilized by compared algorithms are declared. Finally, experiments on compared algorithms are performed and their results measured by the selected performance indicators are analyzed. In addition, empirical experiments on investigating the diversity repairing, dimension reduction, and neighbor size are performed to highlight their superiority and promote

![img-4.jpeg](img-4.jpeg)

Fig. 4. An example to highlight the quality of generated solutions that the non-dominated solution $A$ is included in the neighbor solutions (see Fig. 4b) or not (see Fig. 4a). Especially, the blue line denotes the reference vector, $A, B, C, D$ and $E$ denote the current population, rectangle area denotes the built model based on the selected neighbor solutions, and red circles are the generated solutions.
efficacy in addressing real-word problems.

## A. Benchmark Test Problems

DTLZ1-DTLZ4 problems which are from the scalable benchmark test suit DTLZ are considered as the test instances in these experiments. Specifically, each $M$-objective test problem is with $n=M+k-1$ decision variables where $k$ is specified as 5 for DTLZ1 and 10 for DTLZ2-DTLZ4. Furthermore, the Pareto-optimal solutions of DTLZ1-DTLZ4 in the normalized $M$-dimensional objective space have the form formulated by Equation 2.

$$
\sum_{i=1}^{M} f_{i}(x)^{p}=1
$$

where $p=1$ for DTLZ1 and $p=2$ for DTLZ2-DTLZ4. Because the employed reference vectors $r_{0, i}=\left[r_{0, i}^{1}, \cdots, r_{0, i}^{M}\right]$ generated by the systematical Das and Dennis's method in the proposed algorithm are with the form $\sum_{j=1}^{M} r_{0, i}^{j}=1$ that is similar to Equation 2, DTLZ test problems are considered less challengeable. To this end, the DTLZ1 ${ }^{-1}$-DTLZ4 ${ }^{-1}$ problems from the DTLZ ${ }^{-1}$ test suit, which is a variant of the DTLZ by multiplying the negative sign to each test problem in DTLZ, are included into the considered benchmark test problems for their more complicated PF shapes which especially challenges algorithms based on reference vectors.

## B. Performance Metrics

Two widely used performance metrics, Inverted Generational Distance (IGD) [64] and Hypervolume (HV) [24] which can simultaneously quantify the performance in convergence
and diversity of the algorithms, are adopted in these experiments. The results generated by these compared algorithms are normalized to $[0,1]$ priori to employing the performance indicators, which is in the same manner [65]. In addition, 100, 000 reference points are uniformly sampled from Equation 2 for the calculation of IGD, and $[1.1, \cdots, 1.1]$ is specified as the reference point for the calculation of HV. Furthermore, Monte Carlo simulation [12] is applied for the calculation of HV when $M \geq 10$, otherwise the exact approach proposed in [66] is utilized due to the computation cost dramatically increasing as the number of objectives grows.

## C. Parameter Settings

In this subsection, the parameter settings are presented. First, the general settings for most compared algorithms are listed. Thereafter, special settings for partial algorithms are specified.

1) Crossover and Mutation: SBX [67] and polynomial mutations [68] are employed as the crossover operator and mutation operator, respectively. Furthermore, the probabilities for SBX and polynomial mutation, and the crossover distribution index are set to be $1.0,1 / n$ ( $n$ is the number of decision variables), and 20 , respectively. In addition, the distribution index of NSGA-III is set to be 30 according to the suggestions in [14] while others are set to be 20 .
2) Population Size: The population size can be set arbitrarily for executing experiments. However, reference vectors assisted algorithms, such as NSGA-III, require the same number of the population size to that of reference vectors, other peer algorithms adopt the same population size for a fair comparison. Furthermore, only boundary reference vectors are generated when the division numbers is less than $M$ in the phase of sampling of reference vectors. To this end, the two-layer approach [14] is employed for generating the reference vectors. In addition, the implementation of GrEA and NSGA-III require the population size to be a multiple of 4. In summary, the settings for reference vector and population size are listed in Table I.

TABLE I
SETTINGS FOR REFERENCE VECTORS AND POPULATION SIZE.


3) Special Settings: The grid sizes of GrEA varying in $\{6,7,8,9,10,11,12\}$ are tested individually and the best scores selected based on the corresponding performance indicators are picked up for comparisons. Because RM-MEDA is originally designed for MOEAs, the default configurations are not suitable for solving MaOPs in this experiments. Consequently, the parameter setting in RM-MEDA is slightly modified for maximizing its performance to deal with MaOPs. Specifically, the number of clusters in local PCA varies in $\{10,20,30,50\}$; the maximum iterations of local PCA with

$\{50,100\}$ are tested individually with the maximum generations at 500 for selecting the best mean value indicated by performance metrics, while the parameters settings in MBNEDA are set based on the developers' suggestion in [21]. In addition, both the thresholds for dimension reduction and model building are set to be 0.96 according to the convention of the community. Furthermore the neighbor size is specified as 25 for the investigation in Subsection IV-E, and the enlargement factor is set to be 0.5 . In the settings of the proposed algorithm, the generation number of PCSEA for obtaining training data is specified as 50 , and the population size is set to be 100 .

The proposed MaOEDA-IR is based on the training data obtained by PCSEA. For a fair comparison, the termination criterion of MaOEDA-IR should be set to the total function evaluation number of the peer competitors minus that of PCSEA. Table II ${ }^{4}$ shows these particular settings for each considered number of objective.

TABLE II
THE SETTINGS FOR THE MAXIMAL FUNCTION EVALUATION NUMBERS.

## D. Performance on DTLZ and DTLZ ${ }^{-1}$

The HV results of the proposed MaOEDA-IR against its peer competitors (NSGA-III, MOEA/D, GrEA, HypE, MBNEDA, and RM-MEDA) on DTLZ1-DTLZ4 and DTLZ1- DTLZ4- test problems with 3-, 5-, 8-, 10-, and 15-objective are presented in Table III. Furthermore, each compared algorithm is independently performed 30 runs and the best median HV results are highlighted in bold face. Moreover, the Mann-Whitney-Wilcoxon rank-sum test [69] with a $5 \%$ significance level is used to conduct the HV results due to the heuristic nature of peer algorithms, and the symbols " + ," " $\infty$," and "-" denote whether the HV results of the proposed MaOEDA-IR are statistically better than, equal to, or worse than those of the corresponding peer competitors. In addition, the last row in Table III summarizes how many times the proposed MaOEDAIR is better than, equal to, or worse than its respective peer competitor.

The results in Table III indicate that the proposed MaOEDAIR obtains the best performances on the DTLZ4, DTLZ1 ${ }^{-}$, and DTLZ3 ${ }^{-}$test problems with all considered objective numbers except for 5 -objective DTLZ1 ${ }^{-}$and 8 -objective DTLZ3 ${ }^{-}$which are worse than GrEA. Moreover, MaOEDAIR is superior to others over DTLZ3 with 8 - and 10-objective while is inferior to GrEA with 3-objective, RM-MEDA with 5 -objective, and NSGA-III with 15 -objective. Although the

[^0]![img-5.jpeg](img-5.jpeg)

Fig. 5. IGD values of the results generated by $3-, 5-, 8-, 10-$, and $15-$ objective DTLZ1 test problems with different neighbor sizes varying in $\{5,10,15,20,25,30,35,40,45,50\}$.
performance of MaOEDA-IR in DTLZ2 and DTLZ4 ${ }^{-}$are worse than those of NSGA-III and GrEA on the 3 -, and 5 objective, respectively, the proposed MaOEDA-IR performs better than others on the 10-objective. In addition, the proposed MaOEDA-IR outperforms peer competitors on DTLZ1 and DTLZ2 ${ }^{-}$test problems with 8 -, and 15 -objective. In summary, the HV results of the proposed MaOEDA-IR against selected compared algorithms over eight test problems with 3 -, 5 -, $8-, 10-$, and 15 -objective indicate that MaOEDA-IR has a comparable performance by winning 181 test scores out of 240 comparisons, and performing equally well to 10 comparisons.

## E. Investigation on Neighbor Size

To investigate how the neighbor size $T$ affecting the performance of the proposed MaOEDA-IR, a series of experiments is performed by varying $T$ in $[5,50]$ with an interval of 5 . Specifically, the results measured by IGD on DTLZ1 test problems with considered objective numbers are plotted in Fig. 5 in which it is clearly shown that appreciable changes have been taken place in $T$ with smaller numbers and gradually remain steady as $T$ increases. It is interpreted that the neighbor solutions with one particular reference vector for building the model are from other reference vectors when $T$ is with a smaller number, which causes the inaccuracy of the built model based on which new solutions are generated that led to deteriorating IGD values. Especially, most IGD values remain level when $T>25$ in Fig. 5 (it is actually applicable to other tested benchmark problems based on the investigations.), and the $T$ with larger size will increase the computational cost by introducing more initialized solutions. As a consequence, $T$ is specified as 25 in our experiments.

## F. Investigation on Diversity Repairing Mechanism

As discussed in Subsection III-J, both the diversity and convergence have been improved with the diversity repairing mechanism. To this end, experimental comparisons on the test problems with and without the diversity repairing mechanism are performed. Specifically, the IGD values and the HV values of the evolution trajectory results generated by 10 and 15 -objective DTLZ2 test problems over 200 generations are illustrated in Figs. 6a, 6b, 7a, and 7b, respectively. In these figures, the red lines denote the results without the diversity repairing mechanism, while the blue lines refer to


[^0]:    ${ }^{4}$ These settings apply only to the experimental results in Subsection IV-D. For other experiments, the terminated criteria for MaOEDA-IR and PCSEA are specified as 200 generations, and the population size for PCSEA is set to be 100 for $M \leq 10$ and 200 for $M>10$.

TABLE III
HV RESULTS OF MAOEDA-IR AGAINST NSGA-III, MOEA/D, GrEA, HYPE, MBN-EDA, AND RM-MEDA OVER THE DTLZ1, DTLZ2, DTLZ3, DTLZ4, DTLZ1 ${ }^{-}$, DTLZ2 ${ }^{-}$, DTLZ3 ${ }^{-}$, AND DTLZ4 ${ }^{-}$TEST PROBLEMS WITH 3-, 5-, 8-, 10-, AND 15-OBJECTIVE. EACH COMPARED ALGORITHM IS INDEPENDENTLY PERFORMED 30 RUNS, AND THE REST MEDIAN HV RESULTS ARE HIGHLIGHTED IN ROLES FACE. THE SYMBOLS " + , " " = " AND "-" DENSITE WHETHER THE HV RESULTS OF THE PROPOSED MAOEDA-IR ARE STATISTICALLY RETTER THAN, EQUAL TO, OR WORKS THAN THAT OF THE CORRESPONDING PEER COMPETITORS WITH A SIGNIPICANT LEVEL $5 \%$, RESPECTIVELY.

those with the diversity repairing. To be specific, both the IGD values of 10 -objective DTLZ2 with and without the diversity repairing mechanism sharply decrease during the first 20 generations then gradually remain stable, while those of 15 -objective DTLZ2 smoothly decline throughout the entire evolution. For both the HV values of 10 -objective DTLZ2 with and without the diversity repairing mechanism, they grow substantially before the 40 -th generation then go up moderately as the evolution continues, while the ones resulted by the proposed algorithm without the diversity repairing mechanism may lower than those with the diversity repairing mechanism during the entire evolution. In summary, both the best IGD results in Figs. 6a and 6b and the HV results in Figs. 7a and 7b demonstrate the promising performance of the proposed MaOEDA-IR when the diversity repairing mechanism is employed.

Furthermore, Table IV shows the extensive experimental
comparisons between the proposed algorithm with and without the diversity repairing mechanism. Specifically, these experiments are independently performed 30 runs over each test problem, and their results are measured by HV. Moreover, the best median HV results are highlighted in bold face, and the symbols " + , " $=$ ", and "-" denote whether the HV results of the proposed algorithm with the diversity repairing mechanism are statistically better than, equal to, or worse than those of the proposed algorithm without the diversity repairing mechanism with a significant level $5 \%$, respectively. In addition, the last row in Table 3 summarizes how many times the proposed algorithm with the diversity repairing mechanism are better than, equal to, or worse than itself without this mechanism. It is clearly shown in Table IV that when the diversity repairing mechanism is employed, the proposed algorithm obtains all the best median HV results and most best statistical results against its competitors over DTLZ1-DTLZ4, and DTLZ1 ${ }^{-}$-

![img-6.jpeg](img-6.jpeg)

Fig. 6. IGD values of the results generated by 10-(Fig. 6a) and 15-objective (Fig. 6b) DTLZ2 with and without diversity repairing over 200 generations.
![img-7.jpeg](img-7.jpeg)

Fig. 7. HV values of the results generated by 10-(Fig. 7a) and 15-objective (Fig. 7b) DTLZ2 with and without diversity repairing over 200 generations.

DTLZ4 ${ }^{-}$with $3-, 5-, 8-, 10-$, and 15 -objective, while the proposed algorithm without the diversity repairing mechanism could not even obtain effective solutions over DTLZ2 ${ }^{-}$with 15-objective, DTLZ3 ${ }^{-}$with 8 - and 15-objective, and DTLZ4 with 8-objective (i.e., the HV results upon these generated solutions are approximately zeros, which is caused by the domination by the employed reference points for the calculation of HV). In addition, it also can be observed that the diversity repairing mechanism may not significantly improve the performance in solving MOPs where the phenomenon

TABLE IV
HV RESULTS OF MAOEDA-IR WITH AND WITHOUT THE DIVERSITY REPAIRING MECHANISM OVER THE DTLZ1, DTLZ2, DTLZ3, DTLZ4, DTLZ1 ${ }^{-}$, DTLZ2 ${ }^{-}$, DTLZ3 ${ }^{-}$, AND DTLZ4 ${ }^{-}$TEST PROBLEMS WITH 3-, $5-, 8-, 10-$, AND 15-OBJECTIVE. EACH COMPARED ALGORITHM IS INDEPENDENTLY PERFORMED 30 RUNS, AND THE BEST MEDIAN HV RESULTS ARE HIGHLIGHTED IN BOLD FACE. THE SYMBOLS " + ," " $=$," AND "-" DENOTE WHETDER THE HV RESULTS OF THE MAOEDA-IR WITH DR MECHANISM ARE STATISTICALLY BETTER THAN, EQUAL TO, OR WORSE THAN THAT OF THE CORRESPONDING MAOEDA-IR WITHOUT THE DIVERSITY REPAIRING MECHANISM WITH A SIGNIFICANT LEVEL $5 \%$, RESPECTIVELY.


of diversity losing is not severe. For example, the proposed algorithm obtains the same statistical results over 3-objective DTLZ1, DTLZ3, and DTLZ4 test problems no matter if the diversity repairing mechanism is employed. In summary, the diversity repairing mechanism can significantly improve the performance of the proposed algorithm especially in solving MaOPs.

## G. Investigation on Dimension Reduction

It is expected that dimension reduction in the decision variable space is capable of reducing the computational complexity of the proposed MaOEDA-IR. In this situation, two

types of experiments are to be performed in order to draw any meaningful conclusion. The first one is to measure the performance of the solutions generated by the proposed algorithm with and without the dimension reduction within the same generation numbers. The other one is to compare the generation numbers when the same performance is achieved by the proposed algorithm with and without the dimension reduction. In the following, the first experimental results would be shown, while the second experimental comparisons are presented in Supplemental Materials. Specifically, the generation number of the first experiment is adopted from the parameter settings in Subsection IV-C (i.e., 200). The experimental comparisons are independently performed 30 runs by the proposed algorithm with and without the dimension reduction over DTLZ1-DTLZ4 and DTLZ1 ${ }^{-}$-DTLZ4 ${ }^{-}$with $3-, 5-, 8-$ , $10-$, and 15 -objective. Then their results are measured by HV and shown in Table V where the best median HV results are highlighted in bold face, and the symbols " + ," " $=$," and "-" denote whether the HV results of the proposed algorithm with the dimension reduction are statistically better than, equal to, or worse than that of the proposed algorithm without the dimension reduction with a significant level $5 \%$, respectively. Furthermore, the last row in Table V summarizes how many times the proposed algorithm with the dimension reduction are better than, equal to, or worse than itself without this technique. It is obvious from Table V that the proposed algorithm obtains the significant performance improvement when the dimension reduction is employed. Furthermore, without the dimension reduction, the proposed algorithm cannot perform well over several test problems, such as the DTLZ1 ${ }^{-}$with 5 -objective, DTLZ2 ${ }^{-}$with 10 - and 5 -objective, and DTLZ3 ${ }^{-}$as well as DTLZ4 ${ }^{-}$with $8-, 10-$, and 15 -objective (their HV results are zeros). In summary, the proposed algorithm shows its superiority when the dimension reduction is utilized.

## V. CONCLUSION

In solving many-objective optimization problems, the performance of most multi-objective evolutionary algorithms often deteriorate appreciably because of the loss of selection pressure during the evolution process. This is largely due to the selected parents solutions not generating promising individuals with the conventional genetic operators to direct the search towards the Pareto-optimal front. An improved regularitybased estimation of distribution algorithm, which generates new solutions with a probabilistic model built from the solutions the algorithm has visited, is proposed in this paper. To be specific, the proposed algorithm made an innovation in the following aspects: 1) devising a diversity repairing mechanism to reduce the risk of dominance resistant solutions and 2) generating promising solutions with the statistics of regularity learnt from the neighboring solutions with respect to the representatives which are uniformly distributed in the objective space. These two steps works in conjunction with each other to direct the search towards Pareto-optimal front. In addition, dimension reduction technique is utilized to reduce the cost of exploitation and exploration. Furthermore, in addition to the investigations are performed on the diversity repairing and

TABLE V
HV RESULTS OF MAOEDA-IR WITH AND WITHOUT THE DIMENSION REDUCTION OVER THE DTLZ1, DTLZ2, DTLZ3, DTLZ4, DTLZ1 ${ }^{-}$, DTLZ2 ${ }^{-}$, DTLZ3 ${ }^{-}$, AND DTLZ4 ${ }^{-}$YEST PROBLEMS WITH $3-, 5-, 8-, 10-$, AND 15-ORJECTIVE. EACH COMPARED ALGORITHM IS INDEPENDENTLY
PERFORMED 30 RUNS, AND THE BEST MEDIAN HV RESULTS ARE HIGHLIGHTED IN ROLD FACE. THE SYMBOLS " + ," " $=$," AND "-" DENOTE
WHETHER THE HV RESULTS OF THE MAOEDA-IR WITH THE DIMENSION REDUCTION ARE STATISTICALLY BETTER THAN, EQUAL TO, OR WORSE
THAN THAT OF THE CORRESPONDING MAOEDA-IR WITHOUT THE DIMENSION REDUCTION WITH A SIGNIFICANT LEVEL 5\%, RESPECTIVELY.


dimension reduction, investigation is also performed based on the size of neighbors affecting the performance of the proposed algorithm to give the guideline for decision-marker. Extensive experiments are performed and the results measured by the chosen performance metrics indicate that the proposed algorithm shows superiority in tackling many-objective optimization problems. In our future research, we will extend the proposed algorithm to deal with highly constrained manyobjective optimization problems in which complicated regularity of Pareto fronts often exists.
