# An efficient mixture sampling model for gaussian estimation of distribution algorithm 

Qianlong Dang ${ }^{a}$, Weifeng Gao ${ }^{\mathrm{a}, *}$, Maoguo Gong ${ }^{\mathrm{b}}$<br>${ }^{a}$ School of Mathematics and Statistics, Xidian University, Xi'an 710126, China<br>${ }^{\mathrm{b}}$ Key Laboratory of Intelligent Perception and Image Understanding, International Research Center for Intelligent Perception and Computation, Ministry of Education, Xidian University, Xi'an 710071, China

## A R T I C L E I N F O

Article history:
Received 22 March 2021
Received in revised form 23 February 2022
Accepted 2 July 2022
Available online 06 July 2022

## Keyword:

Gaussian estimation of distribution algorithm
Evolutionary algorithm
Premature convergence
Efficient mixture sampling model

## A B S T R A C T

Estimation of distribution algorithm (EDA) is a stochastic optimization algorithm based on probability distribution model and has been widely applied in global optimization. However, the random sampling of Gaussian EDA (GEDA) usually suffers from the poor diversity and the premature convergence, which severely limits its performance. This paper analyzes the shortcomings of the random sampling and develops an efficient mixture sampling model (EMSM). EMSM can explore more promising regions and utilize the unsuccessful mutation vectors, which achieves a good tradeoff between the diversity and the convergence. Moreover, the feasibility analysis of EMSM is studied. A new GEDA variant named EMSM-EDA is developed, which combines EMSM with enhancing Gaussian estimation of distribution algorithm (EDA ${ }^{2}$ ). The experimental results on IEEE CEC2013 and IEEE CEC2014 test suites demonstrate that EMSM-EDA is efficient and competitive.
(c) 2022 Elsevier Inc. All rights reserved.

## 1. Introduction

Many engineering and scientific problems can be transformed into black-box optimization problems, and these problems are important and difficult. Since the analytical expression of black-box optimization problems is unknown, the traditional gradient-based algorithms are difficult to optimize. In recent years, evolutionary algorithms (EAs) [1-5] have been found to be effective in solving these types of optimization problems. Estimation of distributed algorithm (EDA) [6-10] is a special branch of EAs. Compared with crossover and mutation operators of other EAs, EDA generates candidate solutions by sampling from the probability distribution. Specifically, EDA utilizes the statistical information of good solutions to establish the corresponding probability distribution. Then, new candidate solutions are generated by utilizing the established probabilistic distribution. Over the last decades, EDA has been applied to many discrete optimization problems [11-15], continuous optimization problems [16-19], and mixed-variable optimization problems [20,21].

The traditional Gaussian sampling in EDA has the drawbacks of the poor diversity and the premature convergence. Thus, some efforts have been made to solve these issues and improve the performance of EDA. Zhou et al. [22] utilized Gaussian distribution and Cauchy distribution to propose a sampling algorithm for EDA, which makes use of the good exploration ability of Cauchy distribution. Velasco et al. [23] presented an adaptive Gibbs sampling, which combines with a local search strategy to generate new promising solutions. Ren et al. [24] constructed a novel probability density estimator to improve

[^0]
[^0]:    * Corresponding author.

the sampling quality. Lin et al. [25] proposed a mutual information EDA with hybrid sampling mechanism to relieve the premature convergence. Although the above work has been done in sampling phase of EDA, these methods still utilize the random sampling to generate the mutation vectors and suffer from the poor diversity and the premature convergence in some degree.

The general EDA randomly generates the mutation vectors by the multivariate Gaussian distribution. Nevertheless, this method often suffers from a biased sample that does not uniformly cover the sampling space. Therefore, the Gaussian sampling is unable to explore some promising regions and has the poor diversity, which results in the premature convergence. From this point of view, this paper introduces the idea of the uniform design sampling for improving the diversity and the exploration ability of the mutation vectors in GEDA. Moreover, the orthogonal method orthogonalizes the mutation vectors and ensures that more dimension directions are explored. Thus, the diversity of the mutation vectors can be further enhanced. In addition, GEDA only selects a part of the successful mutation vectors in each generation to construct the Gaussian distribution of the next generation. Most of the unsuccessful mutation vectors do not contribute to the search of the algorithm. It is considered that the reverse direction of the unsuccessful mutation vectors may be the promising direction. Therefore, the mirrored sampling is used to extract the useful information of the unsuccessful mutation vectors.

In this paper, an EMSM method is proposed for improving the performance of GEDA. The main contributions are listed as follows:

1. The random sampling suffers from the poor diversity and the premature convergence. To solve these issues, this paper designs an EMSM method that can maintain a good tradeoff between the diversity and the convergence by the uniform method, the orthogonal method, and the mirrored sampling.
2. The theoretical analysis of EMSM is given. Firstly, the diversity of the uniform method and the orthogonal method is studied by the volume of the probability density ellipsoid. Then, the convergence and the diversity of the mirrored sampling are analyzed by the progress rate and the volume of the probability density ellipsoid. Finally, it can be concluded that EMSM can improve the exploration ability and the convergence rate.
3. EMSM is applied to $\mathrm{EDA}^{2}$ [26] that is an excellent variant of GEDA, named EMSM-EDA. Compared with existing EAs, the experimental results demonstrate that EMSM-EDA is a global optimization algorithm with excellent performance.

The remainder of this paper is arranged as follows. Section 2 reviews the related work on basic GEDA and a series of the sampling methods. Section 3 presents the research motivation and the details of an efficient mixture sampling model. Section 4 gives the theoretical analysis of the efficient mixture sampling model. The experimental results are presented in Section 5. Finally, Section 6 summarizes this paper and prospects the potential future research directions.

# 2. Background and Related Work 

### 2.1. Basic GEDA

Basic GEDA [27] utilizes the probability distribution to generate the population. The details of basic GEDA are shown in Algorithm 1.

```
Algorithm 1: Basic GEDA
Input: initial population and relevant parameters.
Output: optimal solution.
    While (termination conditions are not satisfied) do
        Evaluate the quality of each solution in the current population;
        Select the excellent sample set by the selection rule;
        Estimate the Gaussian model based on the excellent sample set;
        Sample according to the Gaussian model, and build a new population;
        Update the obtained optimal solution;
    End While
```

GEDA adopts Gaussian probability model (GPM) to control the distribution of better solutions. For an $n$-dimensional random vector $\mathbf{x}$, the joint probability density function of GPM can be expressed as:

$$
G_{(\mu, C)}(\mathbf{x})=\frac{(2 \pi)^{-n / 2}}{(\operatorname{det} C)^{1 / 2}} \exp \left(-(\mathbf{x}-\mu)^{\mathrm{T}}(C)^{-1}(\mathbf{x}-\mu) / 2\right)
$$

where $\mu$ is the mean and $C$ is the covariance matrix. In each generation, GEDA usually selects excellent sample set $S^{t}$ based on the truncation selection rule, and estimates the new $\mu$ and $C$ with maximum likelihood estimation:

$\bar{\mu}^{t+1}=\frac{1}{\left|S^{t}\right|} \sum_{i=1}^{\left|S^{t}\right|} S_{i}^{t}$
$\bar{C}^{t+1}=\frac{1}{\left|S^{t}\right|} \sum_{i=1}^{\left|S^{t}\right|}\left(S_{i}^{t}-\bar{\mu}^{t+1}\right)\left(S_{i}^{t}-\bar{\mu}^{t+1}\right)^{T}$
where $\left|\cdot\right|$ denotes the cardinality of a set.
Fig. 1 is a schematic that describes the search process of basic GEDA. This search process includes populations of two successive generations, and the Gaussian model of population can be defined by the probability density ellipsoid (PDE). As shown in Fig. 1, the blue ellipse and the green ellipse represent the PDEs of the $t$-th generation and $(t+1)$-th generation, respectively. Specifically, the center and shape of the PDE are determined by $\bar{\mu}$ and $\bar{C}$, respectively.

# 2.2. Uniform design sampling and orthogonal method 

The uniform design sampling (UDS) [28] is a method to generate uniform sampling points in the sampling space, which has better sample representativeness than the random sampling. According to the uniformity of UDS, Zhou et al. [29] redesigned the sampling point operation in EAs. In order to find suitable parameters, Huang et al. [30] applied UDS method to support vector machines. In addition, the idea of USD can also be found in a novel artificial bee colony algorithm [31] and orthogonal learning particle swarm optimization [32]. The details of UDS are presented in Algorithm 2.

```
Algorithm2: Uniform Design Sampling
Input: nonnegative integers \(n, t\).
Output: uniform vectors \(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{t}\right\}\).
1: Select \(n\) nonnegative integers \(h_{1}, h_{2}, \ldots, h_{n}\);
2: \(\eta_{1}, \eta_{2}, \ldots, \eta_{n}\) are independent identically distributed samples of multinomial distribution;
3: \(x_{k j}\) is defined as \(x_{k j}=\left\{\frac{k h_{j}+h_{j}-1}{n}\right\}+\frac{n_{j j}}{n}\), where \(k=1, \ldots, t, j=1, \ldots, n, u_{k j}\) and \(\eta_{i}\) are sampled uniformly in \(\left[-\frac{1}{2}, \frac{1}{2}\right]\);
4: The \(k\)-th uniform vector is \(\mathbf{x}_{k}=\left(x_{k 1}, \ldots, x_{k n}\right)\);
5: The uniform vectors are \(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{t}\right\}\).
```

![img-0.jpeg](img-0.jpeg)

Fig. 1. Schematic to explain the search process of basic GEDA.

The orthogonal method can be found in adaptive coordinate descent [33] and enhance the diversity of the mutation vectors. In general, the algorithm generates a set of random vectors $\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}\right\}$ by the random sampling. Then, the GramSchmidt orthogonal method is used to orthogonalize the randomly vectors. Finally, a set of orthogonal vectors $\left\{\overline{\mathbf{x}}_{1}, \overline{\mathbf{x}}_{2}, \ldots, \overline{\mathbf{x}}_{n}\right\}$ are generated, which can obtain high quality sample.

# 2.3. Mirrored sampling 

Brockhoff et al. [34] first proposed the mirrored sampling method that generates two sample points. The advantage of the mirrored sampling is that there is always a good one of the two mirrored directions. Therefore, the mirrored sampling can accelerate the convergence of the algorithm and has been widely used. Auger et al. [35] applied the mirrored sampling to evolution strategy with weighted multi-recombination. Wang et al. [36] presented a mirrored orthogonal sampling method and applied it to covariance matrix adaptation evolution strategy. Auger et al. [37] made a refined single parent evolution strategy by the mirrored sampling. The details of the mirrored sampling are given in Algorithm 3.

```
Algorithm 3: Mirrored Sampling
Input: distribution mean \(\mu\), mutation strength \(\sigma\).
Output: mirrored vectors \(\mathbf{x}_{2 i-1}\) and \(\mathbf{x}_{2 i}\).
1: Generate a random vector \(\mathbf{z}_{2 i-1}\) by the random sampling method;
2: Generate two mirrored vectors \(\mathbf{x}_{2 i-1}\) and \(\mathbf{x}_{2 i}\) by \(\mu, \sigma\), and \(\mathbf{z}_{2 i-1}\); where \(\mathbf{x}_{2 i-1}=\mu+\sigma \mathbf{z}_{2 i-1}, \mathbf{x}_{2 i}=\mu-\sigma \mathbf{z}_{2 i-1}\).
```


## 3. Proposed Approach

This section gives the research motivation by analyzing the biased mutation samples and the unsuccessful mutation samples, and proposes EMSM method in detail. Moreover, EMSM-EDA is developed, which combines EMSM with EDA ${ }^{2}$.

### 3.1. Motivation

The mutation vectors are important factors affecting the search of GEDA, which are generated by a simple random sampling from the multivariate Gaussian distribution. In the Gaussian distribution $\mathbf{x}_{i} \sim \mathscr{N}(\bar{\mu}, \bar{C}), \bar{\mu}$ is the mean of the current population and $\bar{C}$ is covariance matrix. However, the random sampling has two disadvantages. Firstly, the vectors generated by the random sampling are not uniform and cannot effectively explore the local function landscape. Secondly, most of the unsuccessful mutation vectors do not contribute to the search of GEDA.

Fig. 2 shows an example of biased mutation samples in which six mutation vectors are generated from the random sampling. The black solid star denotes the optimal solution of the function. The grey dotted line denotes the function contour.
![img-1.jpeg](img-1.jpeg)

Fig. 2. Example of a set of biased mutation samples.

![img-2.jpeg](img-2.jpeg)

Fig. 3. Example of a set of unsuccessful mutation samples.

The blue ellipsoid denotes the sample space. It is clear that six mutation vectors are concentrated in a small region. Therefore, this sampling method has poor diversity. It is hard to find the promising descent directions, which results in the premature convergence.

As illustrated by Fig. 3, a set of unsuccessful mutation samples are given. Although the distribution of the mutation vectors are relatively uniform, most of the mutation vectors are unsuccessful, which makes the population move toward the opposite direction to the optimal solution and affects the convergence of the algorithm. Therefore, how to utilize the unsuccessful mutation vectors for improving the convergence is worth studying.

# 3.2. The proposed EMSM method 

As discussed in the above subsection, the random sampling of GEDA has the poor diversity and suffers from the premature convergence. To solve these problems, this paper achieves a good tradeoff between the diversity and the convergence by the uniform method, the orthogonal method, and the mirrored sampling.

```
Algorithm 4: A Simple Uniform Method
Input: vector dimension \(n\), population size \(p\), distance threshold \(\bar{d}\).
Output: uniform vectors \(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{p}\right\}\).
    While (number of the mutation vectors is no more than \(p\) ) do
        Generate one vector \(\mathbf{x}_{1}\) by using the random sampling;
    The normalized mutation vector \(\overline{\mathbf{x}}_{1}\) is generated by \(\mathbf{x}_{1}\);
        For \(i=2\) to \(p\)
            Generate one vector \(\mathbf{x}_{i}\) by using the random sampling;
            The normalized mutation vector \(\overline{\mathbf{x}}_{i}\) is generated by \(\mathbf{x}_{i}\);
            For \(j=1\) to \(i-1\)
                Calculate the distance \(d\left(\overline{\mathbf{x}}_{i}, \overline{\mathbf{x}}_{j}\right)\) of the vector \(\overline{\mathbf{x}}_{i}\) and the vector \(\overline{\mathbf{x}}_{j}\) according to the Eq. (5);
            End For
            Calculate the diversity index \(\mathrm{DI}\left(\mathbf{x}_{i}\right)\) of \(\mathbf{x}_{i}\) according to the Eq. (4);
            If \(\mathrm{DI}\left(\mathbf{x}_{i}\right)>\bar{d}\) then
                Retain \(\mathbf{x}_{i}\);
            Else
                Delete \(\mathbf{x}_{i}\) and go to step 4;
            End If
        End For
    End While
```

A simple uniform method is proposed for maintaining the diversity of the mutation vectors. Firstly, the diversity index is defined by distance metric. Then, the diversity index of each mutation vector is calculated and compared to the distance threshold. Finally, a set of uniform mutation vectors are generated. The detailed process of the uniform method is given in Algorithm 4.

The definition process of the diversity index is as follows: Firstly, the normalized mutation vectors $\left\{\overline{\mathbf{x}}_{1}, \overline{\mathbf{x}}_{2}, \ldots, \overline{\mathbf{x}}_{p}\right\}$ are obtained from the mutation vectors $\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{p}\right\}$. Secondly, the diversity index $\operatorname{DI}\left(\mathbf{x}_{i}\right)$ of $\mathbf{x}_{i}$ is calculated as follows:

$$
\operatorname{DI}\left(\mathbf{x}_{i}\right)=\min \left\{d\left(\overline{\mathbf{x}}_{i}, \overline{\mathbf{x}}_{1}\right), \ldots, d\left(\overline{\mathbf{x}}_{i}, \overline{\mathbf{x}}_{j}\right), \ldots, d\left(\overline{\mathbf{x}}_{i}, \overline{\mathbf{x}}_{p}\right)\right\}
$$

where $d$ is the distance metric and $\overline{\mathbf{x}}_{i} \neq \overline{\mathbf{x}}_{j}$. The distance metric $d\left(\overline{\mathbf{x}}_{i}, \overline{\mathbf{x}}_{j}\right)$ of $\overline{\mathbf{x}}_{i}$ and $\overline{\mathbf{x}}_{j}$ is computed as

$$
d\left(\overline{\mathbf{x}}_{i}, \overline{\mathbf{x}}_{j}\right)=\sum_{i=1}^{n}\left|\bar{x}_{i t}-\bar{x}_{j t}\right|
$$

where $\bar{x}_{i t}$ and $\bar{x}_{j t}$ are the $t$-th components of $\overline{\mathbf{x}}_{i}$ and $\overline{\mathbf{x}}_{j}$, respectively. Finally, the uniform mutation vectors are generated by comparing the diversity index $\mathrm{DI}\left(\mathbf{x}_{i}\right)$ of $\mathbf{x}_{i}$ with the distance threshold $\bar{d}$. The setting of the distance threshold $\bar{d}$ will be discussed in Section 5.2.

Fig. 4 is an example that explains the advantage of the uniform method. The red circle represents the origin point, the blue circle represents the mutation point, and the arrow represents the mutation vector. As shown in Fig. 4, the mutation vectors are uniformly distributed in the sampling space and have the rich diversity.

However, the uniform method may lose some directions. Thus, the orthogonal method is used to explore more promising directions, which can further enhance the diversity. In addition, the mirrored sampling is used to extract the useful information of the unsuccessful mutation vectors and accelerate the convergence of the algorithm.

Based on the above discussion, an efficient mixture sampling model is proposed in Algorithm 5. Firstly, the algorithm generates a set of uniform vectors by the uniform method. Secondly, the uniform mutation vectors are orthogonalized by the Gram-Schmidt orthogonal method. Thirdly, the mirrored vectors are generated by the mirrored sampling. Therefore, the algorithm can maintain a good tradeoff between the diversity and the convergence. Compared with the random sampling, EMSM has the following three advantages.

```
Algorithm 5: An Efficient Mixture Sampling Model
Input: vector dimension \(n\), distribution mean \(\mu\), covariance matrix \(C\), populationsize \(p\).
Output: uniform orthogonal vectors.
1: Utilize Algorithm 4 to generate a set of simple uniform vectors \(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{p}\right\}\);
2: If \(n \geqslant p\) then
3: Get the uniform orthogonal vectors \(\left\{\overline{\mathbf{x}}_{1}, \ldots, \overline{\mathbf{x}}_{p}\right\}\) by orthogonalizing the uniform vectors \(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{p}\right\}\) based on the
    Gram-Schmidt orthogonal method;
4: Else
5: Get the uniform orthogonal vectors \(\left\{\overline{\mathbf{x}}_{1}, \ldots, \overline{\mathbf{x}}_{n}\right\}\) by orthogonalizing the uniform vectors \(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{n}\right\}\) based on the
    Gram-Schmidt orthogonal method; Then, merge sets \(\left\{\overline{\mathbf{x}}_{1}, \ldots, \overline{\mathbf{x}}_{n}\right\}\) and \(\left\{\mathbf{x}_{n+1}, \ldots, \mathbf{x}_{p}\right\}\);
6: End If
7: Select half of the uniform orthogonal vectors to generate the mirrored vectors by Algorithm 3.
```

![img-3.jpeg](img-3.jpeg)

Fig. 4. Example to explain the advantage of a simple uniform method to the rich diversity of the mutation vectors.

1. EMSM adopts the uniform method and the orthogonal method to generate the uniform mutation vectors. Compared with the random sampling, the mutation vectors generated by EMSM have the rich diversity. In the black-box optimization problems, there are a lot of local optimal solutions. With the rich diversity, EMSM can skip the local optimal solutions more easily. Finally, EMSM can explore more promising regions and avoid the premature convergence.
2. EMSM can utilize the unsuccessful mutation vectors to generate the successful mutation vectors by the mirrored sampling, which accelerates the convergence of the algorithm. In addition, the successful mutation vectors produce some unsuccessful mutation vectors by the mirrored sampling. In GEDA, only part of the successful mutation vectors are selected to construct the Gaussian distribution model. Therefore, these unsuccessful mutation vectors are discarded and do not affect the convergence.
3. By introducing the distance threshold $\bar{d}$, EMSM can flexibly control the uniform degree of the mutation vectors. In the low-dimensional space, a small distance threshold $\bar{d}$ can be adopted. Moreover, a large distance threshold $\bar{d}$ can be used for the high-dimensional space.

# 3.3. Applying EMSM to $E D A^{2}$ 

EMSM is an excellent sampling method and can be easily applied to various GEDA variants. In this part, EMSM-EDA is presented, which combines EMSM with EDA ${ }^{2}$. The details of EMSM-EDA are shown in Algorithm 6. Firstly, the algorithm initializes the relevant parameters. Secondly, EMSM is utilized to generate the mutation vectors in steps 1 and 13. Finally, EMSM-EDA adopts the archiving mechanism of EDA ${ }^{2}$ to construct the mean and the covariance matrix to guide the search.

```
Algorithm 6: EMSM-EDA
Input: population size \(p\), selection ratio \(\tau\), archive length \(l\).
Output: optimal solution \(b^{t}\).
1: Generate the initial population \(P^{t}\) by Algorithm 5;
2: Set \(A^{t}=\varnothing, t=0\), and \(j=0\);
3: While (termination conditions are not satisfied) do
4: Evaluate all solutions in population \(P^{t}\) and obtain the best solution \(b^{t}\);
5: \(\quad\) Generate elite population \(S^{t}\) by selecting the best \(\lfloor\tau p \mid\) solutions from \(P^{t}\);
6: Compute the mean \(\bar{\mu}^{t+1}\) with \(S^{t}\) according to the Eq. (2);
7: Compute the covariance matrix \(\bar{C}^{t+1}\) with \(S^{t}\) and \(A^{t}\) based on the Eq. (3);
8: \(\quad\) If \(j<l\) then
9: $\quad A^{t+1}=A^{t} \cup S^{t} ;\)
10: Else
11: \(\quad A^{t+1}=A^{t} \cup S^{t} \backslash S^{t-l}\);
12: End If
13: Generate \(p-1\) solutions \(M^{t+1}\) by Algorithm 5, \(\bar{\mu}^{t+1}\), and \(\bar{C}^{t+1}\);
14: Set \(P^{t+1}=M^{t+1} \cup b^{t}\);
15: End While
```


### 3.4. Complexity Analysis

The computational cost of EMSM is mainly involved in: 1) the uniform method; 2) the orthogonal method; and 3) the mirrored sampling. Firstly, the computational complexity of uniform method is $O\left(n^{2}\right)$, where $n$ is the dimension of the mutation vector. Secondly, the computational cost of orthogonal method is spent in calling the Gram-Schmidt orthogonal process, which is $O\left(n k^{2}\right) \cdot k=\min (p, n)$, where $p$ is population size. According to the parameter setting in Section 5.2, $p>n$. Thus, $O\left(n k^{2}\right)=O\left(n^{3}\right)$. Thirdly, the computational complexity of mirrored sampling is $O(n)$. Overall, the computational complexity of EMSM is $O\left(n^{3}\right)$. In [26], the computational complexity of EDA ${ }^{2}$ is $O\left(n^{3}\right)$. Since EMSM-EDA is proposed by combining EMSM with EDA ${ }^{2}$, its computational complexity is $O\left(n^{3}\right)$. Therefore, EMSM-EDA can effectively improve the performance of EDA ${ }^{2}$ without increasing the computational complexity.

## 4. Theoretical Analysis

In this section, this paper firstly analyzes the diversity of the uniform method and the orthogonal method, and then studies the convergence and the diversity of the mirrored sampling.

# 4.1. Uniform and orthogonal method 

This part mainly compares the diversity of the uniform method and the orthogonal method with the random sampling. The diversity of two methods is described by the volume of PDE. As shown in Fig. 5, it is a 3-dimensional example that the volume of the cube represents the volume of PDE in the random sampling and EMSM. For EMSM and the random sampling, the volumes of their PDEs are $V_{\text {EMSM }}$ and $V_{\text {RS }}$, respectively. The volume of PDE for the multivariate Gaussian distribution is $V_{\text {Gauss }}$. Compared with $V_{\text {Gauss }}$, the volume errors of two sampling methods are $E_{\text {EMSM }}$ and $E_{\text {RS }}$, respectively, where $E_{\text {EMSM }}=V_{\text {Gauss }}-V_{\text {EMSM }}$ and $E_{\text {RS }}=V_{\text {Gauss }}-V_{\text {RS }}$. Thus, if a sampling method has smaller volume error, it has the richer diversity.

Property 1. The volume error of EMSM is not greater than the random sampling.
Analysis. Let the axis length of PDE of multivariate Gaussian distribution be composed of a set of $n$ orthogonal vectors $d_{1}, d_{2}, \ldots, d_{n}$. Therefore, the volume of PDE is

$$
V_{\text {Gauss }}=\prod_{i=1}^{n}\left|d_{i}\right|
$$

In GEDA, population size $p$ can be larger than or smaller than dimension $n$. The following two cases are discussed.
Case 1, $n \geqslant p$.
The volume of this PDE in EMSM is

$$
V_{\text {EMSM }}=\prod_{i=1}^{n}\left|d_{i}\right|
$$

The volume of this PDE in the random sampling is

$$
V_{\mathrm{RS}}=\prod_{i=1}^{n}\left|\mathbf{x}_{i}\right| \cdot \prod_{i=1}^{n}\left|\cos \theta_{i}\right|
$$

where $\cos \theta_{i}$ is the cosine of the smallest angle between the mutation vector $\mathbf{x}_{i}$ and other mutation vectors.
The volume errors of two sampling methods are

$$
\begin{aligned}
& E_{\text {EMSM }}=V_{\text {Gauss }}-V_{\text {EMSM }} \\
& E_{\mathrm{RS}}=V_{\text {Gauss }}-V_{\mathrm{RS}}
\end{aligned}
$$

Obviously, $\prod_{i=1}^{n}\left|d_{i}\right| \geqslant \prod_{i=1}^{p}\left|\mathbf{x}_{i}\right|, 0 \leqslant \prod_{i=1}^{p}\left|\cos \theta_{i}\right| \leqslant 1$. Then, $V_{\text {EMSM }} \geqslant V_{\text {RS }}$. Therefore, $E_{\text {EMSM }} \leqslant E_{\text {RS }}$.
Case 2, $n<p$.
The volume of this PDE in EMSM is

$$
V_{\text {EMSM }}=\prod_{i=1}^{n}\left|d_{i}\right|
$$

The volume of this PDE in the random sampling is
![img-4.jpeg](img-4.jpeg)
(a) The random sampling
![img-5.jpeg](img-5.jpeg)
(b) EMSM

Fig. 5. Example to explain the volumes of PDEs in the random sampling and EMSM.

$$
V_{R S}=\prod_{i=1}^{n}\left|\mathbf{x}_{i}\right| \cdot \prod_{i=1}^{n}\left|\cos \theta_{i}\right|
$$

From the conclusion of case 1, it is easy to obtain $\prod_{i=1}^{n}\left|d_{i}\right| \geqslant \prod_{i=1}^{n}\left|\mathbf{x}_{i}\right| \cdot \prod_{i=1}^{n}\left|\cos \theta_{i}\right|$. Thus, $V_{\text {EMSM }} \geqslant V_{R S}$ and $E_{\text {EMSM }} \leqslant E_{R S}$.
By the above analysis, the final conclusion is that the volume error of EMSM is not greater than the random sampling. Therefore, EMSM has the richer diversity.

# 4.2. Mirrored sampling method 

This part firstly utilizes the progress rate to research the convergence of the mirrored sampling and the random sampling. Specifically, the progress rate is the distance that the overall probability distribution moves in the descent direction. Then, the diversity of the mirrored sampling is analyzed by the volume of PDE.

Property 2. The mirrored sampling has the greater progress rate than the random sampling.
Analysis. The progress rate is defined based on the method in references [38-40]. As shown in Fig. 6, PDE is a sphere. $\mathbf{C}$ is the center of PDE, and $\mathbf{O}$ is the optimal solution to the sphere function. The distance between $\mathbf{C}$ and $\mathbf{O}$ is $\|\mathbf{C O}\|=L . \mathbf{x}$ is the mutation sample. $-\mathbf{x}$ is the mirrored sample. The vector $z$ is the projection of the mutation sample $\mathbf{x}$ onto the line CO.

The progress rate can be calculated by the projection of the vector $z$ onto CO. Let $\tilde{z}$ be the largest projection of the mutation vectors $\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{p}\right\}$ onto CO. Based on reference [39], the progress rate $\phi$ can be expressed as

$$
\phi=L \cdot \mathbb{E}\left\{1-\left(1+\frac{\pi}{2 L^{2}}-\frac{\tilde{z}}{L}\right)\right\}
$$

where E is the expectation. The cumulative probability distribution of the largest order statistic for EMSM can be computed as

$$
\begin{aligned}
& P_{\text {EMSM }}=(Z \leqslant z)=|P r(-z<Z \leqslant z)|^{p / 2} \\
& =|\Phi(z)-\Phi(-z)|^{p / 2} \\
& =|2 \Phi(z)-1|^{p / 2}
\end{aligned}
$$

where $\Phi(\cdot)$ stands for a standard Gaussian random variable. Let define $\forall z \geqslant 0$. The probability density function of EMSM is

$$
f_{\mathrm{EMSM}}=p f(z)|2 \Phi(z)-1|^{p / 2-1}
$$

where $f(\cdot)$ is the probability density function of a standard Gaussian distribution. Based on reference [38], the probability density function of the random sampling is

$$
f_{R S}=p f(z) \Phi(z)^{p-1}
$$

From the conclusions of references [38,39], the progress coefficients of EMSM and the random sampling are

$$
c_{\mathrm{EMSM}}=p \int_{-\infty}^{\infty} z f(z)|2 \Phi(z)-1|^{p / 2-1} d z
$$

![img-6.jpeg](img-6.jpeg)
optimal solution
center of PDE
mutation sample
mirrored sample

Fig. 6. The progress rate on the sphere function.

$$
c_{\mathrm{RS}}=p \int_{-\infty}^{\infty} z f(z) \Phi(z)^{p-1} d z
$$

Moreover, the normalized progress rates of EMSM and the random sampling are

$$
\begin{aligned}
& \phi_{\mathrm{EMSM}}^{*}=\sigma^{*} c_{\mathrm{EMSM}}-\frac{\left(\sigma^{*}\right)^{2}}{2} \\
& \phi_{\mathrm{RS}}^{*}=\sigma^{*} c_{\mathrm{RS}}-\frac{\left(\sigma^{*}\right)^{2}}{2}
\end{aligned}
$$

where $\phi^{*}=\phi_{X}^{n}, \sigma^{*}=\frac{n}{2}$. By doing a simple calculation, it is easy to get $c_{\text {EMSM }} \geqslant c_{\text {RS }}$ and $\phi_{\text {EMSM }}^{*} \geqslant \phi_{\text {RS }}^{*}$. Compared with the random sampling, the mirrored sampling has the greater progress rate and the faster convergence.

Property 3. The greater progress rate of the mirrored sampling can obtain the larger volume of PDE.
Analysis. The volume of PDE in the mirrored sampling is studied in reference [41]. For different selection methods of solutions, the algorithm can get different progress rates. Therefore, the means $\widehat{\mu}_{\text {EMSM }}$ and $\widehat{\mu}_{\text {EMSM }}$ can be obtained for two different progress rates, where $\widehat{\mu}_{\text {EMSM }}=\widehat{\mu}_{\text {EMSM }}+\delta$ and $\delta \geqslant 0$. The corresponding covariance matrices are $\widehat{C}_{\text {EMSM }}$ and $\widehat{C}_{\text {EMSM }}$, respectively. The eigenvalues of the covariance matrices $\widehat{C}_{\text {EMSM }}$ and $\widehat{C}_{\text {EMSM }}$ are $\left\{\widehat{\lambda}_{1}, \ldots, \widehat{\lambda}_{n}\right\}$ and $\left\{\widehat{\lambda}_{1}, \ldots, \widehat{\lambda}_{n}\right\}$, respectively. For the two progress rates, the volumes of PDEs are $\prod_{i=1}^{n} \widehat{\lambda}_{i}$ and $\prod_{i=1}^{n} \widehat{\lambda}_{i}$, respectively. The covariance matrix $\widehat{C}_{\text {EMSM }}$ can be computed as follows:

$$
\begin{aligned}
& \widehat{C}_{\text {EMSM }}=\frac{1}{\left|\hat{\gamma}_{i}^{t}\right|} \sum_{i=1}^{\left|\hat{\gamma}_{i}^{t}\right|}\left(S_{i}^{t}-\widehat{\mu}_{\text {EMSM }}\right)\left(S_{i}^{t}-\widehat{\mu}_{\text {EMSM }}\right)^{T} \\
& =\frac{1}{\left|\hat{\gamma}_{i}^{t}\right|} \sum_{i=1}^{\left|\hat{\gamma}_{i}^{t}\right|}\left(S_{i}^{t}-\widehat{\mu}_{\text {EMSM }}-\delta\right)\left(S_{i}^{t}-\widehat{\mu}_{\text {EMSM }}-\delta\right)^{T} \\
& =\frac{1}{\left|\hat{\gamma}_{i}^{t}\right|} \sum_{i=1}^{\left|\hat{\gamma}_{i}^{t}\right|}\left(S_{i}^{t}-\widehat{\mu}_{\text {EMSM }}\right)\left(S_{i}^{t}-\widehat{\mu}_{\text {EMSM }}\right)^{T}+\delta \delta^{T} \\
& -\frac{1}{\left|\hat{\gamma}_{i}^{t}\right|} \sum_{i=1}^{\left|\hat{\gamma}_{i}^{t}\right|}\left(S_{i}^{t}-\widehat{\mu}_{\text {EMSM }}\right)^{T} \delta-\frac{1}{\left|\hat{\gamma}_{i}^{t}\right|} \sum_{i=1}^{\left|\hat{\gamma}_{i}^{t}\right|}\left(S_{i}^{t}-\widehat{\mu}_{\text {EMSM }}\right) \delta^{T} \\
& =\widehat{C}_{\text {EMSM }}+\delta \delta^{T}
\end{aligned}
$$

$\widehat{C}_{\text {EMSM }}$ and $\widehat{C}_{\text {EMSM }}$ are semi-positive definite matrices. Then, the matrix $\widehat{C}_{\text {EMSM }}$ is decomposed into

$$
\widehat{C}_{\mathrm{EMSM}}=Q \Lambda Q^{T}
$$

where $Q$ is an orthogonal matrix and $\Lambda$ is a diagonal matrix. The orthogonal transformation of the matrix $\widehat{C}_{\text {EMSM }}$ is obtained

$$
\begin{aligned}
& Q \widehat{C}_{\mathrm{EMSM}} Q^{T}=Q \widehat{C}_{\mathrm{EMSM}} Q^{T}+(Q \delta)(Q \delta)^{T} \\
& =\Lambda+(Q \delta)(Q \delta)^{T}
\end{aligned}
$$

Obviously, $\Lambda+(Q \delta)(Q \delta)^{T}$ is the rank-one modification of $\Lambda$. According to the conclusion in reference [42], the characteristic polynomial of $\widehat{C}_{\text {EMSM }}$ is

$$
F(\lambda)=\prod_{i=1}^{n}\left(\lambda-\lambda_{i}\right)-\sum_{k=1}^{n}\left((Q \delta)^{2} \cdot \prod_{i=1, i=k}^{n}\left(\lambda-\lambda_{i}\right)\right)
$$

Let $\lambda=0, F(\lambda)$ is transformed into

$$
F(\lambda)=\prod_{i=1}^{n} \lambda_{i}+\sum_{k=1}^{n}(Q \delta)^{2} \cdot \prod_{i=1, i=k}^{n} \lambda_{i}
$$

Based on the definition of the characteristic polynomial, the characteristic polynomial of $\widehat{C}_{\text {EMSM }}$ is

$$
F(\lambda)=\prod_{i=1}^{n} \widehat{\lambda}_{i}
$$

From the above two formulas, the following relation can be obtained

$$
\prod_{i=1}^{n} \bar{\lambda}_{i}=\prod_{i=1}^{n} \bar{\lambda}_{i}+\sum_{k=1}^{n}(Q \delta)^{2} \cdot \prod_{i=1, j \neq k}^{n} \bar{\lambda}_{i}
$$

where $\sum_{k=1}^{n}(Q \delta)^{2} \cdot \prod_{i=1, j \neq k}^{n} \bar{\lambda}_{i} \geqslant 0$. Then, $\prod_{i=1}^{n} \bar{\lambda}_{i} \geqslant \prod_{i=1}^{n} \bar{\lambda}_{i}$. Therefore, the greater progress rate of the mirrored sampling can obtain the larger volume of PDE and the richer diversity.

# 5. Experimental Studies 

### 5.1. Experimental settings

The experiments are carried out on 58 test functions with 30 dimensions (30D), 50 dimensions (50D), and 100 dimensions (100D) of IEEE CEC2013 [43] and IEEE CEC2014 [44] test suites, respectively. For the convenience of description, we denote 28 functions for IEEE CEC2013 as $\mathrm{CEC} 2013_{1}-\mathrm{CEC} 2013_{28}$ and 30 functions for IEEE CEC2014 as CEC2014 ${ }_{1}-$ $\mathrm{CEC} 2014_{30}$. According to their characteristics, 58 test functions can be categorized in Table 1. Specifically, the unimodal functions have only global optimum and their structure is smooth. The simple multimodal functions have a large number of local optimum, which can reflect the characteristics of real-world problems as well. For the hybrid functions, the variables are

Table 1
Test functions.

![img-7.jpeg](img-7.jpeg)

Fig. 7. Box plots of the FEVs provided by EMSM-EDA with different distance thresholds on 4 functions of CEC2014 benchmark functions with 50D.

Table 2
The means and the standard deviations of FEVs obtained by the two algorithms over 25 independent runs on 28 CEC2013 benchmark functions with 30D, 50D, and 100D.
"+", "-", and " $\approx$ " indicate that the performance of $\mathrm{EDA}^{2}$ algorithm is better than, worse than, and similar to that of EMSM-EDA, respectively.

randomly divided into some subcomponents and then different basic functions are used for different subcomponents. Then, the different subcomponents of the variables may have different properties. With hybrid functions as the basic functions, the composition function can have different properties for different variables subcomponents and keep continuous around the global/local optima. Moreover, these test functions are all minimization problems with a single objective and their detailed description can be found in [43], [44].

In the comparison experiments, each algorithm independently runs 25 times, and $10000^{*} D$ total fitness evaluation is used as the termination condition. All the experiments are implemented on a PC with Intel Core i5-8400 CPU @ 4.00 GHz and 64bit Windows 10 operating system. The function error value (FEV) is the difference between the best solution of the algorithm and the true optimal solution. When the FEV is less than $1.0 \mathrm{E}-8$, it is taken as 0 .

# 5.2. Influence of parameters 

EMSM-EDA has four parameters: archive length $l$, selection ratio $\tau$, population size $p$, and distance threshold $\bar{d}$. Specifically, $l$ and $\tau$ are set to 20 and 0.35 , respectively. $p$ is set to 100,200 , and 200 on test functions with 30D, 50D, and 100D, respectively. In addition, $\bar{d}$ is the parameter that controls the diversity of the mutation vectors. A large $\bar{d}$ can increase the diversity, but it requires a large amount of computation. Moreover, a small $\bar{d}$ has little effect on the diversity. Based on the experimental analysis, when $\bar{d}$ is set to $28.5,47.5$, and 95 with 30D, 50D, and 100D, EMSM-EDA can obtain a good test result and have an acceptable amount of computation.

Fig. 7 shows the results provided by EMSM-EDA with different $\bar{d}$ on $\mathrm{CEC} 2014_{8}, \mathrm{CEC} 2014_{10}, \mathrm{CEC} 2014_{11}$, and $\mathrm{CEC} 2014_{18}$ with 50D. When $\bar{d}$ increases from 10 to 47.5 , FEV decreases rapidly. However, as $\bar{d}$ continues to increase, FEV is almost constant. In particular, the algorithm needs to consume a large amount of computation with the increase of $\bar{d}$. Therefore, $\bar{d}$ is set to 47.5 . In the case of 30D and 100D, the variations of $\bar{d}$ and FEV are similar to 50D.

### 5.3. Effectiveness of EMSM

To verify the effectiveness of EMSM, this paper firstly compares EMSM-EDA with EDA ${ }^{2}$ on two test suites with 30D, 50D, and 100D, respectively. Then, the evolution curves of EMSM-EDA and EDA ${ }^{2}$ on 8 test functions with 100D are compared.

Table 2 summarizes the test results of EMSM-EDA and EDA ${ }^{2}$ on IEEE CEC2013 test suite. For the benchmark functions with 30D, 50D, and 100D, EMSM-EDA outperforms EDA ${ }^{2}$ on 16, 16, and 19 functions, respectively. In the case of 100D, EMSM-EDA improves EDA ${ }^{2}$ on $\mathrm{CEC} 2013_{3}$ and $\mathrm{CEC} 2013_{7}$ functions by three order of magnitude. Meanwhile, EMSM-EDA also improves EDA ${ }^{2}$ on $\mathrm{CEC} 2013_{13}$ and $\mathrm{CEC} 2013^{27}$ functions by one order of magnitude. Moreover, EMSM-EDA provides the optimal solutions with $100 \%$ successful rate on the functions $\mathrm{CEC} 2013_{2}$ and $\mathrm{CEC} 2013_{5}$. Furthermore, we conduct Wilcoxon's test [45] on two algorithms based on the total 28 test functions with 30D, 50D, and 100D. Table 3 presents the test results that EMSM-EDA has higher $R^{+}$values than $R^{-}$values and corresponding $p$ values are much less than 0.05 . The results show that EMSM-EDA is superior to EDA ${ }^{2}$ on the total 28 test functions.

The results of EMSM-EDA and EDA ${ }^{2}$ are summarized in Table 4 on IEEE CEC2014 test suite with 30D, 50D, and 100D, respectively. It is clear that EMSM-EDA outperforms EDA ${ }^{2}$ on 20, 19, and 24 functions, respectively. Specially, for the case of 100D, EMSM-EDA improves EDA ${ }^{2}$ on $\mathrm{CEC} 2014_{20}$ and $\mathrm{CEC} 2014_{29}$ functions by one order of magnitude. Moreover, EMSM-EDA improves EDA ${ }^{2}$ on $\mathrm{CEC} 2014_{17}$ and $\mathrm{CEC} 2014_{18}$ functions by two order of magnitude. In addition, EMSM-EDA improves EDA ${ }^{2}$ on $\mathrm{CEC} 2014_{1}$ and $\mathrm{CEC} 2014_{2}$ functions by nine order of magnitude. Furthermore, Wilcoxon's test is conducted on IEEE CEC2014 test functions with 30D, 50D, and 100D. The test results in Table 5 show that EMSM-EDA performs better than EDA ${ }^{2}$.

Fig. 8 and Fig. 9 present the evolution curves of FEVs on $\mathrm{CEC} 2013_{2}, \mathrm{CEC} 2013_{3}, \mathrm{CEC} 2013_{5}, \mathrm{CEC} 2013_{7}, \mathrm{CEC} 2014_{1}, \mathrm{CEC} 2014_{2}$, $\mathrm{CEC} 2014_{17}$, and $\mathrm{CEC} 2014_{18}$ with 100D. For all 8 test functions, EDA ${ }^{2}$ suffers from the premature convergence. However, EMSM-EDA can obtain better solutions. In addition, EMSM-EDA finds the optimal solutions on $\mathrm{CEC} 2013_{2}$ and $\mathrm{CEC} 2013_{5}$.

All the above investigations verify that EMSM is effective, which can accomplish a good tradeoff between the diversity and the convergence, and thus obtain better solutions and faster convergence speed in EMSM-EDA. Moreover, the simple structure of EMSM is easily applied to various GEDA variants.

Table 3
Results obtained by the Wilcoxon's test on CEC2013 Benchmark functions with 30D, 50D, and 100D.
Table 4
The means and the standard deviations of FEVs obtained by the two algorithms over 25 independent runs on 30 CEC2014 benchmark functions with 30D, 50D, and 100D.
"+", " ", and " $\times$ " indicate that the performance of $\mathrm{EDA}^{2}$ algorithm is better than, worse than, and similar to that of EMSM-EDA, respectively.

Table 5
Results obtained by the Wilcoxon's test on CEC2014 Benchmark functions with 30D, 50D, and 100D.
![img-8.jpeg](img-8.jpeg)

Fig. 8. Evolution of FEVs derived from EMSM-EDA and EDA ${ }^{2}$ on 4 functions of CEC2013 benchmark functions with 100D.

# 5.4. The effectiveness of EMSM's three components 

The aim of this subsection is to identify the effectiveness of three components of EMSM: the simple uniform method, the orthogonal method, and the mirrored sampling. For this reason, the above three components are applied to EDA ${ }^{2}$, and these three algorithms are named as U-EDA ${ }^{2}$, O-EDA ${ }^{2}$, and M-EDA ${ }^{2}$, respectively. Moreover, EDA ${ }^{2}$ and EMSM-EDA are chosen as a basic comparison.

Table 6 summarizes the test results of EDA ${ }^{2}$, U-EDA ${ }^{2}$, O-EDA ${ }^{2}$, M-EDA ${ }^{2}$, and EMSM-EDA on IEEE CEC2014 test suite with 30D. From Table 6, EMSM-EDA and EDA ${ }^{2}$ have the best and the worst performance among the five comparison algorithms, respectively. Moreover, U-EDA ${ }^{2}$, O-EDA ${ }^{2}$, and M-EDA ${ }^{2}$ surpass EDA ${ }^{2}$ on 16,18 , and 16 test functions, which means the three components of EMSM play an important role in improving the performance of EDA ${ }^{2}$. In addition, U-EDA ${ }^{2}$, O-EDA ${ }^{2}$, and MEDA $^{2}$ is surpassed by EMSM-EDA on 18, 21, and 21 test functions, respectively. It means that the three components jointly improve the performance of EDA ${ }^{2}$.

### 5.5. Comparisons with the other EAs on IEEE CEC2013 test suite

This part compares EMSM-EDA with EMNA ${ }_{8}$ [8], AMaLGaM [6], JADE [46], and SaDE [47] on IEEE CEC2013 test suite. The parameter settings of EMNA $_{8}$, JADE, and SaDE are based on the above references. The test results of AMaLGaM are directly obtained from reference [6]. To analyze the comparison results more clearly, this paper utilizes the method in [48], where "

![img-9.jpeg](img-9.jpeg)

Fig. 9. Evolution of FEVs derived from EMSM-EDA and EDA ${ }^{5}$ on 4 functions of CEC2014 benchmark functions with 100D.
+", "-", and " $\approx$ " denote that the performance of the comparison algorithm is better than, worse than, or similar to EMSMEDA, respectively. In addition, the best results are bolded. Tables 7 and 8 summarize the test results obtained by the five algorithms on test suite with 30D and 50D. The comments can be obtained.

1. CEC2013 ${ }_{1}$-CEC2013 ${ }_{5}$ are unimodal functions, on which EMSM-EDA shows the best performance among the five algorithms with 30D and 50D. For the case of 30D, EMSM-EDA can find the optimal solutions for all 5 test functions, while AMaLGAM fails on CEC2013 ${ }_{3}$. Moreover, JADE and SaDE only obtain the optimal solutions of CEC2013 ${ }_{1}$ and CEC2013 ${ }_{5}$. In particular, $\mathrm{EMNA}_{8}$ cannot find the optimal solutions for all 5 test functions. When the test functions are 50D, EMSM-EDA finds the optimal solutions for 4 test functions and still has the best performance.
2. For basic multimodal functions $\mathrm{CEC} 2013_{6}$ - $\mathrm{CEC} 2013_{20}$ with 30D, EMSM-EDA and JADE have better performance which can obtain 8 best solutions for all 15 test functions. AMaLGAM obtains 2 best solutions. Moreover, there is only 1 best solution for $\mathrm{EMNA}_{8}$ and SaDE. In the case of 50D, EMSM-EDA is surpassed by JADE on 9 functions, which indicates the performance of JADE is superior to EMSM-EDA. EMSM-EDA performs much better than $\mathrm{EMNA}_{8}$, AMaLGAM, and SaDE.
3. For complicated composition functions $\mathrm{CEC} 2013_{21^{-}} \mathrm{CEC} 2013_{28}$ with 30D, EMSM-EDA finds 4 best solutions for all 8 test functions and has the best performance. As a comparison, AMaLGAM and JADE can obtain 3 best solutions. Moreover, SaDE only finds 1 best solution. As a classical GEDA, $\mathrm{EMNA}_{8}$ cannot obtain best solutions. For test functions with 50D, although EMSM-EDA defeats $\mathrm{EMNA}_{8}$, JADE, and SaDE, it is surpassed by AMaLGAM on 4 test functions, which indicates AMaLGAM performs better than EMSM-EDA.

The overall results of five algorithms are summarized in the last rows of Tables 7 and 8 . It is clear that EMSM-EDA outperforms $\mathrm{EMNA}_{8}$, AMaLGAM, JADE, and SaDE on IEEE CEC2013 test suite. To vividly describe the advantage of EMSM-EDA, Fig. 10 and Fig. 11 present the evolution curves of five comparison algorithms on 4 test functions with 30D and 50D, respectively. The evolution curves indicate that EMSM-EDA has the fastest convergence speed and excellent performance.

Table 6
The means and the standard deviations of FEVs obtained by the five algorithms over 25 independent runs on 30 CEC2014 benchmark functions with 30D.
"+", "--", and " $\infty$ " indicate that the performance of the corresponding algorithm is better than, worse than, and similar to that of EMSM-EDA, respectively.

# 5.6. Comparisons with the other EAs on IEEE CEC2014 test suite 

To further evaluate the performance of EMSM-EDA, this paper compares it with five algorithms on IEEE CEC2014 test suite with 30D and 50D. Besides $\mathrm{EMNA}_{8}, \mathrm{AMaLGAM}, \mathrm{JADE}$, and SaDE , one new competitor is added to the experiment, i.e., BL-PSO-5 [49]. The parameter settings of $\mathrm{EMNA}_{8}, \mathrm{AMaLGAM}, \mathrm{JADE}$, and SaDE are the same as IEEE CEC2013 test experiment. The test results of BL-PSO-5 are directly obtained from reference [49]. Tables 9 and 10 summarize the test results of the six algorithms. The comments can be obtained.

1. For the unimodal functions CEC2014 $_{1}$ -CEC2014 $_{2}$ with 30D and 50D, EMSM-EDA can find the optimal solutions for all 3 test functions and achieve perfect performance. AMaLGAM fails on CEC2014 ${ }_{2}$ with 50D. EMNA ${ }_{8}$ and BL-PSO-5 cannot find the optimal solutions. For JADE, it obtains the optimal solutions on CEC2014 ${ }_{2}$ with 30D and 50D. SaDE only obtains the optimal solution on CEC2014 ${ }_{2}$ with 30D.
2. CEC2014 $4_{4}$ -CEC2014 $4_{16}$ are simple multimodal functions, on which EMSM-EDA finds 5 best solutions and demonstrates the best performance among the six algorithms with 30D. AMaLGAM, BL-PSO-5, JADE, and SaDE find 2, 3, 4, and 1 best solution for all 13 test functions, respectively. $\mathrm{EMNA}_{8}$ cannot find the best solutions for all 13 test functions with 30D. Nevertheless, EMSM-EDA is surpassed by JADE on 5 out of total 13 functions with 50D, which means JADE performs better than EMSM-EDA. AMaLGAM and BL-PSO-5 can obtain 3 best solutions for all 13 test functions. For $\mathrm{EMNA}_{8}$ and SaDE, they cannot obtain the best solutions.
3. Regarding to the hybrid functions CEC2014 $4_{17}$ -CEC2014 $2_{22}$ with 30D and 50D, EMSM-EDA finds the best solutions for all 6 functions. Specifically, EMSM-EDA improves $\mathrm{EMNA}_{8}, \mathrm{AMaLGAM}, \mathrm{BL}$-PSO-5, JADE, and SaDE on $\mathrm{CEC} 2014_{17}$ and $\mathrm{CEC} 2014_{18}$ functions with 30D by two order of magnitude. Meanwhile, EMSM-EDA also improves $\mathrm{EMNA}_{8}, \mathrm{AMaLGAM}, \mathrm{BL}$-PSO-5, JADE, and SaDE on $\mathrm{CEC} 2014_{17}, \mathrm{CEC} 2014_{18}$, and $\mathrm{CEC} 2014_{19}$ functions with 50D by one order of magnitude. To summarise, EMSM-EDA is the champion on the hybrid functions.
4. EMSM-EDA has the excellent performance on composition functions $\mathrm{CEC} 2014_{23}$ - $\mathrm{CEC} 2014_{30}$. For the functions with 30D, EMSM-EDA outperforms five comparison algorithms on $8,4,5,5$, and 7 out of all 8 functions, respectively. In addition, EMSM-EDA and AMaLGAM find 4 best solutions for 8 test functions with 50D and perform better than the other four algorithms. Specially, $\mathrm{EMNA}_{8}$, BL-PSO-5, JADE, and SaDE find 0, 2, 3, and 1 best solution for 8 test functions with 50D, respectively.

Table 7
The means and the standard deviations of FEVs obtained by the five algorithms over 25 independent runs on 28 CEC2013 benchmark functions with 50D.
"+", "-", and " $=$ " indicate that the performance of the corresponding algorithm is better than, worse than, and similar to that of EMSM-EDA, respectively.

Table 8
The means and the standard deviations of FEVs obtained by the five algorithms over 25 independent runs on 28 CEC2013 benchmark functions with 50D.
"+", " ", and " $=$ " indicate that the performance of the corresponding algorithm is better than, worse than, and similar to that of EMSM-EDA, respectively.

![img-10.jpeg](img-10.jpeg)

Fig. 10. Evolution of FEVs derived from EMNA ${ }_{8}$, AMaLGaM, JADE, SaDE, and EMSM-EDA on 4 functions of CEC2013 benchmark functions with 30D.
![img-11.jpeg](img-11.jpeg)

Fig. 11. Evolution of FEVs derived from EMNA ${ }_{8}$, AMaLGaM, JADE, SaDE, and EMSM-EDA on 4 functions of CEC2013 benchmark functions with 50D.

Table 9
The means and the standard deviations of FEVs obtained by the six algorithms over 25 independent runs on 30 CEC2014 benchmark functions with 30D.
$"+$ ", ", ", and " $\approx$ " indicate that the performance of the corresponding algorithm is better than, worse than, and similar to that of EMSM-EDA, respectively.

Table 10
The means and the standard deviations of FEVs obtained by the six algorithms over 25 independent runs on 30 CEC2014 benchmark functions with 50D.
"+", ".,", and " $\infty$ " indicate that the performance of the corresponding algorithm is better than, worse than, and similar to that of EMSM-EDA, respectively.

![img-12.jpeg](img-12.jpeg)

Fig. 12. Evolution of FEVs derived from EMNA ${ }_{8}$, AMaLGaM, JADE, SaDE, and EMSM-EDA on 4 functions of CEC2014 benchmark functions with 30D.
![img-13.jpeg](img-13.jpeg)

Fig. 13. Evolution of FEVs derived from EMNA ${ }_{8}$, AMaLGaM, JADE, SaDE, and EMSM-EDA on 4 functions of CEC2014 benchmark functions with 50D.

Table 11
Rankings of EMSM-EDA, AMaLGaM, JADE, and SaDE on 58 test functions based on Friedman's test.
![img-14.jpeg](img-14.jpeg)

Fig. 14. Runtime of $\mathrm{EDA}^{2}$ and EMSM-EDA on 28 CEC2013 benchmark functions and 30 CEC2014 benchmark functions with 30D, 50D, and 100D.

From the last rows of Tables 9 and 10, it is obvious that EMSM-EDA outperforms EMNA ${ }_{8}$, AMaLGAM, BL-PSO-5, JADE, and SaDE on IEEE CEC2014 test suite with 30D and 50D. To vividly depict the excellent performance of EMSM-EDA, Fig. 12 and Fig. 13 present the evolution curves of five comparison algorithms on 4 test functions with 30D and 50D, respectively. Compared with EMNA ${ }_{8}$, AMaLGaM, JADE, and SaDE, EMSM-EDA has the better solutions and faster convergence speed.

Tables 7-10 summarize all the results that EMSM-EDA, AMaLGaM, JADE, and SaDE are competitive on IEEE CEC2013 and IEEE CEC2014 test suites. Furthermore, this paper conducts Friedman's test [45] on four excellent algorithms based on two test suites with 30D and 50D. Table 11 presents the test results that EMSM-EDA has the best ranking with 30D and 50D, sequentially followed by AMaLGaM, JADE, and SaDE.

In summary, EMSM-EDA performs better than five excellent EAs on two test suites with 30D and 50D. Specifically, EMSMEDA is the champion on unimodal functions and hybrid functions. For the simple multimodal functions and the hybrid functions, the performance of EMSM-EDA is competitive.

# 5.7. Computational Complexity of EMSM-EDA 

EMSM is designed to improve the performance of $\mathrm{EDA}^{2}$, which causes extra computational burden. Thus, we study the computational cost of EMSM through the runtime of EDA ${ }^{2}$ and EMSM-EDA on 28 CEC2013 benchmark functions and 30 CEC2014 benchmark functions with 30D, 50D, and 100D. Fig. 14 presents the runtime of EDA ${ }^{2}$ and EMSM-EDA on CEC2013 and CEC2014 benchmark functions with 30D, 50D, and 100D, respectively. As shown in Fig. 14, the extra runtime of EMSM is the same order of magnitude as the runtime of corresponding EDA ${ }^{2}$. Therefore, EMSM can significantly improve the performance of EDA ${ }^{2}$ without too much additional computing burden.

### 5.8. Real-World Optimization Problems

Seven real-world engineering optimization problems in different fields such as radar system, power system, economic scheduling, and spacecraft trajectory optimization are selected to evaluate the performance of EMSM-EDA. These seven real-world optimization problems (called $P_{1}-P_{7}$ in this paper) are collected at the 2011 IEEE Congress on Evolutionary Computation [50], which show different engineering characteristics and are very difficult to solve. Specifically, the details of these seven real-world optimization problems are summarized in Table 12. In this experiment, $\mathrm{EMNA}_{\mathrm{g}}, \mathrm{AMaLGaM}, \mathrm{JADE}$, and SaDE are used as comparison algorithms to compare the performance of EMSM-EDA. In addition, the parameter settings of the five comparison algorithms are the same as IEEE CEC2013 test experiment. Finally, each algorithm is run 25 times, and 150,000 total fitness evaluation is used as the termination condition.

Table 13 summarizes the test results of $\mathrm{EMNA}_{\mathrm{g}}, \mathrm{AMaLGaM}, \mathrm{JADE}, \mathrm{SaDE}$, and EMSM-EDA on seven real-world optimization problems. As shown in Table 13, EMSM-EDA surpasses $\mathrm{EMNA}_{\mathrm{g}}, \mathrm{AMaLGaM}, \mathrm{JADE}$, and SaDE on 7, 5, 6, and 6 problems, respectively. As a contrast, EMSM-EDA is defeated by $\mathrm{EMNA}_{\mathrm{g}}, \mathrm{AMaLGaM}, \mathrm{JADE}$, and SaDE on 0, 1, 0, and 0 problems, respectively. This indicates that EMSM-EDA can achieve the excellent performance on these real-world optimization problems. Furthermore, this paper conducts Friedman's test on five comparison algorithms based on seven real-world engineering optimization problems. Table 14 shows that EMSM-EDA has the best ranking, sequentially followed by AMaLGaM, SaDE, JADE, and $\mathrm{EMNA}_{\mathrm{g}}$. Therefore, the above results demonstrate the potential of EMSM-EDA on the real-world optimization problems.

Table 12
Seven real-world optimization problems.
Table 13
The means and the standard deviations of FEVs obtained by the five algorithms over 25 independent runs on seven real-world optimization problems.

"+", "", and " $\approx$ " indicate that the performance of the corresponding algorithm is better than, worse than, and similar to that of EMSM-EDA, respectively.

Table 14
Rankings of EMSM-EDA, AMaLGaM, SaDE, JADE, and EMNA $_{\mathrm{R}}$ on seven real-world optimization problems based on Friedman's test.
# 6. Conclusion 

In this paper, an efficient mixture sampling model named EMSM is presented. Different from the random sampling, EMSM maintains a good tradeoff between the diversity and the convergence by the uniform method, the orthogonal method, and the mirrored sampling. Moreover, EMSM-EDA is developed, which combines EMSM with EDA ${ }^{2}$. In addition, the diversity and convergence of EMSM are analyzed. Finally, the experimental results indicate that EMSM-EDA is a global optimization algorithm with excellent performance.

Although EMSM-EDA has achieved competitive performance, there are still several issues worth investigating in the future. Firstly, we will further study new sampling methods to explore promising regions. For example, the Markov Chain Monte Carlo method can be used to adaptively select the promising mutation vectors according to the historical performance, so as to obtain better solutions. Secondly, the computational complexity of EMSM-EDA is $O\left(n^{2}\right)$, which can lead to longer runtime in solving large-scale problems. The calculation cost of EMSM-EDA mainly depends on the calculation and decomposition of covariance matrix. It is very interesting to try to use the new matrix decomposition method to improve the computational efficiency. Finally, it is meaningful to extend EMSM-EDA to multimodal optimization problems, dynamic optimization problems, constrained optimization problems, and large-scale optimization problems.

## CRediT authorship contribution statement

Qianlong Dang: Conceptualization, Methodology, Software, Writing - original draft, Visualization. Weifeng Gao: Methodology, Supervision, Funding acquisition. Maoguo Gong: Validation, Writing - review \& editing, Software.

## Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## Acknowledgment

This work was supported in part by the National Nature Science Foundation of China under Grant 61772391 and Grant 61966030, in part by the Natural Science Basic Research Plan in Shaanxi Province of China under Grant 2022JQ-670, in part by the Fundamental Research Funds for the Central Universities under Grant YJS2215.
