# PLOS ONE 

## RESEARCH ARTICLE

## Solving dynamic multi-objective problems with a new prediction-based optimization algorithm

Qingyang Zhang ${ }^{1 *}$, Shouyong Jiang ${ }^{2}$, Shengxiang Yang ${ }^{3}$, Hui Song ${ }^{1}$<br>1 School of Computer Science and Technology, Jiangsu Normal University, Xuzhou, CO, China, 2 School of Computer Science, University of Lincoln, Lincoln, CO, United Kingdom, 3 School of Computer Science and Informatics, De Montfort University, Leicester, United Kingdom<br>* sweqyian @ 126.com

## Abstract

This paper proposes a new dynamic multi-objective optimization algorithm by integrating a new fitting-based prediction (FBP) mechanism with regularity model-based multi-objective estimation of distribution algorithm (RM-MEDA) for multi-objective optimization in changing environments. The prediction-based reaction mechanism aims to generate high-quality population when changes occur, which includes three subpopulations for tracking the moving Pareto-optimal set effectively. The first subpopulation is created by a simple linear prediction model with two different stepsizes. The second subpopulation consists of some new sampling individuals generated by the fitting-based prediction strategy. The third subpopulation is created by employing a recent sampling strategy, generating some effective search individuals for improving population convergence and diversity. Experimental results on a set of benchmark functions with a variety of different dynamic characteristics and difficulties illustrate that the proposed algorithm has competitive effectiveness compared with some state-of-the-art algorithms.


## 1 Introduction

The progress of optimizing multiple mutually conflicting objectives simultaneously and obtaining a set of tradeoff solutions is regarded as Multi-objective optimization problems (MOPs) [1], which involves different fields, including controller design [2], weapon selection [3] and machine learning [4]. Simultaneously, various multiobjective optimization algorithms have been proposed for solving MOPs successfully. Considering a minimization multiobjective optimization problem as follows,

$$
\min _{x \in \Omega} F(x)=\left(f_{1}(x), f_{2}(x), \ldots, f_{m}(x)\right)^{T}
$$

where $\Omega=\prod_{i=1}^{D}\left[L_{i}, U_{i}\right] \subset R^{D}$ is the feasible area of the decision space, and $F$ consists of $m$ time-varying objective functions. $x=\left(x_{1}, x_{2}, \ldots, x_{D}\right)$ defines the decision vector involving $D$ variables, $L_{i}$ and $U_{i}$ represent the lower and upper bounds of the $i$ th variable $x_{i}$, respectively.

62006103 and 61872168, in part by the Jiangsu national science research of high education under Grand 20KJB110021.

Competing interests: The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

For two given decision vectors $x$ and $y$, if $\forall j \in[1, m], f_{i}(x) \leq f_{i}(y)$ and $\exists l \in[1, m] f_{l}(x)<f_{l}(y)$, then, $x$ dominates $y$ regarded as $x \prec y$. If a vector $x^{\circ}$ can dominate any other solutions, $x^{\circ}$ is defined as Pareto optimal solution.

However, recent years, there exist an increasing number of multi-objective optimization problems recognised in various fields, such as scheduling [5, 6], planning [7, 8], resources allocation [9, 10], constrained optimization [11], and machine learning [12], needed to be solved in dynamic or uncertainties environment, which are named dynamic multi-objective optimization problems (DMOPs). The main characteristic of this kind of problem is that the constraints, the Pareto optimal set (POS) or Pareto-optimal front (POF), and the relevant control parameters can change dynamically, which brings great challenges to optimization algorithms. It has attracted a growing attention for exploring efficient optimization algorithms and obtaining high quality optimal solution sets. Although there may exist different classes of dynamic optimization problems, according to [1], this paper considers the following mathematical model of DMOPs.

$$
\min _{x \in \mathcal{M}} F(x, t)=\left(f_{1}(x, t), f_{2}(x, t), \ldots, f_{m}(x, t)\right)^{T}
$$

where $t$ is the time instant of the problem.
Compared with MOPs, dynamic dynamic multi-objective optimization problems have two important features: multiobjectivity and dynamism. It is generally known that multiobjectivity usually involves multiple conflicting objectives, which means the optimal solution of the problem will no longer be a single optimal value, but an optimal solution set containing tradeoff solutions. Dynamism in constraints and/or parameters causes the change of POF or POS and poses big difficulties to evolutionary algorithms. DMOPs are challenging due to the dynamic nature. They can be divided into a sequence of MOPs over the course of time. That is, the optimization goal is to obtain a sequence of approximations to the moving POS/POF.

## 2 Related work

In recent years, much effort has been devoted to designing efficient and effective dynamic multi-objective evolutionary algorithms (DMOEAs). A widely used framework of DMOEAs in literature can be described as Algorithm 1. As shown in this framework, the whole procedure of solving DMOPs contains two main components: change detection and multi-objective algorithms including MOEAs and DMOEAs.

Algorithm 1 The basic framework of DMOEA
1: Initialize time instance $t=1$;
2: Generate an initial population Pop ${ }_{1}$;
3: While the termination criterion is not satisfied
4: Change Detection
5: If change is not detected, evolve population using MOEA;
6: Otherwise, evolve population using DMOEA;
7: Return 3.

### 2.1 Change detection

As a significant component of DMOEAs, change detection is responsible for determining whether the environment has changed and in turn whether to implement a reaction mechanism. The existing dynamic extraction methods contain two categories: re-evaluating solutions [13-15] and checking population statistical information [16]. The former is more widely used in many algorithms because it is simple and easy to implement, but it is likely sensitive to

noise. In contrast, the latter is robust to noise, but it needs some additional parameters. Each method has its advantages and limitations for different DMOPs.

# 2.2 Multi-objective optimization algorithms 

Apart from the dynamic reaction mechanism, MOEAs are significant components of solvers for DMOPs, since DMOPs can be regards as a sequence of MOPs. That is, any MOEAs can be directly used to evolve the population during the (short) period of any static environments.

As one of the most attractive and popular areas in intelligent computing field, the existing Multi-objective optimization algorithms can be classified into three categories as follows. The first class is Pareto ranking-based algorithms, which are designed based on the dominated relationships among population individuals. Some representative algorithms include the nondominated sorting genetic algorithm II (NSGA-II) [17], and strength Pareto evolutionary algorithm (SPEA2) [18]. Besides that, some classic and recent proposed efficient swarm intelligence algorithms inspired by different nature behaviors have also used to solve MOPs, such as multi-obejctve particle swarm optimization (MOPSO) [19], Multi-Objective Grasshopper Optimisation Algorithm (MOGOA) [20], Multi-Objective Multi-Verse Optimizer (MOMVO) [21], Multi-Objective Ant Lion Optimizer [22], and Multi-objective Salp Swarm Algorithm (MSSA) [23], and so on. Although the non-dominant ranking strategy can well screen out excellent individuals, it also produces marginal individuals, which generate negative effect on the whole optimization process. These algorithms can obtain good local optimal solutions, but it is difficult to achieve ideal global optimal solutions.

The second class is indicator-Based algorithms, which are designed based on the performance indicators. The hypervolume [24], the epsilon indicator and the R2 one are the most utilized for proposed various algorithms, such as, indicator-based EA (IBEA) [25], S-metric selection EMO algorithm [26], R2 EMO algorithm (R2EMOA) [27], and approximationguided EMO (AGE) [28]. The last class is decomposition-Based algorithms, which aim to decompose the MOP into some optimization sub-problems and solve them simultaneously. The most used algorithms are NSGA-III [29, 30], and MOEA based on decomposition (MOEA/D) [31, 32]. Although this kind of algorithm is efficient, the division of sub-problems depends on the weight deeply.

### 2.3 Dynamic multi-objective optimization algorithms

Depending on the frequency or severity of change, changes may present various challenges, such as the finite computational time or resources to overcome the change, time-varying feasible region and constraint conditions. Therefore, effective and efficient dynamic multi-objective optimization algorithms are indeed important. Diversity and convergence are two important aspects in designing high-quality optimization methods, since the former aims to prevent the search from local optima whereas the latter helps algorithms to find promising solutions rapidly. Designing an effective strategy that is able to balance the diversity and convergence is one of the key topics in DMOPs. Existing Dynamic multi-objective optimization algorithms can be divided into four categories: diversity based algorithms, memory based algorithms, multi-population based algorithms, and prediction based algorithms.

The main purpose of diversity based algorithms is to maintain the search population diversity for avoiding local optima when a change is detected. Recently, a increasing number of diversity maintenance methods have been proposed. A general framework proposed by Li [33] maintains the diversity by utilizing hierarchical linkage clustering, which is able to generate subpopulations with good diversity while avoiding overlapping. Query-based strategy proposed by Chang et al [34] increases the population diversity by providing a guidance to

particles. Immigration-based strategy aims to prevent local optima and achieve better search ability, such as hybrid immigration [35], memory-based immigration [36] and elitism-based immigration [37]. Besides that, hyper-mutation has been employed to combine with the nondominated sorting genetic algorithm II (NSGAII) [38] to create two different dynamic versions for DMOPs.

The main idea of memory based algorithms is to record some historical information, which can be reused to accelerate the convergence of algorithms whenever a change occurs. Branke [39] suggests that the best individuals in previous change environments were stored in an archive firstly, and used to replace some members of the existing population. Goh [40] proposed a strategy that employs an new population to replace the out-of-date archived members, which integrates competitive and cooperative mechanisms for DMOPs. In [41], memory, local search and random techniques are integrated, and an adaptive hybrid population management strategy is proposed by authors. Jiang and Yang [42] used a steady-state manner to respond to changes. These kinds of algorithms performs well on problems with periodical changing feature.

The main idea of multi-population based algorithms is that multiple subpopulations can be advantageous at maintaining diversity. In [43], a self-organizing scouts method is proposed by dividing the search population into two subpopulations, which are used to search in feasible regions. Li [44] combined an island model with particle swarm optimization for dealing with dynamic vehicle routing problems. Yang [45] employs hierarchical clustering to divide the population into several subpopulations of different sizes for effective diversity maintenance [46].

Prediction based algorithms aims to predict a possible POF/POS locations of new environments based on the solutions in previous environments. These algorithms are much popular in DMOEAs, since prediction-based mechanisms could help tracking the moving POS/POF if solutions in new environments are well predicted. Muruganantham [47] proposed a DMOEA by combining Kalman filter with evolutionary methods for solving DMOPs. The multimodal prediction approach proposed by Rong [48] refers to generate an effective initial population for the subsequent evolution. Population Prediction Strategy (PPS) [49] proposed by Zhou et al. is used to predict the manifold of the whole search population by using the univariate auto-regression (AR) model. Besides that, many other prediction approach have been proposed in different ways, such as multi-directions [50], knee points [51], center points [52], and boundary points [53].

Most of the existing DMOEAs have been proposed, showing promising performance in various applications. However, they neglect properties of decision variables, which is an important part of discovering high-quality search individuals. Simultaneously, according to [54,55], curve fitting technique is a classic and popular technique, which can reflect the distribution relationship between variables to a certain extent and predict possible regions or directions. Motivated by this, this paper proposed a novel method for predicting a high-quality population based on the distribution and classification characteristics of variables after a change is detected. The proposed algorithm contains three different parts, firstly, a simple linear prediction strategy with two different stepsizes is designed to predict non-dominated solutions based on the information of previous environments. The second strategy is proposed by integrating fitting-based strategy for generating new members and improving the quality of population based on the probability distribution of variables. The last strategy aims to generate well-distributed individuals based on the classification features of decision variables. Numerical results on 14 benchmark functions show that the proposed algorithm performs well on tracking time-varying POF or POS.

The following summarizes the organization of this work. Section 2 presents the related work. The proposed algorithm is provided in Section3. In Section 4, the performance of the proposed technique is validated and analyzed on a comprehensive set of benchmark functions. Section 5 gives further discussion about the proposed algorithm. Section 6 concludes the paper.

# 3 Proposed DMOEA 

This section mainly provides the main content of the proposed algorithm in detail. Like other predicted algorithms, our hypothesis is that there is sort of similarity between two consecutive changes. As obtained from the basic framework of the proposed algorithm listed in Algorithm 2, the main idea combines RM-MEDA with a new prediction-based dynamic reaction mechanism, which has three different strategies for predicting a new high-quality search population that tracks the new POS/POF efficiently and effectively.

Algorithm 2 The overall framework of the proposed algorithm
1: Initialize parameter settings.
2: Initialize and evaluate population ( $\mathrm{Pop}_{\text {Gen }}$ ) and set Gen $=1$.
3: If the stop condition is not satisfied.
4: If change detected, go to step 5; otherwise, go to step 10.
5: Generate the first subpopulation $\left(\right.$ SubPop $_{1}$ ) using a linear prediction model.
6: Generate the second subpopulation $\left(\right.$ SubPop $_{2}$ ) based on new fittingbased strategy.
7: Generate the third subpopulation $\left(\right.$ SubPop $_{3}$ ) by recent proposed sampling strategy [56].
8: Merge these subpopulations MixPop $=$ SubPop $_{1}$ USubPop $_{2}$ U SubPop $_{3}$.
9: Obtain a population of Popsize by non-dominated sorting the merged population.
10: Optimize population using RM-MEDA.
11: Gen = Gen + 1, return to 3.

### 3.1 Linear prediction model

This subsection mainly employs a simple linear prediction model with two different stepsizes for predicting non-dominated set. From statistical point of view, the geometric center is an important characteristic and can be used to represent the changing trend of population to some extent. Here, we compute the moving direction of the center points of the last two consecutive populations and use it to predict the position of the non-dominated members of current population in the new environment.

Suppose that $P c_{t}$ is the centroid of population $\left(P o p_{t}\right)$ and $P o s_{t}$ is the non-dominated sets of $P o p_{t}$ at the time $t$. Then, the $p c_{t}$ can be calculated as follows.

$$
P c_{t}=\frac{\sum_{x_{i} \in P o p_{t}} x_{i}}{\left|P o p_{t}\right|}
$$

where $\left[\operatorname{Pop}_{t}\right]$ is the population size, $x_{i}=\left(x_{i}^{1}, x_{i}^{2}, \ldots, x_{i}^{D}\right)$ defines the decision vector of a solution at time $t$. Then, the moving direction $\left(\operatorname{dir}_{t}\right)$ of center points at time $t$ can be calculated by

$$
d i r_{t}=p c_{t}-p c_{t-1}
$$

Then, the new position of members in $\operatorname{Pos}_{t}$ at time $t+1$ can be obtained by $d i r_{t}$ and $P o s_{t}$ according to the following formula:

$$
P o s_{t+1}=P o s_{t}+d i r_{t} \times s t e p
$$

![img-0.jpeg](img-0.jpeg)

Fig 1. Illustration of linear prediction model.
https://doi.org/10.1371/journal.pone.0254839.g001
where step refers to the moving stepsize along the moving direction of $d i r_{t}$. Here, two different values of step (i.e., 0.3 and 1.0, ) are used, representing a small and large movement of $\operatorname{Pos}_{t}$, respectively. Fig 1 illustrates the prediction process.

As shown in Fig 1, $p c_{t}$ and $p c_{t-1}$ (black points) are utilized to obtain $d i r_{t} . \operatorname{Pos}_{t}$ moves to three different regions described by $P o s_{t+1}^{t+1}$ and $P o s_{t+1}^{t+2}$ using the suggested step values. A combination of these two solution predictions is more likely to approximate the true POS of population $\left(\operatorname{Pos}_{t+1}^{t n}\right)$ at time $t+1$. Algorithm 3 provides the implementation of this prediction strategy.

Two questions may arise here, on the one hand, the motivation about the two-step prediction strategy to produce good individuals. In the ideal environment, The widely used one-step strategy assumes the change between two continuous times is same to some extent. This proves effective in various algorithms and we would also like to keep it in our algorithm. However, as suggested by [38], sometimes a small variation to the population can be very effective. This inspires us that a smaller stepsize than the previous stepsize would be helpful in creating population individuals for environments that do not change significantly. That is, a smaller moving step may ensure that the predicted solution is much closer to the new POS after a change. As a result, this work attempts to design a two-step prediction strategy for DMOPs.

On the other hand, how to determine stepsize parameters is a major issue. The proposed strategy employs two stepsize values ( 0.3 and 1.0), which represents two different moving levels (small and normal). There are two reasons for this setting. First, step $=1$ for the normal level is set according to fuzzy systems [57], which means that the change is similar to the previous change (normal changes). Second, the stepsize step $=0.3$ for the small level should be

smaller than that for the normal level. The stepsize setting is chosen not only for simplicity but also by sensitivity analysis as will be detailed in in Section 4.

Algorithm 3 Linear prediction model
1: Retrieve the populations $P o p_{t}$ and $P o p_{t-1}$ at time $t$ and $t-1$, respectively;
2: Calculate the population centers according to Eq (2);
3: Predict moving direction according to Eq (3);
4: Generate three subpopulations $P o n_{t+1}^{p t}$ and $P o n_{t+1}^{p q}$ using Eq (4) with different step values;
5: Save the subpopulations to SubPop ${ }_{1}$.

### 3.2 Curve fitting-based strategy

This subsection proposes a curve fitting-based strategy for generating high-quality search individuals based on the distribution relationship of variables. As suggested in [56], the variables can be classified into two parts: principal and non-principal parts. We believe the correlation between principal variables and non-principal variables can be exploited to speed up the search. For example, if a variable $x_{2}$ is highly correlated with another variable $x_{1}$, then we can generate values for $x_{2}$ based on the values of $x_{1}$. As shown in Fig 2, the curve fittings at time $t-1$ and $t$, denoted $C F_{t-1}$ and $C F_{t}$ respectively, are computed by a polynomial fitting strategy on the corresponding non-dominated set. Then, the relationship between variables in the new environment $\left(C F_{t+1}\right)$ can be predicted using the last two consecutive $C F_{t-1}$ and $C F_{t}$.

$$
\text { move }_{t}=C F_{t}+\left(C F_{t}-C F_{t-1}\right)
$$

![img-1.jpeg](img-1.jpeg)

Fig 2. Illustration of curve fitting-based strategy.

Then, the possible curve fitting characteristic of at time $t+1$ can be calculated as

$$
C F_{t+1}=C F_{t}+\text { move }_{t}
$$

In addition, individuals in the third subpopulation can be generated using the following formula,

$$
\operatorname{Ind}_{t+1}=C F_{t+1}+c r \times N D_{p}
$$

where $c r$ is the compression radio, which ensures that the newly generated individual surrounds the curve fitting closely. $N D_{p}$ refers to the normal distribution based on the $p$ th variable, since this can make that the newly generated variables meet the characteristics of curve fitting as much as possible.

The implementation of this strategy is shown in Algorithm 4. Specifically, the the most principal variable is identified by the correlation matrix of variables, and the other variables are regards as non-principal variable. Then, for each non-principal variable, the corresponding values can be predicted by the curve fitting model which uses the values of the principal variable sampled from normal distribution. After, another subpopulation can be created by concatenating all the variables.

```
Algorithm 4 Implementation details of Curve fitting-based strategy
    1: Find the populations (Pop, and Pop, -1) at time t and t - 1,
respectively.
2: Compute the correlation matrix for each non-principal variable \(x\);
at time \(t-1\).
3: Estimate the new curve fitting feature for each \(x_{\lambda}\) at time \(t+1\)
according to Eq (7).
4: Create a subpopulation SubPop, by sampling from the decision space.
5: Calculate the bounds of \(x_{\lambda}\).
```


# 4 Experimental studies 

This section evaluates the performance of the proposed algorithm through experimental studies. It includes details about benchmark functions, performance indicators, compared methods, parameter settings and numerical results.

### 4.1 Test instances

This work utilizes a set of recently proposed DF problems with various difficulties, such as variable linkage, disconnectivity, irregular POF shapes, and time-dependent geometries. All parameter settings keep the same with the suggestion according to the literature [58].

### 4.2 Performance indicators

This study employs three widely used performance indicators described as follows for evaluating the effectiveness of the proposed algorithm.
4.2.1 Mean Inverted generational distance (MIGD). The first performance indicator is MIGD, which is utilized to evaluate the convergence and diversity of solutions obtained by an algorithm, and the mathematical equation is provided as follows [56, 59].

$$
I G D\left(P O F_{t}^{s}, P O F_{t}^{s b}\right)=\frac{\sum_{g \in P O F_{t}^{s}} d\left(g, P O F_{t}^{s b}\right)}{\left|P O F_{t}^{s}\right|}
$$

where $P O F_{t}^{s}$ is the true POF solutions, $P O F_{t}^{s b}$ is a POF approximation, $d\left(g, P O F_{t}^{s b}\right)$ is the minimum Euclidian distance between $g$ and the points in $P O F_{t}^{s b}$, and $\left|P O F_{t}^{s}\right|$ is the number of

solution in $P O F_{t}^{*}$. Then, the MIGD can be computed as

$$
M I G D=\frac{\sum_{t \in T} I G D\left(P O F_{t}^{*}, P O F_{t}^{* h}\right)}{|T|}
$$

where $T$ is a set of times instance and $|T|$ is the total number of changes in a run.
4.2.2 Mean Schott's Spacing Metric (MSP). The second performance indicator is the Schott's spacing metric, which is used to measure the distribution of the obtained solutions $P O F_{t}^{* h}$ using the following formula:

$$
S P\left(P O F_{t}^{* h}\right)=\sqrt{\frac{1}{\left|P O F_{t}^{* h}\right|-1}\left(\sum_{i=1}^{\left|P O F_{t}^{* h}\right|}\left(D_{i}-\bar{D}\right)\right)}
$$

where $D_{i}$ represents the Euclidean distance between the $i$ th point in $P O F_{t}^{* h}$ and its nearest point in $P O F_{t}^{* h} . \bar{D}$ is the average value of $D_{i}$. The MSP can be defined as follows:

$$
M S P=\frac{\sum_{t \in T} S P\left(P O F_{t}^{* h}\right)}{|T|}
$$

4.2.3 Hypervolume metric. The second performance indicator is Hypervolume (HV) [48, 53], which is a important metric for evaluating solutions. Different from the other indicators mentioned above, HV needs to set a reference vector dominated by any points in the $P O F_{t}^{*}$.

$$
H V_{t}=H V\left(P O F_{t}^{* h}\right)
$$

where $H V\left(P O F_{t}^{* h}\right)$ refers to the hypervolume [52] of set $P O F_{t}^{* h}$. The reference point for the computation of hypervolume is $\left(z_{j}+0.5, j=1, \ldots, m\right)$, where $z_{j}$ is the maximum value of the $j$ th objective of true POF. The MHV can be calculated as follows:

$$
M H V=\frac{\sum_{t \in T} H V_{t}}{|T|}
$$

4.2.4 T-test. To determine whether the results obtained by the proposed algorithm are essentially difference from the results computed by other algorithms, the t-test at a 0.05 significance level is employed to check the experimental results of all optimization methods [60]. A $p$-value less than 0.05 indicates that the performance of two compared techniques is statistically different $(h=1)$, otherwise, there is no significant difference $(h=0)$. Meanwhile, the bottom of each Table summarizes the comparison results, $\mathbb{1}, \dagger$ and $t$ indicate that the performance of FBP is better than, worse than and similar to that of the corresponding algorithm, respectively.

### 4.3 Compared algorithms

In this section, several existing approaches are selected to compare with the proposed technique. A brief description of these algorithms and parameter settings is summarized as follows.
4.3.1 Population Prediction Strategy (PPS). The main idea of PPS is to divide the PS/PF into two parts: population center and manifold. Autoregression (AR) model is adopted to predict the next population center based on a time series of historical population centers.

Similarly, historical manifolds are also used to predict new manifold. Then, A new population will be assembled based on the predicted population center and manifold [48].
4.3.2 TrDMOEA. TrDMOEA is an approach integrating transfer learning strategy and evolutionary algorithms to solve DMOPs. This main idea of this technique is that the agents at different times have different distributions for generating an effective search population. More details can be found in the literature [4].
4.3.3 MOE. MOE is a mixture-of-experts-based computation framework with multiple prediction mechanism for generating robust POS and enhancing the overall prediction quality in dealing with DMOPs. Experimental results illustrate that MOE has significant performance with respective to other dynamic optimization algorithms. More details can be found in the literature [61].
4.3.4 MOEA/D-FD. First-order difference model-based MOEA/D algorithm (MOEA/ D-FD) [62] utilizes historical information to predict the location of the new POS after a change is detected. The new population is composed of two kinds of solutions: the old solutions and the predicted ones. The movement of population centroid defines a predicted direction. To make the new population diversified, evenly-distributed individuals selected from the previous population are used in the prediction.
4.3.5 MOGOA. The Grasshopper Optimisation Algorithm (GOA) models is proposed according to the behaviour of grasshopper swarms in nature, and a multi-objective version of Grasshopper optimization algorithm, MOGOA, is also designed for solving different multiobjective optimization problems. To enhance the distribution of solutions, an archive and a roulette wheel selection technique are integrated to the algorithm, and the individuals with uncrowded distance tend to be deleted for avoiding premature convergence. More details can be found in the literature [20].
4.3.6 MOMVO. The multi-verse optimization is proposed by imitating the white hole, black hole and wormhole mechanisms, which correspond to three different search strategies, exploration, exploitation, and local search, respectively. Meanwhile, a multi-objective version of multi-verse optimization, MOMVO, is also designed for solving different multiobjective optimization problems. In which, a leader selection strategy is utilized to choose the better agents from the archive, in addition, all the individuals will be ranked based on crowded distance with its neighbourhoods, and will be selected using the roulette wheel strategy for maintaining the convergence and diversity. More details can be found in the literature [21].
4.3.7 MOALO. The ALO algorithm, a new population-based optimization technique, is proposed by simulating the interaction and hunting behaviors of antlions in nature. Recently, it is also considered as an extended version, Multi-objective ant lion optimizer (MOALO), in which the non-dominated relationships and roulette wheel strategy are utilized to generating promising solutions. In addition, a set of benchmark functions and some constrained engineering design problems are cited to check the performance of MOALO. More details can be found in the literature [22].
4.3.8 MSSA. The SSA algorithm is designed based on the swarming behavior of salps when navigating and foraging in oceans for solving various optimization problems. Recently, it is also considered as an extended version, Multi-objective Salp Swarm Algorithm (MSSA), in which the guidance solution is selected from a set of non-dominated solutions based on ranking process and roulette wheel selection strategies, and the individuals with low rank tend to be deleted probability for maintaining the scale of archive. More details can be found in the literature [23].

# 4.4 Parameter settings 

The parameters of the MOEAs considered in the experiment were referenced from their original papers. Some key parameters in these algorithms were set as follows.
4.4.1 Population size. The population size $(N)$ in all the algorithms was set to 100 . Around 1000 points were uniformly sampled from the true POF for computing the performance metrics in both bi- and three-objective cases.
4.4.2 Other parameters. All the parameters in the compared algorithms used the same settings as in their original studies.

Parameters in FBP: the degree of polynomial fit ( $d p f$ ) is set to 2 , the size of $\operatorname{SubPop}_{2}$ was set to 0.4 N , the parameters of the third strategy are seen [56].
4.4.3 Stopping criterion and the number of executions. Each algorithm terminates after a prespecified number of generations and should cover all possible changes. To minimize the effect of static optimization, we gave 50 generations for each algorithm before the first change occurs. The total number of generations was set to $3 n_{t} \tau_{t}+50$, which ensures there are $3 n_{t}$ changes during the evolution. Additionally, each algorithm was executed 25 independent times on each test instance.
4.4.4 Change detection. For all the algorithms, a maximum number of $10 \%$ population are re-evaluated for change detection.

### 4.5 Experimental results

The severity of change $\left(n_{t}\right)$ and the frequency of change $\left(\tau_{t}\right)$ are two significant parameters in benchmark functions. To investigate the influence of these parameters on algorithms' performance, they are set to different values $(5,10,20)$ in this section. Tables 1-9 summarize the numerical results obtained by different algorithms, and the best values are also highlighted in bold face.

The MIGD results of all the algorithms are recorded in Tables 1-3, and it can be seen that FBP has the best values compared with its peers for most of the benchmark functions. However, for two functions DF4 and DF8, FBP is not able to obtain the best value, but the difference is not large according to the statistical $p$-values. When the $\tau_{t}$ is set to 10 , FBP generates the best result on DF1. Meanwhile, for different levels of $n_{t}$ and $\tau_{t}$, the proposed technique still can achieve the best result on majority of the functions. This shows that the designed prediction strategies can generate good population tracking the true POF closely in dynamic environments.

As shown in Tables 4-6, which summarizes the MHV values of all the algorithms, although FBP has great better MHV values than the other techniques on a majority of the problems, it is not effective enough for solving DF5, DF6 and DF11 based on the statistical $t$ - test results. In addition, MOE has little advantage over the others on DF9 and DF14. Therefore, the MHV metric further demonstrates that the proposed strategy responds to changes well.

It is observed from Tables 7-9, which lists MSP results obtained by all the algorithms, that although FBP can obtain the best solution on most of two bi-objective problems, e.g., DF1, DF5 and DF7, it seems ineffective in a few three objective problems, but the difference is not significant according to the statistical $p$-values. MOEA/D-FD obtains best distribution of solutions in other cases. MOEA/D-FD benefits from the even weights in its decomposition approach that improves the distribution of solutions. On the contrary, the other MOEAs utilizes dominance-based environmental selection approaches, which may not generate as uniform solutions as the decomposition-based technique, especially in three-objective problems. Besides that, well-distributed solutions does not mean that they approximate the true POF closely. MOEA/D-FD performs better than FBP in terms of MSP, but it is weaker than FBP on

Table 1. Mean and standard deviation values of MIGD obtained by five algorithm for $\left(n_{i}, \tau_{i}\right)=(5,20)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t001

Table 2. Mean and standard deviation values of MIGD obtained by five algorithm for $\left(n_{t}, \tau_{t}\right)=(10,10)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t002

Table 3. Mean and standard deviation values of MIGD obtained by five algorithm for $\left(n_{i}, \tau_{i}\right)=(10,20)$.
https://doi.org/10.1371/journal.pone.0254839.t003
the other two indicators, i.e., MIGD and MHV, which are more reliable to distinguish between algorithms in terms of the overall performance.

As described before, it is obvious that the frequency of changes exerts a certain influence on algorithms' performance. In three-objective functions, frequent changes increase the difficulty

Table 4. Mean and standard deviation values of MHV obtained by five algorithms for $\left(n_{i}, \tau_{i}\right)=(5,20)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t004

Table 5. Mean and standard deviation values of MHV obtained by five algorithms for $\left(n_{i}, \tau_{i}\right)=(10,10)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t005

Table 6. Mean and standard deviation values of MHV obtained by five algorithms for $\left(n_{i}, \tau_{i}\right)=(10,20)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t006

Table 7. Mean and standard deviation values of MSP obtained by five algorithms for $\left(n_{t}, \tau_{t}\right)=(5,20)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t007

Table 8. Mean and standard deviation values of MSP obtained by five algorithms for $\left(n_{i}, \tau_{i}\right)=(10,10)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t008

Table 9. Mean and standard deviation values of MSP obtained by five algorithms for $\left(n_{i}, \tau_{i}\right)=(10,20)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t009

of finding high-quality approximations to the POF, as shown by the large MIGD and MHV results recorded in Tables 1-9, respectively. Overall, FBP seems less sensitive to the frequency and severity of change, as can be observed from its gradual improvement on the three measures when $\tau_{r}$ and $n_{t}$ increase in most cases, for which the compared algorithms have drastic changes in their performance.

Fig 3 presents some convergence graphs of the mean IGD values for a majority of the benchmark functions. It is obvious that FBP shows more stable ability and recovers faster from dynamic changes in most case, thereby gaining higher convergence process compared with the others. For DF10, FBP does not perform well for the first a few environments, but it has significant advantage over its peers in later environments. The overall performance of FBP is better than the others on DF8.

Figs 4-7 plot some POF approximation on DF3, DF5, DF7 and DF8, which are intuitive representations of the solutions. It is obvious that FBP performs better than the compared
![img-2.jpeg](img-2.jpeg)

Fig 3. Mean IGD curves for different problems with $n_{t}=10$ and $\tau_{r}=1$.
https://doi.org/10.1371/journal.pone.0254839.g003

![img-3.jpeg](img-3.jpeg)

Fig 4. POF approximations of five algorithms for DF3 with $n_{2}=10$ and $r_{2}=10$.
https://doi.org/10.1371/journal.pone.0254839.g004
algorithms. The approximations demonstrate clearly that FBP has excellent tracking ability in varying environments, but it may generate some boundary individuals in DF8.

Apart from the above analysis, to investigate the performance of the proposed dynamic dynamic multiobjective algorithm further, some recent MO algorithms (MOGOA, MOMVO, MOALO and MSSA) are employed for comparisons. They are equipped with the same reaction mechanism used in FBP, Tables 10-12 record the simulation results including mean values, standard deviation and t-test values. It can be seen that FBP outperforms the compared algorithms on the majority of test problems based on MIGD and MHV results, and the $p$-values summarized in the bottom of Tables also indicate that the differences among them are significant. For the MSP, the advantages of the algorithm are not obvious on the three functions

![img-4.jpeg](img-4.jpeg)

Fig 5. POF approximations of five algorithms for DF5 with $n_{t}=10$ and $\tau_{t}=10$.
https://doi.org/10.1371/journal.pone.0254839.g005
(DF2, DF11 and DF13), but the $p$-values show that the differences among them are not significant. Totally, FBP is able to generate competitive results with respective to other compared approaches.

# 5 Discussion 

### 5.1 Component analysis

As mentioned before, the proposed strategy contains three different key components. This subsection aims to discuss the role that each component plays in dealing with dynamic environment. Specifically, to demonstrate the importance of the linear prediction model with two

![img-5.jpeg](img-5.jpeg)

Fig 6. POF approximations of five algorithms for DF7 with $n_{2}=10$ and $r_{2}=10$.
https://doi.org/10.1371/journal.pone.0254839.g006
different stepsizes, a one step prediction model is utilized to replace the proposed two steps strategy for predicting non-dominated solutions. This is, the step value is set to one (step $=1$ ), which is a common setting in most existing prediction-based techniques, and the variant is named FBPV1. To demonstrate that the fitting-based strategy has important effect on the proposed strategy, FBPV2 is designed by removing the sampling strategy; in the other words, FBPV2 just has two prediction strategies. Similarly, to study the role of the third strategy, FBP is also modified by excluding the reference sampling strategy, called FBPV3.

These three variants are compared with the original FBP, and Table 13 report the corresponding computing results. The following discusses the influence of each component in detail.

![img-6.jpeg](img-6.jpeg)

Fig 7. POF approximations of five algorithms for DF8 with $\boldsymbol{n}_{\boldsymbol{i}}=\mathbf{1 0}$ and $\boldsymbol{\tau}_{\boldsymbol{t}}=\mathbf{1 0}$.
https://doi.org/10.1371/journal.pone.0254839.g007
5.1.1 Linear prediction mode. It is clear that FBP is much superior to FBPV1 in terms of MIGD on some cases, but the differences among them are not too significant in most of test problems based on the $p$-values. The reason may come from the fact that FBP utilizes a twostep based prediction strategy, which would generate more boundary individuals than FBPV1.

Table 10. Performance comparison of different multiobjective algorithms variants on MIGD.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t010

Table 11. Performance comparison of different multiobjective algorithms variants on MHV.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t011

Table 12. Performance comparison of different multiobjective algorithms variants on MSP.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t012

Table 13. Performance comparison of different FBP variants on MIGD.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t013

Therefore, the population diversity can be affected by too much non-dominated boundary solutions immediately. Despite that, the overall performance of the two-step technique performs much better than one-step strategy for the majority of the benchmark functions.
5.1.2 Curve fitting-based strategy. It is not difficult to observe from the results that FBP outperforms the modified variant FBPV2 on most of the test functions. This means that the curve fitting-based strategy indeed helps improve the quality of population in varying environments. The reason may originate from the fact that the curve fitting-based strategy is designed by considering interlinks between variables, which helps to generate promising solutions to some extent.

The comparison between the three different variants and the proposed FBP illustrates that each part has an significant effect on the performance of FBP, and removing any of them reduces performance. Therefore, it is important to combine them together as in the FBP strategy.
5.1.3 Sampling strategy. All the results illustrate that FBP performs much better than FBPV2 for almost all test problems, although FBP is slightly weaker than FBPV3 for DF14 problem. Thus, the designed sampling technique is able to improve the search ability of population in each varying environment clearly and can further improve the effectiveness of the proposed dynamic multiobjective optimization algorithm.

### 5.2 Influence of step values

As described before, the linear prediction model employs two different stepsizes, which are set to 1 and 0.3 for predicting non-dominated solutions, respectively. Here, to study whether the step values are well configured, step $=1$ is fixed as it has proven effective in many prediction algorithms, and the other step is set to an increment of 0.2 from 0.1 to 0.7 (FBPS1-FBPS3). Numerical results in Table 14 for the fourteen functions shows that the algorithms become ineffective when step is too large shown by t-test values. The results illustrate that FBP outperforms other three versions on a majority of functions, although the differences between us are not very large on some cases. Therefore, it can be concluded from the experiment that FBP should utilize two different stepsize values ( 1 and 0.3 ) reasonably.

### 5.3 Influence of degree of polynomial regression

As a importance part of FBP, the curve fitting-based strategy has a significant parameter, the degree of polynomial regression $(d p f)$. Here, the $d p f$ is set to different values, with an increment of 1 , from 1 to 4 (FBPL1-FBPL3) for exploring its influence on algorithms' performance. The comparison results recorded in Table 15 show that the proposed technique is superior to the other versions on almost all the test problems. Although the higher the degree, the better the goodness of fit, too high degree may result in over-fitting. Thus, it is important to properly select the degree of polynomial regression and the experimental analysis supports the decision made to choose a degree of two.

### 5.4 Influence of $c r$ values

In the third strategy, the new prediction fitting curve is obtained based on Eqs (6) and (7). After that, it will be used to generate new individuals using (Eq 8), which involves two important parameters, the compression ratio (cr) and subpopulation $\left(\operatorname{Subpop}_{2}\right)$ size. The former is discussed in this subsection, and the latter will be analyzed below. $c r$ ranges from 0.1 to 0.7 , with an increment of 0.2 (FBPR1-FBPR3), and the results are summarized in Table 16. It is obvious that the original variant performs much better than the other three versions in almost

Table 14. Performance comparison of FBP variants on MIGD for $\left(n_{i}, \tau_{i}\right)=(10,10)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t014

Table 15. Performance comparison of FBP variants on MIGD for $\left(n_{i}, \tau_{i}\right)=(10,10)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t015

Table 16. Performance comparison of FBP variants on MIGD for $\left(n_{i}, \tau_{i}\right)=(10,10)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t016

all the problems. Especially in some cases, the difference between them are quite significant i.e., DF1, DF5, DF10. Therefore, 0.1 is the best one for $c r$ in this study.

# 5.5 Influence of Subpop $_{2}$ size 

Another important parameter is the Subpop $_{2}$ size. To investigate its influence, the Subpop $_{2}$ size changes with an increment of 0.1 from 0.2 to 0.5 times of the total population size (FBPQ1-FBPQ3). The comparison results recorded in Table 17 show that there is no best values for this parameter for all the test functions. For instance, some cases (e.g. DF3 and DF13) are sensitive to the parameter value, while other cases (e.g. DF1 and DF2) are not affected by this parameter too much. This experiment supports that FBP has much better performance compared with the other variants when Subpop $_{2}$ is defined as around 0.4 N , although it is not always the best. Thus, 0.4 N is chosen for this parameter in FBP.

### 5.6 Different Multi-objective algorithms

This subsection aims to verify the feasibility of the proposed dynamic reaction mechanism by combining it with four efficient and new proposed multiobjective algorithms.

### 5.7 More discussion

Apart from the aforementioned component and parameter analysis, this subsection further discusses the advantages and disadvantages of each strategy of the proposed technique. Firstly, the linear prediction strategy utilizes the two-step strategy for predicting non-dominated solutions, which increases the quality of the population in dynamic environments and improves the optimization performance. However, improvement comes at the cost of complexity, since compared with one-step strategy, the two-step strategy tends to generate more solutions. Meanwhile, these solutions contain some boundary individuals, which are not beneficial for global search, as shown in the numerical results where these boundary individuals are nondominated. Therefore, this strategy should be modified by controlling the boundary members effectively.

Secondly, to obtain well-distributed solutions, FBP employs a recent sampling strategy by classifying decision variables into two groups. Experimental results also show that it is also an effective way for solving multiobjective problems in varying environments. However, the strategy heavily depends on variable classification. This study assumes that there exists principle and non-principle variables, but it not clear about the generalisation of this assumption. Thus, this strategy also needs to be improved effectively to avoid the principal being misidentified.

Thirdly, the curve-fitting based strategy aims to predict a subpopulation based on the distribution characteristic among variables in two consecutive environments. Simulation results show that it enhances performance in bi-objective problems, but is not helpful for triple-objective problems. Therefore, further improvement should be make on this strategy.

## 6 Conclusion

This paper proposed a new dynamic multiobjective optimization algorithm, named FBP, for dealing with multiobjective problems in changing environments. FBP mainly includes three different components, that is, a two-step approach for predicting non-dominated solutions, a sampling strategy and a curve-fitting strategy. Each component has an important role for create high-quality population, improving either diversity or convergence, when a change occurs in the environment. To verify the effectiveness of our algorithm, a recent test suite with different characteristics is utilized. Experimental comparisons demonstrate that FBP has better

Table 17. Performance comparison of FBP variants on MIGD for $\left(n_{i}, \tau_{i}\right)=(10,10)$.
[^0]
[^0]:    https://doi.org/10.1371/journal.pone.0254839.t017

performance than the other algorithms on most cases, showing the proposed algorithm has a good tracking ability and responds fast to environmental changes. Besides, the role that each component and parameter plays in the proposed algorithm is also analysed and discussed extensively. In our future work, we will further improve the proposed algorithm by addressing some parameter issues as discussed previously.

# Supporting information 

## S1 File.

(ZIP)

## Acknowledgments

The authors express sincerely appreciation to the anonymous reviewers for their helpful opinions.

## Author Contributions

Investigation: Qingyang Zhang.
Methodology: Qingyang Zhang, Shouyong Jiang, Shengxiang Yang, Hui Song.
Resources: Shengxiang Yang.
