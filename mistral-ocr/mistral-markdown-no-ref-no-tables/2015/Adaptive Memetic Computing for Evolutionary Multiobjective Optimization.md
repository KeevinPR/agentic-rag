# Adaptive Memetic Computing for Evolutionary Multiobjective Optimization 

Vui Ann Shim, Kay Chen Tan, and Huajin Tang


#### Abstract

Inspired by biological evolution, a plethora of algorithms with evolutionary features have been proposed. These algorithms have strengths in certain aspects, thus yielding better optimization performance in a particular problem. However, in a wide range of problems, none of them are superior to one another. Synergetic combination of these algorithms is one of the potential ways to ameliorate their search ability. Based on this idea, this paper proposes an adaptive memetic computing as the synergy of a genetic algorithm, differential evolution, and estimation of distribution algorithm. The ratio of the number of fitter solutions produced by the algorithms in a generation defines their adaptability features in the next generation. Subsequently, a subset of solutions undergoes local search using the evolutionary gradient search algorithm. This memetic technique is then implemented in two prominent frameworks of multiobjective optimization: the domination- and decomposition-based frameworks. The performance of the adaptive memetic algorithms is validated in a wide range of test problems with different characteristics and difficulties.

Index Terms-Differential evolution, estimation of distribution algorithm, evolutionary gradient search, genetic algorithm, memetic computing, multiobjective optimization.


## I. INTRODUCTION

MANY real-world problems involve multiple objectives that are required to be optimized simultaneously. The problems become more difficult to solve when the objectives are conflicting and bounded by constraints. This type of problems is commonly known as multiobjective optimization problems (MOPs). Without loss of generality, minimization problems are considered throughout the paper. Mathematically, an MOP is interpreted as follows:

$$
\begin{gathered}
\text { Minimize: } \mathbf{f}(\mathbf{x})=\left(f_{1}(\mathbf{x}), f_{2}(\mathbf{x}), \ldots, f_{m}(\mathbf{x})\right) \\
\text { subject to: } \mathbf{g}(\mathbf{x}) \leq 0 \text { and } \mathbf{h}(\mathbf{x})=0
\end{gathered}
$$

where $\mathbf{f}(\mathbf{x})$ is the objective vector, $\mathbf{f}(\mathbf{x}) \in \mathbb{R}^{m}, \mathbb{R}^{m}$ is the objective space, $m$ is the number of objective functions, $\mathbf{x}=\left\{x_{1}, x_{2}, \ldots, x_{n}\right\}$ is the decision vector, $\mathbf{x} \in \mathbb{R}^{n}, \mathbb{R}^{n}$ is

[^0]the decision space, $n$ is the number of decision variables, $\mathbf{g}$ is the set of inequality constraints, and $\mathbf{h}$ is the set of equality constraints. Due to the conflicting property of the objective function, the optimal solutions consist of a set of tradeoff solutions rather than a single optimal solution. Thus, the definitions of optimality, which have been defined as Pareto set (PS) and Pareto front (PF), is different from single-objective problems. PS is the optimal set of solutions in the decision space when no other solutions are dominating them. The mapped solution set in the objective space is PF.

Over the past two decades, the studies of MOP have been surrounded on two main issues-the algorithm and framework issues. The algorithm issue focuses on the implementation of different optimizers, such as genetic algorithm (GA) [1], differential evolution (DE) [2], and estimation of distribution algorithm (EDA) [3], to explore and exploit the search space. Many studies showed that these algorithms are efficient in solving particular test problems but none of them can be treated as a universal optimizer [4]. On the other hand, the framework issue tackles the problems of how to define the superiority of solutions and how to maintain the multiple tradeoff Pareto optimal solutions. Two prominent frameworks of multiobjective evolutionary algorithms (MOEAs) are dominationand decomposition-based frameworks. In the domination-based MOEAs, the fitness of a solution is assigned according to some principles that define the domination behaviors among the solutions. Then, all objectives are optimized simultaneously [5]. In the decomposition-based framework, an MOP is first transformed into multiple scalar optimization problems (known as sub-problems). All sub-problems are then optimized concurrently. Even though the optimization performance of these frameworks has been extensively studied, their comparison in a wide range of test problems is considerably lacking.

Inspired by the notion of a meme and natural evolution, memetic algorithm (MA) has been introduced as populationbased hybrid GAs with local refinement capability [6]. Since then, many variants of MAs have been proposed that involve the synergy or combination of simple agents and memes [4], multiple heuristics or operators [7], multiple populationbased global search algorithms [8], or a marriage between a population-based global search and local search algorithms [9], [10]. Besides, instead of merging several existing optimizers or operators, there were other MAs that employed randomized perturbations to the population [11], different stages of exploration which performed periodically [12], and parallel alteration of a solution through different strategies [13]. In [4], MAs are defined as evolutionary algorithms with local

[^1]
[^0]:    Manuscript received December 23, 2013; revised April 10, 2014 and June 17, 2014; accepted June 17, 2014. Date of publication July 8, 2014; date of current version March 13, 2015. This paper was recommended by Associate Editor H. Ishibuchi.
    V. A. Shim and H. Tang are with the Institute for Infocomm Research, Singapore 138632 (e-mail: shimva@i2r.a-star.edu.sg; htang@i2r.a-star.edu.sg).
    K. C. Tan is with the Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117576 (e-mail: eletankc@nus.edu.sg).
    Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.

    Digital Object Identifier 10.1109/TCYB.2014.2331994

[^1]:    2168-2267 (C) 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

search within the generation cycle while memetic computing approaches involve hybrid structures such as perturbing a single solution. Though distinctions exist among those MAs, all of them stand on the conjecture that synergy among different optimizers, operators, or strategies is a promising way to achieve a common goal through the propagation of memes into a subpopulation of recipients or sharing information within populations.

The aims of this paper are threefold. First, an adaptive memetic computing with the synergy of a GA, DE, and EDA is proposed. The ratio of the number of fitter solutions produced by the algorithms in a particular generation defines their adaptability features in the next generation. Subsequently, a subset of the produced solutions undergoes local search in order to exploit a fitter solution. This synergy implies the communication among different pools of agents such that memes, which are the information or features that can improve the fitness of a chromosome [14], are shared and broadcasted into the whole population. Second, the adaptive memetic computing is implemented in the domination- and decomposition-based frameworks of multiobjective optimization, thus two MAs are devised. Third, the optimization performance of the proposed algorithms are validated and analyzed in 38 benchmark bounded-constrained test instances with different characteristics and difficulties. The preliminary results of this paper have been presented in [15]. A more detailed description, result, analysis, and discussion are provided in this paper.

The proposed MAs lie on two main issues, how to adaptively exchange information according to the peculiarity of the fitness landscapes or problem characteristics and how to choose suitable optimizers such that sharing information among the optimizers is beneficial. For the first issue, if an optimizer has identified a promising search region, more information is drawn from this optimizer as it is closer to that landscape. However, a control mechanism should be introduced to progressively monitor the search such that the balance of information sharing can be governed. For the second issue, we choose optimizers that have vastly different characteristics in producing offspring. EDA considers probability distribution of solutions in the fitness landscape and only uses global probability information throughout the evolutionary process, DE has high exploration capability to generate diverse solution set, GA has a good balance between exploration and exploitation, and EGS only uses local information to perform exploitation. With regards to the above design principles, the contributions of this paper are summarized as follows.

1) We devise an adaptive memetic computing for multiobjective optimization. Though the adaptive principle proposed in this paper has similar concept as AMALGAM [16] and Borg MOEA [17], a progressively control paradigm is missing in both algorithms.
2) We consider multiple global and a local search algorithm in the adaptive memetic computing. Both AMALGAM and Borg MOEA do not involve any local search algorithm. Besides, different optimizers are implemented in the proposed algorithms.
3) We implement and compare the proposed adaptive memetic computing in the domination- and
decomposition-based frameworks of multiobjective optimization. AMALGAM is developed in a domination-based framework while Borg MOEA is developed in an indicator-based framework.
4) Our MAs outperform many other state-of-the-art algorithms, including AMALGAM and Borg MOEA, in a wide variety of well-known test problems.
The structure of the paper is as follows. Section II presents the relevant background information. The adaptive MAs are described in Section III. The experimental setup and test problems are depicted in Section IV. Section V presents the simulation results and discussion, and conclusion is drawn in the last section.

## II. BACKGROUND INFORMATION

## A. MOEAs

1) MOEAs Using Genetic Algorithms: GA is one of the most popular algorithms used to solve MOPs. NSGA-II [1] is a well-known MOEA that implements GA in the dominationbased framework. MOEA/D [18], on the other hand, implements GA in the decomposition-based framework and has become an eminent algorithm due to its superior optimization performance. Both of these algorithms uses simulated binary crossover (SBX) and polynomial mutation operators as their variation operators to explore and exploit the search space. In the SBX operator, the exploration is performed by mating two solutions that are far from each other in the search space. It is also effective in exploiting the promising search regions by generating child solutions that are arbitrary close to the parent solutions. However, the SBX operator suffers several limitations. As pointed out in [19], it may lose diversity when an MOP has a complicated PS and it often generates inferior solutions during an evolutionary process. A detailed description of the SBX can be referred to [20].
2) MOEAs Using Differential Evolution: DE has gained increasing interest from the research community due to its simplicity and strong exploration capability. Some examples of DE for solving MOPs are nondominated sorting differential evolution (NSDE) and MOEA/D with DE [19]. For NSDE, Iorio and Li [21] took advantages from the framework of NSGA-II and applied DE as its genetic operator instead of GA. NSDE showed a better performance in rotated MOPs than NSGA-II. Li and Zhang [19] implemented DE in MOEA/D and the algorithm showed superior performance in MOPs with complicated PS. This is because DE is able to generate solutions with huge dissimilarity in their decision vectors, thus producing a set of diverse child solutions. However, DE suffers several limitations such as it has a poor performance in not linearly separable problems [22] and is easily to be trapped in local optima in some problem landscapes [23]. The technical details of DE can be referred to [19].
3) MOEAs Using Estimation of Distribution Algorithms: GA and DE employ the concept of genetic variations, such as crossover and mutation, in producing child solutions. EDAs, on the other hand, have a different variation concept that the child solutions are generated by the probability distribution of parent solutions. Some examples of those techniques

are Bayesian network [24] and restricted Boltzmann machine (RBM) [25]. Experimental studies indicated that EDAs are well suited for epistatic and deceptive test problems [24], [26], are robust in noisy environments [27], [28], and are wellperformed in high-dimensional MOPs [29]. The downsides of EDAs include the building of the probabilistic model is time consuming [30], they are weak in producing a diverse set of solutions, and are easily trapped in local optima. This is due to the fact that EDAs only use global information, in terms of probability distribution, and do not utilize location information of the known solutions in a reproduction stage [30], [31]. In this paper, only EDAs with RBM (REDA) as its modeling approach is considered. A complete evolutionary process of REDA involves three main steps. First, the parent solution are used as the training data for RBM. It is then trained by the unsupervised learning of contrastive divergence [32], [33] such that the network reaches a certain level of thermal equilibrium. In the second step, a probabilistic model is constructed based on the energy function of the network. Third, child solutions are produced by sampling the constructed probabilistic model. The technical details of REDA can be referred to [31].
4) Evolutionary Gradient Search Algorithm: Local search algorithms introduce a small perturbation to existing solutions such that slightly different solutions are obtained. The hybridization between global and local search algorithms has been experimentally shown to provide a better search performance [34], [35]. In this paper, we implement the EGS as a local search algorithm which utilizes the gradient information of the trajectory movement of solutions. The detail of the algorithm is presented in Algorithm 1. The $\varepsilon$ is set to 1.8 as suggested in [36]. The EGS works as follows. An initial solution $\left(\mathbf{x}^{j}\right)$ is randomly selected from a population or selection pool. Normal mutation with zero mean and variance of $\sigma^{2}$ is applied to introduce a small perturbation to $\mathbf{x}^{j}$ in order to create $L$ local neighbors. The local neighbors are evaluated by calculating their objective functions. The global gradient direction is then computed as illustrated in Step 5 of Algorithm 1. An offspring is subsequently generated based on the global gradient direction and step size. Next, step size and solution are updated. The selected solution is the one with a lower objective value. In this way, the EGS can search for a fitter solution progressively. The above process is repeated, until the stopping criterion is reached, by treating the selected solution as an initial solution.

## B. Algorithm Frameworks

1) Domination-Based Framework: In domination-based MOEAs, the fitness of a solution is determined by the domination principle. This fitness becomes the criterion in natural selection. The final output is a set of tradeoff and nondominated solutions. For preserving the diversity of solutions, techniques such as crowding distance or niche sharing are applied. Some MOEAs under this framework are strength Pareto evolutionary algorithm (SPEA) [37] and NSGA-II. However, this approach suffers several limitations. A large number of nondominated solutions exist in many-objective problems. This leads to a phenomenon called dominance resistance [17], [38], [39], in which it is hard to determine which

Algorithm 1 Pseudo Code of an Evolutionary Gradient Search (EGS) Algorithm

```
Begin
1. Input: Define initial step size \(\sigma_{0}\)
Do while (Stopping criterion is not met)
    For \(j=1: L S\) (Solutions to undergo local search)
        2. Initial solution: Select a solution \(\mathbf{x}^{j}\) from the selec-
        tion pool
        3. Reproduction: Create \(L\) local neighbors \(\mathbf{r}^{j}, i \in\)
        \(\left\{1,2, \ldots, L\right\}\) by perturbing \(\mathbf{x}^{j}\) using normal mutation
        \(N\left(0, \sigma^{2}\right)\)
        4. Evaluation: Calculate the objective values of \(\mathbf{r}^{j}, F\left(\mathbf{r}^{j}\right)\)
        5. Direction: Estimate the global gradient direction
        \(\hat{v}=\frac{\sum_{i=1}^{L}\left[F\left(\mathbf{r}^{j}\right)-F\left(\mathbf{r}^{j}\right)\right]\left(\mathbf{r}^{j}-\mathbf{r}^{j}\right)}{\left\|\sum_{i=1}^{L}\left[F\left(\mathbf{r}^{j}\right)-F\left(\mathbf{r}^{j}\right)\right]\left(\mathbf{r}^{j}-\mathbf{r}^{j}\right)\right\|}\)
        6. Offspring generation
        \(\mathbf{y}=\mathbf{x}^{j}-\sigma_{t} \hat{v}\)
        7. Mutation step size update
        \(\sigma_{t+1}= \begin{cases}\sigma_{t} \varepsilon & \text { if } F(\mathbf{y})<F\left(\mathbf{x}^{j}\right) \\ \sigma_{t} / \varepsilon & \text { otherwise }\end{cases}\)
```

11: 8. Solution update:
if $F(\mathbf{y})<F\left(\mathbf{x}^{j}\right)$ then
$\mathbf{x}^{j}=\mathbf{y}$
end if
12: 9. Output: Output $\mathbf{x}^{j}$
13: End For
14: End Do
15: End
solutions are promising in producing a fitter offspring. More information on many-objective problems can be referred to the study of NSGA-III [40], HypE [41], PICEA-g [42], and cornet sort approach [43]. Besides, a diversity preservation scheme is necessary to maintain the diversity of solutions. Even through a diverse solution set exists, the distribution of the solutions may not be well maintained [18].
2) Decomposition-Based Framework: In decompositionbased MOEAs, an MOP is decomposed into several scalar optimization sub-problems. Subsequently, a tradeoff optimal solution set is determined by optimizing all the sub-problems concurrently. This framework has gained increasing interest from the research community after Zhang and Li [18] devised a multiobjective optimization algorithm based on decomposition (MOEA/D). In MOEA/D, an MOP is decomposed into $N$ scalar optimization sub-problems using weighted sum, Tchebycheff, or boundary intersection approaches. The superiority of solutions is determined by comparing their scalar objective values. Subsequently, new solutions are produced by mating the solution of a sub-problem with its neighboring solutions. In [44], a new MOEA/D with dynamic resource allocation (MOEA/D-DRA) was proposed and tested on CEC09 unconstrained MOPs test problems. In this MOEA/D, different computational efforts are assigned to different subproblems. This algorithm has been granted the best MOEA in the CEC09 competition for unconstrained multiobjective optimization.

## C. Memetic Algorithm

In this section, only a brief review of MAs in the perspective of MO is presented. In [45], a multiobjective genetic local search algorithm (MOGLS) was introduced. In this MA, the multiple objectives of an MOP are aggregated using a weighted sum approach. A weight vector is randomly generated for each solution that undergoes local search. Another attempt to use the weighted sum approach in performing local search for solving MOPs was suggested in [46]. The weighted sum approach is introduced to NSGA-II, in which the weight vector is not randomly generated, but determined by the information carried on each solution. Besides, the weight vector can also be determined by using a simulated annealing algorithm as described in [47].
In [48], the authors proposed an MA for Pareto archived evolution strategy algorithm (M-PAES). An evolution strategy acts as a local search algorithm and recombination of archived solutions acts as a global search mechanism. Goh et al. [36] developed a multiobjective evolutionary gradient search (EGS) algorithm. The population-based approach of EAs is adapted into the EGS algorithm, such that the EGS algorithm can perform a multidirectional search in the search space. Several approaches are used to derive the gradient information of multiple objectives of an MOP, such as a weighted sum approach with random weight vector, a goal programming technique, and a hypervolume indicator-based approach. The detailed coverage of this topics can be referred to $[4],[8]$.

## III. PROPOSED ALGORITHMS

Motivated by the synergetic effect of multiple EAs, an adaptive memetic computing is suggested to ameliorate the optimization performance of each individual EA. The proposed memetic technique is implemented in the dominationand decomposition-based MOEAs. Thus, two MAs, namely memetic nondominated evolutionary algorithm (mNSEA) and memetic MOEA/D (mMOEA/D), are designed.
The operations of the adaptive memetic computing are as follows. In the initialization stage, each optimizer has an equal probability to produce initial solutions. Then, fitter solutions are selected and stored in an archive. Before child solutions are generated, the reproduction probability of each optimizer is calculated as illustrated in Algorithm 2. It is based on the idea that a higher probability is given to the optimizers that produce a higher number of fitter solutions in the previous generation, and vice versa. Besides, a control parameter called learning rate $(\epsilon)$ is introduced to govern the adaptive activity.
Let $\psi$ denotes the solutions in an archive. First, calculate the number of solutions in $\psi$ that are generated by each EA, denoted as $D_{g}^{E A_{i}}$, where $i \in\{1, \ldots, M\}, M$ is the number of EAs that are involved in the adaptive memetic process. In this paper, three EAs are considered, including a GA, DE, and EDA. Thus, the number of solutions in $\psi$ that are generated by each EA is denoted as $D_{g}^{E A_{1}}=D_{g}^{G A}, D_{g}^{E A_{2}}=D_{g}^{D E}$, and $D_{g}^{E A_{3}}=D_{g}^{E D A}$. Afterward, the adaptive proportion rate $\left(A r_{g}^{E A_{i}}\right)$

```
Algorithm 2 Pseudo Code of the Adaptive Memetic
Computing
    1: \%\%Given a set of selected solutions that are stored in an
    archive \((\psi)\)
    Step 1: Calculate the number of solutions in \(\psi\) that are gen-
    erated by each EA, denoted as \(D_{g}^{E A_{i}}\), where \(i \in\{1, \ldots, M\}, M\)
    is the number of EAs that are involved in the adaptive memetic
    process
    Step 2: Calculate the adaptive proportion rate, denoted as \(A r_{g}^{E A_{i}}\)
    of each EA
    Step 3: Check for the lower bound (l_bound) of the adaptive
    proportion rate
        For \(i=1: M\)
            if \(A r_{g}^{E A_{i}}<l\) _bound then
                \(A r_{g}^{E A_{i}}=l_{\text {_bound }}\)
            end if
            End For
        Step 4: Normalize the adaptive proportion rate so that the sum
        of the adaptive proportion rates is equal to 1.0
            For \(i=1: M\)
                \(A r_{g}^{E A_{i}}=\frac{A r_{g}^{E A_{i}}}{\sum_{j}^{M} A r_{g}^{E A_{j}}}
\)
        End For
```

at generation $g$ for each EA is calculated as follows:

$$
\begin{aligned}
& A r_{g}^{E A_{i}}=A r_{g-1}^{E A_{i}}+\epsilon \times P r_{g}^{E A_{i}} \\
& P r_{g}^{E A_{i}}=\frac{D_{g}^{E A_{i}}}{N}
\end{aligned}
$$

where $A r_{g}^{E A_{i}}$ is the adaptive proportion rate at generation $g$ for $i^{\text {th }} \mathrm{EA}, \epsilon$ is the learning rate, $P r_{g}^{E A_{i}}$ is the current proportion rate, and $N$ is the archive size or the number of solutions in an archive. The learning rate $(\epsilon>0)$ is introduced to the updating rule to moderate the influence of an optimizer to the whole evolutionary process and it is a scalar parameter to be determined by the user. Next, the adaptive proportion rate is set to a lower bound (lbound) value if it is lower than the lbound as indicated in Step 3 of Algorithm 2. This is to prevent the stagnation of adaptivity of the MAs during evolutionary processes. Finally, the adaptive proportion rates are normalized such that the summation of the rates is equal to 1.0 (Step 4 of Algorithm 2). After obtaining the proportion rates, a typical evolutionary process continues.

The proposed adaptive method in this paper shares similar adaptive principle as AMALGAM [16], [49] and Borg MOEA [17]. The primarily difference lies in how to determine the final reproduction rate. In AMALGAM, instead of determining reproduction rate, it determines the number of offspring to be generated by each optimizer based on the ratio of the number of offspring generated by the optimizer in the new population. Borg MOEA determines the reproduction rate by calculating the ratio of the number of offspring generated by an algorithm in an $\epsilon$-box dominance archive. Both algorithms determine the final reproduction rate or number of offspring to be generated by each optimizer based on the solutions in an archive or population. The proposed adaptive method, on the

```
Algorithm 3 Pseudo-Code of the Adaptive Memetic
Nondominated Sorting Evolutionary Algorithm (mNSEA)
    Begin
        1. Initialization: At generation \(g=0\), randomly generate \(N\)
        solutions to be the initial population \(\operatorname{Pop}(g)\)
        2. Evaluation: Evaluate each solution in the population
        Do while (Stopping criterion is not met)
            3. Fitness assignment: Apply the Pareto ranking and crowd-
            ing distance over the population.
            4. Selection: Select \(N\) solutions (binary tournament)
            5. Adaptive: Calculate the adaptive proportion rate
            6. Reproduction: Build a probabilistic model \(p_{g}(x)\)
            For \(i=1: N\)
            Generate a random value between \([0,1]\) (u)
            if \(u<A r_{g}^{G A}\) then
            Generate an offspring using GA operators
            else if \(u \geq A r_{g}^{G A} \& u<A r_{g}^{G A}+A r_{g}^{D E}\) then
            Generate an offspring using DE operators
            else
            Sample an offspring from \(p_{g}(x)\) (EDA)
            end if
            End For
            7. Evaluation: Evaluate all child solutions
            8. Archiving: Store the parents and child solutions in an
            archive. Perform the Pareto ranking and crowding distance
            over the solutions in the archive
            9. Elitism: Select \(N\) solutions with the lowest Pareto rank
            or highest crowding distance from the archive
            10. Local Search: Generate a random value between [0, 1]
                (u)
            if \(u<L S\) (local search rate) then
                For \(i=1: N\)
                Generate another random value between \([0,1](u)\)
            if \(u<K\) (\% of solutions undergoing local search)
                then
                    Perform EGS algorithm to generate an offspring
                    end if
            End For
            end if
            Perform archiving and elitism to the parents and child
            solution to form the new population. \(g++\)
    End Do
    End
```

other hand, introduces a control mechanism ( $\epsilon$ or learning rate) to progressively monitor the contribution of each optimizer to an evolutionary process. This is to ensure that a balance of information sharing can be governed. This is due to the fact that some optimizers have different convergence rate at different stage of evolutions. Without the learning rate, certain optimizers may promptly dominate the reproduction process, thus ignoring the contributions of other optimizers.

The process flow of the proposed adaptive memetic evolutionary algorithm with nondominated sorting approach (mNSEA) is presented in Algorithm 3. The initial solutions are randomly generated and then evaluated. Based on their objective values, the solutions are sorted into different levels of domination. The solutions in the first level are not dominated by any other solutions while the solutions in the second level are only dominated by the solutions in the first level, and so on. In order to determine
the superiority of solutions located on the same level, crowding distance is calculated. A solution is fitter when it is in a higher level and has a larger crowding distance. Based on these criteria, $N$ promising solutions are selected using the binary tournament selection operator. Next, the adaptive proportion rate for each EA is calculated as presented in Algorithm 2. To generate child solutions, a random number is used to determine which EAs are activated to reproduce. If GA is activated, the SBX and polynomial mutation operators are used to create an offspring. If DE is activated, the DE and polynomial mutation operators are used to generate an offspring. Similarly, if EDA is activated, then an offspring is sampled from the constructed probabilistic model. In the archiving state, the parent and child solutions are stored in an archive ( $2 N$ size) and their levels of domination and crowding distance are determined. In the elitism state, the best $N$ solutions are selected to be a new population. Subsequently, a subset of the population ( $K \%$ solutions) is chosen to undergo local search using the EGS algorithm. Note that local search is only activated in certain generations, in which the activation is governed by a local search rate $(L S)$. The archiving and elitism stages are also applied to the solutions generated by the EGS algorithm. The similar process repeats until the stopping criterion is reached.

The adaptive memetic computing is also implemented in MOEA/D, and dubbed as mMOEA/D. The complete operation of mMOEA/D is presented in Algorithm 4. In this paper, we apply the MOEA/D proposed by Li and Zhang [19]. The Tchebycheff approach is used to decompose an MOP into $T$ scalar sub-problems. Its weight vectors $\left(\lambda^{i}, \ldots, \lambda^{N}\right)$ are uniformly generated before the evolutionary process starts. It is to be noted that $T=N$, which means that each sub-problem is represented by a solution in the population. The neighboring solutions of a sub-problem are determined by the Euclidean distance among the weight vectors. Each sub-problem is assigned $Q$ neighboring solutions, denoted as $B(i)=\left\{i_{1}, \ldots, i_{Q}\right\}, i \in[1, N]$. A set of initial solutions is randomly generated and evaluated. Note that the fitness of a solution is set to be the value of the scalar function. The initial reference point of the Tchebycheff approach $\left(\mathbf{z}^{*}\right)$ is initialized to be the lowest objective values of the solutions in the population. Next, the evolutionary process starts.

Similar to mNSEA, a random number is used to determine which EAs are activated to reproduce. If GA is activated, a child solution is generated by mating two randomly selected neighboring solutions of solution $i$ using the SBX operator and then followed by the perturbation of polynomial mutation operator. If DE is activated, three solutions are involved in the reproduction. The solution $i$ is modified through the mating with another two randomly selected neighboring solutions of solution $i$. The polynomial mutation with a probability of $p_{m}$ is applied to the resultant child solution. If EDA is activated, a child solution is sampled from the probabilistic models. Two probabilistic models are constructed by the RBM. The first model constructs the probabilistic model from all solutions while the second model only builds the probability distribution of the neighboring solutions of solution $i$. Next, the newly generated solutions are evaluated, and reference point $\left(\mathbf{z}^{*}\right)$ is updated. If the child solution $i$ is fitter than the parent

Algorithm 4 Pseudo-Code of the Adaptive Memetic MOEA/D (mMOEA/D)

```
Begin
    1. Initialization:
a. Generate a set of uniformly distributed weight vectors
    \(\left(\lambda^{1}, \ldots, \lambda^{N}\right)\)
b. Calculate the Euclidean distance between the weight
    vectors. Determine \(Q\) neighboring solutions \((B(i)=\)
    \(\left\{i_{1}, \ldots,\{g\}, i \in\{1, N\}\right)\) for each weight vectors
    according to the short Euclidean distance
c. At generation \(g=0\), randomly generate \(N\) solutions
    to be the initial population \(\operatorname{Pop}(g)\)
d. Initialize reference point of the Tchebycheff approach
    \(\left(\mathbf{z}^{*}\right)\) by setting the value of \(\mathbf{z}^{*}\) to be the lowest
    objective values of the solutions
    Do while (Stopping criterion is not met)
    Build a probabilistic model of the whole population \(p_{g}(x)\)
        For \(i \neq 1: N\)
        2. Reproduction: Generate a random value between
        \([0,1](u)\)
        if \(u<A r_{g}^{G A}\) then
            Randomly select two solutions from \(B(i)\), and then
            generate an offspring using GA operators
        else if \(u \geq A r_{g}^{G A} \& u<A r_{g}^{G A}+A r_{g}^{D E}\) then
            Randomly select three solutions from \(B(i)\), and
            then generate an offspring using DE operators
        else
            Build another probabilistic model \(p_{g}^{\prime}(\mathbf{x})\) which
            only considers the neighboring solutions.
            Generate a random value between \([0,1](u)\)
            For \(j=1: n\) (number of decision variables)
                if \(u<0.5\) then Sample from \(p_{g}(\mathbf{x})\)
                else Sample from \(p_{g}\) end if
            End For
            end if
    3. Evaluation: Evaluate the generated offspring \(\mathbf{y}\) to
obtain the corresponding objective values \(\mathbf{f}(\mathbf{y})\)
4. Update of \(\mathbf{z}^{*}\) : For \(j=1, \ldots, m\), if \(z_{j}^{*}>f_{j}(\mathbf{y})\) then
set \(z_{j}^{*}=f_{j}(\mathbf{y})\)
5. Fitness assignment: Assign fitness to each solution
    \(\left(g^{t e}\right)\) using Tchebycheff method
    6. Update solution: For \(j \in B(i)\), if \(g^{t e}\left(\mathbf{y} \mid \lambda^{j}, \mathbf{z}^{*}\right) \leq$
    \(g^{t e}\left(\mathbf{x}^{j} \mid \lambda^{j}, \mathbf{z}^{*}\right)\), then set \(\mathbf{x}^{j}=\mathbf{y}\) and \(F V^{j}=\mathbf{f}(\mathbf{y})\)
End For
7. Local search: Perform the EGS algorithm if it is acti-
vated (similar to hNSEA). Then, apply Steps 4-6 to
update the reference point and the neighboring solutions.
$g++$
9. End
10. End
11. End
solution $i$ and its neighboring solutions, the parent solution $i$ and the neighboring solutions are discarded and replaced by the child solutions $i$. Next, if local search is activated, the EGS algorithm is used to produce child solutions as illustrated in Algorithm 1. The similar process repeats until the stopping criterion is reached.

## IV. Problem Description and IMPLEMENTATION

The optimization performance of the proposed algorithms (mNSEA and mMOEA/D) was validated in 38 benchmark test

TABLE I
PARAMETER SETTINGS FOR EXPERIMENTS


instances with different characteristics and difficulties. The test problems include five ZDT problems [50], seven DTLZ problems [51] with three objectives and five objectives respectively, ten UF problems [52], and nine WFG problems [53].

Besides, their performance was compared with other eight MOEAs, including NSGA-II with SBX operator (NSGA-II-SBX) [1], NSDE [21], MOEA/D with SBX operator (MOEA/D-SBX) [18], MOEA/D with DE (MOEA/D-DE) [19], NSREDA [25], MOEA/D with REDA (MOEA/D-REDA), AMALGAM [16], and Borg MOEA [17]. The algorithms start with NS are the domination-based MOEAs, and those start with MOEA/D are the decompositionbased MOEAs. The SBX operator was used in GA, DE/rand/1/bin operator was used in DE, and RBM was used in EDA. We also constructed REDA in the decomposition-based framework (MOEA/D-REDA) for a thorough comparison. Note that we implemented the adaptive updating rule proposed in [16] into our domination-based algorithm and the algorithm is dubbed as AMALGAM (a term used in [16]). In other words, the difference between mNSEA and AMALGAM is the implementation of different adaptive updating rule, in which a control parameter is introduced in the updating rule of mNSEA while this feature is missing in AMALGAM. The source code of Borg MOEA was obtained from the authors of [17]. It is to be noted that no local search is involved in AMALGAM and Borg MOEA.

The optimization performance of the algorithms was accessed using inverted generational distance (IGD). IGD is a unary indicator which measures the distance of each solution in the optimal PF to the obtained PF. In this indicator, both convergence and diversity are taken into consideration. A lower value of IGD implies that an algorithm has better optimization performance. Based on IGD values, the ranking of ten algorithms was performed. The details of the parameter settings were summarized in Table I.

## V. RESULTS AND DISCUSSIONS

## A. Comparison Results

The performances measured in IGD for all test problems are presented in Tables II and III. The number of decision

TABLE II
Results in Terms of IGD Measurement for ZDT, DTLZ, UF, and WFG Test Problems

variables ( $n$ ) and objective functions ( $m$ ) of the test problems are indicated by the parentheses next to the test problems. The results (mean $\pm$ standard deviation) are the averaging IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$ standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the average IGD of the test problem. The results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean $\pm$standard deviation) are the results (mean

TABLE III
Results in Terms of IGD Measurement for DTLZ6-DTLZ7 (Five Objectives) Test Problems

ZDT problems, which possess two objective functions and a scalable number of decision variables, are a set of simple MOPs. These problems can be easily solved by all algorithms. In order to increase its difficulty, we enlarged the search space by increasing the number of decision variables ten times greater than its original setting. The results show that mNSEA has a superior performance compared to other algorithms. The ranking is then followed by Borg MOEA, mMOEA/D, and AMALGAM. In other words, all MAs have better optimization performance than individual optimizers. In ZDT4, all algorithms fail to approach PF (indicated by large IGD values). This is due to the fact that ZDT4 is an extremely multimodal problem which consists of many local optima. Comparing the performance of different EAs, EDA has a better result than GA and DE. Comparing between the two frameworks, the domination-based algorithms show a better result than the decomposition-based algorithms. The overall ranking of the algorithms in ZDT problems is mNSEA, Borg MOEA, mMOEA/D, AMALGAM, NSREDA, MOEA/D-REDA, NSGA-II-SBX, MOEA/D-SBX, NSDE, and MOEA/D-DE.

DTLZ problems are well-known for their scalability in both decision and objective spaces. To increase the decision space, we set the number of decision variables to 120 except DTLZ1 and DTLZ3 (12 decision variables) which are highly multimodal test problems. In DTLZ problems with three objective functions, Borg MOEA has the best IGD performance in three test problems while mNSEA has the best performance in two test problems. However, the overall ranking suggests that mNSEA has the best rank while Borg MOEA is only placed at fifth rank (Table IV). This is because Borg MOEA has a lower rank in another three test problems while mNSEA performs well on other test problems. It is also observed that GA with the SBX operator is well-performed in multimodality problems (DTLZ1, and DTLZ3). The performance of EDA and DE are inferior in these test problems. This result is identical to the observation in [25] which explains that EDA, which only uses global probability information, is susceptible to be trapped into local optima. DTLZ4 has a bias landscape when mapping from the decision space to the objective space. In this test problem, NSGA-II-SBX and mNSEA obtain a reasonable result while algorithms with DE operator have a worse performance. No clear indication about the superiority

TABLE IV
Ranking of the Algorithms in Various Test Problems


of both frameworks in these test problems. The overall ranking is mNSEA, NSGA-II-SBX, MOEA/D-SBX, mMOEA/D, Borg MOEA, AMALGAM, MOEA/D-REDA, MOEA/D-DE, NSREDA, and NSDE.

In many-objective problems, the assignation of fitness to each solution become more complex. This challenges an optimizer in differentiating the superiority of solutions. The DTLZ with five objective functions belongs to this class of problems. Without performing dimension reduction to the objective function or modifying the selection operator, we directly implemented the proposed algorithms to deal with these problems. It is observed that Borg MOEA achieves the best IGD results in four test problems and is placed at the top rank among all algorithms. The employment of $\epsilon$-box dominance archive in Borg MOEA allows the algorithm to maintain the best solutions in terms of convergence and diversity based on $\epsilon$-dominance criterion. This criterion is efficient to determine the superiority of solutions even in many-objective problems. It is clearly observed that algorithms with the decomposition-based frameworks outperform algorithms with the domination-based framework. This is due to the fact that the superiority of a solution in the decompositionbased framework can be easily identified using the value of the aggregated or scalar sub-problems. In the dominationbased framework, the domination behavior of the solutions is hardly been determined in these problems. The results also indicated that EDA and GA has a better performance than DE. The overall ranking is Borg MOEA, mMOEA/D, MOEA/DSBX, MOEA/D-REDA, mNSEA, NSREDA, NSGA-II-SBX, MOEA/D-DE, AMALGAM, and NSDE.

UF problems were used in the CEC 2009 competition and well-known for their complicated shapes of PS. The first seven problems have two objective functions while the rest of the problems possess three objective functions. MOEA/D-DE has the best performance in four UF test problems, followed by Borg MOEA and mMOEA/D in two test problems, respectively. However, mNSEA is placed at the top rank as it has a

consistent performance in all UF test problems. The algorithms with EDA show inferior performance in most of these problems. This may be attributed to the fact that EDAs have a weak diversification [30] since only global information is used and they do not leverage the benefits of location information. It is also reported in [19] that the ability to produce diverse solutions is critical to address UF problems. Both dominationand decomposition-based algorithms show competitive performance. The overall ranking is mNSEA, mMOEA/D, MOEA/D-DE, AMALGAM, NSDE, Borg MOEA, NSGA-IISBX, MOEA/D-SBX, MOEA/D-REDA, and NSREDA.

WFG problems are scalable problems in both objective and decision spaces. The problems involve multiple transformations such that various properties, such as deceptive, multimodal, bias, etc., are introduced. In this implementation, 30 decision variables ( 28 distance-related parameters and two position-related parameters) and two objective functions were applied. mMOEA/D shows the best performance in four WFG problems and has the best rank among all algorithms. Besides, algorithms with GA generally outperform algorithms with DE and EDA. Comparing both frameworks, the decompositionbased algorithms slightly outperform the domination-based algorithms. The overall ranking is mMOEA/D, MOEA/DSBX, NSGA-II-SBX, mNSEA, Borg MOEA, AMALGAM, MOEA/D-DE, NSDE, NSREDA, and MOEA/D-REDA.

Table IV shows that, overall, mMOEA/D has the best rank, followed by mNSEA and Borg MOEA. These results indicate that the proposed algorithms leverage the benefits of each optimizer in an adaptive memetic manner in most of the test problems. However, the MAs fail in certain test problems such as ZDT4, in which they have the worst IGD results. Comparing each individual optimizer, GA has the best performance in DTLZ (three and five objective functions) and WFG problems. It has medium performance in ZDT and UF problems. For DE, its performance is superior in UF problems, medium in WFG problems, and inferior in ZDT and DTLZ problems with three and five objective functions. For EDA, its performance is superior in ZDT problems, medium in DTLZ problems with three and five objective functions, and inferior in UF and WFG problems. Comparing between the MOEAs frameworks, the domination-based MOEAs are superior to the decomposition-based algorithms in ZDT problems, inferior in DTLZ problems with five objective functions, and comparable in the other test problems. The overall ranking of the algorithms in all the test problems is mMOEA/D, mNSEA, Borg MOEA, MOEA/D-SBX, NSGA-II-SBX, AMALGAM, MOEA/D-DE, MOEA/D-REDA, NSDE, and NSREDA.

Several observations can be made from the above results. First, the adaptive updating rule in mNSEA has superior performance compared to updating rule in AMALGAM. The control parameter introduced in mNSEA moderates the influence of an optimizer to the whole evolutionary process such that every optimizer has a longer life span to contribute during the evolutionary cycle. Second, we infer that indicatorbased MOEAs (e.g., Borg MOEA) is more effective than decomposition-based MOEAs in differentiating the superiority of solutions in many-objective problems while dominationbased MOEAs are less effective than the former MOEAs.
![img-0.jpeg](img-0.jpeg)

Fig. 1. Effects of local search rate ( $x$-axis) on optimization performance.

Third, the proposed adaptive memetic computing is a generic approach which can be implemented in any framework of MOEAs. Lastly, it is to be noted that any optimizer can be integrated into the adaptive memetic computing. However, a better performance is expected when the optimizers show vastly different search characteristics. As an example, GA has a good balance between exploration and exploitation, DE has a strong exploration capability, and EDA has a mechanism to trace the distribution of promising solutions.

## B. Effects of Local Search on Optimization Performance

Fig. 1 shows the effects of different local search rate on optimization performance. Six cases are shown in order to highlight different observations. Overall, the performance of mNSEA is better in most of the test problems when local search is activated (local search rate, $L S=0$ means local search is deactivated). The box-plot for ZDT1 shows this observation. However, it may also happen that mNSEA has a better performance when $L S=0$ as shown in DTLZ2 and WFG6. In UF3, $L S=0.5$ gives the best performance compared to other settings. An interesting observation can be seen in UF6 that the activation of local search reduces the number of outliers. Fig. 2 shows the effects of the number of solutions which undergoes local search on optimization performance. In ZDT1, 25\% of solutions in the population undergoing local search give the best performance compared to other settings. In DTLZ2, the performance deteriorates when a higher number of solutions is exposed to local search. The figure of WFG2 shows the reverse observation as DTLZ2, in which the performance is better

![img-3.jpeg](img-3.jpeg)

Fig. 2. Effects of the percentage of local search ( $x$-axis) on optimization performance.
![img-3.jpeg](img-3.jpeg)

Fig. 3. Adaptive activation of different EAs.
when a larger number of solutions undergoes local search. In UF1, no clear pattern can be observed. In conclusion, the optimal number of solutions to be involved in the local search is problem dependent. However, a smaller number of solutions to undergo local search is better since it may cause extra fitness evaluations in one generation when a large number of solutions is exposed to local search.

## C. Effects of Adaptive Feature on Optimization Performance

Three EAs are considered in the adaptive memetic computing. The adaptive activation of the different EAs is shown in Fig. 3. All plots in the figure show that GA dominates the search at the later stages of evolutions except UF1. The figure also shows that EDA has a higher activation at the early stages of evolutions in ZDT1 and DE has a higher activation at the later stages of evolutions in UF1. These observations suggest that GA is the main algorithm in guiding the search process while DE and EDA aid in exploring some search regions that
![img-3.jpeg](img-3.jpeg)

Fig. 4. Effects of lbound ( $x$-axis) on optimization performance.
are not explored by the GA. Since a lbound is applied, a population will always consist of solutions generated by different optimizers.

There are two parameters introduced to the adaptive mechanism-lbound and learning rate $\epsilon$. lbound is important in the case where an algorithm dominates other algorithms at the early stages of evolutions. In this case, the adaptive feature is ceased and the optimization process will only be continued by a single optimizer. Fig. 4 shows the optimization performance in IGD measurement with regards to different settings of lbound. Only results for four test problems with different performance are shown. lbound $=0$ means that no lbound is set. The figure indicates that the settings of lbound gives superior optimization performance in some test problem (e.g., ZDT4), inferior performance in some test problems (e.g., ZDT1), and comparable performance in other test problems (e.g., DTLZ5 with three objectives). lbound $=0.33$ means that the adaptivity is deactivated. The IGD results for lbound $=0.33$ are inferior in most of the test problems (e.g., ZDT1, ZDT4, and DTLZ5 with three objectives) except in UF8.
$\epsilon$ is introduced to the proposed adaptive mechanism to moderate the influences of the proportion of the number of selected solutions in generation $g$ to the whole evolutionary process. Fig. 5 shows the effects of the learning rate on optimization performance. Only results for four test problems with different performances are shown. In ZDT1, a smaller value of $\epsilon$ gives better optimization results. In DTLZ2 with three objectives, a higher value of $\epsilon$ gives better optimization results. In UF8 and WFG2, no clear trend is observed in the settings of $\epsilon$. Overall, a smaller value of $\epsilon(0.1-0.5)$ gives better optimization results in most of the test problems.

## D. Possible Applications

The proposed MAs have advantages on problems with different fitness landscapes. For example, in dynamic optimization problems [54], fitness landscapes are changing over

![img-4.jpeg](img-4.jpeg)

Fig. 5. Effects of learning rate ( $x$-axis) on optimization performance.
time either periodically or nonperiodically. Different operators or optimizers may have a better search performance on a particular landscape. The proposed MAs can adaptively use different optimizers to generate offspring upon considering the peculiarity of the fitness landscapes. Some dynamic optimization problems are resource allocation [55] and power scheduling [56].

## VI. CONCLUSION

The synergetic combination of multiple EAs in an adaptive memetic manner has been proposed in this paper. The memetic feature has leveraged the strengths of each individual EA while the adaptability property has provided a clue to produce a set of promising tradeoff solutions. The proposed technique is a generic synergetic approach to consolidate any EA together such that the mutual genetic information is shared through the propagation of memes into the whole population. The implementation of the adaptive memetic computing in the domination- and decomposition-based MOEAs has not only showed the strengths of the proposed MAs but at the same time provided a deeper comparison between these two frameworks. The validation in a wide range of test problems has proved the efficiency of the proposed algorithms.
