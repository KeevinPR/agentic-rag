# An order-based estimation of distribution algorithm for stochastic hybrid flow-shop scheduling problem 

Sheng-yao Wang*, Ling Wang, Min Liu and Ye Xu<br>Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Automation, Tsinghua University, Beijing 10084, China

(Received 18 January 2013; accepted 22 December 2013)


#### Abstract

In view of the stochastic nature of the data in real-world manufacturing systems, it is crucial to develop effective algorithms to solve the scheduling problems with uncertainty. In this paper, an order-based estimation of distribution algorithm (OEDA) is proposed to solve the hybrid flow-shop scheduling problem (HFSP) with stochastic processing times. Considering the effectiveness and robustness of a schedule, it aims to minimise the makespan of the initial scenario as well as the deviation of all results of the stochastic scenarios and the initial one. To be specific, a bi-objective function is used to evaluate the individuals of the population, and a probability model is designed to describe the probability distribution of the solution space. Meanwhile, optimal computing budget allocation (OCBA) technique is employed to provide a reliable identification to the good solutions among the population. A mechanism is also presented to update the probability model with the superior individuals that are identified by the OCBA. The new individuals are generated by sampling the probability model to track the area with promising solutions. In addition, the influence of parameter setting is investigated based on Taguchi method of design-of-experiment (DOE), and a suitable parameter setting is suggested. Extensive numerical testing results and comparisons with the existing algorithm are provided, which demonstrate the effectiveness of the proposed OEDA.


Keywords: hybrid flow-shop scheduling problem; stochastic processing time; estimation of distribution algorithm; optimal computing budget allocation; robust scheduling

## Notation

$n$ : the number of the jobs to be processed;
$S$ : the number of the stages;
$m_{k}$ : the number of the machines at stage $k$;
$J=\left\{J_{1}, J_{2}, \ldots, J_{n}\right\}$ : the set of $n$ jobs to be processed;
$R_{i}$ : the releasing time of job $J i$;
$S_{i k}$ : the starting time of job $J i$ at stage $k$;
$t_{i k}$ : the processing time of job $J i$ at stage $k$;
$C_{i k}$ : the completing time of job $J i$ at stage $k$;
$X_{i j k}$ : a binary variable that is equal to 1 if job $J j$ is assigned to machine $i$ at stage $k, 0$ otherwise;
$Y_{i j k}$ : a binary variable that is equal to 1 if job $J i$ precedes job $J j$ at stage $k, 0$ otherwise;
$L$ : a large enough constant;
$\alpha$ : the uncertainty degree of the processing times;
$I$ : the initial scenario of the problem;
$\zeta_{i}(I)$ : the $i$ th scenario with parameters sampled from $I$;
$I_{\min }$ : the minimal scenario with the minimal processing time, i.e., $I_{\min }=I-\alpha I$;
$I_{\max }$ : the maximal scenario with the maximal processing time, i.e., $I_{\max }=I+\alpha I$;
$C_{\max }^{\zeta}(x)$ : the makespan of the solution $x$ for the scenario $\zeta$;
$n_{x}$ : the number of the stochastic scenarios for evaluating solution $x$;
$T_{f}$ : the processing time of the initial scenario;
$\lambda$ : the weight coefficient of the evaluation function;
$L B$ : the lower bound of the initial scenario;
$n_{0}$ : the number of initial simulation replications in the OCBA;
$n_{x}(h)$ : the number of the simulation replications for evaluating solution $x$ in the $h$ th iteration of the OCBA;
$N(h)$ : the number of already assigned simulations in the $h$ th iteration of the OCBA;
$N_{\text {total }}$ : the total number of simulation replications in each generation of the OEDA;
$\Delta$ : the increment of the computing budget in the iteration of the OCBA;
$s_{i}$ : the standard deviation among the makespan values from the already realised simulations for solution $i$;
$\delta_{b, i}$ : the difference in average makespan value between solution $i$ and the best solution $b$;
Psize: the population size of the OEDA;
SPsize: the superior sub-population size of the OEDA;
$P$ : the probability matrix;
$p_{i j}(l)$ : the element of $P$ at generation $l$;
$\beta$ : the learning rate for updating the probability model;
$\eta$ : the percentage of the superior sub-population from the whole population.

[^0]
[^0]:    *Corresponding author. Email: wangshengyao@tsinghua.org.cn

## 1. Introduction

The hybrid flow-shop scheduling problem (HFSP) is an extension of the classical flow-shop scheduling problem (FSP) (Wang and Zheng 2003) in the flexible manufacturing systems, which contains certain parallel machines at some processing stages. The HFSP is very close to the real manufacturing processes (Wang 2003; Desprez, Chu, and Chu 2009; Luo, Li, and Xue 2010; Wang, Xu, et al. 2012, 2013c). Many real scheduling problems in the fields of chemical, metallurgy, textile, machinery, semiconductor, logistics, architecture and papermaking can be modelled as the HFSP (Ruiz and Vázquez-Rodríguez 2010; Ribas, Leisten, and Framinan 2010). As a combination of the classical FSP and parallel machine scheduling problem, the HFSP is more difficult to solve than the classical FSP. The HFSP has been proved to be strongly non-deterministic polynomial-time (NP)-hard even in a two-stage case (Gupta 1988). Therefore, it is very important to develop effective and efficient solution algorithms for the HFSP.

Early research on the HFSP mainly focused on the exact solution methods (Moursli and Pochet 2000) and the heuristic algorithms (Santos, Hunsucker, and Deal 2001). However, the exact solution methods are inefficient in solving the large-scale problems due to the tremendous computational effort and memory requirement. Although the heuristic algorithms can obtain a solution in relatively short time, it is difficult to guarantee the quality of the solution. With the development of intelligent computing, some meta-heuristic algorithms have been proposed for the HFSP during recent years. Relevant results in this area include the following. The clonal selection principle and affinity maturation mechanism were used to design an artificial immune system algorithm (Engin and Döyen 2004). A simulated annealing-based algorithm was presented by using different dispatching rules, and a method was introduced to calculate the lower bound for the HFSP (Jin, Yang, and Ito 2006). An ant colony optimisation was presented based on local and global pheromone evaluations to solve the HFSP with the serial scheduling strategy (Alaykyran, Engin, and Döyen 2007). Besides, an improved genetic algorithm (GA) was presented by Kahraman, Engin, and Kaya (2008), and a quantuminspired immune algorithm was proposed by hybridising the quantum rotation gate and immune algorithm (Niu, Zhou, and Ma 2009). Very recently, a particle swarm optimisation algorithm was presented by employing a bottleneck heuristic to fully exploit the bottleneck stage of the HFSP (Liao, Tjandradjaja, and Chung 2012).

In the literature, it often assumes that the processing times are deterministic. In reality, however, several types of uncertainty may affect the production situation (Chryssolouris and Subramaniam 2000, 2001). Thus, the processing times of the operations are stochastic in practice. The HFSP with stochastic processing time is named
the stochastic HFSP (SHFSP). Very recently, a GA was developed by Chaari et al. (2011) for solving the SHFSP, where a robust bi-objective evaluation function was defined to obtain robust and effective solutions. To the best of the authors' knowledge, it is the only presented algorithm for the SHFSP. The study on the SHFSP is still in its infancy. So, it is important to develop novel algorithms to solve the problem effectively.

Estimation of distribution algorithm (EDA) (Larranaga and Lozano 2002) is a novel kind of evolutionary algorithm based on statistical learning, which has gained increasing studies and wide applications during recent years. Due to different kinds of the relationships among variables, EDA has different models. Accordingly, it can be classified as univariate model, bivariate model or multivariate model. Univariate model assumes that the variables are independent of each other, e.g., the population-based incremental learning (Baluja 1994), the univariate marginal distribution algorithm (Mühlenbein and Paass 1996) and the compact GA (Harik, Lobo, and Goldberg 1999). Bivariate model assumes that each variable is associated with another one, e.g., the mutual information maximisation for input clustering (De Bonet, Isbell, and Viola 1997), the combining optimisers with mutual information trees (Baluja and Davies 1997) and the bivariate marginal distribution algorithm (Pelikan and Mühlenbein 1999). Multivariate model considers the relationship between all the variables, e.g., the factorised distribution algorithms (Mühlenbein and Mahnig 1999), the extended compact GA (Harik 1999) and the Bayesian optimisation algorithm (Pelikan 2005). So far, EDA has been applied to a variety of academic and engineering problems, such as feature selection (Saeys et al. 2003), inexact graph matching (Cesar et al. 2005), software testing (Sagarna and Lozano 2005), flow-shop scheduling (Jarboui, Eddaly, and Siarry 2009), resource-constrained project scheduling (Wang and Fang 2012), multidimensional knapsack problem (Wang, Wang, and Fang 2012), flexible job-shop scheduling (Wang, Wang, Xu, et al. 2012, 2013b; Wang, Wang, and Liu 2013) and distributed scheduling (Wang et al. 2013a).

In this paper, an order-based EDA (OEDA) is proposed to solve the SHFSP. Due to the stochastic nature of the problem, optimal computing budget allocation (OCBA) technology based on the idea of ordinal optimisation (Chen et al. 2000) is used in the algorithm. To be specific, a probability model is designed to describe the probability distribution of the solution space, and an updating mechanism is provided to update the probability model with the superior individuals. By sampling the probability model, new individuals can be generated to track the search area with promising solutions. Meanwhile, the OCBA technology is used to evaluate solutions and identify the superior ones in the random environment. In addition, the parameter

setting is investigated, and extensive numerical testing results are provided. The comparisons with the existing algorithm demonstrate the effectiveness and robustness of the proposed OEDA.

The remainder of the paper is organised as follows. In Section 2, the SHFSP is described. In Section 3, the basic EDA is introduced briefly and the OEDA for the SHFSP is presented in detail. The influence of parameter setting is investigated in Section 4, and the testing results and comparisons are provided as well. Finally, the paper ends with some conclusions and scope for future work in Section 5.

## 2. Problem description

### 2.1. The HFSP

Generally, the HFSP can be described as follows. There are a set of $n$ jobs $J=\left\{J_{1}, J_{2}, \ldots, J_{n}\right\}$ to be processed at $S$ sequential stages, where stage $k(k=1,2, \ldots, S)$ contains $m_{k}$ ( $m_{k} \geq 1$ ) identical parallel machines. Each job should be processed at all the stages sequentially, and it can be processed by any of the machines at each stage. Typically, it assumes that all the jobs are independent and available to be processed at initial time; the releasing time of any machine is not considered or set as 0 ; buffers between the stages are unlimited; one machine can process only one operation and one job can be processed on only one machine at a time; for all the jobs, the processing times at each stage are known in advance; the transportation times between different machines are negligible or are included in the processing times (Wang, Xu, et al. 2012, 2013c); once an operation is started, it cannot be interrupted. The HFSP is to determine both the assignment of machines at each stage and the sequence of operations on all the machines to minimise a certain scheduling objective function, e.g., the maximum complete time of all the jobs (i.e., makespan $C_{\max }$ ). An example is illustrated in Figure 1.

The HFSP can be formulated as follows (PaterninaArboleda et al. 2008):

Minimise : $C_{\max }=\max C_{i k}, k=1,2, \ldots, S ; i=1,2, \ldots, n$

Subject to:

$$
C_{i k}=S_{i k}+t_{i k}
$$

![img-0.jpeg](img-0.jpeg)

Figure 1. An example of the HFSP.

$$
\begin{gathered}
\sum_{i=1}^{m_{k}} X_{j k}=1 \\
S_{i k} \geq C_{i, k-1}, k=2,3, \ldots, S \\
S_{i k} \geq C_{j k}-L Y_{i j k}, \text { for all the pairs }(i, j) \\
S_{j k} \geq C_{i k}-(1-L) Y_{i j k}, \text { for all the pairs }(i, j)
\end{gathered}
$$

where Equation (1) implies that the objective is to minimise the makespan, Equation (2) is for calculating the $C_{i k}$, Equation (3) ensures that each job is processed once at each stage, Constraints (4) and (5) ensure that one machine can process only one job at one time and Constraint (6) means that one job cannot be processed until its preceding job is finished.

### 2.2. The SHFSP

For the SHFSP, the processing times are described as random variables. Machine breakdowns and maintenance tasks are not taken into consideration in this paper. To evaluate a solution of such a stochastic problem, a scenario modelling approach is often adopted to represent the uncertain characteristic (Chaari et al. 2011). Usually, a set of stochastic scenarios are constructed, in which the processing times are sampled according to a certain probability distribution.

Same as the literature (Chaari et al. 2011), uniform distribution is considered in this paper. To be specific, the initial scenario $I$ is a deterministic scenario, and the set of the stochastic scenarios is represented by sampling the initial scenario $I . \zeta(I)$ is uniformly distributed between $\left[T_{I}-\alpha T_{J}, T_{I}+\alpha T_{J}\right]$, where $\alpha \in[0,1]$ is the uncertainty degree of the processing time. For a given solution $x$, the average of the makespan values yielded from all the $n_{x}$ scenarios is regarded as its expected makespan.

## 3. The OEDA for SHFSP

### 3.1. The basic EDA

Estimation of distribution algorithm (EDA) provides a framework for a group of probability-distribution-based algorithms. The idea is to predict and track the area with promising solutions in the search space by using a probability model (Larranaga and Lozano 2002). The general flow chart of the basic EDA is illustrated in Figure 2.

The core of the above flow chart is to estimate the probability distribution. Probability model is used in the EDA to describe the distribution of the solution space, and it is updated with the superior individuals to help the algorithm focus on the region with promising solutions. Therefore, a proper probability model and a suitable updating mechanism are crucial to the EDA, which should

![img-1.jpeg](img-1.jpeg)

Figure 2. The general flow chart of the EDA.
be well designed for different problems. Next, the OEDA will be introduced in detail to solve the SHFSP.

### 3.2. Evaluation function

Considering the effectiveness and robustness of the schedule, an evaluation function (Chaari et al. 2011) aggregating two objectives is employed to evaluate a solution $x$, which is expressed as follows:

$$
\begin{aligned}
f(x)= & \lambda \frac{C_{\max }^{I}(x)-L B}{L B}+(1-\lambda) \\
& \times \frac{\sqrt{\frac{1}{n_{c}} \sum_{i=1}^{n_{c}}\left[C_{\max }^{I_{c}(j)}(x)-C_{\max }^{I}(x)\right]^{2}}}{D E V \_M A X\left(x^{*}\right)}
\end{aligned}
$$

where $\lambda \in(0,1)$ represents the weight coefficient, $x^{*}$ is the best solution obtained for the initial scenario $I$ and $D E V \_M A X\left(x^{*}\right)=\max \left\{\left(C_{\max }^{I}\left(x^{*}\right)-C_{\max }^{I_{\max }}\left(x^{*}\right)\right)\right.$, $\left.\left(C_{\max }^{I_{\max }}\left(x^{*}\right)-C_{\max }^{I}\left(x^{*}\right)\right)\right\}$. The two values $C_{\max }^{I_{\max }}\left(x^{*}\right)$ and $C_{\max }^{I_{\max }}\left(x^{*}\right)$ are obtained by integrating the solution $x^{*}$ in the minimal and maximal scenarios.

It can be seen that the schedule objective is to minimise the makespan of the initial scenario as well as the deviation of all the makespan values of the stochastic scenarios and the initial one. Clearly, both the effectiveness and robustness of a schedule are concerned. Note that $L B$ and $D E V \_M A X\left(x^{*}\right)$ are used to normalise the two different types of objectives. Obviously, the smaller the objective function value, the better is the solution.

### 3.3. Encoding and decoding

Each individual of the population is a solution of the SHFSP, expressed by an integer sequence with a length of $n$, which determines the processing order of jobs at the first stage. For example, a solution $x=\{5,2,3,1,4\}$ represents that job 5 is processed first at the first stage and next are job 2 , job 3 and job 1 in sequence. Job 4 is the last one to be processed at the first stage.

To decode a sequence is to assign the jobs to the machines at each stage so as to yield a feasible schedule and calculate the schedule objective value. For the SHFSP, the decoding procedure is to decide both the order of jobs and the machines assignment. For the order of jobs, the processing order of jobs at the first stage is decided referring to the order of the job numbers in the solution. From stage 2 onwards, the processing order of jobs at the current stage is decided according to their completion times at the previous stage in a nondecreasing order. As for the machines assignment, the first available machine rule (Brah and Luan 1999) is employed. That is, the job is assigned to the machine with the earliest release time.

For a given solution $x$, all the makespan values of the initial scenario and the stochastic scenarios are obtained with the above decoding method. Then, the objective function value $f(x)$ of solution $x$ is obtained.

### 3.4. Evaluation by OCBA

Based on the basic idea of order comparison and goal softening, ordinal optimisation (Ho, Sreenivas, and Vakili 1992) is an effective way to solve complex stochastic optimisation problems. Later, Chen et al. $(2000,2009)$ presented the OCBA technique to intelligently allocate a limited computing budget to efficiently evaluate and reliably identify good solutions. In the OEDA, a number of simulation replications, $N_{\text {total }}$, are given in each generation; then, OCBA is used to assign the simulation replications iteratively to each individual according to the acquired knowledge. The procedure of the OCBA is described as follows (Chen et al. 2009; Zhang, Song, and Wu 2012).

Step 1: Perform $n_{0}$ simulation replications for all solutions, and calculate the average makespan value and $s_{i}$ for each solution $i$. Set $h=0, n_{1}(h)=n_{2}(h)=\ldots=n_{P s t i z e}$ $(h)=n_{0}$ and $N(h)=n_{0} \cdot$ Psize.

Step 2: If $\sum_{i=1}^{P s t i z e} n_{i}(h) \geq N_{\text {total }}$, stop the procedure and set the number of the stochastic scenarios for evaluating solution $i$ as $n_{i}=n_{i}(h)$.

Step 3: Let $N(h+1)=N(h)+\Delta$, and compute the new budget allocation $n_{1}(h+1), n_{2}(h+1), \ldots, n_{P s t i z e}(h+1)$ using the following sub-procedure.

Step 3.1: Locate the best solution and record its index as $b$. Then randomly find $j$ such that $j \neq b$.

Step 3.2: Calculate $n_{i}(h+1)$ as follows.

$$
\begin{aligned}
n_{i}(h+1)= & N(h+1) \\
& \times\left\{\sum_{\substack{i=1 \\
i \neq h}}^{P \text { size }}\left(\frac{s_{i} / \delta_{b, i}}{s_{j} / \delta_{b, i}}\right)^{2}+s_{b} \times\left[\sum_{\substack{i \neq 1 \\
i \neq h}}^{P \text { size }} \frac{1}{s_{i}^{2}}\left(\frac{s_{i} / \delta_{b, i}}{s_{j} / \delta_{b, i}}\right)^{4}\right]^{\frac{1}{2}}\right\}^{-1}
\end{aligned}
$$

Step 3.3: Calculate $n_{i}(h+1)$ (For all $i \neq j$ and $i \neq b$ ) as follows.

$$
n_{i}(h+1)=n_{j}(h+1) \times\left(\frac{s_{i} / \delta_{b, i}}{s_{j} / \delta_{b, i}}\right)^{2}
$$

Step 3.4: Calculate $n_{b}(h+1)$ as follows.

$$
n_{b}(h+1)=n_{j}(h+1) \times s_{b} \times\left[\sum_{\substack{i=1 \\ i \neq b}}^{P \text { size }} \frac{1}{s_{i}^{2}}\left(\frac{s_{i} / \delta_{b, i}}{s_{j} / \delta_{b, i}}\right)^{4}\right]^{\frac{1}{2}}
$$

Step 4: Perform additional $\max \left\{0, n_{i}(h+1)-n_{i}(h)\right\}$ simulations for evaluating solution $i(i=1,2, \ldots, P$ size $)$ and update the average makespan value and $s_{i}$ for each solution $i$ if necessary.

Step 5: Let $n_{i}(h+1)=\max \left\{n_{i}(h+1), n_{i}(h)\right\}$. Let $h=h+1$ and go to Step 2.

For a solution $x$ in the population, its simulation replications $n_{x}$ is obtained by the OCBA. Then, its objective value $f(x)$ is calculated as in Section 3.2. From Equation (9), it can be seen that very few replications are provided by OCBA for the inferior solution with a high $\delta_{b, i}$. Thus, the inferior solution will have few chances to be evaluated in the subsequent allocation. In contrast, more simulation replications are allocated by OCBA to the solutions with high $s_{i}$ to promote the evaluation accuracy of performance.

To balance the simulation budget and the accuracy, both $n_{0}$ and $\Delta$ should not be too small or too large. According to the suggested range of $n_{0}$ and $\Delta$ (Chen et al. 2000, 2009), it sets $n_{0}=10$ and $\Delta=10$ in this paper.

### 3.5. Probability model and updating mechanism

The EDA produces a new population by sampling the probability model. In this paper, the probability model is designed as a probability matrix $P$, where the element $p_{i j}(l)$ represents the probability that job $J j$ appears before or in position $i$ of the solution sequence at generation $l$. The value of $p_{i j}$ implies the importance of a job when deciding the jobs order. For all $i$ and $j, p_{i j}$ is initialised to $p_{i j}(0)=1 /$ $n$, which ensures that the whole solution space can be sampled uniformly.

In each generation of the OEDA, the new individuals are generated via sampling the probability matrix $P$. For every position $i$, job $J j$ is selected with the probability $p_{i j}$. If job $J j$ has already appeared, the whole $j$ th column of probability matrix $P$ will be set as zero and all the elements of $P$ will be normalised to maintain that each row sums up to 1 . An individual is constructed until all the jobs appear in the sequence. In such a way, Psize individuals are generated.

Next, it determines the superior sub-population with the best SPsize solutions identified by OCBA. Then, the probability matrix $P$ is updated as follows:

$$
p_{i j}(l+1)=(1-\beta) p_{i j}(l)+\frac{\beta}{i \times \text { SPsize }} \sum_{s=1}^{S P \text { size }} I_{i j}^{s}, \forall i, j
$$

where $\beta \in(0,1)$ is the learning rate of $P$, and $I_{i j}^{s}$ is the following indicator function of the $s$ th individual in the superior sub-population.

$$
I_{i j}^{s}=\left\{\begin{array}{l}
1, \text { if job } J_{j} \text { appears before or in position } i \\
0, \text { else }
\end{array}\right.
$$

The above updating process can be regarded as a kind of increased learning, where the second term on the right hand side of Equation (11) represents the learning information from the superior sub-population.

### 3.6. Procedure of the OEDA for SHFSP

With the above design, the procedure of the OEDA for solving the SHFSP is illustrated in Figure 3.

In each generation, Psize individuals are generated by sampling the probability matrix after the matrix is initialised. Then, the best SPsize individuals identified by OCBA are selected to update the probability model. If the stopping condition is met, it will stop the algorithm and output the optimal solution; otherwise, it will generate Psize new individuals for the next generation. Since the probability model is updated with elite individuals, it helps the algorithm track the search region with promising solutions. So, it is expected to solve the SHFSP effectively.

### 3.7. Computational complexity analysis

For each generation of the OEDA, its computational complexity can be roughly analysed as follows.

For the sampling process, every position is generated with the roulette strategy by sampling the probability matrix $P$. A sequence is constructed with the complexity $O\left(n^{2}\right)$ and

![img-2.jpeg](img-2.jpeg)

Figure 3. The procedure of the OEDA for the SHFSP.

Psize individuals can be generated with the complexity $O$ ( $n^{2}$ Psize $)$.

For the updating process, first it is with the computational complexity $O($ Psize $\log$ Psize $)$ by using the quick sorting method to select the best SPsize individuals from population; then, it is with the complexity $O[n$ $(S P s i z e+n)]$ to update all the $n \times n$ elements of $P$. Thus, the computational complexity for updating process is $O[n(S P s i z e+n)+$ Psize $\log$ Psize $]$.

It can be seen that the complexity of the proposed OEDA is not large and the algorithm could be efficient.

## 4. Numerical results and comparisons

To test the performance of the proposed OEDA, numerical tests are carried out by using the benchmarks provided by Chaari et al. (2011). Table 1 lists the configurations and the $L B$ values of these benchmarks. The algorithm is coded in C and is run on Thinkpad T420 with a 2.3 GHz processor/2GB RAM. For each benchmark instance, it sets

Table 1. Configurations and $L B$ values of the benchmarks.
the total number of evaluations as $10^{5}$, which is used as the stopping condition for the OEDA.

### 4.1. Parameters setting

The OEDA contains four key parameters: Psize (population size), $\eta$ (percentage of superior sub-population from population, $\eta=$ SPsize $/$ Psize $\times 100$ ), $N_{\text {total }}$ (the total computing budget in each generation) and $\beta$ (learning rate of $P$ ). To investigate the influence of these parameters on the performance of the OEDA, the Taguchi method of design-of-experiment (DOE) (Montgomery 2005) is implemented by using a moderate-scale instance No. 9 with $\alpha=10 \%$ and $\lambda=0.5$. Combinations of different values of these parameters are listed in Table 2.

For each combination, the OEDA is run 10 times independently. The average response variable (ARV) value is the average objective function value of 10 runs, i.e., $\operatorname{ARV}=\sum_{i=1}^{10} f_{i}(x) / 10$. Clearly, the smaller the ARV, the better is the combination. Considering 4 factor levels for each parameter, an orthogonal array $L_{16}\left(4^{4}\right)$ is shown in Table 3 together with the obtained ARV values.

Table 2. Combinations of parameter values.
Table 3. Orthogonal array and ARV values.
![img-3.jpeg](img-3.jpeg)

Figure 4. Factor level trend of the OEDA.

According to the orthogonal table, the trend of each factor level is illustrated in Figure 4. Then, the response value of each parameter is figured out to analyse the significance rank of each parameter. Besides, the best level of each parameter is obtained, e.g., level 3 of Psize, level 1 of $\eta$, level 2 of $N_{\text {total }}$ and level 1 of $\beta$. The results are listed in Table 4.

From Table 4, it can be seen that $N_{\text {total }}$ is the most significant one among the four parameters. That is, the total computing budget in each generation is crucial to the OEDA. A small value of $N_{\text {total }}$ could lead to insufficient evaluation for the solutions of the population. With a fixed total number of evaluations, however, a larger value of $N_{\text {total }}$ will lead to insufficient evolution with fewer generations. The significance of the learning rate $\beta$ of $P$ ranks the second. A large value of $\beta$ could lead to premature convergence. Besides, the population size Psize ranks the third. An appropriate population size ensures that the whole solution space can be sampled sufficiently. As for $\eta$, it has the slightest influence, while a smaller value is preferable to build the probability model. According to the above analysis, the parameter

Table 4. Response value $\left(\times 10^{-3}\right)$ and rank of each parameter.
Note: The bold values mean the best results.
setting for the OEDA is suggested as Psize $=50, \eta=10$, $N_{\text {total }}=1000$ and $\beta=0.1$, which will be used for the following tests.

### 4.2. Testing results and comparisons

To compare the proposed OEDA to the GA (Chaari et al. 2011), the best solution obtained by the algorithm is evaluated extra 100 times to confirm the accuracy as given in the literature (Chaari et al. 2011). Furthermore, the number of evaluations in the GA (Chaari et al. 2011) is no less than $2 \times 10^{6}$ and it is only $10^{5}$ in the OEDA. Besides, to study the influence of the OCBA on the OEDA, the results of the algorithm without the OCBA are also given. For a fair comparison, the algorithm without the OCBA has the same parameter setting as the OEDA, and 20 stochastic scenarios are evaluated for each solution in the population.

The detailed results are summarised in Tables 5-7 for the benchmarks with the uncertainty degree $10 \%, 25 \%$ and $50 \%$, respectively. The column ' $C_{\text {max }}^{I}$ ' presents the best makespan value of the initial scenario. The column 'STD' presents the deviation of all the makespan values of the stochastic scenarios and the initial one, which is calculated as follows:

$$
S T D=\sqrt{\frac{1}{100} \sum_{i=1}^{100}\left(C_{\max }^{I_{i}(I)}-C_{\max }^{I}\right)^{2}}
$$

The column ' $A V G$ ' presents the average makespan value of 100 stochastic scenarios. The column ' $D E V$ ' (in \%) presents the deviation between $A V G$ and $C_{\text {max }}^{I}$, which is calculated as follows:

![img-4.jpeg](img-4.jpeg)

![img-5.jpeg](img-5.jpeg)

![img-6.jpeg](img-6.jpeg)

$$
D E V=\frac{A V G-C_{\max }^{I}}{C_{\max }^{I}} \times 100
$$

According to Tables 5-7, as the value of $\lambda$ decreases, the values of $C_{c}^{I}{ }_{\max }$ increase and the values of STD decrease. The reason is that it only minimises the $C_{\text {max }}^{I}$ when $\lambda=1$, while it only minimises the STD when $\lambda=0$. For $\lambda=0.5$, it makes a trade-off between the two objectives.

From Tables 5-7, it can also be seen that the OEDA performs better than both the GA and the EDA without OCBA for solving the SHFSP. The $C_{c}^{I}{ }_{\max }$ values and the $A V G$ values by the OEDA are smallest among the three algorithms for all the instances, showing that the OEDA is most effective in solving the initial scenario and the stochastic scenarios. In addition, the STD values by the OEDA are also the smallest for all the instances, showing that the OEDA is most robust on the data uncertainty. To make easy reading of Tables 5-7, the relative percentage
improvement of the OEDA to the GA is shown in Table 8, where $P I_{-} C_{\max }^{I}, P I_{-} S T D$ and $P I_{-} A V G$ are calculated as follows:

$$
\begin{aligned}
& P I_{-} C_{\max }^{I}=\frac{C_{\max }^{I}(\mathrm{GA})-C_{\max }^{I}(\mathrm{EDA})}{C_{\max }^{I}(\mathrm{GA})} \times 100 \\
& P I_{-} S T D=\frac{S T D(\mathrm{GA})-S T D(\mathrm{EDA})}{S T D(\mathrm{GA})} \times 100 \\
& P I_{-} A V G=\frac{A V G(\mathrm{GA})-A V G(\mathrm{EDA})}{A V G(\mathrm{GA})} \times 100
\end{aligned}
$$

From Table 8, it can be seen that the values of $P I_{-} C_{c}^{I}{ }_{\max }$, $P I_{-} S T D$ and $P I_{-} A V G$ are all non-negative, which means that the OEDA can improve all the results of the GA for all the instances.

Table 8. Relative percentage improvement of the OEDA to the GA.
![img-7.jpeg](img-7.jpeg)

Figure 5. Computing budget allocation of OCBA in the OEDA.

Besides, the allocation of computing budget in a certain generation of the OEDA is shown in Figure 5, where the solutions among the population are ordered from the best to the worst. It implies that the OCBA is able to intelligently allocate computing budget for solutions. Assigning more for superior solutions, it helps the OEDA to effectively update the probability model and produce more promising solutions.

All in all, the OEDA is an effective and robust algorithm for solving the SHFSP. Its superiority owes to the following aspects. (1) With the well-designed probability model and the suitable updating mechanism, it is helpful to explore the search space effectively. (2) With the OCBA, it is helpful to identify superior solutions reasonably and update the probability model effectively. (3) With the suitable parameter setting, it is helpful to perform order-based EDA search.

## 5. Conclusions

This was the first research work to propose a novel EDAbased algorithm by employing the OCBA technique to solve the HFSP with stochastic processing time effectively. A bi-objective evaluation function was employed considering the effectiveness and robustness of the schedule. A specific probability model was designed, and a mechanism was presented for the OEDA to solve the problem. The OCBA was employed to evaluate solutions and identify superior ones for updating the model. The influence of parameter setting was investigated, and the suitable values were recommended. Extensive numerical testing results verified the contribution of employing the OCBA in the OEDA. Moreover, the comparison with the existing genetic algorithm demonstrated the effectiveness and robustness of the proposed OEDA. The OEDA enriches the tool-kit of SHFSP and can be generalised to other kinds of problems. The future work is to consider the SHFSP with other kinds of distribution laws or other schedule objective functions.

The scheduling problems with machine breakdowns or maintenance tasks as well as the problems with fuzzy or interval processing times are also worthy of being studied.

## Acknowledgements

The authors would like to thank Sondes Chaabane for providing the benchmark problems.

## Funding

This research is partially supported by the National Key Basic Research and Development Program of China [grant number 2013CB329503], the National Science Foundation of China [grant number 61174189] and the Doctoral Program Foundation of Institutions of Higher Education of China [grant number 20130002110057].
