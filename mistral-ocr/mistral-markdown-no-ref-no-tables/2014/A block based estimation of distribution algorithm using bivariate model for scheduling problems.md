# A block based estimation of distribution algorithm using bivariate model for scheduling problems 

Pei-Chann Chang $\cdot$ Meng-Hui Chen

(C) Springer-Verlag Berlin Heidelberg 2013


#### Abstract

Recently, estimation of distribution algorithms (EDAs) have gradually attracted a lot of attention and have emerged as a prominent alternative to traditional evolutionary algorithms. In this paper, a block-based EDA using bivariate model is developed to solve combinatorial problems. Instead of generating a set of chromosomes, our approach generates a set of promising blocks using bivariate model and these blocks are reserved in an archive for future use. These blocks will be updated every other k generation. Then, two rules, i.e., AC 1 and AC 2 , are developed to generate a new chromosome by combining the set of selected blocks and rest of genes. This block based approach is very efficient and effective when compared with the traditional EDAs. According to the experimental results, the block based EDA outperforms EDA, GA, ACO and other evolutionary approaches in solving benchmark permutation problems. The block based approach is a new concept and has a very promising result for other applications.


Keywords Combinatorial problems - Estimation of distribution algorithms $\cdot$ Bivariate probabilistic model $\cdot$ Artificial chromosomes

## 1 Introduction

Recently, the estimation of distribution algorithms (EDAs) has been one of the major evolutionary computing paradigms

[^0]applied in solving the combinatorial optimization problems (Ceberio et al. 2012). EDAs provide another way to evaluate solutions instead of using the crossover and mutation. EDAs build a probabilistic model based on the statistical information extracted from the selected solutions and generate new solutions by sampling from the probabilistic model (LarraÃ±aga and Lozano 2002). These new solutions are sampled from the model to replace the old population fully or in part. EDAs are good at solving hard problems when we do not have prior knowledge of the problems characteristics.

In order to obtain the better performance, some researchers have attempted to combine local heuristics with evolutionary algorithm. Chang et al. (2008a) propose a genetic algorithm by injecting artificial chromosomes (ACs) into new population to solve the single machine scheduling problems. A roulette wheel selection method is applied to generate an artificial chromosome by assigning genes onto each position according to the probability model. By injecting these artificial chromosomes, the genetic algorithm will speed up the convergence of the evolutionary processes. Other heuristics combining with different nature inspired approaches are also developed to solve permutation flow-shop scheduling problems (PFSP) such as (Ahmadizar 2012; Costa et al. 2012; Pen and Ruiz 2012; Dong et al. 2013; Tasgetiren et al. 2011).

In this paper, a block-based EDA using bivariate model is developed to solve combinatorial problems. Instead of generating a set of chromosomes, our approach generates a set of promising blocks using bivariate model and these blocks are reserved in an archive for future use. These blocks will be updated every other k generation. Then, a new solution is generated by combining the set of selected blocks and rest of genes. The block based approach is very efficient and effective when compared with the traditional EDAs.


[^0]:    Communicated by Y. Jin.
    P.-C. Chang ( $\boxtimes$ ) $\cdot$ M.-H. Chen

    Department of Information Management, Yuan Ze University, Chung-Li, Taoyuan 32026, Taiwan, ROC
    e-mail: iepchang@saturn.yzu.edu.tw

## 2 Literature review

### 2.1 Flow-shop scheduling problems

Permutation flow-shop scheduling problems are different kinds of the combinatorial problems. To find the solution of an optimization problem is a common challenge in which an algorithm may be trapped in the local optima of the objective function when the complexity is high, and there are several local optima in solution space.

According to Baker (1974) and Chang et al. (2008b) flowshop scheduling problem is a scheduling problem of combinatorial issues. Garey and Johnson (1979) also categorized these kinds of problems as NP-hard (non-deterministic polynomial-time hard). NP-hard in computational complexity theory, is a class of problems that are "at least as hard as the hardest problems in NP", meaning that the cost will be enormous to reach the best permutation of the PFSP problem. Therefore, most of the research works emerged to develop effective heuristics and meta-heuristics.

Flow-shop scheduling can be defined as follows: if $\mathrm{p}(\mathrm{i}, \mathrm{j})$ is the processing time for Job i on Machine j , and a job permutation is represented by $\left\{\pi_{1}, \pi_{2}, \ldots, \pi_{\mathrm{n}}\right\}$, when there are n jobs and m machines, the completion times $\mathrm{C}\left(\pi_{i}, \mathrm{j}\right)$ is calculated as the follows:

$$
\begin{aligned}
& \mathrm{C}\left(\pi_{1}, 1\right)=\mathrm{p}\left(\pi_{1}, 1\right) \\
& \mathrm{C}\left(\pi_{\mathrm{i}}, 1\right)=\mathrm{C}\left(\pi_{\mathrm{i}-1}, 1\right)+\mathrm{p}\left(\pi_{\mathrm{i}}, 1\right), \text { for } \mathrm{i}=2, \ldots, \mathrm{n} \\
& \mathrm{C}\left(\pi_{1}, \mathrm{j}\right)=\mathrm{C}\left(\pi_{1}, \mathrm{j}-1\right)+\mathrm{p}\left(\pi_{1}, \mathrm{j}\right), \text { for } \mathrm{j}=2, \ldots, \mathrm{~m} \\
& \mathrm{C}\left(\pi_{\mathrm{i}}, \mathrm{j}\right)=\max \left\{\mathrm{C}\left(\pi_{\mathrm{i}}-1, \mathrm{j}\right), \mathrm{C}\left(\pi_{\mathrm{i}}, \mathrm{j}-1\right)\right\}+\mathrm{p}\left(\pi_{\mathrm{i}}, \mathrm{j}\right) \\
& \text { for } \mathrm{i}=2, \ldots, \mathrm{n} ; \text { for } \mathrm{j}=2, \ldots, \mathrm{~m}
\end{aligned}
$$

The makespan is defined as:
$\mathrm{C} \max (\pi)=\mathrm{C}\left(\pi_{\mathrm{n}}, \mathrm{m}\right)$
There are some important additional conditions to this problem:

- Operations are independent and available for processing at time zero.
- Each machine i processes at most a job j at a time.
- M machines are continuously available.
- All jobs j can be processed only on one machine i at a time.
- Setup and removal times are independent from process sequence and are included in the processing times.

There are many other criteria that can be considered for the purpose of optimization. We refer the reader to Bagchi (1999) for a detailed discussion of scheduling using GA. For details of the flow-shop as well as other scheduling and sequencing problems, we refer the reader to Baker (1975). As proposed
by Reeves (1995), a more general flow-shop scheduling problem can be defined by allowing the permutation of jobs to be different on each machine. However, the more general flowshop scheduling problem tends to show only small improvement in solution quality over the PFSP, while it substantially increases the complexity of the problem. The size of the solution space increases from n ! to ( $\mathrm{n}!) \mathrm{m}$. Other objective functions for PFSP have also received a lot of attention, such as the mean flow-time (the time a job spends in the process), or minimizing the mean tardiness (assuming some deadline for each job).

Some of the PFSP problems in manufacturing industries like their jobs may have non-identical release dates, and there may be sequence-dependent setup times, and limited buffer storage between machines and so on. The problem in real world will make the problem more complex. However, genetic algorithm provides a more realistic view to solve the problem. Since it can generate alternatives of sequences to the decision maker and a more applicable sequence can be decided to solve the current problem with satisfactory results.

### 2.2 Estimation of distribution algorithms

In EDAs, the problem specific interactions among the variables of individuals are taken into consideration. The two main steps of EDAs are to estimate the probability distribution of selected solutions and to generate new chromosomes by sampling this probability distribution. In Evolutionary Computations the interactions are kept implicitly in mind whereas in EDAs the interrelations are expressed explicitly through the joint probability distribution associated with the individual variable selected at each generation. The probability distribution is calculated from a database of selected individuals from the previous generation. An offspring is obtained from the sampling of this probability distribution. Neither crossovers nor mutations have been applied in EDAs. However, to derive the estimation of the joint probability distribution associated with the database containing the selected individuals is a challenge task. The flowchart of EDA is shown in Fig. 1.

Paul and Iba (2002) proposed three probability distribution models. In this paper, independent uni-variate marginal distribution is estimated from marginal frequencies as shown in the following:
$p l\left(X_{i}\right)=\frac{\sum_{j=1}^{N} \delta_{i}\left(X_{i}=x_{i} \mid D_{i-1}^{S e}\right)}{N}$
$\delta_{i}\left(x_{i}=x_{i} \mid D_{i-1}^{S e}\right)=1$ if in jth individual $x_{i}$ has its ith value; otherwise it is zero.

Different EDAs use different models for the estimation of probability distribution. Population based incremental learn-

![img-0.jpeg](img-0.jpeg)

Fig. 1 EDA flowchart
ing (PBIL) proposed by Baluja (1994) and compact genetic algorithm (CGA) proposed by Harik et al. (1999) treat each variable of solutions as independent from one another. As a result, n-dimensional joint probability distribution factorizes as a product of $n$ univariate and independent probability distributions. PBIL and CGA both use a probability vector while the update rules of probability vector of PBIL and CGA are different.

In order to estimate the relationship between variables, a bivariate dependencies model is built. Zhang (2004) proposed univariate marginal distribution algorithm (UMDA) and another factorized distribution algorithm (FDA) for discrete optimization problems. He introduced the heuristic functions and the limit models of these two algorithms and analyzed the stability of these limit models. Higher order statistics is used to improve the chance of finding the global optimal solution. For applications of UMDA with Laplace corrections in binary and permutation domains is proposed by Paul and Iba (2002).

Edge histogram sampling algorithm (EHBSA) is a local search mechanism proposed by Tsutsui (2002). He proposed probabilistic model-building genetic algorithms (PMBGAs) in permutation representation domain using edge histogram based sampling algorithms (EHBSAs). He also proposed two types of sampling mechanisms, EHBSA/WO without template and EHBSA/WT with template which execute based on different situations. Tsutsui et al. (2006) modify his previous research using the EHBSA and proposes another histogram based model which called the node histogram sampling algo-
rithm (NHBSA). In this paper we employee EHBSA be one of local search mechanism.

In recent researches, some researchers hybridize EDA with other Meta-heuristic algorithms to form a powerful algorithm. Chen et al. (2012) proposed a new probability model by combining the uni-variate and bivariate probability models of EDA together. They define a new binary variable to indicate relationship between a pair of jobs. This variable collects the job which is immediately following another job in each chromosome. By combining the uni-variate probability model with bivariate probability model, a new probability model is formed which contain two different statistical information. The new probability model is used to generate new population. This hybrid heuristic algorithm is able to obtain good performance on flow-shop problem. Tzeng et al. (2012) proposed an algorithm hybridizing EDA with ant colony system (ACS) for the minimization of makespan in PFSP. They propose a new filter strategy and local search method to evaluate the solution and to use these solutions to generate new pheromone trails. A solution construction method of ACS is applied to generate members for the next iteration. In addition, they developed a new jump strategy to create more diversity for the solution to escape local optimal. Liu et al. (2011) proposed an algorithm to solve PFSP that combined particle swarm optimization (PSO) with EDA. The combination of the EDA procedure with PSO enables the sharing of information from the collective experience of the swarm and use a local optimal mechanism to increase the primitive intelligence for each particle.

In this paper, a bivariate probabilistic model is employed to generate a set of promising blocks instead of chromosomes. Then, these blocks will be recombined with rest of genes to form a legal chromosome. The new population generated from this block based approach has a very good fitness performance when compared with the traditional EDAs. In addition, a new local search mechanism is also proposed to speed up the convergence of the block based EDA.

## 3 A block based estimation of distribution algorithm

Basically, in BBEDA, we use a gene linkage probability matrix to identify a set of suitable blocks which is a small part of the chromosome to be recombined into a complete legal chromosome. In order to understand the behavior of block based evolution from the point of view of blocks creation and composition, there is a need for a very simple and direct representation of blocks. The block consists of a series of genes linked to each other continuously. To mine the blocks from the set of high fit chromosomes, two methods can be applied: static block size, on which equally sized blocks are created or dynamic block size where blocks are created with random sizes. In this research, we will focus on static block

size. In order to maintain diversity as well as to encourage the proper recombination, the size of block is very important while dealing with the problem with different number of jobs.

The first step is to collect the top $M$ chromosomes in fitness performance generated from the initial solutions. Then, convert the statistical information from those M chromosomes into dominance matrix and dependency matrix Chen et al. (2012). A set of blocks then can be generated using this bivariate probabilistic model. These blocks will be ranked according to the sum of the probability in each position. A fixed number of blocks will be maintained in the archive.

In the second step, two different types of combinations mechanism of artificial chromosome are developed. Using these blocks, ACs are generated by combining blocks and the rest of genes together. In the third step, we modify the edge histogram based on the sampling algorithm with template (EHBSA) mechanism form EDA proposed by Tsutsui (2002). The local search strategy is named as mEHBSA and is executed in the later iterations. The mEHBSA can speed up the convergence of BBEDA.

For further explanation of the flow, the pseudo-code is given as follows.

1. Initial population();
2. Calculate fitness();
3. While stopping criterion is not satisfied.
3.1 If refresh counter $>$ threshold $_{M}$ then

Refresh dominance and dependency matrix. Endif
3.2 Update dominance and dependency matrix by best $\mathrm{N} \%$ of population.
3.3 If counter of generate $\mathrm{AC}>$ threshold $_{A}$ then Mining blocks and generate artificial chromosomes. Endif
4. Execute mEHBSA();
5. Calculate fitness and sorting population by Cmax.
6. Use tournament selection to choose chromosomes to next iteration.
7. Endwhile

The flowchart of BBEDA is represented in Fig. 2.
As for the detailed procedure of each step, i.e., the generation of dominance and dependence matrix, the block mining process, and the modified EHBSA, they are explained as follows:

### 3.1 Dominance and dependency matrix

In this research, BBEDA applies two mechanisms to establish the probabilistic models sampling from the set of top
![img-1.jpeg](img-1.jpeg)

Fig. 2 The architecture of BBEDA
$M$ chromosomes. One is dominance matrix, and another is dependency matrix. By combining these two matrices with different ration, the blocks and AC generated will be more diversified.

Dominance matrix emphasizes the relationship of jobs between positions. The top M solutions are selected and accumulated in the archive. Then, the statistical information, i.e., the frequencies of each job shown up in each position, is calculated in the dominance matrix in each generation. The composition of the matrix is updated by the accumulation of the probability.

To build up the dominance matrix, a set of $m$ better chromosomes $\left(\mathrm{C}^{1}, \mathrm{C}^{2}, \ldots, \mathrm{C}^{\mathrm{m}}\right)$ are selected from the current generation $\mathrm{t} . \mathrm{X}_{\mathrm{ij}}^{\mathrm{k}}$ is a binary variable and can be treated like a gene within chromosome $\mathrm{C}^{\mathrm{k}}$ (As Eq. 7).

$$
\begin{aligned}
X_{i j}^{k} & =\left\{\begin{array}{l}
1 \text { if job i at position } j \\
0 \text { otherwise }
\end{array}\right. \\
i & =1 \cdots n ; j=1 \cdots n ; k=1 \cdots m
\end{aligned}
$$

Then the $\vartheta_{\mathrm{ij}}(\mathrm{t})$ in Eq. 8 which represents the number of times that job $i$ at position $j$ at the current generation $t$ is found from summing up the statistic information from all $m$ chromosomes to the $\mathrm{X}_{\mathrm{ij}}^{\mathrm{k}}$. The total number of generations is G .

$$
\begin{aligned}
& \emptyset_{i j}(t)=\sum_{k=1}^{m} X_{i j}^{k} \\
& i=1 \cdots n ; j=1 \cdots n ; t=1 \cdots G ; k=1 \cdots m
\end{aligned}
$$

We transform the dominance matrix into dominance probability matrix, and each element of the dominance probability matrix is defined as follows:
$P_{i j}(t)=\frac{\emptyset_{i j}(t)}{m} i=1 \cdots n ; j=1 \cdots n ; t=1 \cdots G$
A schematic diagram of the dominance matrix and dominance probability matrix is shown in Fig. 3.

The relationship of jobs between different jobs also has a significant influence for the new chromosomes to be generated, therefore, we established dependency matrix to store the information. In BBEDA, the dependency matrix is established in the following.

Each element in the dependency matrix and the dependency probability matrix is defined as in Eq. 10. To build up the dependency matrix, a set of $m$ better chromosomes $\left(\mathrm{C}^{1}, \mathrm{C}^{2}, \ldots, \mathrm{C}^{\mathrm{m}}\right)$ are selected from the current generation t . $\mathrm{Y}_{\mathrm{ij}}^{\mathrm{k}}$ is a binary variable and can be treated like a gene within chromosome $\mathrm{C}^{\mathrm{k}}$.

$$
\begin{aligned}
Y_{i j}^{k} & = \begin{cases}1 & \text { if job i is next to job } j \\
0 & \text { otherwise }\end{cases} \\
i & =1 \cdots n ; j=1 \cdots n ; k=1 \cdots m
\end{aligned}
$$

Then $\theta_{\mathrm{ij}}(\mathrm{t})$ in Eq. 13 which represents the number of times that job $i$ is next to job $j$ at the current generation $t$ is found from summing up the statistic information from all $m$ chromosomes to the $\mathrm{Y}_{\mathrm{ij}}^{\mathrm{k}}$. The total number of generations is G .

$$
\begin{gathered}
\theta_{i j}(t)=\sum_{k=1}^{m} Y_{i j}^{k}, i=1 \cdots n \\
j=1 \cdots n ; t=1 \cdots G ; k=1 \cdots m
\end{gathered}
$$

We transform the dependency matrix into dependency probability matrix, and each element of the dependency probability matrix is defined as follows
$P_{i j}(t)=\frac{\theta_{i j}(t)}{m} i=1 \cdots n ; j=1 \cdots n ; t=1 \cdots G$
A schematic diagram of the dependency matrix and dependency probability matrix is shown in Fig. 4.

In addition, the value of every element in dominance matrix and dependency matrix is set by 0.1 from the beginning (accumulated any solutions yet). The purpose is to increase the diversity of the new blocks and ACs generated. The blocks and ACs are generated by the roulette wheel selection method. Some position of the jobs are never accumulated which give them a better chance to be selected.

To sum up, a block is a more focused area within the chromosome to be looked into closely. Since, there are more chances that these blocks might be included in the final optimal solution. Of course, these blocks have to be evolved generation by generation. As a result, the new chromosome generated by recombining these blocks and the rest of genes will be also improved iteratively through the evolution procedure. In addition, by combining these two matrices, i.e., the dominance and dependency probability matrix, a set of more diversified blocks will be generated.

# 3.2 A block based approach 

Combinatorial optimization problems (COPs) are intractable owing to the computational complexities. Different Metaheuristics have been proposed to solve COPs. Problem

Fig. 3 Dominance matrix and dominance probability matrix

Fig. 4 Dependency matrix and dependency probability matrix
![img-2.jpeg](img-2.jpeg)

decomposition is one of the feasible approaches to reduce the problem complexities. Our attempt is to decrease the complexity of COPs by proposing blocks as virtual jobs and these virtual jobs certainly can reduce the problem dimensionality. Then, a better solution can be reached using the same amount of computation resources. For example, a single machine scheduling problem with ten jobs, it has 10 ! feasible solutions. As shown in Fig. 5, if we could find some information to combine a set of jobs to be grouped together. For example, job 1 with job 8 , job 4 with job 7 , and job 10 with job 5 , and the total number of feasible solutions will be 7 ! only. If a longer block is defined, the total number of feasible solutions will be reduced even more abruptly. However, the quality of the blocks will be the key to the block based approach.

According to previous researches, EDA use different kinds of probabilistic models to generate new solutions. Instead of generating chromosomes, we propose a mechanism which is called "block evolving" to decrease the complexity of COPs. We transform the dominance and dependency matrix into a probability matrix, and using the probability matrix to generate blocks. There are two advantages to using blocks instead of a chromosome. One is as mentioned above; blocks when compared with genes can decrease the complexity of COPs. The other is blocks are more effective than chromosomes in terms of convergence speed. To generate a chromosome, the probability model will be applied $n$ times if the chromosome length is $n$. To generate a block instead, the probability model will be applied only $m$ times if the block size is $m$. For example as shown in Fig. 6, an instance of probability matrix shows the probability of each job assigned in each position. A chromosome is generated with the sequence of $(3,2,5,4$, 1) and the probability will be $0.2 \times 0.2 \times 0.1 \times 0.2 \times 0.1$. However, if we generate a block with the size of 2 , and the block is $(5,4)$ since job 5 has the largest probability in posi-
![img-3.jpeg](img-3.jpeg)

Fig. 5 Complexity reduction


Fig. 6 An instance of a Probability matrix
tion one and job 4 in position 2. The probability of generating block $(5,4)$ will be $0.6 \times 0.6$ which is much larger than the probability of generating chromosome (3, 2, 5, 4, 1). After generating a set of high quality blocks, we can recombine them with the rest of genes and to form a legal chromosome. The quality or probability of this chromosome is better or higher than that of the traditional EDA.

But, relatively speaking, it is noteworthy that the block size cannot be too long either. For example, the probability of job $i$ in correct position is $x_{i}$, the probability of jobs $j$ in correct position is $x_{j}$ and the probability of jobs $k$ in correct position is $\mathrm{x}_{\mathrm{k}}$. A block performance for a pair of jobs i and j , is defined as follows:
$H_{i, j}=\mathrm{p}\left(x_{i}, x_{j}\right)=p\left(x_{i}\right) p\left(x_{j}\right)$
Then another block for a three of jobs $\mathrm{i}, \mathrm{j}$ and k , is defined as follows:
$H_{i, j, k}=\mathrm{p}\left(x_{i}, x_{j}\right)=p\left(x_{i}\right) p\left(x_{j}\right) p\left(x_{k}\right)$
Since $\mathrm{p}\left(\mathrm{x}_{\mathrm{i}}\right), \mathrm{p}\left(\mathrm{x}_{\mathrm{j}}\right)$ and $\mathrm{p}\left(\mathrm{x}_{\mathrm{k}}\right)$ are all smaller than or equal to one, the probability of $B_{i, j, k}$ is smaller than or equal to $B_{i, j}$.

A block with size L can be generated according to the following procedures:
$\mathrm{U}_{J}=\left\{\mathrm{J}_{1}, \mathrm{~J}_{2}, \ldots, \mathrm{~J}_{n}\right\}$ : A set of unselected jobs and $\mathrm{U}_{G}=$ $\left\{\mathrm{G}_{1}, \mathrm{G}_{2}, \ldots, \mathrm{G}_{n}\right\}$ : A set of unselected positions.
$\mathrm{S}_{j}=\emptyset$ : Scheduled job.
$L$ : block size.
B: block archive.
Step1. According to the dominance and dependency probability matrix, randomly select a $\mathrm{G}_{j}$ from $\mathrm{U}_{G}$ and using RWS to select $\mathrm{J}_{i}$ from $\mathrm{U}_{j}$ for $\mathrm{G}_{j} . \mathrm{U}_{G}=\mathrm{U}_{G} / \mathrm{G}_{j}$, $\mathrm{U}_{j}=\mathrm{U}_{j} / \mathrm{J}_{i}, \mathrm{~S}_{j}=\mathrm{S}_{j} \cup \mathrm{~J}_{i}$.
Step2. $j=j+1$. Then using RWS to select $\mathrm{J}_{i}$ from $\mathrm{U}_{j}$ for $\mathrm{G}_{j} . \mathrm{U}_{G}=\mathrm{U}_{G} / \mathrm{G}_{j}, \mathrm{U}_{j}=\mathrm{U}_{j} / \mathrm{J}_{i}, \mathrm{~S}_{j}=\mathrm{S}_{j} \cup \mathrm{~J}_{i}$.
Step3. Repeat step2 until $j=L-1$. A block is generated and reserve the block in B with the best fitness.

A schematic diagram of the block mining procedure is shown in Fig. 7.

After block mining, two different rules using block are designed to generate ACs. These two rules are called AC1 and AC2. Some symbols and the procedure of AC1 and AC2 are as follows:
$\mathrm{J}_{i}$ : Job number is i .
$\mathrm{P}_{j}$ : position is j .
U: Unscheduled jobs, $\mathrm{U}=\{1,2, \ldots, \mathrm{n}\}$.
S: Scheduled jobs, $\mathrm{S}=\{\phi\}$.
B: Block archive, $\mathrm{B}=\left\{\mathrm{b}_{1}, \mathrm{~b}_{2}, \ldots\right\}$.
AC1:
Step1. Select a $\mathrm{J}_{i}$ by using RWS and put it in homologous position, then $\mathrm{U}=\mathrm{U} /\{\mathrm{i}\}, \mathrm{S}=\mathrm{S}+\{\mathrm{i}\}$. If i is a starting job of a $\mathrm{b}_{m}$, select the block and $\mathrm{B}=\mathrm{B} /\left\{\mathrm{b}_{m}\right\}$

Dominance matrix

Calculate Cmax
![img-4.jpeg](img-4.jpeg)

Fig. 7 Block mining procedure

Step2. If $\mathrm{U}=\phi$, stop. Otherwise, go to step1.
AC2:
Step1. Put all blocks in the homologous position, and B $=\phi$.

Step2. Select a $\mathrm{J}_{i}$ by using RWS and put it in homologous position, then $\mathrm{U}=\mathrm{U}\{\mathrm{i}\}, \mathrm{S}=\mathrm{S}+\{\mathrm{i}\}$.

Step3. If $\mathrm{U}=\phi$, stop. Otherwise, go to step2.
The reasons for using AC1 and AC2 to generate ACs are for increasing the diversity and convergence speed simultaneously. AC1 only applying part of the blocks will have more diversity than AC2. AC2 using all blocks will be focused only for convergence speed.

As shown in Fig. 8, there are two different situations when applying AC1 to generate new chromosomes. One is that when jobs of blocks are not selected in the homologous positions, then the blocks will not be selected afterwards in this generation. The other is when the job is a starting job in a block and in the homologous position. The block will be placed into the homologous position of the chromosome.

Otherwise, as shown in Fig. 9, AC2 puts all blocks into homologous positions in the beginning, and then different
![img-5.jpeg](img-5.jpeg)

Fig. 8 AC1 mechanism
![img-6.jpeg](img-6.jpeg)

Fig. 9 AC2 mechanism
jobs are selected into each empty position from unscheduled jobs by roulette wheel selection (RWS) method.

AC1 and AC2 both use roulette wheel selection to select jobs by these probabilities. Each job's probability is based on the hybrid probability that merges dominance and dependency probability matrix. The combination probability ( CPjob n ) is defined as follows:
$C P_{\text {jobn }}=\left(W_{\text {dom }} * P_{\text {jobn }}^{d o m}\right)+\left(W_{\text {dep }}+P_{\text {jobn }}^{\text {dep }}\right)$

The value of weight of dominance and dependency probability matrix $\left(\mathrm{W}_{\text {dep }}\right)$ are defined by user. In this research, the value of $\mathrm{W}_{\text {dep }}$ is defined between 0.3 and 0.7 . $\mathrm{W}_{\text {dep }}$ increases as generation goes on. Number of $\mathrm{W}_{\text {dep }}$ is one minus $\mathrm{W}_{\text {dep }}$.

![img-7.jpeg](img-7.jpeg)

Fig. 10 A schematic diagram of the mEHBSA procedure

### 3.3 Modified EHBSA

EHBSA is an important heuristic in improving the quality of chromosomes generated by EDA. EHBSA/WT uses a template in sampling a new string from the selected chromosome. When n cut points are obtained for the template, the template should be divided into n segments. Then, we choose one segment randomly and sample nodes for the segment. Nodes in other $\mathrm{n}-1$ segments remain unchanged. This sampling method is denoted by EHBSA/WT/n.

In this paper, we propose a modified rule instead of the original EHBSA/WT. The detail of mEHBSA is described in the followings:

Step1. Randomly select a segment from the chromosome. Step2. Starting from the first gene of the segment, switch each gene with the next gene in order until it reaches the last position.
Step3. Calculate the fitness of the new chromosome after each exchange.
Step4. Then, the procedure repeats again based on the new chromosome generated in the last round of switch in step 3.
Step3. Select the chromosome with the best fitness among these new generated chromosomes as a new AC.

A schematic diagram of the mEHBSA procedure is shown in Fig. 10.

## 4 Experimental results

In this section the experimental results of BBEDA are presented and compared to the performance of other algorithms. The test instances for comparisons are adopted from Reeves ${ }^{1}$ and Taillard ${ }^{2}$ in OR-Library to validate the performance of BBEDA. Each instance is executed for thirty runs. The job number is n and the machine number is m . The population

[^0]size is set to 100 , and the termination criteria is set to when the total number of chromosomes generated is equal to job number * machine number * 50 in Reeves instance. The termination criteria for other instances are attributed to PSOLian et al. (2006). The EDA to be compared with in this research uses univariate probability model and the sampling method is applying the solutions with top $20 \%$ performance.

The error rate of each algorithm is defined as follows:

$$
\begin{aligned}
& \text { Error rate }(\%) \\
& =\frac{\text { observed value }- \text { optimal (opt.) value }}{\text { optimal value }} \times 100 \%
\end{aligned}
$$

Table 1 shows the performance comparison on Taillard's instances. The comparison standard is based on Chang et al. (2010). From the result, the average error rate of BBEDA is $0.62 \%$ which outperforms the other algorithms.

Table 2 shows ANOVA result of BBEDA when compared with AC2GA, PSO-Lian et al. (2006) and HGIA individually. The factor algorithms have a P-values are 0.002 . Therefore, all these algorithms which listed in Table 1 performed very differently.

Table 3 shows the performance comparison on Reeves's instances. The comparison standard is based on Chang et al. (2011). The average error rate is $0.60 \%$ which outperforms the other three algorithms. It can be concluded that BBEDA performs the effective searching ability on different problems and complexities.

Table 4 shows the performance comparison on Taillard's instances however, the comparison standard is based on Chen and Chen (2013). Again, the average error rate of BBEDA is $1.42 \%$ which outperforms the other algorithms.

Table 5 shows ANOVA result of BBEDA when compared with algorithms listed in Table 4 individually. The factor algorithms have a P-values are 0.000 . Therefore, all these algorithms which listed in Table 4 performed very differently.

Tables 6 and 7 show the result for testing BBEDA integrated with AC1or AC2 on Taillard's instances without local search strategy. The performance of BBEDA combined with AC1 has the best performance with the average error rate of $3.71 \%$. It shows that block mechanism and different recombination strategy can be helpful in increasing the convergence ability.

The three methods do not include any local search strategy. The result shows a significant difference in average value of error ratio and minimum value of error rate between BBEDAAC1, BBEDA-AC2 and the original EDA. The result also proves that block based approach and different recombination strategies have effectively reduced the problem size and achieved good convergence ability.

Tables 8 and 9 show the performance comparisons for EDA, BBEDA+AC1 and BBEDA+AC2 with local search on Taillard's instances.


[^0]:    ${ }^{1}$ http://people.brunel.ac.uk/ mastjjb/jeb/orlds/files/flowshop1.txt.
    ${ }^{2}$ http://mistic.heig-vd.ch/taillard/problemes.dir/ordonnancement.dir/ ordonnancement.html.

Table 1 Performance comparison on Taillard's instances

Table 2 ANOVA results on the error rate of the final solutions obtained by the algorithms listed in Table 1 on the 8 instances

As shown in Figs. 11 and 12, these three algorithms, i.e., EDA, EDA-AC1, and EDA-AC2, are not applied with local search rules when solving Ta030 and Ta060 problems. The convergence speed of AC 1 and AC 2 are both better than EDA only. The reason is because blocks can effectively decompose the original problem sizes and these blocks are also very effective when recombined with the rest of genes. And as shown in Figs. 13 and 14, these three algorithms are also used with local search strategy, i.e., mEHBSA. The result also shows that the convergence speed of AC 1 and AC 2 are still

Table 3 Performance comparison on Reeves's instances
Table 4 Performance comparison on Taillard's instances based on Chen and Chen (2013)

Table 5 ANOVA results on the error rate of the final solutions obtained by the algorithms listed in Table 4 on the 24 Instances

Table 6 Average error rate comparison on Taillard's instances with AC1 or AC2
Table 7 Min error rate comparison on Taillard's instances
Table 8 Performance comparison on Taillard's instances with local search
Table 9 Min error rate comparison on Taillard's instance with local search
![img-8.jpeg](img-8.jpeg)

Fig. 11 Ta030 convergence graph
![img-9.jpeg](img-9.jpeg)

Fig. 12 Ta060 convergence graph
![img-10.jpeg](img-10.jpeg)

Fig. 13 Ta030 convergence graph with local search strategy
![img-11.jpeg](img-11.jpeg)

Fig. 14 Ta060 convergence graph with local search strategy
better than EDA only. As for the abrupt drop in the convergence diagram, that is because of the application of mEHBSA afterwards.

## 5 Conclusion

In this study, we presented a block based EDA approach using bivariate model for solving the flow shop scheduling problems. The blocks are basically the subset of the building blocks of the chromosomes and two rules, i.e., AC1 and AC2 are developed to generate new chromosomes. The quality of the blocks mined from the previous population has a great impact on behavior of BBEDA. In addition, the blocks can be continuously updated based on the combination of dominance and dependency matrices. The length of the block is kept constant and is varied as per the size of the problem in this research. It is also ensured that the block size is not large. If the block lengths are too big, they may contain redundant information which will be carried over throughout the evolutionary process and the final result may not be good. If the block length is too small, the information contained within each block may be too little and the recombination process may not be able to come out with good quality of chromosomes.

The main contributions of this paper were to demonstrate that the BBEDA can be successfully extended to deal with "hard" optimization problems such as the Flow shop and the effectiveness and efficiency are largely improved when compared with previous researches in solving benchmark flow shop problems. Two effective rules, i.e., AC1 and AC2 are

also developed to generate new chromosomes in combining these effective blocks. In addition, a modified EHBSA is also proposed to further fine tune the chromosomes generated by BBEDA. The experimental results indicate BBEDA is very effective and efficient. Future works can be extended in applying this method on several other combinatorial problems.
