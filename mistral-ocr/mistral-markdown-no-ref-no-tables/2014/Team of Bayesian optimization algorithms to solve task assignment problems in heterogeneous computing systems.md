# Team of Bayesian Optimization Algorithms to Solve Task Assignment Problems in Heterogeneous Computing Systems 

Jie Li ${ }^{1}$, JunQi Zhang ${ }^{1, \dagger}$, Qi Kang ${ }^{2}$, Member, IEEE and ChangJun Jiang ${ }^{1}$, Member, IEEE<br>${ }^{1}$ Department of Computer Science and Technology, Tongji University, Shanghai, China<br>Key Laboratory of Embedded System and Service Computing, Ministry of Education<br>${ }^{2}$ Department of Control Science and Engineering, Tongji University, Shanghai, China<br>${ }^{\dagger}$ Corresponding author. Email: lijietjsh@gmail.com, \{zhangjunqi, qkang, cjjiang\}@tongji.edu.cn


#### Abstract

A Bayesian optimization algorithm (BOA) belongs to estimation of distribution algorithms (EDAs). It is characterized by combining a Bayesian network and evolutionary algorithms to solve nearly decomposable optimization problems. BOA is less popularly applied to solve high dimensionality complex optimization problems. A key reason is that the cost of training all dimensions by BOA becomes expensive with the increase of problem dimensionality. Since data are relatively sparse in a high dimensional space, even though BOA can train all dimensions simultaneously, the interdependent relations between different dimensions are difficult to learn. Its search ability is thus significantly reduced. In this paper, we propose a team of Bayesian optimization algorithms (TBOA) to search and learn dimensionality. TBOA consists of multiple BOAs, in which each BOA corresponds to a dimension of the solution domain and it is responsible for the search of this dimension's value region. The proposed TBOA is used to solve the real problem of task assignment in heterogeneous computing systems. Extensive experiments demonstrate that the computational cost of the overall training in TBOA is decreased very significantly while keeping high solution accuracy.


Keywords-Estimation of distribution algorithm (EDA), Bayesian optimization algorithm (BOA), dimensionality learning.

## I. Introduction

An estimation of distribution algorithm (EDA) is an advanced evolutionary algorithm first presented in [1]. Its most significant characteristic is that it can explicitly build the distribution of promising solutions and estimate their correlations. It has been successfully applied to solve various problems, e.g., multiobjective multiple traveling salesman [2], maximum diversity [3] and multiobjective resource-constrained project [4].

A Bayesian optimization algorithm (BOA) [5] belongs to EDAs. Its estimation and sampling are based on a constructed Bayesian network, which is designed to automatically estimate and construct the joint probability distribution of promising solutions in a search space. New candidate solutions are generated by sampling this Bayesian network and then they are incorporated into the current population to eliminate some relatively worse solutions. As an important evolutionary algorithm, BOA has been widely employed to solve all kinds of nearly decomposable optimization and scheduling problems, such as electric equipment configuration problem in a power
plant [6], decomposition problems [7], fault identification on flight control system [8], nurse scheduling [9] and directed acyclic graph (DAG) scheduling [10]. The work [11] presents a hierarchical Bayesian optimization algorithm (hBOA) that can solve hierarchical problems accurately and reliably by exploiting hierarchical decomposition as opposed to decomposition in a single level. The work [12] presents a mixed Bayesian optimization algorithm (MBOA) that tries to learn a Bayesian network via a set of decision trees with univariate normal-kernel leaves. The work [13] presents a real-coded Bayesian optimization algorithm (rBOA) that uses a Bayesian network with a mixture of normal distributions sampling to solve continuous optimization problems.

Although the aforementioned works have obtained satisfactory results, there is still a drawback in BOA. It is only used to deal with optimization problems with low dimensionality, and is rarely applied for solving high dimensionality complex optimization problems. The most important reason is that as the problem dimensionality increases, a BOA with a single Bayesian network will spend enormous cost to train all dimensions. Moreover, in a high dimensional space, even though a BOA can train all dimensions at the same time, the interdependent relations between different dimensions cannot be effectively represented because of the relatively sparse data issue. The search ability of BOA can thus be decreased dramatically.

How to provide an effective algorithm that can adapt to multi-dimensional problems without reducing its search ability becomes an important and interesting problem. This work for the first time proposes a team of BOAs (TBOA) to search and learn dimensionality. Each BOA only corresponds to a dimension and it is responsible for the search of this dimension's value region. It is divided into different parts, each of which corresponds to a continuous interval if it is continuous or to an integer value if it is discrete. Each BOA undertakes dimensional search and can estimate the joint probability distribution of different values that belong to this dimension, thereby leading to its own Bayesian network.

Compared with the previous works, TBOA has two significant characteristics:

1) It can obtain better solutions than traditional BOA via dimensional search precisely; and

2) It needs less computational cost of the overall training since the Bayesian network of each BOA in TBOA is only responsible for the training of a dimension, while in traditional BOA, a Bayesian network needs to train all dimensions simultaneously.
The proposed TBOA is applied to solve a task assignment problem in heterogeneous computing (HC) systems. The goal of this problem is to find an appropriate allocation that minimizes the total costs of execution and communication without violating any of the constraints. Extensive experiments are carried out on this problem to demonstrate the superiority of TBOA over BOA. The experimental results illustrate that the computational cost of the overall training in TBOA is dramatically decreased by more than $80 \%$ while keeping high solution accuracy.

Next, a task assignment problem is formulated in Section II. In Section III, BOA algorithm is introduced. The proposed TBOA is presented in Section IV. In Section V, it is applied to solve task assignment problems. Extensive simulation results comparing TBOA and BOA are presented in Section VI. Finally, concluding remarks are given in Section VII.

## II. TASK ASSIGNMENT PROBLEM FORMULATION

A task assignment problem in HC systems has the following characteristics: a task interaction graph (TIG) $G(V, E)$ is used to represent a distributed application, where $V$ is a set of $N$ tasks and $E$ is a set of edges that define the communication requirements among $N$ tasks. Precedence relations among tasks are not considered. A weight $w_{i j}$ represents the amount of data to be transferred between tasks $i$ and $j$. The estimated execution cost matrix $H=\left[y_{i k}\right]_{N \times M}$ where $y_{i k}$ is the cost to execute task $i$ on processor $k, N$ is the number of tasks, and $M$ is the number of processors. Next, $d_{h q}$ is defined as communication cost associated with one unit of data transferred from processors $h$ to $q$. It is symmetric, i.e., $d_{h q}=d_{q h}$. Here, different communication cost is incurred if an identical amount of data is transmitted through different communication channels. Thus, $w_{i k} d_{h q}$ defines a communication cost if tasks $i$ and $k$ are executed on processors $h$ and $q$, respectively.

The resource requirement by the $i$-th task, denoted as $r_{i}$, and the available resource capacity of the $p$-th processor, denoted as $O_{p}$ constitute the constraints. The task assignment problem in HC systems can be formulated as:

$$
\begin{aligned}
\min \operatorname{cost}(\theta)=\sum_{i=1}^{N} y_{i \theta[i]}+ & \sum_{i=1}^{N-1} \sum_{k=i+1}^{N} w_{i k} d_{\theta[i] \theta[k]} ; \forall \theta \in \vartheta \\
\text { s.t. } & \sum_{i: \theta[i] \mathrm{op}} r_{i} \leq O_{p} p \in\{1,2, \ldots, M\}
\end{aligned}
$$

where $\theta$ is an integer vector that represents the assignment of a particular task and $\vartheta$ is the set of all mappings. $\forall \theta \in \vartheta$, $\theta[i]=p$ denotes task $i$ being allocated to processor $p$. Eq. (1) consists of two parts. The first part is the sum of the execution costs and the second part is the sum of the communication costs among interacting tasks located at different processors. The scheduling goal is to minimize the sum of the two parts' costs. Eq. (2) is a constraint that ensures the total resource requirements of those tasks assigned to each processor must not exceed its resource availability.

## III. BOA

BOA [5][11] belongs to a class of EDAs. It is based on the estimation and sampling of a constructed Bayesian network, which is designed to automatically encode promising solutions' structure and estimate their joint probability distribution. New offspring are generated by sampling the constructed Bayesian network and are incorporated into the population such that some worse solutions can be eliminated. Algorithm 1 is a description of BOA.

## Algorithm 1 BOA

1: Generate the initial population randomly;
2: Select some promising solutions from the current population using a certain selection procedure, e.g., tournament selection or truncation selection;
3: Construct a Bayesian network by estimating these selected promising solutions;
4: Sample new offspring according to the joint probability distribution encoded by this Bayesian network;
5: Use new offspring to replace some worse solutions in the previous population;
6: If the end condition is not satisfied, go to Step 1.

Bayesian network [14] is a directed acyclic graph (DAG) that is composed of nodes, edges and the conditional probability table (CPT) that can identify the conditional dependencies. Bayesian network can be used to calculate the joint probability distribution among different nodes. It can be decomposed into the production of the marginal distribution randomly, that is:

$$
\begin{gathered}
P\left(x_{1}, x_{2}, \ldots \mid \mathbf{B}\right)=\prod_{i=1}^{L} P\left(x_{i}, \delta_{i}\right) \\
=\frac{\sum_{A-\left\{x_{i}\right\}_{i} \mid \delta_{i}} P\left(x_{1}, x_{2}, \ldots\right)}{\sum_{A-\left\{x_{i}\right\}} P\left(x_{1}, x_{2}, \ldots\right)}
\end{gathered}
$$

where $x_{1}, x_{2}, \ldots$ represent the set of nodes in Bayesian network, $L$ is the number of nodes. $\mathbf{B}$ is denoted as a Bayesian network, $\delta_{i}: i=1,2, \ldots$ means the set of the $i$-th node's parent nodes.

## IV. PROPOSED TBOA

The significant characteristic of BOA is that it owns an excellent estimation characteristic through a Bayesian network. However, it is difficult to estimate the interdependent relations between different dimensions as problem dimensionality increases. Inspired by swarm intelligence approaches [15] and clustering [16], this work proposes an effective algorithm to search and learn dimensionality, called a team of BOAs (TBOA). It consists of multiple BOAs, in which the number of BOAs used is equal to problem dimension count. Each BOA is responsible for searching in a dimension. It is used to search this dimension's value region, then a Bayesian network is constructed by estimating the interdependent relationships between different values and building the joint probability distribution of different values. Next, each BOA uses this Bayesian network to sample some new values. In the end, new offspring of problem are composed by all BOAs' sampling values.

![img-0.jpeg](img-0.jpeg)

Fig. 1. The structure of TBOA where Q is current population.

## A. Structure of TBOA

TBOA consists of multiple BOAs. Each BOA is responsible for searching in a dimension, and dimensional values in this dimension are sampled by its Bayesian network. Fig. 1 illustrates its structure. It consists of $R$ BOAs. $R$ is an optimization problem dimension count. Each BOA is considered as an independent entity and responsible for searching in a dimension. The Bayesian network of the $i$-th BOA can be defined as $<B N_{i}, C P T_{i}>, 1 \leq i \leq R$, in which $B N_{i}$ is the $i$-th Bayesian network structure, and $C P T_{i}$ is as conditional probability table. $x_{i, 1}, x_{i, 2}, \ldots, x_{i, U}$ denote the values in the $i$-th Bayesian network, $U$ is the number of values in the $i$-th dimension's value region. Each value corresponds to a continuous interval if the value region is continuous or corresponds to an integer value if it is discrete. $A_{i}$ denotes the selection states of all values according to those selected promising solutions that correspond to the $i$-th dimension, $C_{i}$ is the new values sampled from the $i$-th Bayesian network. $C=\left\{C_{1}, C_{2}, \ldots, C_{R}\right\}$ is the new offspring set that sampled from all Bayesian networks. "Select" denotes some promising solutions are selected from the current population according to their fitness values via a certain selection procedure. "Replace" denotes some worse solutions in the population are replaced by new offspring.

Next, we describe the process of TBOA. First, a promising set $A$ of solutions is selected from the current population $Q$. Then $A$ is mapped into $A_{i}, 1 \leq i \leq R$ that denotes selection states of all values in the $i$-th dimension according to those selected promising solutions. Next, BOA $i$ uses its $A_{i}$ to estimate the joint probability distribution of different values and construct the $i$-th Bayesian network, and then obtains the interdependent relationships between different values. The interdependent relationships indicate that whether to select a value is influenced by the selection states of this value's parents. Third, each Bayesian network samples the joint probability distribution to obtain a certain number of new candidate
values $C_{i}$, which are integrated into the new offspring set $C=\left\{C_{1}, C_{2}, \ldots, C_{R}\right\}$. At last, $C$ is used to replace some worse solutions in population $Q$, as shown in Fig. 1.

## B. Generation of Multiple Bayesian Networks

Now, we describe how to generate multiple Bayesian networks in TBOA. Each Bayesian network corresponds to a dimension that contains $U$ values. First, some promising solutions are selected from the current population according to their fitness values via a selection procedure. Second, construct multiple Bayesian networks according to the selected promising solutions. The structure of each Bayesian network in TBOA is encoded by a directed acyclic graph with the nodes correspond to the values in this dimension's value region and the edges correspond to their conditional dependencies. In order to map the selected promising solutions to each dimension, we define a new data set as follows. Let an $S \times R$ matrix $Z$ denote the selected promising solutions, in which $S$ is the number of promising solutions, and $R$ is the number of dimensions.

$$
Z=\left[\begin{array}{ccc}
z_{1,1} & \ldots & z_{1, R} \\
\ldots & z_{s, r} & \ldots \\
z_{S, 1} & \ldots & z_{S, R}
\end{array}\right]
$$

where $z_{s, r}$ indicates the promising value of the $r$-th dimension in the $s$-th promising solution, $1 \leq s \leq S, 1 \leq r \leq R$. Next, $Z$ is decomposed and converted to a new matrix set $A^{r}$, in which each matrix corresponds to a dimension of the solution domain.

$$
A^{r}=\left\{\left[\begin{array}{ccc}
a_{1,1}^{1} & \ldots & a_{1, S}^{1} \\
\ldots & a_{u, s}^{1} & \ldots \\
a_{U, 1}^{r} & \ldots & a_{U, S}^{1}
\end{array}\right] \ldots\left[\begin{array}{ccc}
a_{1,1}^{R} & \ldots & a_{1, S}^{R} \\
\ldots & a_{u, s}^{R} & \ldots \\
a_{U, 1}^{R} & \ldots & a_{U, S}^{R}
\end{array}\right]\right\}
$$

where $a_{u, s}^{r}$ indicates the $u$-th value's selection state of the $r$-th dimension in the $s$-th promising solution. $1 \leq u \leq U$. Matrix

set $A^{r}$ is used to estimate and construct Bayesian networks in TBOA. Each element $a_{n, s}^{i}$ has only two values, i.e., $a_{n, s}^{i} \in$ $\{0,1\}$. " 1 " means the $r$-th dimension selects the $u$-th value in the $s$-th promising solution, while " 0 " means "not".

## C. Sampling and Analysis of Each Bayesian Network

The sampling process in TBOA is different from traditional BOA. In traditional BOA, sampling is carried out according to the interdependent relationships between different dimensions, then a new offspring is generated by the dimensions order. In TBOA, sampling is carried out according to the interdependent relationships between different values in each dimension, so each dimension can sample its values independently, then a new offspring is generated by the combination of the values of different dimensions. The process of sampling in each BOA consists of two steps. The first step computes an ancestral ordering of the values, where each value is preceded by its parents. The basic idea is that the selection states of the parents of each value are generated prior to the generation of the selection state of the value itself. In the second step, the selection states of all values are generated according to the ancestral ordering. Assuming that we have a vector $\mathbf{b}_{r}$ for the $r$-th dimension at each sample, which is described as $\mathbf{b}_{r}=\left(b_{r}^{1}, \ldots, b_{r}^{U}\right)$. Every element in $\mathbf{b}_{r}$ indicates whether this dimension selects the corresponding value. " 1 " means yes, while " 0 " no. Note that all elements in $\mathbf{b}_{r}$ may be " 1 ", which means all values are selected by this BOA sampling. A preservation mechanism can be used to preserve those desired values, e.g., preserving the first sampled " 1 " value or random preservation.

Next, we will briefly analyze the characteristics of each Bayesian network in TBOA. We assume that the structure of the $r$-th Bayesian network is shown in Fig. 2(a). It owns four values. At the initial iteration, the conditional probabilities between these values are similar, as described in the conditional probability table of Fig. 2(b). Sampling it may yield a vector $\mathbf{b}_{r}=(1,1,1,1)$, which means the diversity of values is very high. It has a broad selection region and owns a high global search ability. With the increased number of iterations, TBOA may converge. The conditional probabilities among the four values also converge, as described in the conditional probability table of Fig. 2(c). Bayesian network $r$ may be sampled to generate a new vector $\mathbf{b}_{r}=(0,0,0,1)$, which means the diversity is very low, implying its high local search ability.
![img-2.jpeg](img-2.jpeg)
(a)
Bayesian network structure
![img-2.jpeg](img-2.jpeg)
(c)

Conditional probability table

Fig. 2. An example of sampling Bayesian network $r$. (a) Bayesian network structure and (b) (c) Conditional probability table.

## V. TASK ASSIGNMENT IN HC SYSTEMS

We now apply the proposed TBOA to solve a task assignment problem in HC systems. First, solution representation and how to generate initial population are given. Next, the structure of a Bayesian network is described. At last, a complete TBOA algorithm is represented.

## A. Solution Representation and Initial Population

Solution representation is important for TBOA. A welldesigned representation can make problem-solving easy. A potential solution for solving task assignment in HC systems is an integer vector $T[n], n \in\{1,2, \ldots, N\}$ with $N$ elements, $T[n]=m, m \in\{1,2, \ldots, M\}$ denotes that processor $m$ is assigned to the $n$-th task. $N$ is the number of tasks and $M$ is the number of processors. Initially, a population $Q$ is randomly generated by a uniform distribution, the $i$-th dimension of the $g$-th solution is described as $x_{g, i}=\operatorname{randint}(1, M)$, $i \in\{1,2, \ldots, N\}$ and $g \in\{1,2, \ldots, Q\}$. randint is a function that generates an integer uniformly distributed in the integer range $[1, M]$. Then, the cost of a solution described in Eq. (1) is used as its fitness value.

## B. Structures of Multiple Bayesian Networks

We use Eqs. (4) and (5) to construct the structures of multiple Bayesian networks for solving this task assignment problem. First, some promising solutions are selected from the current population according to their fitness values via a selection procedure. Next, the structures of multiple Bayesian networks in this task assignment problem are constructed according to Eqs. (4) and (5), where $S$ is the number of promising solutions, $N \times R$ and $M \times U$. Each matrix in Eq. (5) is used to estimate and construct a Bayesian network that searches a task assignment domain. Thus, each element in matrix $A_{m, s}^{n}$ can be " 1 " meaning that the $n$-th task is assigned on the $m$-th processor in the $s$-th promising solution, and" 0 " means "not".

## C. Complete TBOA Algorithm

At each iteration, some new values (processors) are sampled in each dimension (task) according to the joint probability distribution encoded by the constructed Bayesian network. Note that there may be many 1's are sampled by a Bayesian network in a dimension. Implying that a task is assigned to different processors. This is unlikely to happen because a task can only be assigned to one processor in every assignment. This work adopts the first " 1 " sampled by a Bayesian network. After all tasks are assigned to their processors, a new offspring is generated. Thus, some new offspring are generated by all tasks sample their Bayesian networks repeatedly, then they replace some worse solutions in the previous population. The process of TBOA for solving a task assignment problem in HC systems is presented in Algorithm 2.

## VI. EXPERIMENTAL EVALUATION

Extensive simulations are carried out in order to compare TBOA with BOA [5]. They are coded in MATLAB-R2010a and dedicated simulations are executed on a 3.20GHz Core i3 processor with 4GB main memory running under Windows 7 environment.

TABLE I. EXPERIMENTAL RESULTS OVER 20 RUNS ON EACH OF 9 INSTANCES WITH 200 itRRA TIONS.


## Algorithm 2 TBOA

1: At iteration $t \geq 1$, calculate the fitness value for each potential solution in the population;
2: Select two solutions from the current population randomly;
3: Compare the fitness values of the selected two solutions through a tournament selection procedure and the better one is retained. Then repeat Steps 2 and 3 until $S$ promising solutions are selected;
4: Then Eqs. (4) and (5) utilize $S$ promising solutions to construct multiple Bayesian networks;
5: Some new offspring are generated by all tasks sample their constructed Bayesian networks repeatedly;
6: $S$ worse solutions in the current population are replaced by the top $S$ new offspring regardless of their merits. Because these Bayesian networks are constructed by using promising solutions, most of new offspring are excellent;
7: If the end condition is not satisfied, go to Step 1.

The main parameters are $N, M$ and $D$ as the task interaction density that is a probability between two tasks. $C C R$ is the communication to computation time ratio, which is the ratio of the average communication cost to the average computation cost. Three levels of task interaction density $D=(0.3,0.5$ and 0.8 ) and three $C C R=(0.5,1.0$ and 2.0$)$ are tested, resulting a total of 9 benchmark instances. In our experiments, the values of $(N, M)$ are set to $(50,5)$. The number of initial population for two algorithms is set to $100, S=50$. In order to compare both algorithms fairly, a solution obtained by greedy constructive heuristic (GCH) algorithm [17] is integrated into the initial population. Their termination condition is that a maximum number of iterations, i.e., 200, is reached. Due to their stochastic nature, each of their independent runs may yield a different result. Thus, we run each algorithm 20 times for every benchmark instance and report the statistical results.

The experimental results are obtained by the corresponding algorithms to solve the problem instances, as shown in Table I. $M_{\text {avg }}$ denotes the average total costs of execution and communication in Eq. (1), $V_{\text {avg }}$ denotes the average standard deviation, $T_{\text {avg }}$ denotes the average computational cost and $B_{\text {best }}$ denotes the best total costs. The average relative percentage deviations of total costs $R P D_{M}$ and computational $\operatorname{cost} R P D_{T}$ are calculated as follows:

$$
\begin{aligned}
& R P D_{M}=\left(M_{B O A}-M_{T B O A}\right) / M_{T B O A} \times 100 \\
& R P D_{T}=\left(T_{B O A}-T_{T B O A}\right) / T_{T B O A} \times 100
\end{aligned}
$$

where $M_{T B O A}$ and $T_{T B O A}$ are the average total costs and the average computational cost across 20 independent runs obtained by TBOA. $M_{B O A}$ and $T_{B O A}$ are the ones provided by BOA for each instance.

It is observed from Table I that TBOA outperforms BOA in terms of the average total costs and the average computational cost performance among all the instances. The evolution of the mean total costs derived from the two algorithms is displayed in Fig. 3. From these results, we can easily draw a conclusion that TBOA performs much better than BOA. Its convergence is faster than BOA's significantly without losing the accuracy. Especially in terms of efficiency, TBOA can reduce the average computational cost by more than $80 \%$. These results indicate that TBOA is highly competitive at solving the task assignment problems. The superior performance of TBOA can be attributed in large part to the use of a team of BOAs. Each BOA can precisely search promising offspring in a dimension. There seems to be an interesting behavior of TBOA taking a global search in the early stages of runs since multiple Bayesian networks maintain the diversity. Then as iterations proceed, TBOA gradually converge. The conditional probabilities among different values also gradually converge, thereby equipping TBOA with better local search ability.

## VII. ConClusions

In this paper, the contribution of our work is that we for the first time propose a team of Bayesian optimization algorithms (TBOA) to search and learn dimensionality. Each BOA undertakes a dimensional search. The results on a task assignment problem in heterogeneous computing systems show that the proposed TBOA can significantly decrease the computational cost of the overall training while maintaining high solution accuracy. In the future, we will upgrade the performance of TBOA through a coordination mechanism. Besides, we will try to apply TBOA to solve more discrete optimization problems, such as multiobjective resource-constrained project scheduling problems [4] and optimal power flow problems [15].

## ACKNOWLEDGMENT

This work is supported by China NSF under Grants No. 61272271, 71371142, 61332008 and 91218301, Shanghai NSF under Grant No. 12ZR1434000, National Basic Research Program of China under Grant No. 2014CB340404, International Cooperation Project of Chinese Ministry of Science and Technology under Grant No. 2012DFG11580.

![img-3.jpeg](img-3.jpeg)

Fig. 3. The evolution of the mean total costs derived from TBOA and BOA over 20 independent runs on each of 9 instances.
