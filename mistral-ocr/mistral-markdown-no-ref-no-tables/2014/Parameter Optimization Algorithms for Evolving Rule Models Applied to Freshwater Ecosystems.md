# Parameter Optimization Algorithms for Evolving Rule Models Applied to Freshwater Ecosystems 

Hongqing Cao, Friedrich Recknagel, and Philip T. Orr


#### Abstract

Predictive rule models for early warning of cyanobacterial blooms in freshwater ecosystems were developed using a hybrid evolutionary algorithm (HEA). The HEA has been designed to evolve IF-THEN-ELSE model structures using genetic programming and to optimize the stochastical constants contained in the model using population-based algorithms. This paper intensively investigates the performances of the following six alternative population-based algorithms for parameter optimization (PO) of rule models within this hybrid methodology: 1) hill climbing (HC); 2) simulated annealing (SA); 3) genetic algorithm (GA); 4) differential evolution (DE); 5) covariance matrix adaptation evolution strategy (CMA-ES); and 6) estimation of distribution algorithm (EDA). The comparative study was carried out by predictive modeling of chlorophyll-a concentrations and the potentially toxic cyanobacterium Cylindrospermopsis raciborskii cell concentrations based on water quality time-series data in Lake Wivenhoe, Queensland, Australia, from 1998 to 2009. The experimental results demonstrate that with these PO methods, the rule models discovered by the HEA proved to be both predictive and explanatory whose IF condition indicates threshold values for some crucial water quality parameters. When comparing different PO algorithms, HC always performed best followed by DE, GA, and EDA, while CMA-ES performed worst and the performance of SA varied with different data sets.


Index Terms-Cyanobacterial blooms, evolutionary algorithm, genetic programming, population-based algorithms.

## I. INTRODUCTION

EECURRENT cyanobacterial blooms have become a common feature of Australian rivers and lakes [1], [2] and pose significant risks to human health through water consumption [3], [4]. Harmful algal blooms have a high economic cost worldwide and a report in 1999 [5] estimated the annual cost to Australia to be between AUD\$180 and AUD\$240 million. Therefore, there is an immediate need to develop suitable tools for predictive modeling to facilitate early warning and informed management of cyanobacterial blooms.

Traditionally, process-based models that allow simulations of food web dynamics and nutrient cycles over time by using

[^0]ordinary differential equations (ODEs) [6], [7] have been widely used. However, these ODEs are typically calibrated, and are usually only applicable for one specific sampling site.

With the rapid development of advanced computing technology, data mining (DM) techniques [8] have been explored to complement or even replace process-based models. With the aim of prediction, extensive study of machine learning techniques has been conducted in ecological modeling, which typically include artificial neural network (ANN) models [9], [10], and fuzzy logic and neuro-fuzzy models [11], [12]. A major drawback of ANN models is the lack of transparency by not explicitly representing the underlying models. In contrast, fuzzy models attempt to overcome this by introducing fuzzy rules, but the acquisition and reliability of these rules largely depends on the experts' empirical knowledge.

In recent years, the use of evolutionary algorithms (EAs) has gained wide popularity in domains, such as machine learning, pattern recognition, economic prediction, optimization control, and parallel processing [13], due to their capability of self-adaptation, self-organization, self-learning, intrinsic parallelism, and generality. The first application of EAs for modeling cyanobacterial blooms using simple equations was reported in [14]. Recently, we proposed a hybrid evolutionary algorithm (HEA) [15] to model algal blooms using single IF-THEN-ELSE rules that provide high transparency of the ecological driving forces and relationships driving cyanobacterial growth. Even though the IF-THEN-ELSE rule structures are similar to rule-based classifiers [16], they are more flexible in terms of representing input relationships by complex nonlinear functions in both the condition branch and the result branch. Moreover, the IF-THEN-ELSE rules calculate the output by a mathematical expression directly from the selected result branch rather than by execution of a set of rules in order.

Since we found that the IF condition randomly generated by genetic programming (GP) indicates ecological thresholds of crucial water quality parameters, we carried out parameter optimization for each rule. In addition, the optimized random constants in the THEN and ELSE branches help improve the predictive accuracy and explanatory power of the model.

The previous HEA version used only a genetic algorithm (GA) based on multiparent crossover [17] to optimize the parameters of rule models, but did not compare with other optimization algorithms. In this paper, we systematically studied six population-based algorithms for parameter optimization


[^0]:    Manuscript received August 13, 2012; revised March 5, 2013, May 13, 2103, and September 3, 2013; accepted September 24, 2013. Date of publication October 18, 2013; date of current version November 26, 2014.
    H. Cao and F. Recknagel are with the School of Earth and Environmental Sciences, University of Adelaide, Adelaide 5005, Australia (e-mail: hongqing.cao@adelaide.edu.au; friedrich.recknagel@adelaide.edu.au).
    P. T. Orr is with Seqwater, City East Qld 4002, Australia (e-mail: philip.orr@monash.edu).
    Digital Object Identifier 10.1109/TEVC.2013.2286404

![img-0.jpeg](img-0.jpeg)

Fig. 1. Conceptual diagram of HEA for evolving the IF-THEN-ELSE rule models.
within the hybrid methodology. They are: 1) hill climbing (HC); 2) simulated annealing (SA); 3) GA; 4) differential evolution (DE); 5) covariance matrix adaptation evolution strategy (CMA-ES); and 6) estimation of distribution algorithm (EDA). The effectiveness of each algorithm was tested by the validity of the prediction of chlorophyll-a (Chl-a) concentrations and cell concentrations of the potentially toxic cyanobacterium Cylindrospermopsis raciborskii (Wolonszynska) Seenaya and Subba Raju (C. raciborskii) for 11 years of data monitored in Lake Wivenhoe, Queensland, Australia. In this paper, we provide a systematic analysis and comparison of different parameter optimization algorithms, as well as the resulting predictive rule models.

The remainder of this paper is structured as follows. In the next section, we briefly describe the hybrid evolutionary algorithm to develop rule models for predicting the abundance of cyanobacteria that includes the structure optimization using genetic programming and the six population-based algorithms applied to optimize the random constants in the rule. In Section III, we present the experimental results using the six algorithms on two data sets and statistically compare their performances. In Section IV, further experiments are described based on the best models achieved in Section III. Finally, we draw some conclusions and identify future work in Section V.

## II. HyBRID Evolutionary Algorithm FOR Evolving IF-THEN-ElSE RULES

A HEA was proposed to evolve the rule model for predicting the abundance of cyanobacteria. The HEA contains two layers. The first or outer layer searches for model structures using GP, while the second or inner layer searches for the optimal continuous parameters of the model using some populationbased optimization algorithms. Fig. 1 shows a conceptual diagram of HEA based on water-quality data as input and cyanobacterial abundance data as output. The details of the GP and the population-based algorithms are briefly described in what follows.

IF $\mathbf{i}(\mathbf{W T}=22.5) \mathrm{AND}(\mathbf{p H}=10.8))$ OR $\left(\mathrm{TP} * \mathrm{DO}==523.5\right)$ ) THEN Chl-a $=\mathrm{DO} * \exp (\mathrm{pH})+\mathrm{NTU} * \mathrm{SiO},{ }^{*} 34.5$ ELSE Chl-a $=\mathrm{WT} / 2.1+\ln \left(\mathrm{TP} * \mathrm{pH}-25.6\right)$

Fig. 2. Example of a rule model for predicting Chl-a concentrations.

## A. Structure Optimization of Rule Models

1) Model Representation: We used GP [18], [19] as the main technique for evolving the rule model structure. Because GP typically operates on parse trees instead of bit strings as traditional GA does, it is well suited to evolving an equation or formula relating the output and input variables. First, we defined the following three function sets.
1) Logic function set: $F_{\mathrm{L}}=\{A N D, O R\}$
2) Comparison function set: $F_{\mathrm{C}}=\{>,<, \geq, \leq\}$;
3) Arithmetic function set: $F_{\mathrm{A}}=\{+,-, *, /$, exp, $\ln \}$.

In our case, a rule model with an IF-THEN-ELSE structure is represented as a vector of multiple trees in GP with the form (Tree1, Tree2, Tree3). Tree1 denotes the IF condition branch, and Tree2 and Tree3 denote the result branches of the THEN and ELSE branch, respectively. Their function sets are

$$
\mathrm{F}_{\text {Tree } 1}=\mathrm{F}_{\mathrm{L}} \cup \mathrm{~F}_{\mathrm{C}} \cup \mathrm{~F}_{\mathrm{A}} \text { and } \mathrm{F}_{\text {Tree } 2 / \text { Tree } 3}=\mathrm{F}_{\mathrm{A}}
$$

They have the same terminal set as

$$
T=\left\{x_{1}, \ldots, x_{n}, c\right\}
$$

where $n$ is the number of input variables and $c$ is a random constant.

Fig. 2 shows an example of a rule model for predicting Chl-a concentrations. The rule model shows the relationship between Chl-a and water temperature (WT), pH , total phosphorus (TP), dissolved oxygen (DO), turbidity (NTU), and $\mathrm{SiO}_{2}$. Note that we provided this example for the purpose of showing the rule structure only, which may not have any biological meaning.
2) Genetic Operators: We used the standard GP crossover and mutation operators [18] to recombine the rules in the

![img-1.jpeg](img-1.jpeg)

Fig. 3. Locations of three reservoirs in southeastern Queensland, Australia, showing the sampling sites on each reservoir (from [50]).
population to produce new models. The crossover is always performed between the same type of trees (Tree1/Tree2/Tree3). Note that in Tree1 there are three different types of function nodes that come from $\mathrm{F}_{\mathrm{L}}, \mathrm{F}_{\mathrm{C}}, \mathrm{F}_{\mathrm{A}}$, respectively. Only the same types of nodes are selected as the crossover points that ensure the crossover always produces valid condition branches for a rule.
3) Fitness Evaluation: The entire data set was divided into two parts: training data and testing data. As the 11 years recorded water quality data from Lake Wivenhoe reveal different annual patterns, it is difficult to choose which years should be used for training and which years should be left out for testing. To improve the robustness and stability of the model, we used a bootstrap method [20] to choose the training and testing data points randomly. That is, we defined a percentage first ( $75 \%$ in this paper) and in each run that percentage of the whole data set was randomly selected as the training data set and the remainder used as the testing data set. We realized that as bootstrapping requires independent data samples, when applied to time-series data, it inevitably overestimates the model accuracy to some extent, and the model performance is likely to be biased due to this sampling.

The goodness of a model in the population is evaluated by the root mean squared error (RMSE) between the measured training data and the predicted data. The fitness function is defined as

$$
\text { Fitness }=\sqrt{\frac{1}{k} \sum_{i=1}^{k}\left(\hat{y}_{i}-y_{i}\right)^{2}}
$$

where $k$ is the number of training data points, and $y_{i}$ and $\hat{y}_{i}$ are the measured value and the predicted value of the $i$ th data point, respectively. Obviously, here, the lower the fitness value is, the better the model. In addition, as the training data
set and testing data set can vary in each run, to make a fair comparison, we validated the best-evolved rule in each run on all the data (both training and testing data) and calculated its total RMSE as the comparable value among different runs and different algorithms.

## B. Parameter Optimization of Rule Models

1) Six Parameter Optimization Algorithms: One of the problems inherent in ecosystem modeling is that when a model is built to describe an ecosystem, there are always parameters in the model whose values have large uncertainty and can affect the model accuracy significantly. Traditionally, a wide range of mathematical optimization techniques have been applied to ecosystem models to adjust model parameters by minimizing the misfit between the model output and a set of in situ observations. To date, those include among others, least-squares methods [21], [22], modified simplex algorithm [23], conjugate gradient methods [24], [25], and adjoint methods [26], [27]. A drawback common to each of the above techniques is that they are only able to detect a local minimum, and the minimum they find may depend on the parameter settings of the initial values of the model. It is clearly more desirable to have techniques that are able to identify the global minimum. The global stochastic techniques supposedly capable of doing this have been developed and applied to ecosystem models. They include SA [28], [29] and GA [30]. More recently, some new population-based methods have been developed, such as DE, CMA-ES, and EDA. They are considered to be the most competitive representatives of evolutionary computation but so far have not been applied to ecosystem models. In this paper, we selected several state-of-the-art population-based global optimization techniques to optimize the random constants contained in a rule model and compared their performance and efficiency. Those methods

include: 1) HC; 2) SA; 3) GA; 4) DE; 5) CMA-ES; and 6) EDA.

To ensure a fair comparison, for each algorithm we defined the stopping (termination) criterion as being when the total number of fitness evaluation evalnum reaches a maximum number $N_{\text {eval }}(=1000)$. Below we provide brief descriptions of each method as well as the settings of some key control parameters for each algorithm. The parameter values were set based on previous experience and in consideration of available computational time. Readers are strongly recommended to refer to relevant literature for a better understanding of each method.
a) $H C$ : HC was originally a local search technique that starts with a random (potentially poor) solution, and iteratively makes small changes to the solution, each time improving it a little. When the algorithm cannot detect any improvement, it terminates. In this paper, we modified the simple HC method by enabling it to do hill-climbing iteratively, each time with an initial starting point from the best solution in the previous run. The pseudocode of HC is presented in Appendix A. A key control parameter for the algorithm is the neighborhood size $N_{\text {neighbour }}$, which was set as 50 for this paper.
b) SA: SA [31] is a random-search technique that exploits an analogy between the way in which a metal cools and freezes into a minimum energy crystalline structure (the annealing process) and the search for a minimum in a more general system. It forms the basis of an optimization technique for combinatorial and other problems. The pseudocode of SA is presented in Appendix B. The three key control parameters in the algorithm were set as follows: initial temperature $T_{0}=100$, the epoch length $L=100$ (the number of trials allowed at each temperature level), and the maximum Markov chains length $K=10000$ (the total number of trials allowed in the whole SA process).
c) GA: A GA [32] is a search heuristic that mimics the process of biological evolution and the mechanisms of natural selection and genetic variation. Suitable codings are used to represent possible solutions to a problem, and the search is guided by using some genetic operators (crossover, mutation, etc.) and the principle of survival of the fittest. Due to the merits of self-adaptation, self-organization, selflearning, intrinsic parallelism and generality, GAs have been applied successfully in a wide range of economic, engineering, and scientific disciplines [33]. The GA we used in this paper is based on multiparent crossover [17]. The pseudocode of GA and the details of multiparent crossover are presented in Appendix C. The three control parameters ( $M, \mathrm{a}, \mathrm{b}$ ) in the crossover enable the offspring to skip out of the range of parents. In this paper, they were set as $M=8, \mathrm{a}=-0.5, \mathrm{~b}=1.5$, and the population size Popsize was set as 50 .
d) DE: DE is an EA [34] proposed in [35] for solving real-parameter optimization problems. It is an effective, robust, and simple global optimization algorithm that extracts the differential information (i.e., distance and direction information) from the current population of solutions to guide its further search. In this way, no separate probability distribution has to be used which makes the scheme completely selforganizing. According to frequently reported comprehensive
studies [36], [37], DE outperforms many other optimization methods in terms of convergence speed and robustness over common benchmark functions and real-world problems. In this paper, we used the DE algorithm and five alternative DE schemes from [38]. The pseudocode of DE and the details of five alternative DE schemes are presented in Appendix D. The parameter settings in the algorithm are Popsize $=50$, $\lambda=F=0.5$.
e) CMA-ES: CMA-ES is one of the most powerful evolutionary algorithms for real-valued optimization [39], [40] with many successful applications. For an overview see [41]. The main advantage of CMA-ES lies in its invariance properties, which are achieved by carefully designed variation and selection operators, and in its efficient (self-) adaptation of the mutation distribution. The CMA-ES consists of two adaptation phases: the cumulative step-size adaptation (CSA), which is based on the path length control, as well as the actual CMA, which is based on the evolution path. See [42] for an analysis of the two components. The website http://www.lri.fr/ hansen/cmaes_inmatlab.html provides implementations of the CMA-ES and links. The reader can download the source code with various programming languages (ANSI C, C++, Fortran, Java, MATLAB, etc). The implementation of the CMA-ES algorithm used in this paper was based on the pseudocode shown in [40].
f) $E D A$ : EDA [43], [44] is a relatively new branch of EAs. Unlike other EAs, EDAs do not use crossover or mutation. Instead, they explicitly extract global statistical information from the selected solutions and build a probability model of promising solutions based on the extracted information. New solutions are sampled from the model and fully or in part replace solutions in the current population.

Different continuous EDAs have been proposed for the global continuous optimization problem, including univariate marginal distribution algorithm $\left(\mathrm{UMDA}_{\mathrm{c}}\right)$ [45], population based incremental learning (PBIL ${ }_{\mathrm{c}}$ ) [46], mutual information maximization for input clustering (MIMIC ${ }_{\mathrm{c}}$ ) [47], and iterated density evolutionary algorithm (IDEA) [48]. In this paper, we used an improved IDEA called iterated density evolutionary algorithm with adapted maximum-likelihood Gaussian models (AMaLGaM-IDEA) [49] that uses the combination of standard-deviation ratio, adaptive variance scaling, and anticipated mean shift to adaptively change both the covariance and the mean-shift to prevent inefficient sampling. In addition, truncation selection, rejection sampling, and elitist replacement were used in the algorithm. The implementation of the EDA algorithm in this paper was based on the pseudocode shown in [49]. Among these, some parameter settings are Popsize $=50$ and truncation. percentage $=30 \%$.
2) Parameter Set Encoding and Fitness Evaluation: The encoding of the parameter set for each of the six parameter optimization algorithms above is the same. Regarding a specific rule model, we first checked all the constants contained in Tree1, Tree2, and Tree3, including counting the number of constants $l$ and recording their positions. Each individual in the parameter population can then be represented as an $l$-dimensional row vector $\left(c_{1}, c_{2}, \ldots, c_{l}\right)$ where each

component $c_{i}$ for $i=1,2, \ldots, l$ is encoded as a floating number and generated randomly ranging from 0 to a predefined maximum integer ( 500 in this paper) during the initialization of the parameter population.

We chose positive initial values based on the following considerations. First, all the input variables in the test data sets (WT, pH, TP, etc.) allow for positive values only, so if the parameter to optimize appears in Tree1 and indicates a threshold for some specific variable, it is reasonable to initialize it with a positive value. Second, in most cases, the parameter may appear in any position of Tree1, Tree2, or Tree3. This does not matter because we have defined subtraction in the arithmetic function set. If the parameter has a negative value, it is equivalent to subtracting the absolute value of the parameter. Hence, there is no effect caused by defining a positive data range when initializing each parameter as negative values of the parameter can be achieved by evolving a model structure with subtraction operations. Moreover, because most random constants in the rule do not represent any biological meanings, for simplicity, we defined an identical maximum for each parameter. Note that this maximum only applies when initializing the parameter population and when the population is evolved generation by generation, the parameter is no longer restricted by this maximum. Its value will change adaptively by using genetic operators in the parameter optimization (PO) algorithm. In addition, if the parameter indicates a threshold, it will ultimately reach a valid value for the specific water quality parameter. Later on, experiments will verify this.

Before the fitness evaluation of an individual in the parameter population, we first returned to the original rule model and replaced all constants with the corresponding components of the row vector (i.e., the individual) and then followed the same procedure as in Section II-A to calculate the fitness.

## III. EXPERIMENTS

## A. Study Sites and Data

The three drinking water reservoirs-Lake Samsonvale, Lake Somerset, and Lake Wivenhoe-shown in Fig. 3 are the primary raw water sources of drinking water for about 1.5 million people living in the City of Brisbane and the surrounding areas in southeastern Queensland, Australia. In recent years, toxic cyanobacteria have been regularly recorded in each of the reservoirs. In this paper, we used the Chl-a concentration and C. raciborskii abundance data from Lake Wivenhoe to test and compare the performance of different parameter optimization algorithms based on the hybrid evolutionary modeling framework. A number of limnological variables have been collected over an 11 year period (1998-2009), as shown in Table I. As the sampling intervals of the raw data are highly irregular, and sampling dates for physical, chemical, and biological variables do not always coincide, the data need to be interpolated to meet daily time steps for forecasting required by the models. To achieve this, we have evaluated a number of interpolation methods, such as random, Lagrange, Parabola, cubic spline, Aitken, and linear interpolation but we found the difference between these

TABLE I
PARAMETERS USED AS INPUT AND OUTPUT VARIABLES FOR Evolutionary Modeling With the Hea Methodology


interpolation methods did not affect the modeling results. Therefore for simplicity, we used linear interpolation to fill missing values to produce a complete daily time series for this period. In addition, to develop 7-day-ahead predictive models for cyanobacterial abundance using our modeling algorithm, we time-shifted the interpolated daily data by 7 days to use as input data.

## B. Parameter Settings and Measures

For both data sets (Chl-a and C. raciborskii) 100 runs were conducted for each algorithm independently. All the experiments were performed on a Corvus supercomputer (SGI, Altix XE1300) with 69 nodes, each with two Intel quadcore processors provided by eResearch SA. All algorithms were programmed in $\mathrm{C}++$. The GP parameter settings of HEA for structure optimization are: Popsize $=100$, maximum tree depth $=4$, and maximum number of generations $(\operatorname{MAXGENO})=80$.

## C. Results and Discussion

Table II shows the statistical results of six PO algorithms for two test data sets after 100 runs of each algorithm. For Chl-a data, based on the mean total RMSE of 100 runs, we ranked the average performance of the six algorithms from best to worst. The rank order was HC, DE, GA, EDA, SA, and CMA-ES. When applied to C. raciborskii abundance data, SA was better than DE and the rank order was HC, SA, DE, GA, EDA, and CMA-ES.

Surprisingly, both data sets demonstrated that HC was the most time-consuming in terms of computational efforts although intuitively, HC should be faster than other more complex algorithms. The high execution cost of HC implies that it is generating more complex individuals. We rechecked each model obtained by the six PO algorithms over 100 runs and obtained the average numbers of total tree nodes and optimized parameters that are shown in Table II. For both data sets, the rule models obtained by HC were most complicated with on average about 29 nodes and 7 parameters for Chl-a data and 37 nodes and 9 parameters for C. raciborskii data.

TABLE II
Statistical Comparison of the Six Po Algorithms for the Two Test Data Sets After 100 Runs of Each Algorithm


In contrast, the rule models obtained by SA on average had 17 nodes and 3 parameters for Chl-a data, 25 nodes and 5 parameters for C. raciborskii cell data. Undoubtedly, the increased number of parameters and model complexity resulted in the higher execution cost of HC. We have not determined the reason that causes HC to lead to more complex individuals, but this will be the subject of future work.

Table III shows the best rule models obtained by the six PO algorithms for Chl-a data after 100 runs of each algorithm. The IF condition branches of all those rule models clearly indicate threshold values of some water quality parameters (i.e., $\mathrm{SiO}_{2}$, EC, WT, TN, etc.) which stimulate phytoplankton growth and result in higher Chl-a concentrations. This demonstrates that the HEA has the most advantage of automatically discovering the easily understandable IF-THEN-ELSE rules hidden in the measured data, and can be regarded as a powerful intelligent data mining tool for ecological data.

It is interesting to see that each threshold turning up in the six best models has a valid value for the specific water quality parameter. As mentioned previously, all the parameters were initialized with the same range $[0,500]$. However after parameter optimization, they adaptively reached different values within the valid range of the associated water quality parameter. For example, for the best HC model, the threshold values for $\mathrm{SiO}_{2}$ were small values as 3.431 and 7.114 , whereas the threshold for EC was larger value at 356.748 .

For the six best models found by the different PO algorithms, the HC model obviously was the best with the lowest RMSE (3.21), followed by the GA, DE, and EDA models. Although the total RMSEs of those three models were very close, their model structures were quite different. Comparatively, the worst models were the SA and CMA-ES models, both of which had RMSEs greater than 3.9 .

The validation results for the six best Chl-a models on the whole data set are illustrated in Fig. 4. The graph of measured data shows that the two most prominent summer peaks occurred in 2000 and 2002, respectively. Moderate peak events were observed annually in the remaining years except 2003. As shown in Fig. 4, generally all six best models predicted the timing of peak events in every year quite well and their differences lie in the predicted magnitudes of the peaks.

In terms of their predictive power, the HC model appeared best being able to match magnitudes of measured summer peaks very well for most years from 1998 to 2009, although it did slightly overestimate the magnitudes in 2007 and 2008. In contrast, the five other models predicted the magnitudes of peak events reasonably well for the years prior to 2003 but underestimated most peaks from 2003 to 2009.

Table IV shows the best rule models for C. raciborskii data for each of the six PO algorithms over 100 runs. Similar to the Chl-a models, each IF condition branch indicates threshold values of some water quality parameters (i.e., WT, TP, NTU, $\mathrm{SiO}_{2}, \mathrm{DO}, \mathrm{EC}$ ) or their ratios (i.e., $\mathrm{SiO}_{2} / \mathrm{pH}$ ). However, the logic combination of variable conditions becomes more complicated by the significantly increased lengths of their IF branches. The HC and SA models are significantly superior to other models because they both had much lower RMSE. The second group consists of the DE, EDA and GA models whose RMSE values were also very close. While the CMA-ES model performed worst with the largest RMSE ( $>15000$ ).

The modeling results of the six best models validated on the whole C. raciborskii data set are illustrated in Fig. 5. The measured data show the annual summer blooms during the years from 1998 to 2009. Among those, the biggest blooms occurred in 2000, 2001, 2002, 2003, 2006, and 2009. Results from all six models show that the predicted timing of these prominent peaks corresponds very well to the measured data. This confirms the models predictive capability to achieve our aim of giving early warning of cyanobacterial blooms. Moreover, the HC and SA models were best able to forecast the magnitudes of the summer peaks in four successive years from 2000 to 2003, where the predicted values correlated well with the measured data. In contrast, other models underestimated those peaks in most cases. Due to the fact that all six models were unable to accurately predict the magnitudes of the summer peaks in 2006 and 2009, it seems challenging work for the HEA to get good predictions for those two years.

Finally, the results of the experiments were analyzed using a one-way ANOVA test combined with a Tukey's Post Hoc Test for multiple comparisons using a $95 \%$ confidence level as suggested by [51]. The data we used for statistical analysis were the total RMSE values in 100 runs for the six PO

TABLE III
Best Models Obtained for Chl.-a After 100 Runs of Each of the Six Po Algorithms


TABLE IV
Best Models Obtained for C. raciborski After 100 Runs of Each of the Six Po Algorithms


algorithms and two data sets. The results show that the performance order of the six algorithms is slightly different for Chl-a and C. raciborskii.

1) For the Chl-a data, the order is $\mathrm{A} 1(\mathrm{HC}) \rightarrow(\mathrm{A} 3(\mathrm{GA})$, $\mathrm{A} 4(\mathrm{DE})) \rightarrow \mathrm{A} 6(\mathrm{EDA}) \rightarrow(\mathrm{A} 2(\mathrm{SA}), \mathrm{A} 5(\mathrm{CMA}-\mathrm{ES}))$.
2) For the C. raciborskii data, the order is $(\mathrm{A} 1(\mathrm{HC})$, $\mathrm{A} 2(\mathrm{SA})) \rightarrow \mathrm{A} 4(\mathrm{DE}) \rightarrow \mathrm{A} 3(\mathrm{GA}) \rightarrow \mathrm{A} 6(\mathrm{EDA}) \rightarrow \mathrm{A} 5(\mathrm{CMA}-$ ES).
Their main difference is the rank of SA. So it can be concluded that based on the two ecological data sets, HC is statistically the best method, followed by DE, GA, EDA whose performances are comparable, whereas the CMA-ES always performs worst and the performance of SA may vary with different case studies.

## D. Model Selection

In this paper, we presented six parameter optimization algorithms developed under the framework of HEA. We produced a set of candidate ecologically plausible rule models that
comparably match measured data making it difficult to select the best one.

In this section, we applied model selection theory [52] to compare and rank the best models obtained by the six alternative PO algorithms as applied to the two test data sets. We allocated two scores to each model using smallsample corrected Akaike's information criterion (AIC) [53] and minimum description length (MDL) [52] criterion according to the following formulas:

$$
\mathrm{AIC}=D \log (\mathrm{RSS})+2 K\left(\frac{D}{D-(K+1)}\right)
$$

and

$$
\mathrm{MDL}=D \log (\mathrm{RSS})+K \log (D)
$$

where

$$
\mathrm{RSS}=\sum_{i=1}^{D}\left(\hat{y}_{i}-y_{i}\right)^{2}
$$

is the residual sum of square errors, $D$ the number of total data points, and $K$ the number of GP tree nodes for each rule model.

![img-2.jpeg](img-2.jpeg)

Fig. 4. Validation results on the whole data set of $\mathrm{Chl}-a$ for the best models obtained from 100 runs of each of the six PO algorithms. (a) HC, SA, and GA. (b) DE, CMA-ES, and EDA.
$y_{i}$ and $\hat{y}_{i}$ are the measured value and the predicted value of the $i$ th data point, respectively. Both scores are approximations of theoretical measures that are not computable in general. AIC is an approximation of Kullback-Leibler divergence that measures the amount of information lost when building a model. Whereas the MDL criterion aims at capturing the general idea of choosing the model that provides the shortest description of the data. These two scores share the same first part, $D \log (\mathrm{RSS})$, which evaluates how good a model replicates the prefixed behavior. However, they differ in the second part that penalizes complex models with more parameters over simpler ones. This second part aims at preventing the overfitting of the sample data when using complex models. AIC penalizes models with a number of parameters approaching the number of data points more strongly than MDL does.

The scores associated with each model are presented in Table V and they determine the following rank order for the models.
![img-3.jpeg](img-3.jpeg)

Fig. 5. Validation results on the whole data set of $C$. raciborskii for the best models obtained from 100 runs of each of the six PO algorithms. (a) HC, SA, and GA. (b) DE, CMA-ES, and EDA.

1) For the Chl- $a$ data:
$\operatorname{Model}(\mathrm{HC}) \rightarrow \operatorname{Model}(\mathrm{GA}) \rightarrow \operatorname{Model}(\mathrm{DE}) \rightarrow \operatorname{Model}(\mathrm{EDA})$
$\rightarrow \operatorname{Model}(\mathrm{SA}) \rightarrow \operatorname{Model}(\mathrm{CMA}-\mathrm{ES})$.
2) For the C. raciborskii data:
$\operatorname{Model}(\mathrm{HC}) \rightarrow \operatorname{Model}(\mathrm{SA}) \rightarrow \operatorname{Model}(\mathrm{DE}) \rightarrow \operatorname{Model}(\mathrm{EDA})$
$\rightarrow \operatorname{Model}(\mathrm{GA}) \rightarrow \operatorname{Model}(\mathrm{CMA}-\mathrm{ES})$.

## IV. FURTHER EXPERIMENTS

We conducted some further experiments in two aspects. First, all the experiments described above show that when applying different PO algorithms at each generation, it produced a variety of models with different accuracy and structures when the evolution process was completed. So, we wondered what the models would look like if we did not carry out parameter optimization during evolution except during the final generation. That is, only structure optimization using GP was performed during the evolution. When the maximum number of generations MAXGENO was reached, a specific algorithm was applied to optimize the model parameters for each individual in the final population. For each test data set, we conducted 100 independent runs for each PO algorithm and present the statistical results in Table VI. For both data

TABLE V
AIC AND MDL SCORES ASSOCIATED WITH THE BEST MODELS Obtained) USING THE Six PO AlGORITHMS ON THE TWO TEST DATA SETS


TABLE VI
Statistical Comparison of the Six Po Algorithms For the Two Test Data Sets After 100 Runs of Each Algorithm When Parameter Optimization Is Carried Out After Evolution


TABLE VII
Statistical Comparison of 10000 Random Parameter MODELS Based on the Best Chl-a Model Structures Obtained for Each of the Six PO Algorithms

sets, the model accuracy for different PO algorithms measured by the mean total RMSE was very close with similar model complexity in terms of the mean number of total tree nodes and parameters to optimize. By comparing with the results in Table II, we conclude that optimizing the model parameters for rule models during evolution improves the model accuracy significantly, especially when using some better-performing algorithms such as HC and DE. However, it is notable that both the model complexity and computational time increases significantly when the parameter optimization is carried out for every generation. Typically for the HC algorithm, the number of total tree nodes and parameters to optimize increases by $1.5-2$ fold compared with only optimizing parameters at the final generation. Moreover, it takes several hours to finish one run instead of a few minutes without parameter optimization during the evolution.

Second, based on the best Chl-a models shown in Table III, we conducted some experiments to further investigate the
model structures and their linkage to the constant values contained in the model.

As in the first step, for each best model, we froze the model structure and randomly generated a set of floating numbers to fill the corresponding constants in the model within the range of 0 to 500 and then calculated its total RMSE. This procedure was repeated 10000 times and the statistical results are shown in Table VII. Obviously, the six best models were divided into two groups in terms of distinct RMSE values. The HC, CMA-ES, and EDA models (the first group) had lower mean RMSEs ( $<6.0$ ) and standard deviations ( $<1.0$ ). This indicates that those models had relatively stable model structures and the change of the random constants had little effect on the model accuracy. On the contrary, the SA, GA, and DE models (the second group) had larger mean RMSEs ( $>200$ ) and higher standard deviations ( $>60$ ) and their total RMSE changed significantly with the random constant values. For example, for the best DE model, the minimal RMSE was 4.31 while the maximal RMSE could reach 2057.66. Because it is possible for each model in the second group to reach a small minimal RMSE, we considered that these model structures did not perform poorly but their accuracy could be improved significantly by parameter optimization.

By following the above experiments, the next step is that without loss of representability, we chose a model from the second group with moderate RMSE and moderate number of parameters to optimize. In our case, we chose the best GA model that has a mean RMSE of 122.25 and consists of six parameters to be optimized. We rewrote the model structure as follows:

TABLE VIII
Statistical Comparison of the Six Po Algorithms After 100 Runs of Each Algorithm BASED
on the Model Structure of the Best GA Chl- $a$ Model


TABLE IX
Statistical Comparison of Optimized Parameters for the Six Po Algorithms After 100 Runs
of Each Algorithm Based on the Model Structure of the Best GA Chl- $a$ Model


TABLE X
Best Parameter Sets Obtained by the Six Po Algorithms After 100 Runs of Each Algorithm
BASED ON THE MODEL Structure of the Best GA Chl- $a$ Model


$$
\begin{aligned}
& \text { IF }\left(\left(\mathrm{SiO}_{2}\right\rangle=\mathrm{P} 1\right) \mathrm{OR}\left(\left(\mathrm{SiO}_{2}\right\rangle=\mathrm{P} 2\right) \mathrm{AND}(\mathrm{EC}>\mathrm{P} 3) \text { ) } \\
& \text { THEN Chl- } a=\mathrm{pH}-\ln (|\mathrm{EC}-\mathrm{P} 4)|) \\
& \text { ELSE Chl- } a=\ln (|\ln (|(\mathrm{NTU} / \mathrm{P} 5)|)|)+P 6 * \mathrm{TN}+\mathrm{pH} .
\end{aligned}
$$

Using the same random training and test data sets as when the best GA model was obtained, we froze the model structure and applied six PO algorithms to optimize the six parameters ( $\mathrm{P} 1 \sim \mathrm{P} 6$ ) separately. One hundred independent runs were conducted for each algorithm. All the parameter settings for each PO algorithm are the same as previous experiments except $N_{\text {exul }}=10000$.

Table VIII shows the RMSE values after 100 runs of each algorithm when using the six PO algorithms to optimize the six parameters. Based on the three average RMSEs (total, training, and testing) the HC, DE, GA models performed best followed by the SA and EDA models. Again, the CMA-ES was the worst performer. These results compare well with previous results in Section III. As expected, in terms of execution time, HC runs fastest, with the two more complex algorithms, EDA and CMA-ES, running slowest.

Table IX shows the results of the statistical comparison of the six parameters based on 100 runs of each of the six PO
algorithms. We found that for each algorithm the parameters P1 P P4 changed significantly in different runs but not so for P5 and P6. This suggests that a large number of different optimal combinations exist for the six parameters to achieve a small RMSE. Table X shows the best parameter sets obtained by the six PO algorithms. The results prove again that based on the total RMSE, the HC model performed best followed by the SA, GA, and EDA models, and that the CMA-ES model performed worst. In addition, the best parameter sets for the HC, DE, and GA models were very similar.

## V. CONCLUSION AND FUTURE WORK

This paper proposes a novel evolutionary computation method for modeling cyanobacterial blooms. It has the following main features:

1) explanatory single rules with IF-THEN-ELSE structures to model the general algal growth and abundance of the tropical cyanobacterium C. raciborskii;
2) a HEA that performs structural and parameter optimization of rule models simultaneously;

3) six population-based algorithms that are applied to perform parameter optimization within the hybrid methodology including HC, SA, GA, DE, CMA-ES, and EDA.

The effectiveness of the methodology was tested by timeseries modeling of Chl-a concentrations and C. raciborskii abundance in Lake Wivenhoe. Based on the 7-day-ahead forecasting of Chl-a and C. raciborskii, we drew the following conclusions.

1) Whatever PO method was applied in the hybrid EA, our modeling algorithm always found explanatory rule models whose IF conditions indicate threshold values of water quality conditions that trigger or suppress phytoplankton growth, which is reflected in Chl-a concentrations and C. raciborskii cell concentrations. These threshold values provide crucial insights about water conditions that most likely favor growth of cyanobacteria. More interestingly, our algorithm is able to propose a variety of alternative rule models with distinctive IF condition branches. These results can then inform laboratory ecological experiments to test a range of hypothesis implied by the evolved models.
2) Both the experiments and their statistical analysis suggest that when comparing different PO methods, HC always performed best followed by DE, GA, and EDA as a group. CMA-ES always performed worst and the performance of SA was dependent on the data set being used. This result was surprising and we surmised that this is mainly driven by the high complexity and uncertainty of the model parameters contained in the rule model. Unlike some benchmark functions, the number of random constants in a rule is arbitrary and their value ranges can be quite different. Hence, it is hard for some sophisticated methods with complex mathematical computation (i.e., CMA-ES) to capture the hidden relationship among multiple model variables and find the global optima with a small population size. In this paper, we found that the default population size $\lambda$ in CMA-ES was less than 10 in most cases. On the contrary, some classical optimization techniques, such as HC and SA, have the advantage of exploring the irregular search space by using multiple starting points and iterative random searches. They seem to work better for these rule models when optimizing the random constants with arbitrary numbers that are evolved based on real-world ecological data.

To further improve the predictive accuracy of rule models, we will focus our future research on:

1) developing an ensemble of alternative rules and using the averaged result of the alternative rules as output;
2) designing rule set models with multilevel embedded IF-THEN-ELSE structures.

Comparing advantages and disadvantages of model structures based on single rules, an ensemble of multiple single rules, and a rule set with embedded single rules by means of real-world ecological data will further enhance predictive modeling by evolutionary computation.

## APPENDIX A

## Pseudo-code of HC BEGIN

check all constants contained in current rule; evalnum $\leftarrow 0$;
best rule $\leftarrow$ current rule;
best fitness $\leftarrow$ fitness of current rule;
while(evalnum $\left.<N_{\text {eval }}\right)$
\{
$\mathrm{n} \leftarrow 0$;
current rule $\leftarrow$ best rule;
current fitness $\leftarrow$ best fitness;
while( $\mathrm{n}<N_{\text {neighbors }}$ )
\{
produce a temporary rule by performing Gaussian mutations on the constants randomly chosen from Tree1,Tree2 and Tree3 respectively;
calculate the fitness of the temporary rule; evalnum $\leftarrow$ evalnum +1 ;
if (temporary fitness is better than best fitness)
\{
best fitness $\leftarrow$ temporary fitness;
best rule $\leftarrow$ current rule;
\}
$\mathrm{n} \leftarrow \mathrm{n}+1$;
\}
\}
output the best rule and best fitness; END

## APPENDIX B

## Pseudo-code of SA

## BEGIN

check all constants contained in current rule;
evalnum $\leftarrow 0$;
best rule $\leftarrow$ current rule;
best fitness $\leftarrow$ fitness of current rule;
$\mathrm{T} \leftarrow T_{0}$;
frozen $\leftarrow$ false;
totaln $\leftarrow 0$;
while ((!frozen || totaln $<K) \& \&$ evalnum $\left.<N_{\text {eval }}\right)$
\{
cooling $\leftarrow$ false;
$\mathrm{k} \leftarrow 0$;
while $(\mathrm{k}<L)$
\{
produce a temporary rule by performing
Gaussian mutations on the constants randomly chosen from Tree1, Tree2 and Tree3 respectively;
calculate the fitness of the temporary rule;
evalnum $\leftarrow$ evalnum +1 ;
$\triangle \mathrm{E} \leftarrow$ temporary fitness - current fitness;
if $(\triangle \mathrm{E}<0)$
\{
cooling $\leftarrow$ true;
current rule $\leftarrow$ temporary rule;

current fitness $\leftarrow$ temporary fitness; best fitness $\leftarrow$ temporary fitness; best rule $\leftarrow$ current rule;
\}
else if $\left(e^{\frac{i-1 \pi d k}{T}}>\right.$ random $\left.(0,1))$
\{
current rule $\leftarrow$ temporary rule;
current fitness $\leftarrow$ temporary fitness;
\}
$\mathrm{k} \leftarrow \mathrm{k}+1$
totaln $\leftarrow$ totaln +1 ;
\}
if (cooling)
$\mathrm{T} \leftarrow 0.9 * \mathrm{~T}$;
else frozen $\leftarrow$ true;
\}
output the best rule and best fitness;
END

## APPENDIX C

## Pseudo-code of GA

## BEGIN

$\mathrm{t} \leftarrow 0$
evalnum $\leftarrow 0$
randomly initialize a parameter population $P(0)$;
calculate the fitness of the individuals in $P(0)$;
evalnum $\leftarrow$ evalnum +1 (for each calculation);
while (evalnum $<N_{\text {eval }}$ )
\{
sort population $\mathrm{P}(\mathrm{t})$ in terms of fitness;
randomly choose $M$ individuals from $P(\mathrm{t})$ to do
multiparent crossover;
calculate the fitness of the new produced individual;
evalnum $\leftarrow$ evalnum +1 ;
if(new fitness is better than the worst fitness)
replace the worst one with the new individual;
$P(\mathrm{t}) \leftarrow P(\mathrm{t}+1)$;
$\mathrm{t} \leftarrow \mathrm{t}+1$
output the best rule and best fitness;

## END

## Multiparent crossover [17]

Randomly select $M$ different individuals from the population $(M>2)$ denoted as $P_{1}, P_{2}, \ldots, P_{M}$ where $P_{i}=\left(p_{1} p_{2} \ldots p_{k}\right)$ (i:1 $\sim M$ ) and $k$ is the number of parameters to optimize. Accordingly produce $M$ random coefficients $\alpha_{i}$, which satisfies $\mathrm{a} \leq \alpha_{i} \leq \mathrm{b}(\mathrm{a}<0, \mathrm{~b}>1)$ and $\sum_{i=1}^{M} \alpha_{i}=1$, then generate a new individual, $P_{\text {new }}$, by the non-convex linear combination of those $M$ individuals as the following:

$$
P_{\text {new }}=\sum_{i=1}^{M} \alpha_{i} P_{i}
$$

## APPENDIX D

## Pseudo-code of DE BEGIN

$\mathrm{t} \leftarrow 0$
evalnum $\leftarrow 0$
randomly initialize a parameter population $P(0)$;
calculate the fitness of the individuals in $P(0)$;
evalnum $\leftarrow$ evalnum +1 (for each calculation);
while(evalnum $<N_{\text {eval }}$ )
\{
sort population $P(\mathrm{t})$ and get the best individual bestp(t);
for $(\mathrm{i}=0 ; \mathrm{i}<$ Popsize $; \mathrm{i}++$ )
\{
produce a new offspring $p_{i} *(t)$ for individual $p_{i}(\mathrm{t})$ by randomly choosing one of the five alternative DE schemes;
calculate the fitness of $p_{i} *(t)$;
evalnum $\leftarrow$ evalnum +1 ;
if (fitness $\left(p_{i} *(t)\right)$ is better than fitness $\left(\mathrm{p}_{\mathrm{i}}(\mathrm{t})\right)$
$p_{i}(\mathrm{t}+1) \leftarrow p_{i} *(t)$;
else $p_{i}(\mathrm{t}+1) \leftarrow p_{i}(\mathrm{t})$;
\}
$\mathrm{t} \leftarrow \mathrm{t}+1$
output the best rule and best fitness;

## END

## Five Alternative DE Schemes ([38])

1. DE-rand1:
$p_{i} *(t)=p_{r 1}(t)+F *\left(p_{r 2}(t)-p_{r 3}(t)\right)$
2. DE-rand2:
$p_{i} *(t)=p_{r 1}(t)+F *\left(p_{r 2}(t)+p_{r 3}(t)-p_{r 4}(t)-p_{r 5}(t)\right)$
3. DE-best1:
$p_{i} *(t)=\operatorname{bestp}(t)+F *\left(p_{r 1}(t)-p_{r 2}(t)\right)$
4. DE-best2:
$p_{i} *(t)=\operatorname{bestp}(t)+F *\left(p_{r 1}(t)+p_{r 2}(t)-p_{r 3}(t)-p_{r 4}(t)\right)$
5. DE-randtobest1:
$p_{i} *(t)=p_{i}(t)+\lambda *\left(\operatorname{bestp}(t)-p_{i}(t)\right)+F *\left(p_{r 1}(t)-p_{r 2}(t)\right)$
Notes:
r1, r2, r3, r4, r5 $\in[0$,Popsize-1]: randomly chosen integer and mutually different.
$\lambda, F \in[0,2]$ (usually set $\lambda=F$ ).
