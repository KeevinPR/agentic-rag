# Parameter Evolution for a Particle Swarm Optimization Algorithm 

Aimin Zhou ${ }^{1}$, Guixu Zhang ${ }^{1}$, and Andreas Konstantinidis ${ }^{2}$<br>${ }^{1}$ East China Normal University, Shanghai, China<br>\{amzhou, gxzhang\}@cs.ecnu.edu.cn<br>${ }^{2}$ Frederick University of Cyprus, Cyprus<br>com.ca@fit.ac.cy


#### Abstract

Setting appropriate parameters of an evolutionary algorithm (EA) is challenging in real world applications. On one hand, the characteristics of a real world problem are usually unknown. On the other hand, in different running stages of an EA, the best parameters may be different. Thus adaptively tuning algorithm parameters online is preferred. In this paper, we propose to use an estimation of distribution algorithm (EDA) to do this for a particle swarm optimization (PSO) algorithm. The major characteristic of our approach is that there are two evolving processes simultaneously: one for tackling the original problem, and the other for optimizing PSO parameters. For the former evolving process, a set of particles are maintained; while for the later, a probability distribution model of the PSO parameters is maintained throughout the run. In the reproduction procedure, the PSO parameters are firstly sampled from the model, and then new particles are generated by the PSO operator. The feedback from the newly generated particles is used to evaluate the PSO parameters and thus to update the probability model. The new approach is applied to a set of test instances and the preliminary results are promising.


## 1 Introduction

The parameter tuning plays a key role in applying evolutionary algorithms (EAs) to real world applications [1]. The success of an EA depends not only on the algorithm itself but also on the problem to be solved. In algorithm design, we could tune the parameters either by repeated running or by analyzing the properties of benchmark probelems. However in applications, the characteristics of the problems maybe incomplete or the problems may not have closed forms, and the repeated experiments may be expensive. Thus the strategies for tuning parameters in algorithm design are no longer suitable in such cases. Furthermore, to achieve the best performance, the parameters of an algorithm may not be fixed throughout the run. For example, at the beginning stages, to get better diversity, the mutation probability might be high; while at the later stages, to achieve better convergence, the mutation probability needs to be low. To overcome the shortcomings of offline parameter tuning strategies, many research turn to set algorithm parameters adaptively online [2].

Most of widely adaptively parameter tuning methods could be classified into the following categories.

- Randomly selecting parameters: With this strategy, the users give a set of candidate parameter settings by guessing or using prior knowledge. The algorithm then randomly select a parameter setting from the given set 3. The initial parameter set is importance to the algorithm success.
- Adaptively tuning by feedback: By this strategy, the parameters are adaptively adjusted by heuristic rules with take feedbacks from previous parameter changes [4. How to define the feedback is a key issue.
- Encoding parameters into chromosomes: The parameters are incorporated into the chromosomes and evolve with decision variables [5]. There is not much additional work to implement this strategy. However, encoding the parameters increases the complexity of the EA search space and may slow down the search.
- Parameter evolving: Cooperating with the main algorithm, another EA works on the parameters and its optimal solutions, i.e. the best parameters, are used in the main algorithm [6].

In this paper, we follow the idea of parameter evolving strategy. An estimation of distribution algorithm (EDA) 77] 8 is applied to tune the parameters of a particle swarm optimization (PSO) 9] 10. In our approach, an EDA and a PSO evolve simultaneously. The EDA works on the PSO parameter space, while the PSO works on the original problem search space. In the running process, the EDA maintains a multivariate histogram probabilistic model of PSO parameters; and the PSO maintain a set of candidate solutions (particles). In each generation, the PSO parameters are firstly sampled from the EDA model; secondly, new particles are generated by the PSO operator; thirdly, the probability model is updated according to the performances of the sampled parameters.

The rest of the paper is organized as follows. The next section introduces the background of our work, including the problems, the PSO model which are used in the paper, and a brief introduction of EDA. Section 3 presents the details of the proposed method. Section 4 describes and analyzes the experimental results. The final section concludes the paper and outlines future research work.

# 2 Background 

### 2.1 Optimization Problem

In this paper, we consider the following global optimization problems.

$$
\begin{gathered}
\min f(x) \\
\text { s.t } x \in \Omega
\end{gathered}
$$

where $\Omega \subset R^{n}$ is the decision space and $x=\left(x_{1}, \cdots, x_{n}\right)^{T}$ is the decision variable vector. $f: \Omega \rightarrow R$ is a continuous objective function and $R$ is the objective space.

# 2.2 Particle Swarm Optimization 

Among various techniques for global optimization, particle swarm optimization (PSO) is a promising one. PSO is a population based stochastic optimization technique developed by Eberhart and Kennedy in 1995 [9] [10], inspired by social behavior of bird flocking or fish schooling. Mathematically, the $i$ th particle at generation $t, x_{i}(t)$, is updated as,

$$
\left\{\begin{aligned}
v_{i}(t+1)= & w v_{i}(t) \\
& +c_{1} r_{1, i}\left(x^{G}(t)-x_{i}(t)\right) \\
& +c_{2} r_{2, i}\left(x_{i}^{L}(t)-x_{i}(t)\right) \\
x_{i}(t+1)= & x_{i}(t)+v_{i}(t+1)
\end{aligned}\right.
$$

where $v$ denotes the velocity, $x^{G}(t)$ is the global best particle, $x_{i}^{L}(t)$ is the local best particle found so far, $w$ is the inertia weight, $c_{1}, c_{2}$ are the acceleration constants, $r_{1, i}, r_{2, i}$ are two dialog matrix of which the dialog elements are uniformly randomly sampled from $[0,1]$.

Let $\alpha_{i}(t)=\left(w_{i}(t), c_{1, i}(t), c_{2, i}(t)\right)^{T}$ be the parameter vector for generating the $i$ ish particle at generation $t$, the above generation procedure could be denoted as

$$
\left(v_{i}(t+1), x_{i}(t+1)\right):=\operatorname{generate}\left(v_{i}(t), x_{i}(t), x_{i}^{L}(t), x^{G}(t), \alpha_{i}(t)\right)
$$

After the generation process, the global best particle the the local best particle are then updated.

### 2.3 Estimation of Distribution Algorithm

Estimation of distribution algorithms (EDA) are a new evolutionary computation paradigm [7] [8]. A major difference between EDAs and traditional EAs is in the offspring reproduction procedure. There is no crossover or mutation in EDAs. Instead, they build a probability model of promising solutions by extracting the global population distribution information and sample new solutions from the model thus built.

## 3 Evolving PSO Parameters by EDA

### 3.1 Algorithm Framework

To adaptively set the PSO parameters, we use an EDA to evolve these parameters with the main PSO procedure. In each generation $t$, our approach, named parameter evolution guided particle swarm optimization (PEPSO), maintains

- a set of particles: $\left\{x_{i}(t), i=1, \cdots, N\right\}$,
- a set of velocity vectors: $\left\{v_{i}(t), i=1, \cdots, N\right\}$,
- a set of local best particles: $\left\{x_{i}^{L}(t), i=1, \cdots, N\right\}$,
- a global best particle: $x^{G}(t)$, and
- a probability distribution model of parameters: $P(\alpha(t))$.
where $N$ is the population size, and $\alpha(t)=\left(w(t), c_{1}(t), c_{2}(t)\right)^{T}$ denotes the PSO parameter vector.

The main framework of PEPSO is as follows.
Step 0 Initialization: Set $t:=0$. Uniformly randomly generate a set of particle $\left\{x_{i}(t), i=1, \cdots, N\right\}$, a set of velocity vectors $\left\{v_{i}(t), i=1, \cdots, N\right\}$ in the search space $\Omega$. Evaluate these particles by (1) and find the global best particle $x^{G}(t)$. Let $\left\{x_{i}^{L}(t)=x_{i}(t), i=1, \cdots, N\right\}$. Initialize the parameter distribution model $P(\alpha(t))$.
Step 1 Stopping Condition: If stopping condition is met, stop and return $x^{G}(t)$.
Step 2 Reproduction: For each particle $i=1, \cdots, N$, sample a parameter vector $\alpha_{i}(t)$ from $P(\alpha(t))$, generate a new particle $x_{i}(t+1)$ by (3), and evaluate this particle.
Step 3 Particle Updating: Update the global best particle $x^{G}(t+1)$ and local best particle $x_{i}^{L}(t+1), i=1, \cdots, N$.
Step 4 Model Updating: Update the probability model $P(\alpha(t+1))$ by the improvements of the particles.
Step 5 Set $t:=t+1$ and go to Step 1.
In the following, we discuss the probability model definition and implementation.

# 3.2 Probability Distribution Model of Parameters 

Since the algorithm parameters may correlate with each other, we use a multivariate histogram probabilistic model to model the distribution of continuous parameters. Let the boundaries of the parameter vector $\alpha=\left(\alpha_{1}, \cdots, \alpha_{m}\right)^{T}$ be $\left[\alpha_{1}^{L}, \alpha_{1}^{U}\right] \times \cdots \times\left[\alpha_{m}^{L}, \alpha_{m}^{U}\right]$, and each dimension be divided into $D$ subsets, the parameter search space is thus divided into $D^{m}$ bins.

The multivariate histogram probabilistic model is defined as

$$
P(\alpha(t))=\left(p_{1}(t), \cdots, p_{D^{m}}(t)\right)^{T}
$$

where $0 \leq p_{i}(t) \leq 1$ denotes the probability of a parameter vector from the $i$ th bin, and $\sum_{i=1}^{D^{m}} p_{i}(t)=1$.

Fig 1 shows a multivariate histogram probability model in the case of 2dimensional parameter vector. The search space is divided into $5^{2}=25$ bins, and the hight of each bar denotes the probability of a parameter vector is from that bin.

### 3.3 Model Sampling and Updating

Let $q_{i}(t)>0$ denotes a frequency of the $i$ th bin to be used in generation $t$, then the probability is approximated as $p_{i}(t)=\frac{q_{i}(t)}{\sum_{j} q_{j}(t)}$. Instead of maintaining a probability vector, we maintain a frequency vector $q_{i}(t), i=1, \cdots, D^{m}$ in the running process.

In the initialization step, the frequency vector is set to $q_{i}(0)=5.0, i=$ $1, \cdots, D^{m}$.

![img-0.jpeg](img-0.jpeg)

Fig. 1. Illustration of multivariate histogram probabilistic model in the case of 2dimensional parameter vector

In generating the $i$ th particle, a parameter vector $\alpha_{i}(t)$ is sampled as follows. Firstly, we randomly select a bin index $I_{i}(t)$ according to the probability distribution model $P(\alpha(t))$. We then uniformly randomly sample a vector $\alpha_{i}(t)$ from the $I_{i}(t)$ th bin.

Define the improvement of the $i$ th particle as

$$
\Delta_{i}(t)= \begin{cases}f\left(x_{i}(t)\right)-f\left(x_{i}(t+1)\right) & \text { if } f\left(x_{i}(t)\right)>f\left(x_{i}(t+1)\right) \\ 0 & \text { otherwise }\end{cases}
$$

Let

$$
a_{i, j}(t)= \begin{cases}1 & \text { if } I_{i}(t)=j \\ 0 & \text { otherwise }\end{cases}
$$

The contribution of parameters from the $j$ th bin is defined as

$$
C_{j}(t)=\sum_{i=1}^{N} a_{i, j}(t) \frac{\Delta_{i}(t)}{\max _{k} \Delta_{k}(t)}
$$

We then update the frequency vector as follows,

$$
q_{j}(t)= \begin{cases}1.0 & \text { if }(1-\beta) q_{j}(t)+\frac{C_{j}(t)}{\max _{j} C_{j}(t)}<1 \\ 10.0 & \text { if }(1-\beta) q_{j}(t)+\frac{C_{j}(t)}{\max _{j} C_{j}(t)}>10.0 \\ (1-\beta) q_{j}(t)+\frac{C_{j}(t)}{\max _{j} C_{j}(t)} & \text { otherwise }\end{cases}
$$

where $\beta$ is an algorithm parameter and it is set to be 0.75 in the experiments. The frequency is fixed in the range of $[1.0,10.0]$.

# 4 Experimental Results and Analysis 

### 4.1 Test Instances

We use 13 test instances [11] in the experiments. They are Sphere function, Schwefel 2.22 function, Schwefel 1.2 function, Schwefel 2.21 function, Rosenbrock fuction, Step function, Noisy Quartic function, Schwefel 2.26 function, Rastrigin function, Ackley function, Griewank function, and two Penalized functions. Each of these functions has a global minimum value of 0 . The details of these functions are listed in Tab 1.

Table 1. Test instances used in our experiments
### 4.2 Experimental Parameter Setting

In the experiments, we compare the proposed PEPSO with a general PSO. The parameters for PSO are $w=0.5$, and $c_{1}=c_{2}=2.05$ as used in most PSO algorithms. The parameter for PEPSO are as follows: the search range of $w$ is

Table 2. The statistical results (mean $\pm$ std.) of PSO and PEPSO on the 13 test instances over 50 runs after 1000, 3000 and 5000 generations

![img-1.jpeg](img-1.jpeg)

Fig. 2. The mean fitness values versus generations over 50 runs on $f_{1}-f_{8}$

![img-2.jpeg](img-2.jpeg)

Fig. 3. The mean fitness values versus generations over 50 runs on $f_{9}-f_{13}$

$[0.25,0.75]$ and the search range of $c_{1}$ and $c_{2}$ is $[1.5,2.5]$; each range is divided into 20 subranges; to reduce the computational cost, we set $c_{1}=c_{2}$ in the experiments and thus the parameter space is divided into $20^{2}=400$ grids.

The population size for both PSO and PEPSO is 200 and the algorithms will stop after 5000 generations. The decision vector dimensions of all the test problems are set to be $n=30$. Each algorithm is executed independently for each instance for 50 times.

# 4.3 Results and Analysis 

Table 2 shows the means and standard devisions of PSO and PEPSO on the 13 problems over 50 runs after 1000, 2000, and 3000 generations. Figs 2 and 3 illustrate the mean fitness values versus generations over 50 runs on all the test problems.

From the results, we can see that

- For $f_{1}-f_{5}, f_{7}$, and $f_{9}$, PEPSO outperforms PSO not only on the converge speed but also on the quality of the obtained solutions.
- For $f_{6}, f_{10}$, and $f_{13}$, both PSO and PEPSO obtain similar results. However, PEPSO converges faster than PSO.
- For $f_{8}, f_{11}$, and $f_{12}$, PSO shows better performance than PEPSO.

The only difference between PSO and PEPSO is that PEPSO adaptively tunes its parameters in (3). The results indicate that for most of the test instances, adaptively online tuning strategy is better than setting the parameters offline. However, for some problems, PEPSO works worse than PSO. The reason might be that the probability model in PEPSO converges to local optimal solutions, i.e., the probabilities of some bad parameter vectors are much higher than those of good parameter vectors.

## 5 Conclusion and Future Work

In this paper, we proposed a PSO algorithm with an adaptively parameter tuning strategy by an EDA. The proposed algorithm, PEPSO, was compared with a general PSO algorithm on 13 widely used test instances. The preliminary results indicated that for most of the test problems, PEPSO performed better than PSO, either in convergence speed or in both convergence speed and solution quality. Although the adaptive strategy (EDA) still needs some parameters, it leads to the improvements of solution quality and convergence speed.

The research on adaptively tuning EA parameters is still in its very infancy and our work presented in this paper is also rather preliminary. Much work remains to be done in the future, for example, designing more efficient probability model update strategies and comparing PEPSO with other PSO algorithms [12] [13] [14.

# Acknowledgement 

This work is supported by National Science Foundation of China (No. 60773119) and Program for New Century Excellent Talents in University (No. NCET-08-0193).
