{
  "metadata": {
    "file_path": "/Users/id05309/Documents/tfm/mistral/mistral-markdown/2000/Feature Subset Selection by Bayesian network-based optimization.md",
    "filename": "Feature Subset Selection by Bayesian network-based optimization.md",
    "title": "Feature Subset Selection by Bayesian network-based optimization",
    "year": "2000"
  },
  "references": {
    "header": "# References",
    "content": "[1] D.W. Aha, D. Kibler, M.K. Albert, Instance-based learning algorithms, Machine Learning 6 (1991) 37-66.\n[2] D.W. Aha, R.L. Bankert, Feature selection for case-based classification of cloud types: An empirical comparison, in: Proc. AAAI-94, Seattle, WA, 1994, pp. 106-112.\n[3] D.W. Aha, Personal communication, 1999.\n[4] H. Almuallin, T.G. Dietterich, Learning with many irrelevant features, in: Proc. AAAI-91, Anaheim, CA, 1991, pp. 547-552.\n[5] E. Alpaydin, Combined 5x2cv $F$ test for comparing supervised classification learning algorithms, Neural Comput. 11 (1999) 1885-1892.\n[6] J. Bala, K. DeJong, J. Huang, H. Wechsler, H. Vafae, Hybrid learning using genetic algorithms and decision trees for pattern classification, in: Proc. IJCAI-95, Montreal, Quebec, 1995, pp. 719-724.\n[7] S. Baluja, Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning, Technical Report CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, PA, 1994.\n[8] S. Baluja, S. Davies, Using optimal dependency-trees for combinatorial optimization: Learning the structure of the search space, in: Proc. 14th International Conference on Machine Learning, Nashville, TN, 1997, pp. 30-38.\n[9] M. Ben-Bassat, Pattern recognition and reduction of dimensionality, in: P.R. Krishnaiah, L.N. Kanal (Eds.), Handbook of Statistics-II, North-Holland, Amsterdam, 1982, pp. 773-791.\n[10] A.L. Blum, P. Langley, Selection of relevant features and examples in machine learning, Artificial Intelligence 97 (1997) 245-271.\n[11] M. Boddy, T. Dean, Deliberation scheduling for problem solving in time-constrained environments, Artificial Intelligence 67 (2) (1997) 245-285.\n[12] R.R. Bouckaert, Properties of Bayesian belief network learning algorithms, in: Proc. 10th Annual Conference on Uncertainty in Artificial Intelligence, Seattle, WA, 1994, pp. 102-109.\n[13] D. Boyce, A. Farhi, R. Weischedel, Optimal Subset Selection, Springer, Berlin, 1974.\n[14] W. Buntine, Theory refinement in Bayesian networks, in: Proc. 7th Conference on Uncertainty in Artificial Intelligence, Los Angeles, CA, 1991, pp. 52-60.\n[15] L. Breiman, J.H. Friedmann, R.A. Olshen, C.J. Stone, Classification and Regression Trees, Wadsworth, Belmont, CA, 1984.\n[16] C. Cardie, Using decision trees to improve case-based learning, in: Proc. 10th International Conference on Machine Learning, Amherst, MA, 1993, pp. 25-32.\n[17] R. Caruana, D. Freitag, Greedy attribute selection, in: Proc. 11th International Conference on Machine Learning, New Brunswick, NJ, Morgan Kaufmann, Los Altos, CA, 1994, pp. 28-36.\n[18] E. Castillo, J.M. Gutiérrez, A.S. Hadi, Expert Systems and Probabilistic Network Models, Springer, Berlin, 1997.\n[19] B. Cestnik, Estimating probabilities: A crucial task in machine learning, in: Proc. European Conference on Artificial Intelligence (ECAI-90), Stockholm, Sweden, 1990, pp. 147-149.\n[20] M. Chen, J. Han, P. Yu, Data mining: An overview from database perspective, IEEE Transactions on Knowledge and Data Engineering 8 (6) (1996) 866-883.\n[21] D.M. Chickering, D. Geiger, D. Heckerman, Learning Bayesian networks is NP-hard, Technical Report MSR-TR-94-17, Microsoft Research, Advanced Technology Division, Microsoft Corporation, Redmond, WA, 1994.\n[22] D.M. Chickering, D. Geiger, D. Heckerman, Learning Bayesian networks: Search methods and experimental results, in: Preliminary Papers of the 5th International Workshop on Artificial Intelligence and Statistics, Ft. Lauderdale, FL, 1995, pp. 112-128.\n[23] G.F. Cooper, E.A. Herskovits, A Bayesian method for the induction of probabilistic networks from data, Machine Learning 9 (1992) 309-347.\n[24] A.P. Dawid, Conditional independence in statistical theory, J. Roy. Statist. Soc. Ser. B 41 (1979) 1-31.\n[25] J.S. De Bonet, C.L. Isbell, P. Viola, MIMIC: Finding optima by estimating probability densities, in: Th. Petsche, M. Mozer, M. Jordan (Eds.), Advances in Neural Information Processing Systems 9, MIT Press, Cambridge, MA, 1997.\n\n[26] T.G. Dietterich, Approximate statistical tests for comparing supervised learning algorithms, Neural Comput. 10 (7) (1998) 1895-1924.\n[27] J. Doak, An evaluation of feature selection methods and their application to computer security, Technical Report CSE-92-18, University of California at Davis, CA, 1992.\n[28] R. Etxeberria, P. Larrañaga, J.M. Picaza, Analysis of the behaviour of genetic algorithms when learning Bayesian network structure from data, Pattern Recognition Lett. 18 (11-13) (1997) 1269-1273.\n[29] R. Etxeberria, P. Larrañaga, Global optimization with Bayesian networks, in: Proc. II Symposium on Artificial Intelligence (CIMAF99), La Habana, Cuba, 1999, pp. 332-339.\n[30] F.J. Ferri, V. Kadirkamanathan, J. Kittler, Feature subset search using genetic algorithms, in: Proc. IEE/IEEE Workshop on Natural Algorithms in Signal Processing, Essex, 1993, pp. 23/1-23/7.\n[31] F.J. Ferri, P. Pudil, M. Hatef, J. Kittler, Comparative study of techniques for large scale feature selection, in: E.S. Gelsema, L.N. Kanal (Eds.), Multiple Paradigms, Comparative Studies and Hybrid Systems, North Holland, Amsterdam, 1994, pp. 403-413.\n[32] N. Friedman, Z. Yakhini, On the sample complexity of learning Bayesian networks, in: Proc. 12th Conference on Uncertainty in Artificial Intelligence, Portland, OR, 1996, pp. 274-282.\n[33] J.J. Grefenstette, Optimization of control parameters for genetic algorithms, IEEE Trans. Systems Man Cybernet. SMC-16 (1) (1986) 122-128.\n[34] G.R. Harik, F.G. Lobo, D.E. Goldberg, The compact genetic algorithm, IlliGAL Report 97006, University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL, 1997.\n[35] D. Heckerman, D. Geiger, D. Chickering, Learning Bayesian networks: The combinations of knowledge and statistical data, Machine Learning 20 (1995) 197-243.\n[36] M. Henrion, Propagating uncertainty in Bayesian networks by probabilistic logic sampling, in: J.F. Lemmer, L.N. Kanal (Eds.), Uncertainty in Artificial Intelligence 2, Elsevier Science, Amsterdam, 1988, pp. 149-163.\n[37] J.H. Holland, Adaptation in Natural and Artificial Systems, University of Michigan Press, Ann Arbor, MI, 1975.\n[38] I. Inza, Feature weighting for nearest neighbor algorithm by Bayesian networks based combinatorial optimization, in: Proc. Student Session of Advanced Course on Artificial Intelligence (ACAI-99), Chania, Greece, 1999, pp. 33-35.\n[39] A.K. Jain, R. Chandrasekaran, Dimensionality and sample size considerations in pattern recognition practice, in: P.R. Krishnaiah, L.N. Kanal (Eds.), Handbook of Statistics-II, North-Holland, Amsterdam, 1982, pp. 835-855.\n[40] A. Jain, D. Zongker, Feature selection: Evaluation, application, and small sample performance, IEEE Transactions on Pattern Analysis and Machine Intelligence 19 (2) (1997) 153-158.\n[41] G. John, R. Kohavi, K. Pfleger, Irrelevant features and the subset selection problem, in: Proc. 11th International Conference on Machine Learning, New Brunswick, NJ, 1994, pp. 121-129.\n[42] K. Kira, L.A. Rendell, The feature selection problem: Traditional methods and a new algorithm, in: Proc. AAAI-92, San Jose, CA, 1992, pp. 129-134.\n[43] J. Kittler, Feature set search algorithms, in: C.H. Chen (Ed.), Pattern Recognition and Signal Processing, Sijthoff and Noordhoff, Alphen aan den Rijn, The Netherlands, 1978, pp. 41-60.\n[44] R. Kohavi, Feature Subset Selection as search with probabilistic estimates, in: Proc. AAAI Fall Symposium on Relevance, New Orleans, LA, 1994, pp. 122-126.\n[45] R. Kohavi, A study of cross-validation and bootstrap for accuracy estimation and model selection, in: Proc. IJCAI-95, Montreal, Quebec, 1995, pp. 1137-1143.\n[46] R. Kohavi, Feature Subset Selection using the wrapper method: Overfitting and dynamic search space topology, in: Proc. First International Conference on Knowledge Discovery and Data Mining (KDD-95), Montreal, Quebec, 1995, pp. 192-197.\n[47] R. Kohavi, G. John, Wrappers for feature subset selection, Artificial Intelligence 97 (1-2) (1997) 273-324.\n[48] R. Kohavi, D. Sommerfield, J. Dougherty, Data mining using MLC++, a Machine Learning Library in C++, Internat. J. Artificial Intelligence Tools 6 (4) (1997) 537-566.\n[49] R. Kohavi, Personal communication, 1999.\n[50] D. Koller, M. Sahami, Toward optimal feature selection, in: Proc. 13th International Conference on Machine Learning, Bari, Italy, 1996, pp. 284-292.\n[51] L. Kuncheva, Genetic algorithms for feature selection for parallel classifiers, Inform. Process. Lett. 46 (1993) $163-168$.\n\n[52] P. Langley, S. Sage, Induction of selective Bayesian classifiers, in: Proc. 10th Conference on Uncertainty in Artificial Intelligence, Seattle, WA, 1994, pp. 399-406.\n[53] P. Larrañaga, C.M.H. Kuijpers, R.H. Murga, Y. Yurramendi, Learning Bayesian network structures by searching for the best ordering with genetic algorithms, IEEE Trans. Systems Man Cybernet.—Part A: Systems and Humans 26 (4) (1996) 487-493.\n[54] P. Larrañaga, M. Poza, Y. Yurramendi, R.H. Murga, C.M.H. Kuijpers, Structure learning of Bayesian networks by genetic algorithms: A performance analysis of control parameters, IEEE Transactions on Pattern Analysis and Machine Intelligence 18 (9) (1996) 912-926.\n[55] P. Larrañaga, R. Etxeberria, J.A. Lozano, B. Sierra, I. Inza, J.M. Peña, A review of the cooperation between evolutionary computation and probabilistic graphical models, in: Proc. II Symposium on Artificial Intelligence (CIMAF99), La Habana, Cuba, 1999, pp. 314-324.\n[56] S.L. Lauritzen, Graphical Models, Oxford University Press, Oxford, 1996.\n[57] H. Liu, R. Setiono, Feature selection and classification-A probabilistic wrapper approach, in: Proc. 9th International Conference on Machine Learning, Bari, Italy, 1996, pp. 284-292.\n[58] H. Liu, H. Motoda, Feature Selection for Knowledge Discovery and Data Mining, Kluwer Academic, Norwell, MA, 1998.\n[59] H. Liu, R. Setiono, Incremental feature selection, Applied Intelligence 9 (3) (1998) 217-230.\n[60] D. Madigan, A.E. Raftery, C.T. Volinsky, J.A. Hoeting, Bayesian model averaging, in: Proc. AAAI Workshop on Integrating Multiple Learned Models, Portland, OR, 1996, pp. 77-83.\n[61] W. Mendenhall, T. Sincich, Statistics for Engineering and the Sciences, Prentice Hall, Englewood Cliffs, NJ, 1998.\n[62] A.J. Miller, Subset Selection in Regression, Chapman and Hall, Washington, DC, 1990.\n[63] D. Mladenić, Feature subset selection in text-learning, in: Proc. 10th European Conference on Machine Learning, Chemnitz, Germany, 1998, pp. 95-100.\n[64] A.W. Moore, M.S. Lee, Efficient algorithms for minimizing cross validation error, in: Proc. 11th International Conference on Machine Learning, New Brunswick, NJ, 1994, pp. 190-198.\n[65] H. Mühlenbein, G. Paaß, From recombination of genes to the estimation of distributions. Binary parameters, in: H.M. Voigt et al. (Eds.), Parallel Problem Solving from Nature-PPSN IV, Lecture Notes in Computer Science, Vol. 1411, Springer, Berlin, 1996, pp. 178-187.\n[66] H. Mühlenbein, The equation for response to selection and its use for prediction, Evolutionary Comput. 5 (3) (1997) $303-346$.\n[67] H. Mühlenbein, T. Mahnig, A. Ochoa, Schemata, distributions and graphical models in evolutionary optimization, J. Heuristics 5 (1999) 215-247.\n[68] P. Murphy, UCI Repository of Machine Learning Databases, University of California, Department of Information and Computer Science, Irvine, CA, 1995.\n[69] P. Narendra, K. Fukunaga, A branch and bound algorithm for feature subset selection, IEEE Trans. Comput. C-26 (9) (1977) 917-922.\n[70] A.Y. Ng, Preventing 'overfitting' of cross-validation data, in: Proc. 14th Conference on Machine Learning, Nashville, TN, 1997, pp. 245-253.\n[71] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, Palo Alto, CA, 1988.\n[72] M. Pelikan, H. Mühlenbein, The bivariate marginal distribution algorithm, 1999, submitted for publication.\n[73] M. Pelikan, D.E. Goldberg, E. Cantú-Paz, BOA: The Bayesian optimization algorithm, IlliGAL Report 99003, University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL, 1999.\n[74] M. Pelikan, D.E. Goldberg, F. Lobo, A survey of optimization by building and using probabilistic model, IlliGAL Report 99018, University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL, 1999.\n[75] F. Provost, V. Kolluri, A survey of methods for scaling up inductive algorithms, Data Mining and Knowledge Discovery 2 (1999) 131-169.\n[76] P. Pudil, J. Novovicova, J. Kittler, Floating search methods in feature selection, Pattern Recognition Lett. 15 (1) (1994) 1119-1125.\n[77] J.R. Quinlan, Induction of decision trees, Machine Learning 1 (1986) 81-106.\n\n[78] J.R. Quinlan, Inferring decision trees using the minimum description length principle, Inform. and Comput. 80 (1989) 227-248.\n[79] J.R. Quinlan, Programs for Machine Learning, Morgan Kaufmann, San Mateo, CA, 1993.\n[80] G. Schwarz, Estimating the dimension of a model, Ann. Statist. 7 (1978) 461-464.\n[81] W. Siedelecky, J. Skalansky, On automatic feature selection, Internat. J. Pattern Recognition and Artificial Intelligence 2 (1988) 197-220.\n[82] D.B. Skalak, Prototype and feature selection by sampling and random mutation hill-climbing algorithms, in: Proc. 11th International Conference on Machine Learning, New Brunswick, NJ, 1994, pp. 293-301.\n[83] S.D. Stearns, On selecting features for pattern classifiers, in: Proc. 3rd International Conference on Pattern Recognition, Coronado, CA, 1976, pp. 71-75.\n[84] G. Syswerda, Uniform crossover in genetic algorithms, in: Proc. International Conference on Genetic Algorithms 3, Arlington, VA, 1989, pp. 2-9.\n[85] C. Taylor, D. Michie, D. Spiegelhalter, Machine Learning, Neural and Statistical Classification, Paramount Publishing International, 1994.\n[86] H. Vafaie, K. De Jong, Robust feature selection algorithms, in: Proc. 5th International Conference on Tools with Artificial Intelligence, Rockville, MD, 1993, pp. 356-363.\n[87] M.L. Wong, W. Lam, K.S. Leung, Using evolutionary programming and minimum description length principle for data mining of Bayesian networks, IEEE Transactions on Pattern Analysis and Machine Intelligence 21 (2) (1999) 174-178.\n[88] J. Yang, V. Honavar, Feature subset selection using a genetic algorithm, IEEE Intelligent Systems 13 (2) (1998) $44-49$.\n[89] Y. Yang, J.O. Pedersen, A comparative study on feature selection in text categorization, in: Proc. 14th International Conference on Machine Learning, Nashville, TN, 1997, pp. 412-420.",
    "references": [
      {
        "ref_id": "1",
        "text": "D.W. Aha, D. Kibler, M.K. Albert, Instance-based learning algorithms, Machine Learning 6 (1991) 37-66."
      },
      {
        "ref_id": "2",
        "text": "D.W. Aha, R.L. Bankert, Feature selection for case-based classification of cloud types: An empirical comparison, in: Proc. AAAI-94, Seattle, WA, 1994, pp. 106-112."
      },
      {
        "ref_id": "3",
        "text": "D.W. Aha, Personal communication, 1999."
      },
      {
        "ref_id": "4",
        "text": "H. Almuallin, T.G. Dietterich, Learning with many irrelevant features, in: Proc. AAAI-91, Anaheim, CA, 1991, pp. 547-552."
      },
      {
        "ref_id": "5",
        "text": "E. Alpaydin, Combined 5x2cv $F$ test for comparing supervised classification learning algorithms, Neural Comput. 11 (1999) 1885-1892."
      },
      {
        "ref_id": "6",
        "text": "J. Bala, K. DeJong, J. Huang, H. Wechsler, H. Vafae, Hybrid learning using genetic algorithms and decision trees for pattern classification, in: Proc. IJCAI-95, Montreal, Quebec, 1995, pp. 719-724."
      },
      {
        "ref_id": "7",
        "text": "S. Baluja, Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning, Technical Report CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, PA, 1994."
      },
      {
        "ref_id": "8",
        "text": "S. Baluja, S. Davies, Using optimal dependency-trees for combinatorial optimization: Learning the structure of the search space, in: Proc. 14th International Conference on Machine Learning, Nashville, TN, 1997, pp. 30-38."
      },
      {
        "ref_id": "9",
        "text": "M. Ben-Bassat, Pattern recognition and reduction of dimensionality, in: P.R. Krishnaiah, L.N. Kanal (Eds.), Handbook of Statistics-II, North-Holland, Amsterdam, 1982, pp. 773-791."
      },
      {
        "ref_id": "10",
        "text": "A.L. Blum, P. Langley, Selection of relevant features and examples in machine learning, Artificial Intelligence 97 (1997) 245-271."
      },
      {
        "ref_id": "11",
        "text": "M. Boddy, T. Dean, Deliberation scheduling for problem solving in time-constrained environments, Artificial Intelligence 67 (2) (1997) 245-285."
      },
      {
        "ref_id": "12",
        "text": "R.R. Bouckaert, Properties of Bayesian belief network learning algorithms, in: Proc. 10th Annual Conference on Uncertainty in Artificial Intelligence, Seattle, WA, 1994, pp. 102-109."
      },
      {
        "ref_id": "13",
        "text": "D. Boyce, A. Farhi, R. Weischedel, Optimal Subset Selection, Springer, Berlin, 1974."
      },
      {
        "ref_id": "14",
        "text": "W. Buntine, Theory refinement in Bayesian networks, in: Proc. 7th Conference on Uncertainty in Artificial Intelligence, Los Angeles, CA, 1991, pp. 52-60."
      },
      {
        "ref_id": "15",
        "text": "L. Breiman, J.H. Friedmann, R.A. Olshen, C.J. Stone, Classification and Regression Trees, Wadsworth, Belmont, CA, 1984."
      },
      {
        "ref_id": "16",
        "text": "C. Cardie, Using decision trees to improve case-based learning, in: Proc. 10th International Conference on Machine Learning, Amherst, MA, 1993, pp. 25-32."
      },
      {
        "ref_id": "17",
        "text": "R. Caruana, D. Freitag, Greedy attribute selection, in: Proc. 11th International Conference on Machine Learning, New Brunswick, NJ, Morgan Kaufmann, Los Altos, CA, 1994, pp. 28-36."
      },
      {
        "ref_id": "18",
        "text": "E. Castillo, J.M. Gutiérrez, A.S. Hadi, Expert Systems and Probabilistic Network Models, Springer, Berlin, 1997."
      },
      {
        "ref_id": "19",
        "text": "B. Cestnik, Estimating probabilities: A crucial task in machine learning, in: Proc. European Conference on Artificial Intelligence (ECAI-90), Stockholm, Sweden, 1990, pp. 147-149."
      },
      {
        "ref_id": "20",
        "text": "M. Chen, J. Han, P. Yu, Data mining: An overview from database perspective, IEEE Transactions on Knowledge and Data Engineering 8 (6) (1996) 866-883."
      },
      {
        "ref_id": "21",
        "text": "D.M. Chickering, D. Geiger, D. Heckerman, Learning Bayesian networks is NP-hard, Technical Report MSR-TR-94-17, Microsoft Research, Advanced Technology Division, Microsoft Corporation, Redmond, WA, 1994."
      },
      {
        "ref_id": "22",
        "text": "D.M. Chickering, D. Geiger, D. Heckerman, Learning Bayesian networks: Search methods and experimental results, in: Preliminary Papers of the 5th International Workshop on Artificial Intelligence and Statistics, Ft. Lauderdale, FL, 1995, pp. 112-128."
      },
      {
        "ref_id": "23",
        "text": "G.F. Cooper, E.A. Herskovits, A Bayesian method for the induction of probabilistic networks from data, Machine Learning 9 (1992) 309-347."
      },
      {
        "ref_id": "24",
        "text": "A.P. Dawid, Conditional independence in statistical theory, J. Roy. Statist. Soc. Ser. B 41 (1979) 1-31."
      },
      {
        "ref_id": "25",
        "text": "J.S. De Bonet, C.L. Isbell, P. Viola, MIMIC: Finding optima by estimating probability densities, in: Th. Petsche, M. Mozer, M. Jordan (Eds.), Advances in Neural Information Processing Systems 9, MIT Press, Cambridge, MA, 1997."
      },
      {
        "ref_id": "26",
        "text": "T.G. Dietterich, Approximate statistical tests for comparing supervised learning algorithms, Neural Comput. 10 (7) (1998) 1895-1924."
      },
      {
        "ref_id": "27",
        "text": "J. Doak, An evaluation of feature selection methods and their application to computer security, Technical Report CSE-92-18, University of California at Davis, CA, 1992."
      },
      {
        "ref_id": "28",
        "text": "R. Etxeberria, P. Larrañaga, J.M. Picaza, Analysis of the behaviour of genetic algorithms when learning Bayesian network structure from data, Pattern Recognition Lett. 18 (11-13) (1997) 1269-1273."
      },
      {
        "ref_id": "29",
        "text": "R. Etxeberria, P. Larrañaga, Global optimization with Bayesian networks, in: Proc. II Symposium on Artificial Intelligence (CIMAF99), La Habana, Cuba, 1999, pp. 332-339."
      },
      {
        "ref_id": "30",
        "text": "F.J. Ferri, V. Kadirkamanathan, J. Kittler, Feature subset search using genetic algorithms, in: Proc. IEE/IEEE Workshop on Natural Algorithms in Signal Processing, Essex, 1993, pp. 23/1-23/7."
      },
      {
        "ref_id": "31",
        "text": "F.J. Ferri, P. Pudil, M. Hatef, J. Kittler, Comparative study of techniques for large scale feature selection, in: E.S. Gelsema, L.N. Kanal (Eds.), Multiple Paradigms, Comparative Studies and Hybrid Systems, North Holland, Amsterdam, 1994, pp. 403-413."
      },
      {
        "ref_id": "32",
        "text": "N. Friedman, Z. Yakhini, On the sample complexity of learning Bayesian networks, in: Proc. 12th Conference on Uncertainty in Artificial Intelligence, Portland, OR, 1996, pp. 274-282."
      },
      {
        "ref_id": "33",
        "text": "J.J. Grefenstette, Optimization of control parameters for genetic algorithms, IEEE Trans. Systems Man Cybernet. SMC-16 (1) (1986) 122-128."
      },
      {
        "ref_id": "34",
        "text": "G.R. Harik, F.G. Lobo, D.E. Goldberg, The compact genetic algorithm, IlliGAL Report 97006, University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL, 1997."
      },
      {
        "ref_id": "35",
        "text": "D. Heckerman, D. Geiger, D. Chickering, Learning Bayesian networks: The combinations of knowledge and statistical data, Machine Learning 20 (1995) 197-243."
      },
      {
        "ref_id": "36",
        "text": "M. Henrion, Propagating uncertainty in Bayesian networks by probabilistic logic sampling, in: J.F. Lemmer, L.N. Kanal (Eds.), Uncertainty in Artificial Intelligence 2, Elsevier Science, Amsterdam, 1988, pp. 149-163."
      },
      {
        "ref_id": "37",
        "text": "J.H. Holland, Adaptation in Natural and Artificial Systems, University of Michigan Press, Ann Arbor, MI, 1975."
      },
      {
        "ref_id": "38",
        "text": "I. Inza, Feature weighting for nearest neighbor algorithm by Bayesian networks based combinatorial optimization, in: Proc. Student Session of Advanced Course on Artificial Intelligence (ACAI-99), Chania, Greece, 1999, pp. 33-35."
      },
      {
        "ref_id": "39",
        "text": "A.K. Jain, R. Chandrasekaran, Dimensionality and sample size considerations in pattern recognition practice, in: P.R. Krishnaiah, L.N. Kanal (Eds.), Handbook of Statistics-II, North-Holland, Amsterdam, 1982, pp. 835-855."
      },
      {
        "ref_id": "40",
        "text": "A. Jain, D. Zongker, Feature selection: Evaluation, application, and small sample performance, IEEE Transactions on Pattern Analysis and Machine Intelligence 19 (2) (1997) 153-158."
      },
      {
        "ref_id": "41",
        "text": "G. John, R. Kohavi, K. Pfleger, Irrelevant features and the subset selection problem, in: Proc. 11th International Conference on Machine Learning, New Brunswick, NJ, 1994, pp. 121-129."
      },
      {
        "ref_id": "42",
        "text": "K. Kira, L.A. Rendell, The feature selection problem: Traditional methods and a new algorithm, in: Proc. AAAI-92, San Jose, CA, 1992, pp. 129-134."
      },
      {
        "ref_id": "43",
        "text": "J. Kittler, Feature set search algorithms, in: C.H. Chen (Ed.), Pattern Recognition and Signal Processing, Sijthoff and Noordhoff, Alphen aan den Rijn, The Netherlands, 1978, pp. 41-60."
      },
      {
        "ref_id": "44",
        "text": "R. Kohavi, Feature Subset Selection as search with probabilistic estimates, in: Proc. AAAI Fall Symposium on Relevance, New Orleans, LA, 1994, pp. 122-126."
      },
      {
        "ref_id": "45",
        "text": "R. Kohavi, A study of cross-validation and bootstrap for accuracy estimation and model selection, in: Proc. IJCAI-95, Montreal, Quebec, 1995, pp. 1137-1143."
      },
      {
        "ref_id": "46",
        "text": "R. Kohavi, Feature Subset Selection using the wrapper method: Overfitting and dynamic search space topology, in: Proc. First International Conference on Knowledge Discovery and Data Mining (KDD-95), Montreal, Quebec, 1995, pp. 192-197."
      },
      {
        "ref_id": "47",
        "text": "R. Kohavi, G. John, Wrappers for feature subset selection, Artificial Intelligence 97 (1-2) (1997) 273-324."
      },
      {
        "ref_id": "48",
        "text": "R. Kohavi, D. Sommerfield, J. Dougherty, Data mining using MLC++, a Machine Learning Library in C++, Internat. J. Artificial Intelligence Tools 6 (4) (1997) 537-566."
      },
      {
        "ref_id": "49",
        "text": "R. Kohavi, Personal communication, 1999."
      },
      {
        "ref_id": "50",
        "text": "D. Koller, M. Sahami, Toward optimal feature selection, in: Proc. 13th International Conference on Machine Learning, Bari, Italy, 1996, pp. 284-292."
      },
      {
        "ref_id": "51",
        "text": "L. Kuncheva, Genetic algorithms for feature selection for parallel classifiers, Inform. Process. Lett. 46 (1993) $163-168$."
      },
      {
        "ref_id": "52",
        "text": "P. Langley, S. Sage, Induction of selective Bayesian classifiers, in: Proc. 10th Conference on Uncertainty in Artificial Intelligence, Seattle, WA, 1994, pp. 399-406."
      },
      {
        "ref_id": "53",
        "text": "P. Larrañaga, C.M.H. Kuijpers, R.H. Murga, Y. Yurramendi, Learning Bayesian network structures by searching for the best ordering with genetic algorithms, IEEE Trans. Systems Man Cybernet.—Part A: Systems and Humans 26 (4) (1996) 487-493."
      },
      {
        "ref_id": "54",
        "text": "P. Larrañaga, M. Poza, Y. Yurramendi, R.H. Murga, C.M.H. Kuijpers, Structure learning of Bayesian networks by genetic algorithms: A performance analysis of control parameters, IEEE Transactions on Pattern Analysis and Machine Intelligence 18 (9) (1996) 912-926."
      },
      {
        "ref_id": "55",
        "text": "P. Larrañaga, R. Etxeberria, J.A. Lozano, B. Sierra, I. Inza, J.M. Peña, A review of the cooperation between evolutionary computation and probabilistic graphical models, in: Proc. II Symposium on Artificial Intelligence (CIMAF99), La Habana, Cuba, 1999, pp. 314-324."
      },
      {
        "ref_id": "56",
        "text": "S.L. Lauritzen, Graphical Models, Oxford University Press, Oxford, 1996."
      },
      {
        "ref_id": "57",
        "text": "H. Liu, R. Setiono, Feature selection and classification-A probabilistic wrapper approach, in: Proc. 9th International Conference on Machine Learning, Bari, Italy, 1996, pp. 284-292."
      },
      {
        "ref_id": "58",
        "text": "H. Liu, H. Motoda, Feature Selection for Knowledge Discovery and Data Mining, Kluwer Academic, Norwell, MA, 1998."
      },
      {
        "ref_id": "59",
        "text": "H. Liu, R. Setiono, Incremental feature selection, Applied Intelligence 9 (3) (1998) 217-230."
      },
      {
        "ref_id": "60",
        "text": "D. Madigan, A.E. Raftery, C.T. Volinsky, J.A. Hoeting, Bayesian model averaging, in: Proc. AAAI Workshop on Integrating Multiple Learned Models, Portland, OR, 1996, pp. 77-83."
      },
      {
        "ref_id": "61",
        "text": "W. Mendenhall, T. Sincich, Statistics for Engineering and the Sciences, Prentice Hall, Englewood Cliffs, NJ, 1998."
      },
      {
        "ref_id": "62",
        "text": "A.J. Miller, Subset Selection in Regression, Chapman and Hall, Washington, DC, 1990."
      },
      {
        "ref_id": "63",
        "text": "D. Mladenić, Feature subset selection in text-learning, in: Proc. 10th European Conference on Machine Learning, Chemnitz, Germany, 1998, pp. 95-100."
      },
      {
        "ref_id": "64",
        "text": "A.W. Moore, M.S. Lee, Efficient algorithms for minimizing cross validation error, in: Proc. 11th International Conference on Machine Learning, New Brunswick, NJ, 1994, pp. 190-198."
      },
      {
        "ref_id": "65",
        "text": "H. Mühlenbein, G. Paaß, From recombination of genes to the estimation of distributions. Binary parameters, in: H.M. Voigt et al. (Eds.), Parallel Problem Solving from Nature-PPSN IV, Lecture Notes in Computer Science, Vol. 1411, Springer, Berlin, 1996, pp. 178-187."
      },
      {
        "ref_id": "66",
        "text": "H. Mühlenbein, The equation for response to selection and its use for prediction, Evolutionary Comput. 5 (3) (1997) $303-346$."
      },
      {
        "ref_id": "67",
        "text": "H. Mühlenbein, T. Mahnig, A. Ochoa, Schemata, distributions and graphical models in evolutionary optimization, J. Heuristics 5 (1999) 215-247."
      },
      {
        "ref_id": "68",
        "text": "P. Murphy, UCI Repository of Machine Learning Databases, University of California, Department of Information and Computer Science, Irvine, CA, 1995."
      },
      {
        "ref_id": "69",
        "text": "P. Narendra, K. Fukunaga, A branch and bound algorithm for feature subset selection, IEEE Trans. Comput. C-26 (9) (1977) 917-922."
      },
      {
        "ref_id": "70",
        "text": "A.Y. Ng, Preventing 'overfitting' of cross-validation data, in: Proc. 14th Conference on Machine Learning, Nashville, TN, 1997, pp. 245-253."
      },
      {
        "ref_id": "71",
        "text": "J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, Palo Alto, CA, 1988."
      },
      {
        "ref_id": "72",
        "text": "M. Pelikan, H. Mühlenbein, The bivariate marginal distribution algorithm, 1999, submitted for publication."
      },
      {
        "ref_id": "73",
        "text": "M. Pelikan, D.E. Goldberg, E. Cantú-Paz, BOA: The Bayesian optimization algorithm, IlliGAL Report 99003, University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL, 1999."
      },
      {
        "ref_id": "74",
        "text": "M. Pelikan, D.E. Goldberg, F. Lobo, A survey of optimization by building and using probabilistic model, IlliGAL Report 99018, University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL, 1999."
      },
      {
        "ref_id": "75",
        "text": "F. Provost, V. Kolluri, A survey of methods for scaling up inductive algorithms, Data Mining and Knowledge Discovery 2 (1999) 131-169."
      },
      {
        "ref_id": "76",
        "text": "P. Pudil, J. Novovicova, J. Kittler, Floating search methods in feature selection, Pattern Recognition Lett. 15 (1) (1994) 1119-1125."
      },
      {
        "ref_id": "77",
        "text": "J.R. Quinlan, Induction of decision trees, Machine Learning 1 (1986) 81-106."
      },
      {
        "ref_id": "78",
        "text": "J.R. Quinlan, Inferring decision trees using the minimum description length principle, Inform. and Comput. 80 (1989) 227-248."
      },
      {
        "ref_id": "79",
        "text": "J.R. Quinlan, Programs for Machine Learning, Morgan Kaufmann, San Mateo, CA, 1993."
      },
      {
        "ref_id": "80",
        "text": "G. Schwarz, Estimating the dimension of a model, Ann. Statist. 7 (1978) 461-464."
      },
      {
        "ref_id": "81",
        "text": "W. Siedelecky, J. Skalansky, On automatic feature selection, Internat. J. Pattern Recognition and Artificial Intelligence 2 (1988) 197-220."
      },
      {
        "ref_id": "82",
        "text": "D.B. Skalak, Prototype and feature selection by sampling and random mutation hill-climbing algorithms, in: Proc. 11th International Conference on Machine Learning, New Brunswick, NJ, 1994, pp. 293-301."
      },
      {
        "ref_id": "83",
        "text": "S.D. Stearns, On selecting features for pattern classifiers, in: Proc. 3rd International Conference on Pattern Recognition, Coronado, CA, 1976, pp. 71-75."
      },
      {
        "ref_id": "84",
        "text": "G. Syswerda, Uniform crossover in genetic algorithms, in: Proc. International Conference on Genetic Algorithms 3, Arlington, VA, 1989, pp. 2-9."
      },
      {
        "ref_id": "85",
        "text": "C. Taylor, D. Michie, D. Spiegelhalter, Machine Learning, Neural and Statistical Classification, Paramount Publishing International, 1994."
      },
      {
        "ref_id": "86",
        "text": "H. Vafaie, K. De Jong, Robust feature selection algorithms, in: Proc. 5th International Conference on Tools with Artificial Intelligence, Rockville, MD, 1993, pp. 356-363."
      },
      {
        "ref_id": "87",
        "text": "M.L. Wong, W. Lam, K.S. Leung, Using evolutionary programming and minimum description length principle for data mining of Bayesian networks, IEEE Transactions on Pattern Analysis and Machine Intelligence 21 (2) (1999) 174-178."
      },
      {
        "ref_id": "88",
        "text": "J. Yang, V. Honavar, Feature subset selection using a genetic algorithm, IEEE Intelligent Systems 13 (2) (1998) $44-49$."
      },
      {
        "ref_id": "89",
        "text": "Y. Yang, J.O. Pedersen, A comparative study on feature selection in text categorization, in: Proc. 14th International Conference on Machine Learning, Nashville, TN, 1997, pp. 412-420."
      }
    ],
    "reference_count": 89,
    "pattern_matched": "(?:^|\\n)#+\\s*References?\\s*\\n"
  },
  "tables": [
    {
      "table_number": "1",
      "table_title": "Details of experimental domains. $\\mathrm{C}=$ continuous. $\\mathrm{N}=$ nominal",
      "headers": [
        "Domain",
        "Number of instances",
        "Number of classes",
        "Number of features"
      ],
      "rows": [
        [
          1,
          351,
          2,
          "$34(34-\\mathrm{C})$"
        ],
        [
          "(2) Horse-colic",
          368,
          2,
          "$22(15-\\mathrm{N}, 7-\\mathrm{C})$"
        ],
        [
          3,
          898,
          6,
          "$38(32-\\mathrm{C}, 6-\\mathrm{N})$"
        ],
        [
          424,
          1000,
          10,
          "$24(24-\\mathrm{N})$"
        ],
        [
          5,
          2310,
          7,
          "$19(19-\\mathrm{C})$"
        ],
        [
          621,
          2500,
          2,
          "$21(21-\\mathrm{C})$"
        ],
        [
          "(7) Sick-euthyroid",
          3163,
          2,
          "$25(7-\\mathrm{C}, 18-\\mathrm{N})$"
        ],
        [
          8,
          3196,
          2,
          "$36(36-\\mathrm{N})$"
        ]
      ],
      "row_count": 8,
      "column_count": 4
    },
    {
      "table_number": "2",
      "table_title": "A comparison of accuracy percentages of ID3 with and without FSS-EBNA",
      "headers": [
        "Domain",
        "ID3 without FSS",
        "ID3 \\& FSS-EBNA",
        "p-value"
      ],
      "rows": [
        [
          1,
          "$87.97 \\pm 3.68$",
          "$88.77 \\pm 1.99$",
          0.35
        ],
        [
          "(2) Horse-colic",
          "$78.42 \\pm 4.16$",
          "$83.65 \\pm 1.57$",
          0.01
        ],
        [
          3,
          "$99.42 \\pm 0.55$",
          "$99.40 \\pm 0.50$",
          0.99
        ],
        [
          424,
          "$58.21 \\pm 1.73$",
          "$71.40 \\pm 1.72$",
          0.0
        ],
        [
          5,
          "$95.52 \\pm 0.60$",
          "$95.73 \\pm 0.86$",
          0.95
        ],
        [
          621,
          "$79.32 \\pm 1.11$",
          "$79.32 \\pm 1.11$",
          1.0
        ],
        [
          "(7) Sick-euthyroid",
          "$96.78 \\pm 0.36$",
          "$96.78 \\pm 0.41$",
          1.0
        ],
        [
          8,
          "$98.93 \\pm 0.40$",
          "$99.05 \\pm 0.39$",
          0.93
        ],
        [
          "Average",
          86.81,
          89.06,
          ""
        ]
      ],
      "row_count": 9,
      "column_count": 4
    },
    {
      "table_number": "3",
      "table_title": "A comparison of accuracy percentages of NB with and without FSS-EBNA",
      "headers": [
        "Domain",
        "NB without FSS",
        "NB \\& FSS-EBNA",
        "p-value"
      ],
      "rows": [
        [
          1,
          "$84.84 \\pm 3.12$",
          "$92.40 \\pm 2.04$",
          0.0
        ],
        [
          "(2) Horse-colic",
          "$78.97 \\pm 2.96$",
          "$83.53 \\pm 1.58$",
          0.01
        ],
        [
          3,
          "$93.01 \\pm 3.13$",
          "$94.10 \\pm 3.00$",
          0.1
        ],
        [
          424,
          "$72.53 \\pm 0.91$",
          "$72.78 \\pm 0.67$",
          0.96
        ],
        [
          5,
          "$79.95 \\pm 1.52$",
          "$90.01 \\pm 1.83$",
          0.0
        ],
        [
          621,
          "$79.48 \\pm 0.82$",
          "$93.42 \\pm 0.90$",
          0.0
        ],
        [
          "(7) Sick-euthyroid",
          "$84.77 \\pm 2.70$",
          "$96.14 \\pm 0.65$",
          0.0
        ],
        [
          8,
          "$87.22 \\pm 1.79$",
          "$94.23 \\pm 0.35$",
          0.0
        ],
        [
          "Average",
          83.48,
          89.57,
          ""
        ]
      ],
      "row_count": 9,
      "column_count": 4
    },
    {
      "table_number": "4",
      "table_title": "Cardinalities of selected feature subsets for ID3 with and without FSS-EBNA. It must be taken into account that ID3 carries out an embedded FSS and it can discard some of the available features in the construction of the decision tree. The third column shows the full set cardinality",
      "headers": [
        "Domain",
        "ID3 without FSS",
        "ID3 \\& FSS-EBNA",
        "Full set"
      ],
      "rows": [
        [
          1,
          "$9.00 \\pm 1.15$",
          "$6.50 \\pm 1.17$",
          34
        ],
        [
          "(2) Horse-colic",
          "$10.60 \\pm 1.17$",
          "$3.30 \\pm 1.25$",
          22
        ],
        [
          3,
          "$10.00 \\pm 1.15$",
          "$8.70 \\pm 1.22$",
          38
        ],
        [
          424,
          "$24.00 \\pm 0.00$",
          "$7.00 \\pm 0.82$",
          24
        ],
        [
          5,
          "$11.50 \\pm 1.17$",
          "$5.70 \\pm 1.05$",
          19
        ],
        [
          621,
          "$9.00 \\pm 0.00$",
          "$9.00 \\pm 0.00$",
          21
        ],
        [
          "(7) Sick-euthyroid",
          "$9.40 \\pm 1.17$",
          "$4.00 \\pm 0.66$",
          25
        ],
        [
          8,
          "$26.50 \\pm 2.01$",
          "$21.20 \\pm 2.09$",
          36
        ]
      ],
      "row_count": 8,
      "column_count": 4
    },
    {
      "table_number": "5",
      "table_title": "Cardinalities of selected feature subsets for NB with and without FSS-EBNA. It must be taken into account that when no FSS is applied to NB, it uses the full feature set to induce the classification model",
      "headers": [
        "Domain",
        "NB without FSS $=$ Full set",
        "NB \\& FSS-EBNA"
      ],
      "rows": [
        [
          1,
          34,
          "$13.40 \\pm 2.11$"
        ],
        [
          "(2) Horse-colic",
          22,
          "$6.10 \\pm 1.85$"
        ],
        [
          3,
          38,
          "$20.50 \\pm 3.13$"
        ],
        [
          424,
          24,
          "$11.20 \\pm 1.61$"
        ],
        [
          5,
          19,
          "$7.10 \\pm 0.73$"
        ],
        [
          621,
          21,
          "$9.00 \\pm 0.00$"
        ],
        [
          "(7) Sick-euthyroid",
          25,
          "$9.80 \\pm 2.09$"
        ],
        [
          8,
          36,
          "$17.30 \\pm 2.58$"
        ]
      ],
      "row_count": 8,
      "column_count": 3
    },
    {
      "table_number": "6",
      "table_title": "Generation in which stopped each of the ten runs of the $5 \\times 2 \\mathrm{cv}$ procedure. It must be noted that the subset returned by the algorithm was the best subset of the previous generation respect the stop. The initial generation is considered as ' 0 '",
      "headers": [
        "Domain",
        "ID3 \\& FSS-EBNA",
        "NB \\& FSS-EBNA"
      ],
      "rows": [
        [
          1,
          "$2,2,2,2,2,1,2,1,2,1$",
          "$1,1,2,2,2,2,2,2,2,2$"
        ],
        [
          "(2) Horse-colic",
          "$2,2,2,3,3,2,2,2,1,1$",
          "$4,2,2,2,2,2,3,2,3,2$"
        ],
        [
          3,
          "$1,1,1,1,1,1,1,1,1,1$",
          "$2,2,2,2,1,2,1,2,2,2$"
        ],
        [
          424,
          "$2,2,2,2,2,2,2,2,2,2$",
          "$3,3,2,3,3,3,3,3,2,2$"
        ],
        [
          5,
          "$2,1,2,3,1,1,2,2,2,2$",
          "$4,4,4,3,3,3,3,4,4,3$"
        ],
        [
          621,
          "$1,1,1,1,1,1,1,1,1,1$",
          "$3,2,3,2,3,2,3,3,2,2$"
        ],
        [
          "(7) Sick-euthyroid",
          "$2,2,1,1,2,2,3,2,2,2$",
          "$4,4,4,3,5,2,4,2,3,4$"
        ],
        [
          8,
          "$5,5,4,4,4,4,4,4,4,4$",
          "$3,3,3,4,4,3,3,4,3,4$"
        ]
      ],
      "row_count": 8,
      "column_count": 3
    },
    {
      "table_number": "7",
      "table_title": "CPU times, in seconds, for FSS-EBNA. Reported numbers reflect the average times and standard deviation for the ten folds of $5 \\times 2 \\mathrm{cv}$",
      "headers": [
        "Domain",
        "ID3 \\& FSS-EBNA",
        "NB \\& FSS-EBNA"
      ],
      "rows": [
        [
          1,
          "$23,105 \\pm 4830$",
          "$2466 \\pm 842$"
        ],
        [
          "(2) Horse-colic",
          "$28,021 \\pm 5331$",
          "$2901 \\pm 698$"
        ],
        [
          3,
          "$24,127 \\pm 724$",
          "$5213 \\pm 873$"
        ],
        [
          424,
          "$64,219 \\pm 4536$",
          "$6333 \\pm 1032$"
        ],
        [
          5,
          "$103,344 \\pm 24,675$",
          "$15,243 \\pm 1675$"
        ],
        [
          621,
          "$78,218 \\pm 1322$",
          "$14,361 \\pm 1545$"
        ],
        [
          "(7) Sick-euthyroid",
          "$48,766 \\pm 9433$",
          "$15,541 \\pm 3786$"
        ],
        [
          8,
          "$104,229 \\pm 9278$",
          "$16,106 \\pm 2768$"
        ]
      ],
      "row_count": 8,
      "column_count": 3
    }
  ]
}