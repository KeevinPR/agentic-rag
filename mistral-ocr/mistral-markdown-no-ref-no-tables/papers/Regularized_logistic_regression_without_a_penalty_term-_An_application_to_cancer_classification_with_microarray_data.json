{
  "metadata": {
    "file_path": "/Users/id05309/Documents/tfm/mistral/mistral-markdown/2011/Regularized logistic regression without a penalty term- An application to cancer classification with microarray data.md",
    "filename": "Regularized logistic regression without a penalty term- An application to cancer classification with microarray data.md",
    "title": "Regularized logistic regression without a penalty term- An application to cancer classification with microarray data",
    "year": "2011"
  },
  "references": {
    "header": "## References",
    "content": "Aguilera, A. M., Escabias, M., \\& Valderrama, M. J. (2006). Using principal components for estimating logistic regression with high-dimensional multicollinear data. Computational Statistics and Data Analysis, 50, 1905-1924.\nAlon, U., Barkai, N., Notterman, D., Gish, K., Ybarra, S., Mack, D., et al. (1999). Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide microarrays. Proceedings of the National Academy of Sciences USA, 96, 6745-6750.\nAntoniadis, A., Lambert-Lacroix, S., \\& Leblanc, F. (2003). Effective dimension reduction methods for tumor classification using gene expression data. Bioinformatics, 19(5), 563-570.\nBalakrishnan, S., \\& Madigan, D. (2008). Algorithms for sparse linear classifier in the massive data setting. Journal of Machine Learning Research, 9, 313-337.\nBaumgartner, C., Böhm, C., Baumgartner, D., Marini, G., Weinberger, K., Olgemöller, B., et al. (2004). Supervised machine learning techniques for the classification of metabolic disorders in newborns. Bioinformatics, 20, 2985-2996.\nBickel, P. J., \\& Li, B. (2006). Regularization in statistics. Text. 15, 271-344.\nBraga-Neto, U. M., \\& Dougherty, E. R. (2004). Is cross-validation valid for smallsample microarray classification? Bioinformatics, 20, 374-380.\nCawley, G. C., \\& Talbot, N. (2006). Gene selection in cancer classification using sparse logistic regression with Bayesian regularization. Bioinformatics, 22, $2348-2355$.\nCortes, C., \\& Mohri, M. (2004). AUC optimization vs. error rate minimization. Advances in neural information processing systems (Vol. 16). Cambridge, MA: The MIT Press.\nDemšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7, 1-30.\nDudoit, S., Fridlyand, J., \\& Speed, T. P. (2002). Comparison of discrimination methods for the classification of tumors using gene expression data. Journal of the American Statistical Association, 97, 77-87.\n\nEfron, B. (1983). Estimating the error rate of a prediction rule: Improvement on cross-validation. Journal of the American Statistical Association, 78, 316-331.\nEilers, P., Boer, J., van Ommen, G., \\& van Houwelingen, H. (2001). Classification of microarray data with penalized logistic regression. Proceedings of SPEE. Progress in Biomedical Optics and Images, 4266(2), 187-198.\nFan, J., \\& Li, R. (2006). Statistical challenges with high dimensionality: Feature selection in knowledge discovery. In Proceedings of the Madrid international congress of mathematicians (Vol. III, pp. 595-622).\nFirth, D. (1993). Bias reduction of maximum likelihood estimates. Biometrika, 80, 27-38.\nFort, G., \\& Lambert-Lacroix, S. (2005). Classification using partial least squares with penalized logistic regression. Bioinformatics, 21, 1104-1111.\nFrank, I. E., \\& Friedman, J. H. (1993). A statistical view of some chemometric regression tools. Technometrics, 35, 109-148.\nFu, W. J. (1998). Penalized regression: The bridge versus the LASSO. Journal of Computational and Graphical Statistics, 7, 397-416.\nGao, S., \\& Shen, J. (2007). Asymptotic properties of a double penalized maximum likelihood estimator in logistic regression. Statistics and Probability Letters, 77, $925-930$.\nGenkin, A., Lewis, D. D., \\& Madigan, D. (2007). Large-scale Bayesian logistic regression for text categorization. Technometrics, 49, 291-304.\nGohib, T. K, Slonim, D. K., Tamayo, P., Huard, C., Gaasenbeek, M., Mesirov, J. P., et al. (1999). Molecular classification of cancer: Class discovery and class prediction by gene expression monitoring. Science, 286, 531-537.\nGonzález, C., Lozano, J. A., \\& Larrañaga, P. (2002). Mathematical modelling of UMDA, algorithm with tournament selection. Behaviour on linear and quadratic functions. International Journal of Approximate Reasoning, 31, 313-340.\nGuyon, I., Weston, J., Barnhill, S., \\& Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. Machine Learning, 46, $389-422$.\nHanley, J. A., \\& McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143, 29-36.\nHastie, T., \\& Tibshirani, R. (2004). Efficient quadratic regularization for expression arrays. Biostatistics, 5, 329-340.\nHastie, T., Tibshirani, R., \\& Friedman, J. (2001). The elements of statistical learning. Springer.\nHoerl, A. E., \\& Kennard, R. W. (1970). Ridge regression: Biased estimates for nonorthogonal problems. Technometrics, 12, 55-67.\nHosmer, D. W., \\& Lemeshow, S. (2000). Applied logistic regression (2nd ed.). New York: J. Wiley and Sons.\nHuang, J., \\& Ling, C. X. (2005). Using AUC and accuracy in evaluating learning algorithms. IEEE Transactions on Knowledge and Data Engineering, 17, 299-310.\nIhaka, R., \\& Gentleman, R. (1996). R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5, 229-314.\nInza, I., Larrañaga, P., Blanco, R., \\& Cerrolaza, A. (2004). Filter versus wrapper gene selection approaches in DNA microarray domains. Artificial Intelligence in Medicine, 31, 91-103.\nKeerthi, S. S., Duan, K. B., Shevade, S. K., \\& Poo, A. N. (2005). A fast dual algorithm for kernel logistic regression. Machine Learning, 61, 151-165.\nKiang, M. Y. (2003). A comparative assessment of classification methods. Decision Support Systems, 35, 441-454.\nKohavi, R., \\& John, G. H. (1997). Wrappers for feature subset selection. Artificial Intelligence, 97, 273-324.\nKoh, K., Kim, S.-Y., \\& Boyd, S. (2007). An interior-point method for large-scale $I_{1}$ regularized logistic regression. Journal of Machine Learning Research, 8, $1519-1555$.\nLarrañaga, P., Etxeberria, R., Lozano, J. A., \\& Peña, J. M. (2000). Optimization in continuous domains by learning and simulation of Gaussian networks. In Workshop in optimization by building and using probabilistic models. Genetic and evolutionary computation conference, GECCO 2000 (pp. 201-204).\nLarrañaga, P., \\& Lozano, J. A. (Eds.). (2002). Estimation of distribution algorithms. A new tool for evolutionary computation. Kluwer A.P.\nLasko, T. A., Bhagwat, J. G., Zou, K. H., \\& Ohno-Machado, L. (2005). The use of ROC curves in biomedical informatics. Journal of Biomedical Informatics, 38, 404-415.\nLe Cessie, S., \\& van Houwelingen, J. C. (1992). Ridge estimators in logistic regression. Applied Statistics, 41, 191-201.\nLee, S.-I., Lee, H., Abbeel, P., \\& Ng, A. Y. (2006). Efficient L1 regularized logistic regression. Proceedings of the 21st national conference on artificial intelligence (AAAI-06) (pp. 1-9).\n\nLee, J. W., Lee, J. B., Park, M., \\& Song, S. H. (2005). An extensive comparison of recent classification tools applied to microarray data. Computational Statistics and Data Analysis, 48, 869-885.\nLee, A., \\& Silvapulle, M. (1988). Ridge estimation in logistic regression. Communications in Statistics, Part B-Simulation and Computation, 17, 1231-1257.\nLiao, J. G., \\& Chin, K.-V. (2007). Logistic regression for disease classification using microarray data: Model selection in a large $p$ and small $n$ case. Bioinformatics, 23, 1945-1951.\nLiu, Z., Jiang, F., Tian, G., Wang, S., Sato, F., Meltzer, S. J., et al. (2007). Sparse logistic regression with $I_{p}$ penalty for biomarker identification. Statistical Applications in Genetics and Molecular Biology, 6 [Article 6].\nLiu, H., \\& Motoda, H. (2008). Computational methods of feature selection. Chapman and Hall(CRC Press).\nLokhorst, J. (1999) The lasso and generalized linear models. Technical Report, University of Adelaide.\nLozano, J. A., Larrañaga, P., Inza, I., \\& Bengoetxea, E. (Eds.). (2006). Towards a new evolutionary computation. Advances in estimation of distribution algorithms. New York: Springer.\nMa, S., \\& Huang, J. (2005). Regularized ROC method for disease classification and biomarker selection with microarray data. Bioinformatics, 21, 4356-4362.\nMeier, L., van de Geer, S., \\& Bühlmann, P. (2008). The group Lasso for logistic regression. Journal of the Royal Statistical Society, Series B, 70, 53-71.\nNakamichi, R. E., Imoto, S., \\& Miyano, S. (2004). Case-control study of binary disease trait considering interactions between SNPs and environmental effects using logistic regression. In Fourth IEEE symposium on bioinformatics and bioengineering (Vol. 21, pp. 73-78).\nNg, A. (2004). Feature selection, $I_{1}$ vs. $I_{2}$-regularization, and rotational invariants. In Proceedings of the 21st international conference on machine learning.\nNguyen, D. V., \\& Rocke, D. M. (2002). Tumor classification by partial least squares using microarray gene expression data. Bioinformatics, 18, 39-50.\nPark, M. Y., \\& Hastie, T. (2006). $I_{3}$-regularization path algorithm for generalized linear models. Technical Report, Stanford University.\nPelikan, M. (2005). Hierarchical Bayesian optimization algorithm: Toward a new generation of evolutionary algorithms. Springer.\nSha, F., Park, Y. A., \\& Saul, L. K. (2007). Multiplicative updates for $L_{1}$-regularized linear and logistic regression. Lecture notes in computer science (Vol. 4723). Springer.\nShen, L., \\& Tan, E. C. (2005). Dimension reduction-based penalized logistic regression for cancer classification using microarray data. IEEE Transactions on Computational Biology and Bioinformatics, 2, 166-175.\nShevade, S. K., \\& Keerthi, S. S. (2003). A simple and efficient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19, 2246-2253.\nThisted, R. A. (1988). Elements of statistical computing. New York: Chapman and Hall.\nTibshirani, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society, Series B, 58, 267-288.\nTibshirani, R., Saunders, M., Rosset, S., Zhu, J., \\& Knight, K. (2005). Sparsity and smothness via the fused lasso. Journal of the Royal Statistical Society, Series B, 67, $91-108$.\nUncu, O., \\& Türksen, I. B. (2007). A novel feature selection approach: Combining feature wrappers and filters. Information Sciences, 177, 449-466.\nVinterbo, S., \\& Ohno-Machado, L. (1999). A genetic algorithm to select variables in logistic regression: Example in the domain of myocardial infarct. Journal of the American Medical Informatics Association, 6, 984-988.\nWeber, G., Vinterbo, S., \\& Ohno-Machado, L. (2004). Multivariate selection of genetic markers in diagnostic classification. Artificial Intelligence in Medicine, 31, 155-167.\nWest, M., Blanchette, C., Dressman, H., Huang, E., Ishida, S., Spang, R., et al. (2001). Predicting the clinical status of human breast cancer by using gene expression profiles. Proceedings of the National Academy of Sciences USA, 98(20), $11462-11467$.\nYuan, M., \\& Lin, Y. (2006). Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society, Series B, 68, 49-67.\nZhao, P., \\& Yu, B. (2007). Stagewise Lasso. Journal of Machine Learning Research, 8, 2701-2726.\nZhou, H., \\& Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67, 301-320.\nZhu, J., \\& Hastie, T. (2004). Classification of gene microarrays by penalized logistic regression. Biostatistics, 5, 427-443.\nZou, H. (2006). The adaptive Lasso and its oracle properties. Journal of the American Statistical Association, 101, 1418-1429.",
    "references": [
      {
        "ref_id": "1",
        "text": "Aguilera, A. M., Escabias, M., \\& Valderrama, M. J. (2006). Using principal components for estimating logistic regression with high-dimensional multicollinear data. Computational Statistics and Data Analysis, 50, 1905-1924."
      },
      {
        "ref_id": "2",
        "text": "Alon, U., Barkai, N., Notterman, D., Gish, K., Ybarra, S., Mack, D., et al. (1999). Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide microarrays. Proceedings of the National Academy of Sciences USA, 96, 6745-6750."
      },
      {
        "ref_id": "3",
        "text": "Antoniadis, A., Lambert-Lacroix, S., \\& Leblanc, F. (2003). Effective dimension reduction methods for tumor classification using gene expression data. Bioinformatics, 19(5), 563-570."
      },
      {
        "ref_id": "4",
        "text": "Balakrishnan, S., \\& Madigan, D. (2008). Algorithms for sparse linear classifier in the massive data setting. Journal of Machine Learning Research, 9, 313-337."
      },
      {
        "ref_id": "5",
        "text": "Baumgartner, C., Böhm, C., Baumgartner, D., Marini, G., Weinberger, K., Olgemöller, B., et al. (2004). Supervised machine learning techniques for the classification of metabolic disorders in newborns. Bioinformatics, 20, 2985-2996."
      },
      {
        "ref_id": "6",
        "text": "Bickel, P. J., \\& Li, B. (2006). Regularization in statistics. Text. 15, 271-344."
      },
      {
        "ref_id": "7",
        "text": "Braga-Neto, U. M., \\& Dougherty, E. R. (2004). Is cross-validation valid for smallsample microarray classification? Bioinformatics, 20, 374-380."
      },
      {
        "ref_id": "8",
        "text": "Cawley, G. C., \\& Talbot, N. (2006). Gene selection in cancer classification using sparse logistic regression with Bayesian regularization. Bioinformatics, 22, $2348-2355$."
      },
      {
        "ref_id": "9",
        "text": "Cortes, C., \\& Mohri, M. (2004). AUC optimization vs. error rate minimization. Advances in neural information processing systems (Vol. 16). Cambridge, MA: The MIT Press."
      },
      {
        "ref_id": "10",
        "text": "Demšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7, 1-30."
      },
      {
        "ref_id": "11",
        "text": "Dudoit, S., Fridlyand, J., \\& Speed, T. P. (2002). Comparison of discrimination methods for the classification of tumors using gene expression data. Journal of the American Statistical Association, 97, 77-87."
      },
      {
        "ref_id": "12",
        "text": "Efron, B. (1983). Estimating the error rate of a prediction rule: Improvement on cross-validation. Journal of the American Statistical Association, 78, 316-331."
      },
      {
        "ref_id": "13",
        "text": "Eilers, P., Boer, J., van Ommen, G., \\& van Houwelingen, H. (2001). Classification of microarray data with penalized logistic regression. Proceedings of SPEE. Progress in Biomedical Optics and Images, 4266(2), 187-198."
      },
      {
        "ref_id": "14",
        "text": "Fan, J., \\& Li, R. (2006). Statistical challenges with high dimensionality: Feature selection in knowledge discovery. In Proceedings of the Madrid international congress of mathematicians (Vol. III, pp. 595-622)."
      },
      {
        "ref_id": "15",
        "text": "Firth, D. (1993). Bias reduction of maximum likelihood estimates. Biometrika, 80, 27-38."
      },
      {
        "ref_id": "16",
        "text": "Fort, G., \\& Lambert-Lacroix, S. (2005). Classification using partial least squares with penalized logistic regression. Bioinformatics, 21, 1104-1111."
      },
      {
        "ref_id": "17",
        "text": "Frank, I. E., \\& Friedman, J. H. (1993). A statistical view of some chemometric regression tools. Technometrics, 35, 109-148."
      },
      {
        "ref_id": "18",
        "text": "Fu, W. J. (1998). Penalized regression: The bridge versus the LASSO. Journal of Computational and Graphical Statistics, 7, 397-416."
      },
      {
        "ref_id": "19",
        "text": "Gao, S., \\& Shen, J. (2007). Asymptotic properties of a double penalized maximum likelihood estimator in logistic regression. Statistics and Probability Letters, 77, $925-930$."
      },
      {
        "ref_id": "20",
        "text": "Genkin, A., Lewis, D. D., \\& Madigan, D. (2007). Large-scale Bayesian logistic regression for text categorization. Technometrics, 49, 291-304."
      },
      {
        "ref_id": "21",
        "text": "Gohib, T. K, Slonim, D. K., Tamayo, P., Huard, C., Gaasenbeek, M., Mesirov, J. P., et al. (1999). Molecular classification of cancer: Class discovery and class prediction by gene expression monitoring. Science, 286, 531-537."
      },
      {
        "ref_id": "22",
        "text": "González, C., Lozano, J. A., \\& Larrañaga, P. (2002). Mathematical modelling of UMDA, algorithm with tournament selection. Behaviour on linear and quadratic functions. International Journal of Approximate Reasoning, 31, 313-340."
      },
      {
        "ref_id": "23",
        "text": "Guyon, I., Weston, J., Barnhill, S., \\& Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. Machine Learning, 46, $389-422$."
      },
      {
        "ref_id": "24",
        "text": "Hanley, J. A., \\& McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143, 29-36."
      },
      {
        "ref_id": "25",
        "text": "Hastie, T., \\& Tibshirani, R. (2004). Efficient quadratic regularization for expression arrays. Biostatistics, 5, 329-340."
      },
      {
        "ref_id": "26",
        "text": "Hastie, T., Tibshirani, R., \\& Friedman, J. (2001). The elements of statistical learning. Springer."
      },
      {
        "ref_id": "27",
        "text": "Hoerl, A. E., \\& Kennard, R. W. (1970). Ridge regression: Biased estimates for nonorthogonal problems. Technometrics, 12, 55-67."
      },
      {
        "ref_id": "28",
        "text": "Hosmer, D. W., \\& Lemeshow, S. (2000). Applied logistic regression (2nd ed.). New York: J. Wiley and Sons."
      },
      {
        "ref_id": "29",
        "text": "Huang, J., \\& Ling, C. X. (2005). Using AUC and accuracy in evaluating learning algorithms. IEEE Transactions on Knowledge and Data Engineering, 17, 299-310."
      },
      {
        "ref_id": "30",
        "text": "Ihaka, R., \\& Gentleman, R. (1996). R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5, 229-314."
      },
      {
        "ref_id": "31",
        "text": "Inza, I., Larrañaga, P., Blanco, R., \\& Cerrolaza, A. (2004). Filter versus wrapper gene selection approaches in DNA microarray domains. Artificial Intelligence in Medicine, 31, 91-103."
      },
      {
        "ref_id": "32",
        "text": "Keerthi, S. S., Duan, K. B., Shevade, S. K., \\& Poo, A. N. (2005). A fast dual algorithm for kernel logistic regression. Machine Learning, 61, 151-165."
      },
      {
        "ref_id": "33",
        "text": "Kiang, M. Y. (2003). A comparative assessment of classification methods. Decision Support Systems, 35, 441-454."
      },
      {
        "ref_id": "34",
        "text": "Kohavi, R., \\& John, G. H. (1997). Wrappers for feature subset selection. Artificial Intelligence, 97, 273-324."
      },
      {
        "ref_id": "35",
        "text": "Koh, K., Kim, S.-Y., \\& Boyd, S. (2007). An interior-point method for large-scale $I_{1}$ regularized logistic regression. Journal of Machine Learning Research, 8, $1519-1555$."
      },
      {
        "ref_id": "36",
        "text": "Larrañaga, P., Etxeberria, R., Lozano, J. A., \\& Peña, J. M. (2000). Optimization in continuous domains by learning and simulation of Gaussian networks. In Workshop in optimization by building and using probabilistic models. Genetic and evolutionary computation conference, GECCO 2000 (pp. 201-204)."
      },
      {
        "ref_id": "37",
        "text": "Larrañaga, P., \\& Lozano, J. A. (Eds.). (2002). Estimation of distribution algorithms. A new tool for evolutionary computation. Kluwer A.P."
      },
      {
        "ref_id": "38",
        "text": "Lasko, T. A., Bhagwat, J. G., Zou, K. H., \\& Ohno-Machado, L. (2005). The use of ROC curves in biomedical informatics. Journal of Biomedical Informatics, 38, 404-415."
      },
      {
        "ref_id": "39",
        "text": "Le Cessie, S., \\& van Houwelingen, J. C. (1992). Ridge estimators in logistic regression. Applied Statistics, 41, 191-201."
      },
      {
        "ref_id": "40",
        "text": "Lee, S.-I., Lee, H., Abbeel, P., \\& Ng, A. Y. (2006). Efficient L1 regularized logistic regression. Proceedings of the 21st national conference on artificial intelligence (AAAI-06) (pp. 1-9)."
      },
      {
        "ref_id": "41",
        "text": "Lee, J. W., Lee, J. B., Park, M., \\& Song, S. H. (2005). An extensive comparison of recent classification tools applied to microarray data. Computational Statistics and Data Analysis, 48, 869-885."
      },
      {
        "ref_id": "42",
        "text": "Lee, A., \\& Silvapulle, M. (1988). Ridge estimation in logistic regression. Communications in Statistics, Part B-Simulation and Computation, 17, 1231-1257."
      },
      {
        "ref_id": "43",
        "text": "Liao, J. G., \\& Chin, K.-V. (2007). Logistic regression for disease classification using microarray data: Model selection in a large $p$ and small $n$ case. Bioinformatics, 23, 1945-1951."
      },
      {
        "ref_id": "44",
        "text": "Liu, Z., Jiang, F., Tian, G., Wang, S., Sato, F., Meltzer, S. J., et al. (2007). Sparse logistic regression with $I_{p}$ penalty for biomarker identification. Statistical Applications in Genetics and Molecular Biology, 6 [Article 6]."
      },
      {
        "ref_id": "45",
        "text": "Liu, H., \\& Motoda, H. (2008). Computational methods of feature selection. Chapman and Hall(CRC Press)."
      },
      {
        "ref_id": "46",
        "text": "Lokhorst, J. (1999) The lasso and generalized linear models. Technical Report, University of Adelaide."
      },
      {
        "ref_id": "47",
        "text": "Lozano, J. A., Larrañaga, P., Inza, I., \\& Bengoetxea, E. (Eds.). (2006). Towards a new evolutionary computation. Advances in estimation of distribution algorithms. New York: Springer."
      },
      {
        "ref_id": "48",
        "text": "Ma, S., \\& Huang, J. (2005). Regularized ROC method for disease classification and biomarker selection with microarray data. Bioinformatics, 21, 4356-4362."
      },
      {
        "ref_id": "49",
        "text": "Meier, L., van de Geer, S., \\& Bühlmann, P. (2008). The group Lasso for logistic regression. Journal of the Royal Statistical Society, Series B, 70, 53-71."
      },
      {
        "ref_id": "50",
        "text": "Nakamichi, R. E., Imoto, S., \\& Miyano, S. (2004). Case-control study of binary disease trait considering interactions between SNPs and environmental effects using logistic regression. In Fourth IEEE symposium on bioinformatics and bioengineering (Vol. 21, pp. 73-78)."
      },
      {
        "ref_id": "51",
        "text": "Ng, A. (2004). Feature selection, $I_{1}$ vs. $I_{2}$-regularization, and rotational invariants. In Proceedings of the 21st international conference on machine learning."
      },
      {
        "ref_id": "52",
        "text": "Nguyen, D. V., \\& Rocke, D. M. (2002). Tumor classification by partial least squares using microarray gene expression data. Bioinformatics, 18, 39-50."
      },
      {
        "ref_id": "53",
        "text": "Park, M. Y., \\& Hastie, T. (2006). $I_{3}$-regularization path algorithm for generalized linear models. Technical Report, Stanford University."
      },
      {
        "ref_id": "54",
        "text": "Pelikan, M. (2005). Hierarchical Bayesian optimization algorithm: Toward a new generation of evolutionary algorithms. Springer."
      },
      {
        "ref_id": "55",
        "text": "Sha, F., Park, Y. A., \\& Saul, L. K. (2007). Multiplicative updates for $L_{1}$-regularized linear and logistic regression. Lecture notes in computer science (Vol. 4723). Springer."
      },
      {
        "ref_id": "56",
        "text": "Shen, L., \\& Tan, E. C. (2005). Dimension reduction-based penalized logistic regression for cancer classification using microarray data. IEEE Transactions on Computational Biology and Bioinformatics, 2, 166-175."
      },
      {
        "ref_id": "57",
        "text": "Shevade, S. K., \\& Keerthi, S. S. (2003). A simple and efficient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19, 2246-2253."
      },
      {
        "ref_id": "58",
        "text": "Thisted, R. A. (1988). Elements of statistical computing. New York: Chapman and Hall."
      },
      {
        "ref_id": "59",
        "text": "Tibshirani, R. (1996). Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society, Series B, 58, 267-288."
      },
      {
        "ref_id": "60",
        "text": "Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., \\& Knight, K. (2005). Sparsity and smothness via the fused lasso. Journal of the Royal Statistical Society, Series B, 67, $91-108$."
      },
      {
        "ref_id": "61",
        "text": "Uncu, O., \\& Türksen, I. B. (2007). A novel feature selection approach: Combining feature wrappers and filters. Information Sciences, 177, 449-466."
      },
      {
        "ref_id": "62",
        "text": "Vinterbo, S., \\& Ohno-Machado, L. (1999). A genetic algorithm to select variables in logistic regression: Example in the domain of myocardial infarct. Journal of the American Medical Informatics Association, 6, 984-988."
      },
      {
        "ref_id": "63",
        "text": "Weber, G., Vinterbo, S., \\& Ohno-Machado, L. (2004). Multivariate selection of genetic markers in diagnostic classification. Artificial Intelligence in Medicine, 31, 155-167."
      },
      {
        "ref_id": "64",
        "text": "West, M., Blanchette, C., Dressman, H., Huang, E., Ishida, S., Spang, R., et al. (2001). Predicting the clinical status of human breast cancer by using gene expression profiles. Proceedings of the National Academy of Sciences USA, 98(20), $11462-11467$."
      },
      {
        "ref_id": "65",
        "text": "Yuan, M., \\& Lin, Y. (2006). Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society, Series B, 68, 49-67."
      },
      {
        "ref_id": "66",
        "text": "Zhao, P., \\& Yu, B. (2007). Stagewise Lasso. Journal of Machine Learning Research, 8, 2701-2726."
      },
      {
        "ref_id": "67",
        "text": "Zhou, H., \\& Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67, 301-320."
      },
      {
        "ref_id": "68",
        "text": "Zhu, J., \\& Hastie, T. (2004). Classification of gene microarrays by penalized logistic regression. Biostatistics, 5, 427-443."
      },
      {
        "ref_id": "69",
        "text": "Zou, H. (2006). The adaptive Lasso and its oracle properties. Journal of the American Statistical Association, 101, 1418-1429."
      }
    ],
    "reference_count": 69,
    "pattern_matched": "(?:^|\\n)#+\\s*References?\\s*\\n"
  },
  "tables": [
    {
      "table_number": "1",
      "table_title": "Results of UMDA ${ }_{L}^{C c}$ vs. other logistic regressions for breast with the BSS/WSS criterion. and symbols are used for the comparisons UMDA ${ }_{L}^{C c}$ vs. Roca and UMDA ${ }_{L}^{C c}$ vs. Lasso, respectively. ${ }^{*}$ means that Roca or Lasso is statistically superior to UMDA ${ }_{L}^{C c}$ ( $p$-value $<0.05$ ).",
      "headers": [
        "$\\#$ <br> genes",
        "UMDA ${ }_{L}^{C c}$",
        "",
        "Roca",
        "",
        "Lasso",
        ""
      ],
      "rows": [
        [
          "",
          "Accur.",
          "AUC",
          "Accur.",
          "AUC",
          "Accur.",
          "AUC"
        ],
        [
          1,
          0.8613,
          0.9426,
          0.8643,
          0.9405,
          0.8593,
          0.9416
        ],
        [
          2,
          0.8421,
          0.9266,
          "0.8557 -",
          "0.9310 -",
          "0.8517 -",
          0.9235
        ],
        [
          3,
          0.9077,
          0.9577,
          0.9128,
          0.9545,
          0.856,
          0.939
        ],
        [
          4,
          0.9062,
          0.96,
          0.9208,
          0.9557,
          0.8605,
          0.9396
        ],
        [
          5,
          0.8921,
          0.9504,
          0.9104,
          0.9442,
          0.8587,
          0.9328
        ],
        [
          6,
          0.9167,
          0.9743,
          0.9334,
          0.9678,
          0.8605,
          0.9398
        ],
        [
          7,
          0.9103,
          0.9727,
          0.9292,
          0.9665,
          0.8614,
          0.9395
        ],
        [
          8,
          0.9249,
          0.9818,
          0.941,
          0.9771,
          0.8604,
          0.9421
        ],
        [
          9,
          0.9179,
          0.9794,
          0.9357,
          0.9753,
          0.8597,
          0.943
        ],
        [
          10,
          0.9213,
          0.985,
          0.9375,
          0.9747,
          0.8698,
          0.94
        ],
        [
          11,
          0.9307,
          0.9889,
          0.9422,
          0.9787,
          0.8712,
          0.9422
        ],
        [
          12,
          0.9224,
          0.9802,
          0.9348,
          0.9768,
          0.8749,
          0.9427
        ],
        [
          13,
          0.9245,
          0.9844,
          0.9348,
          0.976,
          0.8729,
          0.9406
        ],
        [
          14,
          0.9224,
          0.9861,
          0.9333,
          0.9753,
          0.8711,
          0.9393
        ],
        [
          15,
          0.9257,
          0.9875,
          0.931,
          0.9734,
          0.8719,
          0.9402
        ],
        [
          16,
          0.9258,
          0.9869,
          0.9289,
          0.9729,
          0.8715,
          0.9389
        ],
        [
          17,
          0.9207,
          0.9836,
          0.9267,
          0.9693,
          0.8715,
          0.9386
        ],
        [
          18,
          0.9156,
          0.9819,
          0.9252,
          0.9692,
          0.8679,
          0.9378
        ],
        [
          19,
          0.9165,
          0.9802,
          0.9236,
          0.9649,
          0.8684,
          0.9368
        ],
        [
          20,
          0.9287,
          0.9835,
          0.9334,
          0.9729,
          0.8675,
          0.9382
        ],
        [
          21,
          0.9309,
          0.9855,
          0.9362,
          0.9768,
          0.8668,
          0.9366
        ],
        [
          22,
          0.9327,
          0.985,
          0.9378,
          0.978,
          0.866,
          0.936
        ],
        [
          23,
          0.9371,
          0.9856,
          0.9413,
          0.978,
          0.8643,
          0.9369
        ],
        [
          24,
          0.9355,
          0.9847,
          0.9408,
          0.9779,
          0.8636,
          0.9356
        ],
        [
          25,
          0.9333,
          0.9847,
          0.938,
          0.9768,
          0.8645,
          0.9356
        ],
        [
          26,
          0.934,
          0.9843,
          "",
          "",
          0.8631,
          0.9355
        ],
        [
          27,
          0.9337,
          0.9839,
          "",
          "",
          0.8621,
          0.9346
        ],
        [
          28,
          0.9329,
          0.9834,
          "",
          "",
          0.8648,
          0.9351
        ],
        [
          29,
          0.9325,
          0.9833,
          "",
          "",
          0.8606,
          0.932
        ],
        [
          30,
          0.9345,
          0.9839,
          "",
          "",
          0.8619,
          0.9344
        ],
        [
          31,
          0.9326,
          0.982,
          "",
          "",
          0.863,
          0.9343
        ],
        [
          32,
          0.9305,
          0.9811,
          "",
          "",
          0.8613,
          0.9329
        ],
        [
          33,
          0.931,
          0.981,
          "",
          "",
          0.8631,
          0.932
        ],
        [
          34,
          0.9315,
          0.9813,
          "",
          "",
          0.8597,
          0.9326
        ],
        [
          35,
          0.9301,
          0.9807,
          "",
          "",
          0.8606,
          0.9342
        ],
        [
          36,
          0.9377,
          0.9828,
          "",
          "",
          0.8593,
          0.9341
        ],
        [
          37,
          0.9363,
          0.9822,
          "",
          "",
          0.8607,
          0.9328
        ],
        [
          38,
          0.936,
          0.9817,
          "",
          "",
          0.8585,
          0.9315
        ],
        [
          39,
          0.9379,
          0.9837,
          "",
          "",
          0.8585,
          0.933
        ],
        [
          40,
          0.9416,
          0.9865,
          "",
          "",
          0.8558,
          0.9309
        ],
        [
          41,
          0.9422,
          0.9862,
          "",
          "",
          0.8583,
          0.931
        ],
        [
          42,
          0.94,
          0.985,
          "",
          "",
          0.8583,
          0.9311
        ],
        [
          43,
          0.9395,
          0.9857,
          "",
          "",
          0.8566,
          0.9291
        ],
        [
          44,
          0.9382,
          0.9847,
          "",
          "",
          0.8562,
          0.93
        ],
        [
          45,
          0.9396,
          0.9851,
          "",
          "",
          0.8593,
          0.9315
        ],
        [
          46,
          0.9386,
          0.9844,
          "",
          "",
          0.8564,
          0.9311
        ],
        [
          47,
          0.9408,
          0.9847,
          "",
          "",
          0.8542,
          0.9286
        ],
        [
          48,
          0.9384,
          0.9842,
          "",
          "",
          0.8562,
          0.9299
        ],
        [
          49,
          0.9489,
          0.9894,
          "",
          "",
          0.8538,
          0.9291
        ],
        [
          50,
          0.9502,
          0.9889,
          "",
          "",
          0.8545,
          0.9297
        ]
      ],
      "row_count": 51,
      "column_count": 7
    },
    {
      "table_number": "2",
      "table_title": "Results of UMDA ${ }_{1}^{C}$ vs. other logistic regressions for Colon with the BSS/WSS criterion. and symbols are used for the comparisons UMDA ${ }_{1}^{C}$, vs. Ross and UMDA ${ }_{1}^{C}$ vs. Leao, respectively. $\\boldsymbol{\\nabla}$ means that Ross or Leao is statistically superior to UMDA ${ }_{1}^{C}$ ( $p$ value $<0.05$ ).",
      "headers": [
        "$\\#$ <br> genes",
        "UMDA ${ }_{1}^{C}$",
        "",
        "Ross",
        "",
        "Leao",
        ""
      ],
      "rows": [
        [
          "",
          "Accur.",
          "AUC",
          "Accur.",
          "AUC",
          "Accur.",
          "AUC"
        ],
        [
          1,
          0.8423,
          0.8188,
          0.8487,
          0.8213,
          0.8016,
          0.8208
        ],
        [
          2,
          0.8233,
          0.8163,
          0.8255,
          0.8224,
          0.8142,
          0.8223
        ],
        [
          3,
          0.8204,
          0.8507,
          0.8277,
          0.8591,
          0.7965,
          0.84
        ],
        [
          4,
          0.8151,
          0.8442,
          0.8205,
          0.8443,
          0.7956,
          0.8314
        ],
        [
          5,
          0.8348,
          0.8316,
          0.847,
          0.823,
          0.7927,
          0.829
        ],
        [
          6,
          0.8406,
          0.8365,
          0.8624,
          0.8466,
          0.7907,
          0.8274
        ],
        [
          7,
          0.8521,
          0.8436,
          0.8629,
          0.8484,
          0.8014,
          0.8424
        ],
        [
          8,
          0.8449,
          0.8345,
          0.8634,
          0.8485,
          0.8006,
          0.8423
        ],
        [
          9,
          0.8409,
          0.8256,
          0.852,
          0.8361,
          0.7993,
          0.8407
        ],
        [
          10,
          0.8589,
          0.9219,
          0.9097,
          0.9219,
          0.7883,
          0.8981
        ],
        [
          11,
          0.8663,
          0.9248,
          0.9155,
          0.918,
          0.7864,
          0.896
        ],
        [
          12,
          0.8678,
          0.9224,
          0.9123,
          0.9179,
          0.7842,
          0.8928
        ],
        [
          13,
          0.8625,
          0.9235,
          0.9059,
          0.9087,
          0.7848,
          0.8932
        ],
        [
          14,
          0.8853,
          0.9477,
          0.9126,
          0.9261,
          0.7817,
          0.898
        ],
        [
          15,
          0.8899,
          0.9479,
          0.9158,
          0.9305,
          0.7833,
          0.8963
        ],
        [
          16,
          0.8859,
          0.9453,
          0.9115,
          0.9272,
          0.7839,
          0.8944
        ],
        [
          17,
          0.8985,
          0.9595,
          0.9167,
          0.9387,
          0.7845,
          0.8957
        ],
        [
          18,
          0.8967,
          0.9601,
          0.9154,
          0.9413,
          0.7844,
          0.8948
        ],
        [
          19,
          0.8939,
          0.9547,
          0.9123,
          0.9374,
          0.7846,
          0.8969
        ],
        [
          20,
          0.8921,
          0.9522,
          0.908,
          0.9357,
          0.7841,
          0.8983
        ],
        [
          21,
          0.8869,
          0.949,
          0.9031,
          0.9299,
          0.7841,
          0.8937
        ],
        [
          22,
          0.887,
          0.9507,
          0.905,
          0.9338,
          0.7828,
          0.8942
        ],
        [
          23,
          0.8853,
          0.9488,
          0.9023,
          0.9317,
          0.7828,
          0.8942
        ],
        [
          24,
          0.8871,
          0.9481,
          0.9017,
          0.9283,
          0.7842,
          0.8949
        ],
        [
          25,
          0.8855,
          0.9456,
          0.901,
          0.9283,
          0.7832,
          0.893
        ],
        [
          26,
          0.8826,
          0.9424,
          0.8983,
          0.9227,
          0.7831,
          0.8919
        ],
        [
          27,
          0.8832,
          0.9433,
          0.9025,
          0.9308,
          0.7824,
          0.8936
        ],
        [
          28,
          0.8808,
          0.9404,
          0.899,
          0.9287,
          0.7836,
          0.8949
        ],
        [
          29,
          0.8779,
          0.9395,
          0.898,
          0.9274,
          0.7806,
          0.8933
        ],
        [
          30,
          0.8738,
          0.9363,
          0.8942,
          0.9256,
          0.7829,
          0.8941
        ],
        [
          31,
          0.8785,
          0.9352,
          0.8955,
          0.9237,
          0.7816,
          0.8934
        ],
        [
          32,
          0.8744,
          0.9302,
          0.8924,
          0.9205,
          0.7821,
          0.8932
        ],
        [
          33,
          0.883,
          0.9395,
          0.8996,
          0.9315,
          0.7821,
          0.8928
        ],
        [
          34,
          0.8807,
          0.9371,
          0.8994,
          0.9322,
          0.7809,
          0.8921
        ],
        [
          35,
          0.879,
          0.9355,
          0.9015,
          0.933,
          0.7797,
          0.8961
        ],
        [
          36,
          0.8798,
          0.9362,
          "",
          "",
          0.7808,
          0.8947
        ],
        [
          37,
          0.88,
          0.9366,
          "",
          "",
          0.7795,
          0.896
        ],
        [
          38,
          0.8776,
          0.9347,
          "",
          "",
          0.7779,
          0.8937
        ],
        [
          39,
          0.8763,
          0.9332,
          "",
          "",
          0.7795,
          0.8926
        ],
        [
          40,
          0.8781,
          0.9369,
          "",
          "",
          0.7794,
          0.8951
        ],
        [
          41,
          0.8794,
          0.9339,
          "",
          "",
          0.7785,
          0.8928
        ],
        [
          42,
          0.8767,
          0.9326,
          "",
          "",
          0.7789,
          0.893
        ],
        [
          43,
          0.8807,
          0.9361,
          "",
          "",
          0.7774,
          0.8916
        ],
        [
          44,
          0.8778,
          0.933,
          "",
          "",
          0.7794,
          0.8916
        ],
        [
          45,
          0.8795,
          0.9336,
          "",
          "",
          0.7781,
          0.8922
        ],
        [
          46,
          0.8821,
          0.9365,
          "",
          "",
          0.7782,
          0.8919
        ],
        [
          47,
          0.8805,
          0.9351,
          "",
          "",
          0.7793,
          0.8928
        ],
        [
          48,
          0.8779,
          0.9328,
          "",
          "",
          0.7804,
          0.8923
        ],
        [
          49,
          0.8798,
          0.9353,
          "",
          "",
          0.777,
          0.8905
        ],
        [
          50,
          0.8795,
          0.9343,
          "",
          "",
          0.7797,
          0.8912
        ]
      ],
      "row_count": 51,
      "column_count": 7
    },
    {
      "table_number": "3",
      "table_title": "Results of UMDA ${ }_{1}^{C}$ vs. other logistic regressions for Leukemia with the BSS/WSS criterion. and symbols are used for the comparisons UMDA ${ }_{1}^{C}$ vs. Ross and UMDA ${ }_{1}^{C}$ vs. Leao, respectively. $\\nabla$ means that Ross or Leao is statistically superior to UMDA ${ }_{1}^{C}(p$-value $<0.05)$.",
      "headers": [
        "$\\#$ <br> genes",
        "UMDA ${ }_{1}^{C}$",
        "",
        "Ross",
        "",
        "Leao",
        ""
      ],
      "rows": [
        [
          "",
          "Accur.",
          "AUC",
          "Accur.",
          "AUC",
          "Accur.",
          "AUC"
        ],
        [
          1,
          0.9326,
          0.9793,
          0.9362,
          0.9784,
          0.855,
          0.9786
        ],
        [
          2,
          0.9228,
          0.9815,
          0.9246,
          0.981,
          0.8631,
          0.975
        ],
        [
          3,
          0.9445,
          0.9934,
          0.9559,
          0.9933,
          0.8518,
          0.9796
        ],
        [
          4,
          0.9441,
          0.992,
          0.9578,
          0.9893,
          0.8525,
          0.9816
        ],
        [
          5,
          0.9378,
          0.987,
          0.9494,
          0.9831,
          0.8504,
          0.9805
        ],
        [
          6,
          0.9342,
          0.9817,
          0.9481,
          0.9766,
          0.858,
          0.9769
        ],
        [
          7,
          0.937,
          0.9836,
          0.9544,
          0.9783,
          0.8547,
          0.9768
        ],
        [
          8,
          0.9325,
          0.9817,
          0.9548,
          0.9775,
          0.8536,
          0.9763
        ],
        [
          9,
          0.9382,
          0.9801,
          0.9553,
          0.975,
          0.8529,
          0.9759
        ],
        [
          10,
          0.9383,
          0.9772,
          0.9546,
          0.9713,
          0.8529,
          0.9756
        ],
        [
          11,
          0.9455,
          0.9792,
          0.9587,
          0.9761,
          0.8496,
          0.9747
        ],
        [
          12,
          0.9459,
          0.973,
          0.9553,
          0.9667,
          0.8488,
          0.973
        ],
        [
          13,
          0.9423,
          0.9674,
          0.9537,
          0.9617,
          0.8467,
          0.9714
        ],
        [
          14,
          0.945,
          0.9662,
          0.9541,
          0.9594,
          0.8459,
          0.9711
        ],
        [
          15,
          0.9444,
          0.9693,
          0.9577,
          0.9677,
          0.8469,
          0.9713
        ],
        [
          16,
          0.9448,
          0.9716,
          0.9556,
          0.9659,
          0.8459,
          0.9689
        ],
        [
          17,
          0.9465,
          0.9647,
          0.9551,
          0.9619,
          0.8445,
          0.968
        ],
        [
          18,
          0.9441,
          0.9657,
          0.9562,
          0.9666,
          0.8456,
          0.9691
        ],
        [
          19,
          0.9489,
          0.9627,
          0.9568,
          0.9644,
          0.8436,
          0.9681
        ],
        [
          20,
          0.9506,
          0.9652,
          0.9617,
          0.9685,
          0.8354,
          0.9698
        ],
        [
          21,
          0.9579,
          0.9743,
          0.9649,
          0.9751,
          0.8356,
          0.9716
        ],
        [
          22,
          0.9557,
          0.9738,
          0.9663,
          0.9775,
          0.8347,
          0.9711
        ],
        [
          23,
          0.9576,
          0.9738,
          0.9663,
          0.9782,
          0.8368,
          0.9712
        ],
        [
          24,
          0.9611,
          0.9795,
          0.97,
          0.9812,
          0.8352,
          0.9713
        ],
        [
          25,
          0.9626,
          0.9832,
          0.9716,
          0.9831,
          0.8367,
          0.9717
        ],
        [
          26,
          0.963,
          0.986,
          0.9729,
          0.9842,
          0.8363,
          0.9717
        ],
        [
          27,
          0.9631,
          0.9853,
          0.973,
          0.9826,
          0.8361,
          0.9718
        ],
        [
          28,
          0.9627,
          0.9849,
          0.9728,
          0.9833,
          0.8356,
          0.9711
        ],
        [
          29,
          0.9623,
          0.9846,
          0.9717,
          0.9831,
          0.8358,
          0.971
        ],
        [
          30,
          0.9609,
          0.9846,
          0.9714,
          0.9827,
          0.8349,
          0.9712
        ],
        [
          31,
          0.9618,
          0.9843,
          0.9707,
          0.9822,
          0.836,
          0.9711
        ],
        [
          32,
          0.9626,
          0.9867,
          0.9719,
          0.9844,
          0.8358,
          0.973
        ],
        [
          33,
          0.9622,
          0.9859,
          0.9725,
          0.9841,
          0.8337,
          0.9704
        ],
        [
          34,
          0.9676,
          0.9894,
          0.9795,
          0.9921,
          0.8313,
          0.9718
        ],
        [
          35,
          0.9665,
          0.9897,
          0.9781,
          0.9918,
          0.8325,
          0.9726
        ],
        [
          36,
          0.9725,
          0.9926,
          0.9842,
          0.9955,
          0.832,
          0.9722
        ],
        [
          37,
          0.9713,
          0.9923,
          0.9837,
          0.995,
          0.8313,
          0.9719
        ],
        [
          38,
          0.9702,
          0.992,
          0.9831,
          0.9952,
          0.8322,
          0.9723
        ],
        [
          39,
          0.9698,
          0.9918,
          0.9814,
          0.9945,
          0.8322,
          0.9724
        ],
        [
          40,
          0.9703,
          0.9914,
          0.9824,
          0.9948,
          0.8328,
          0.972
        ],
        [
          41,
          0.9712,
          0.9918,
          "",
          "",
          0.8324,
          0.9724
        ],
        [
          42,
          0.9713,
          0.9927,
          "",
          "",
          0.8326,
          0.9735
        ],
        [
          43,
          0.9711,
          0.9914,
          "",
          "",
          0.8314,
          0.9716
        ],
        [
          44,
          0.9725,
          0.9904,
          "",
          "",
          0.8323,
          0.972
        ],
        [
          45,
          0.9761,
          0.9925,
          "",
          "",
          0.8319,
          0.9716
        ],
        [
          46,
          0.977,
          0.9923,
          "",
          "",
          0.8326,
          0.9721
        ],
        [
          47,
          0.9778,
          0.9938,
          "",
          "",
          0.8326,
          0.972
        ],
        [
          48,
          0.9753,
          0.9926,
          "",
          "",
          0.8316,
          0.9728
        ],
        [
          49,
          0.9757,
          0.9924,
          "",
          "",
          0.8306,
          0.9713
        ],
        [
          50,
          0.9756,
          0.9925,
          "",
          "",
          0.8298,
          0.9716
        ]
      ],
      "row_count": 51,
      "column_count": 7
    },
    {
      "table_number": "4",
      "table_title": "Some statistical measures of the run times (in seconds).",
      "headers": [
        "",
        "min",
        "mean",
        "max"
      ],
      "rows": [
        [
          "Breast",
          "UMDA $_{1}^{C}$",
          0.31,
          1.76
        ],
        [
          "",
          "Ross",
          0.28,
          0.65
        ],
        [
          "",
          "Leao",
          0.27,
          0.45
        ],
        [
          "Colon",
          "UMDA $_{1}^{C}$",
          0.91,
          8.45
        ],
        [
          "",
          "Ross",
          0.27,
          0.78
        ],
        [
          "",
          "Leao",
          0.26,
          0.53
        ],
        [
          "Leukemia",
          "UMDA $_{1}^{C}$",
          0.35,
          3.8
        ],
        [
          "",
          "Ross",
          0.18,
          0.59
        ],
        [
          "",
          "Leao",
          0.18,
          0.39
        ]
      ],
      "row_count": 9,
      "column_count": 4
    }
  ]
}