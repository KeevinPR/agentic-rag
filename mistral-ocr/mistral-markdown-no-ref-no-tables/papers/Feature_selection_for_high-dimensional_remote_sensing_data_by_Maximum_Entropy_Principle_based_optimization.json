{
  "metadata": {
    "file_path": "/Users/id05309/Documents/tfm/mistral/mistral-markdown/2001/Feature selection for high-dimensional remote sensing data by Maximum Entropy Principle based optimization.md",
    "filename": "Feature selection for high-dimensional remote sensing data by Maximum Entropy Principle based optimization.md",
    "title": "Feature selection for high-dimensional remote sensing data by Maximum Entropy Principle based optimization",
    "year": "2001"
  },
  "references": {
    "header": "## References",
    "content": "[1] K. Fukanaga, Introduction to Statistical Pattern Recognition. (Academic Press, San Diego, California, 1990).\n[2] D. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning (Addison-Wesley, Reading, MA, 1989).\n[3] L. Yan and David J. Miller, General statistical inference for discrete and mixed spaces by an approximate application of the maximum entropy principle. IEEE trans. Neural Networks, vol.11, No.3, May 2000.\n[4] H. Muhlenbein, G. Paas, From combination of genes to the estimation of distributions: Binary parameters in H.M. Voigt, et al.(Eds.) Lecture Notes in Computer Science 1411: Parallel Problem Solving from Nature - PPSN IV, 1996, pp. $178-187$.\n[5] S. Baluja, Population-based increment learning: A method for integrating genetic search based function optimization and competitive learning. Technical Report CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, PA, 1994.\n[6] G.R. Harik, F.G. Lobo, D.E. Goldberg, The compact genetic algorithm IlliGAl Report 97006, Urbana: University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory, 1997.\n[7] H. Muhlenbein, The equation for response to selection and its use for prediction, Evolutionary Computation 5 (3) (1997) 303-346.\n[8] G. Syswerda, Uniform crossover in genetic algorithms, in Proceedings of the international conference on genetic algorithm 3, Arlington, VA, 1989, pp. 2-9.\n[9] J.S. De Bonet, C.L. Isbell, P. Viola, MIMIC: Finding optima by estimating probability densities, in M. Mozer, M. Jordan, T. Petsche(Eds.) Advances in Neural Information Processing Systems 9, MIT Press, Cambridge, MA, 1997.\n[10] S. Baluja, S. Davies, Using optimal dependencetrees for combinatorial optimization:learning the structure of the search space, in Proceedings of the fourteenth international conference on machine learning, Nashville, TN, 1997, pp. 30-38.\n[11] M. Pelikan, D.E. Goldberg, E. Cantu-Paz, BOA: the bayesian optimization algorithm,IlliGAL Report 99003, Urbana: University of Illinois at Urbaba-Champaign, Illinois Genetic Algorithms Laboratory, 1999.\n[12] I. Inza, P. Larraaga, R. Etxeberria, B. Sierra, Feature subset selection by bayesian network based optimizaiton, Artificial Intelligence, 123(1-2), 157184, 2000.\n[13] http://dynamo.cen.purdue.edu/ $\\cdot$ bichl/MultiSpec/documentation.html $19 / 2,153,1997$.\n[14] D. Landgrebe, Information Extraction Principles and Methods for Multispectral and Hyperspectral data, in Information processing for Remote Sensing, ed. C.H. Chen (World Scientific, USA, 2000).\n[15] C. Lee and D. Landgrebe, IEEE Transactions on Pattern Analysis and Machine Intelligence, 15/4, 388, 1993.\n[16] E.T. Jaynes, Papers on probability, statistics and statistical physics. Dordrecht, The Netherlands, 1982.\n[17] P. Cheeseman, A method of computing generalized Bayesian probability values for expert systems, in Proc. 8th Int. Joint Conf. AI, vol.1, 1983, pp.198202.\n[18] H.H. Ku and S. Kullback, Approximating discrete probability distributions, IEEE trans. Information Theory, vol.IT-15, No.4, pp.444-447, 1969.\n[19] R.Kohavi, D. Sommerfield, J. Dougherty, Data mining using MLC++, a machine learning libray in $C++$, Int. Journal of Artifical Intelligence Tools 6(4), 1997, pp.537-566.\n[20] http://lancet.mit.edu/ga/\n[21] S.A. Goldman, Efficient methods for calculating maximum entropy distributions, MIT/LCS/TR391, May, 1987.\n[22] A.W. Moore and M.S. Lee. Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets, Carnegie Mellon University, Robotics Institute, CMU-Ri-TR-97-27, July, 1997.",
    "references": [
      {
        "ref_id": "1",
        "text": "K. Fukanaga, Introduction to Statistical Pattern Recognition. (Academic Press, San Diego, California, 1990)."
      },
      {
        "ref_id": "2",
        "text": "D. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning (Addison-Wesley, Reading, MA, 1989)."
      },
      {
        "ref_id": "3",
        "text": "L. Yan and David J. Miller, General statistical inference for discrete and mixed spaces by an approximate application of the maximum entropy principle. IEEE trans. Neural Networks, vol.11, No.3, May 2000."
      },
      {
        "ref_id": "4",
        "text": "H. Muhlenbein, G. Paas, From combination of genes to the estimation of distributions: Binary parameters in H.M. Voigt, et al.(Eds.) Lecture Notes in Computer Science 1411: Parallel Problem Solving from Nature - PPSN IV, 1996, pp. $178-187$."
      },
      {
        "ref_id": "5",
        "text": "S. Baluja, Population-based increment learning: A method for integrating genetic search based function optimization and competitive learning. Technical Report CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, PA, 1994."
      },
      {
        "ref_id": "6",
        "text": "G.R. Harik, F.G. Lobo, D.E. Goldberg, The compact genetic algorithm IlliGAl Report 97006, Urbana: University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory, 1997."
      },
      {
        "ref_id": "7",
        "text": "H. Muhlenbein, The equation for response to selection and its use for prediction, Evolutionary Computation 5 (3) (1997) 303-346."
      },
      {
        "ref_id": "8",
        "text": "G. Syswerda, Uniform crossover in genetic algorithms, in Proceedings of the international conference on genetic algorithm 3, Arlington, VA, 1989, pp. 2-9."
      },
      {
        "ref_id": "9",
        "text": "J.S. De Bonet, C.L. Isbell, P. Viola, MIMIC: Finding optima by estimating probability densities, in M. Mozer, M. Jordan, T. Petsche(Eds.) Advances in Neural Information Processing Systems 9, MIT Press, Cambridge, MA, 1997."
      },
      {
        "ref_id": "10",
        "text": "S. Baluja, S. Davies, Using optimal dependencetrees for combinatorial optimization:learning the structure of the search space, in Proceedings of the fourteenth international conference on machine learning, Nashville, TN, 1997, pp. 30-38."
      },
      {
        "ref_id": "11",
        "text": "M. Pelikan, D.E. Goldberg, E. Cantu-Paz, BOA: the bayesian optimization algorithm,IlliGAL Report 99003, Urbana: University of Illinois at Urbaba-Champaign, Illinois Genetic Algorithms Laboratory, 1999."
      },
      {
        "ref_id": "12",
        "text": "I. Inza, P. Larraaga, R. Etxeberria, B. Sierra, Feature subset selection by bayesian network based optimizaiton, Artificial Intelligence, 123(1-2), 157184, 2000."
      },
      {
        "ref_id": "13",
        "text": "http://dynamo.cen.purdue.edu/ $\\cdot$ bichl/MultiSpec/documentation.html $19 / 2,153,1997$."
      },
      {
        "ref_id": "14",
        "text": "D. Landgrebe, Information Extraction Principles and Methods for Multispectral and Hyperspectral data, in Information processing for Remote Sensing, ed. C.H. Chen (World Scientific, USA, 2000)."
      },
      {
        "ref_id": "15",
        "text": "C. Lee and D. Landgrebe, IEEE Transactions on Pattern Analysis and Machine Intelligence, 15/4, 388, 1993."
      },
      {
        "ref_id": "16",
        "text": "E.T. Jaynes, Papers on probability, statistics and statistical physics. Dordrecht, The Netherlands, 1982."
      },
      {
        "ref_id": "17",
        "text": "P. Cheeseman, A method of computing generalized Bayesian probability values for expert systems, in Proc. 8th Int. Joint Conf. AI, vol.1, 1983, pp.198202."
      },
      {
        "ref_id": "18",
        "text": "H.H. Ku and S. Kullback, Approximating discrete probability distributions, IEEE trans. Information Theory, vol.IT-15, No.4, pp.444-447, 1969."
      },
      {
        "ref_id": "19",
        "text": "R.Kohavi, D. Sommerfield, J. Dougherty, Data mining using MLC++, a machine learning libray in $C++$, Int. Journal of Artifical Intelligence Tools 6(4), 1997, pp.537-566."
      },
      {
        "ref_id": "20",
        "text": "http://lancet.mit.edu/ga/"
      },
      {
        "ref_id": "21",
        "text": "S.A. Goldman, Efficient methods for calculating maximum entropy distributions, MIT/LCS/TR391, May, 1987."
      },
      {
        "ref_id": "22",
        "text": "A.W. Moore and M.S. Lee. Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets, Carnegie Mellon University, Robotics Institute, CMU-Ri-TR-97-27, July, 1997."
      }
    ],
    "reference_count": 22,
    "pattern_matched": "(?:^|\\n)#+\\s*References?\\s*\\n"
  },
  "tables": [
    {
      "table_number": null,
      "table_title": null,
      "headers": [
        "",
        "pop. size",
        "learning alg.",
        "error rate",
        "bands"
      ],
      "rows": [
        [
          "EBNA",
          1000,
          3,
          6.79,
          12
        ],
        [
          "ME",
          1000,
          3,
          4.5,
          15
        ]
      ],
      "row_count": 2,
      "column_count": 5
    }
  ]
}