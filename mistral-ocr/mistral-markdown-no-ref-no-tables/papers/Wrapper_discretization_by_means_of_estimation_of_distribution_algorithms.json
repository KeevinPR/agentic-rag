{
  "metadata": {
    "file_path": "/Users/id05309/Documents/tfm/mistral/mistral-markdown/2007/Wrapper discretization by means of estimation of distribution algorithms.md",
    "filename": "Wrapper discretization by means of estimation of distribution algorithms.md",
    "title": "Wrapper discretization by means of estimation of distribution algorithms",
    "year": "2007"
  },
  "references": {
    "header": "## References",
    "content": "[1] J. Bacardit and J.M. Garrell, Evolving Multiple Discretizations with Adaptive Intervals for a Pittsburgh Rule-Based Learning Classifier System, In Proceedings of the Genetic and Evolutionary Computation Conference, volume 2724 of Lecture Notes in Computer Science, pages 1818-1831, Chicago (Illinois,USA), 2003, Springer-Verlag.\n[2] S.D. Bay, Multivariate Discretization of Continuous Variables for Set Mining, In Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 315-319, Boston (Massachusetts, USA), 2000. ACM Press.\n[3] R. Blanco, I. Inza and P. Larrañaga, Learning Bayesian networks in the space of structures by estimation of distribution algorithms, International Journal of Intelligent Systems 18(2) (2003), 205-220.\n[4] J.S. De Bonet, C. Isbell and P. Viola, MIMIC: Finding optima by estimating probability densities, in: Advances in Neural Information Processing, (Vol. 9), M. Mozer M. Jordan and M. Perrone, eds, Denver 1996, MIT Press, Cambridge.\n[5] M. Boullé, Khiops: A Discretization Method of Continuous Attributes with Guaranteed Resistance to Noise, In Proceedings of the 3rd International Conference on Machine Learning and Data Mining, Volume 2734 of Lecture Notes in Computer Science, Leipzig (Germany), 2003, Springer-Verlag, 50-64.\n[6] M. Boullé, Khiops: A statistical discretization method of continuous attributes, Machine Learning 55(1) (2004), 53-69.\n[7] M. Boullé, A Grouping Method for Categorical Attributes having Very Large Number of Values, In Proceedings of the 4th International Conference on Machine Learning and Data Mining in Pattern Recognition, Volume 3587 of Lecture Notes in Computer Science, pages 228-242, Leipzig (Germany), 2005, Springer-Verlag.\n[8] M. Boullé, Optimal bin number for equal frequency discretizations in supervised learning, Intelligent Data Analysis 9(2) (2005), 175-188.\n[9] R. Butterworth, D.A. Simovici, G.S. Santos and L. Ohno-Machado, A greedy algorithm for supervised discretization, Journal of Biomedical Informatics 37(4) (2004), 285-292.\n[10] E. Castillo, J.M. Gutierrez and A.S. Hadi, Expert Systems and Probabilistic Network Models, Springer-Verlag, 1996.\n[11] J. Catlett, On Changing Continuous Attributes into Ordered Discrete Attributes, In Proceedings of the 5th European Working Session on Learning, Vol. 482 of Lecture Notes in Computer Science, pages 164-178, Porto (Portugal), 1991, Springer-Verlag.\n[12] J.Y. Ching, A.C. Wong and K.C.C. Chan, Class-dependent discretization for inductive learning from continuous and mixed-mode data, IEEE Transactions on Pattern Analysis and Machine Learning 17(7) (1995), 641-651.\n[13] B.S. Chlebus and S.H. Nguyen, Discretization Problem for Rough Set Methods, In Proceedings of the 1st International Conference on Rough Sets and Current Trend in Computing, Volume 1424 of Lecture Notes in Computer Science, pages 545-552, Warsaw (Poland), 1998, Springer-Verlag.\n[14] E. Consortium, Elvira: An Environment for Creating and using Probabilistic Graphical Models, In Proceedings of the 1st European Workshop on Probabilistic Graphical Models, Cuenca (Spain), 2002.\n\n[15] J. Demsar, Statistical comparisons of classifiers over multiple data sets, Journal of Machine Learning Research 7 (2006), $1-30$.\n[16] P. Domingos and M. Pazzani, On the optimality of the simple Bayesian classifier under zero-one loss, Machine Learning 29(2-3) (1997), 103-130.\n[17] J. Dougherty, R. Kohavi and M. Sahami, Supervised and Unsupervised Discretization of Continuous Features, In Proceedings of the 12th International Conference on Machine Learning, Lecture Notes in Artificial Intelligence, pages 194-202, San Francisco (CA, USA), 1995.\n[18] R.O. Duda and P.E. Hart, Pattern Classification and Scene Analysis, John Wiley and Sons, 1973.\n[19] R. Etxeberria and P. Larrañaga, Global Optimization using Bayesian Networks, In Proceedings of the 2nd Symposium on Articial Intelligence Adaptive Systems, Volume 9, pages 332-339, La Habana (Cuba), 1999.\n[20] U.M. Fayyad and K.B. Irani, On the handling of continuous-valued attributes in decision tree generation, Machine Learning 8(1) (1992), 87-102.\n[21] U.M. Fayyad and K.B. Irani, Multi-Interval Discretization of Continuousvalued Attributes for Classification Learning, In Proceedings of the 13th International Joint Conference on Artificial Intelligence, pages 1022-1027, Chambery (France), 1993. Morgan Kaufmann.\n[22] S. Ferrandiz and M. Boullé, Multivariate Discretization by Recursive Supervised Bipartition of Graph, In Machine Learning and Data Mining, Volume 3587 of Lecture Notes in Artificial Intelligence, pages 253-264, Leipzig (Germany), 2005, Springer-Verlag.\n[23] S. Ferrandiz and M. Boullé, Supervised Evaluation of Dataset Partitions: Advantages and Practice, In Machine Learning and Data Mining, Volume 3587 of Lecture Notes in Artificial Intelligence, pages 600-609, Leipzig (Germany), 2005, Springer-Verlag.\n[24] J. Gama, Dynamic Discretization of Continuous Attributes, In Proceedings of the 6th Iberoamerican Conference on Artificial Intelligence, Lecture Notes in Artificial Intelligence, pages 160-169, Lisbon (Portugal), 1998.\n[25] P. Geurts and L. Wehenkel, Investigation and Reduction of Discretization Variance in Decision tree Induction, In Proceedings of the 11th European Conference on Machine Learning, Volume 1810 of Lecture Notes in Computer Science, pages 162-170, Cataluña (Spain), 2000, Springer-Verlag.\n[26] R. Giráldez, J.S. Aguilar-Ruiz and J.C. Riquelme Santos, Natural Coding: A More Efficient Representation for Evolutionary Learning, In Proceedings of the Genetic and Evolutionary Computation Conference, Vol. 2723 of Lecture Notes in Computer Science, pages 979-990, Chicago (Illinois,USA), 2003, Springer-Verlag.\n[27] C. González, J.A. Lozano and P. Larrañaga. Mathematical modeling of UMDA algorithm with tournament selection, International Journal of Approximate Reasoning (31) (2002), 313-340.\n[28] K. Grabczewski, SSV Criterion Based Discretization for Naive Bayes Classifiers, In Proceedings of the International Conference on Artificial Intelligence and Soft Computing, Volume 3070 of Lecture Notes in Computer Science, pages 574-579, Zakopane (Poland), 2004, Springer-Verlag.\n[29] R.C. Holte, Very simple classification rules perform well on most commonly used datasets, Machine Learning 11(1) (1993), 63-91.\n[30] C.-N. Hsu, H-J. Huang and T.-T. Wong, Implications of the Dirichlet assumption for discretization of continuous variables in naive Bayesian classifiers, Machine Learning Journal 53(3) (2002).\n[31] H. Huang and C. Hsu, Bayesian Classification for Set and Interval Data, In Proceedings 2000 International Computer Symposium, ChiaYi (Taiwan), 2000.\n[32] I. Inza, M. Merino, P. Larrañaga, J. Quiroga, B. Sierra and M. Giralá, Feature subset selection by genetic algorithms and estimation of distribution algorithms. A case study in the survival of cirrhotic patients treated with TIPS, Artificial Intelligence in Medicine 23(2) (2001), 187-205.\n[33] R. Kerber, Chimerge: Discretization of Numeric Attributes, In Proceedings of the 10th National Conference on Artificial Intelligence, pages 123-128, San Jose (California,USA), 1992.\n[34] P. Knotkanen, P. Myllymaki and H. Tirri, A Bayesian Approach to Discretization, In Proceedings of the European Symposium on Intelligent Techniques, pages 265-268, Bari (Italy), 1997.\n[35] R. Kohavi and M. Sahami, Error-Based and Entropy-Based Discretization of Continuous Features, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, pages 114-119, Portland (Oregon,USA), 1996.\n[36] R. Kohavi and D.H. Wolpert, Bias Plus Variance Decomposition for Zeroone Loss Functions, In Lorenza Saitta, editor, Proceedings of the 13th International Conference on Machine Learning, pages 275-283. Morgan Kaufmann, 1996.\n[37] A.V. Kozlov and D. Koller, Nonuniform Dynamic Discretization in Hybrid Networks, In Proceedings of the 13th Conference on Uncertainy in Artificial Intelligence, pages 314-325, Providence (Rhode Island, USA), 1997.\n[38] L. Kurgan, Discretization algorithm that uses class attribute interdependence maximization, IEEE Transactions on Knowledge and Data Engineering 16(2) (February 2004), 145-163.\n\n[39] W. Kwedlo and M. Kretowski, An Evolutionary Algorithm using Multivariate Discretization for Decision Rule Induction, In Proceedings of the Third European Conference on Principles of Data Mining and Knowledge Discovery, Volume 1704 of Lecture Notes in Artificial Intelligence, pages 392-397, Prague (Czech Republic), 1999, Springer-Verlag.\n[40] P. Langley, W. Iba and K. Thompson, An analysis of Bayesian Classifiers, In Proceedings of the 10th International Conference on Artificial Intelligence, pages 223-228, 1992.\n[41] P. Larrañaga, R. Etxeberria, J.A. Lozano and J.M. Peña, Combinatorial optimization by learning and simulation of Bayesian networks. In Proceedings of the 16th Annual Conference on Uncertainty in Artificial Intelligence (UAI-00), pages 343-352, San Francisco (USA), 2000. Morgan Kaufmann.\n[42] P. Larrañaga, R. Etxeberria, J.A. Lozano and J.M. Peña, Optimization in Continuous Domains by Learning and Simulation of Gaussian Networks, In Proceedings of the 2000 Genetic and Evolutionary Computation Conference Workshop Program, pages 201-204, 2000.\n[43] P. Larrañaga and J.A. Lozano, Estimation of Distribution Algorithms, A New Tool for Evolutionary Computation. Kluwer Academic Publisher. 2002.\n[44] P. Larrañaga, J.A. Lozano and J.M. Peña, Optimization by Learning and Simulation of Bayesian and Gaussian Networks, Technical Report Technical Report KZZA-IK-4-99, Department of Computer Science and Artificial Intelligence, University of the Basque Country, 1999.\n[45] L. Liu, A.K.C. Wong and Y. Wang, A global optimal algorithm for class-dependent discretization of continuous data, Intelligent Data Analysis 8(2) (2004), 151-170.\n[46] H. Mühlenbein and T. Mahnig, FDA - a scalable evolutionary algorithm for the optimization of additively decomposed functions, Evolutionary Computation 7(4) (1999), 353-376.\n[47] H. Mühlenbein, The equation for the response to selection and its use for prediction, Evolutionary Computation 5(3) (1998), 303-346.\n[48] H. Mühlenbein and G. Paass, From Recombination of Genes to the Estimation of Distributions I. Binary Parameters, In Proceedings of the 4th International Conference on Parallel Problem Solving, Volume 1141 of Nature Lecture Notes in Computer Science, pages 178-187, Berlin (Germany), 1996.\n[49] M. Minsky, Steps toward artificial intelligence, Transactions on Institute of Radio Engineers 49 (1961), 8-30.\n[50] M.J. Pazzani, Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier, In Proceedings of the 13th International Conference on Machine Learning, Volume 29, pages 105-112, Bari (Italy), 1996.\n[51] M. Pelikan and D.E. Goldberg, Genetic Algorithms, Clustering, and the Breaking of Symmetry, In Proceedings of the 6th International Conference on Parallel Problem Solving from Nature, Volume 1917 of Lecture Notes In Computer Science, pages 385-394, London (UK), 2000, Springer-Verlag.\n[52] M. Pelikan and D.E. Goldberg, Hierarchical Problem Solving by the Bayesian Optimization Algorithm, In Proceedings of the Genetic and Evolutionary Computation Conference, Lecture Notes in Computer Science, pages 267-274, Las Vegas, Nevada (USA), 2000. Morgan Kauffman.\n[53] M. Pelikan and D.E. Goldberg, Research on the Bayesian Optimization Algorithm, In Proceedings of the Genetic and Evolutionary Computation Conference, Volume 1 of Lecture Notes in Computer Science, pages 212-215. Morgan Kauffman, 2000.\n[54] M. Pelikan, D.E. Goldberg and F.G. Lobo, A survey of optimization by building and using probabilistic models, Computational Optimization and Applications 21(1) (January 2002), 5-20.\n[55] B. Pfahringer, Compression-Based Discretization of Continuous Attributes, In International Conference on Machine Learning, pages 4560C 463, Tahoe City (California, USA), 1995.\n[56] J.R. Quinlan, Induction of decision trees, Machine Learning 1(1) (1986), 81-106.\n[57] K. Revoredo and G. Zaverucha, Search-Based Class Discretization for Hidden Markov Model for Regression, In Proceedings of the Brazilian Symposium on Artificial Intelligence, Volume 3171 of Lecture Notes in Computer Science, pages 317-325, São Luis (Maranhão, Brazil), 2004.\n[58] H. Scheffe, The Analysis of Variance, John Wiley, 1959.\n[59] M. Sebag and A. Ducoulombier, Extending Population-Based Incremental Learning to Continuous Search Spaces, In Proceedings of the 5th International Conference on Parallel Problem Solving from Nature, Volume 1498 of Lecture Notes in Computer Science, pages 418-427, London (UK), 1998, Springer-Verlag.\n[60] R. Settono and H. Liu, Chi2: Feature Selection and Discretization of Numeric Attributes, In Proceedings of the 7th International Conference on Tool with Artificial Intelligence, pages 3880C391, Whashington DC (USA), 1995.\n[61] R. Shachter and C.R. Kenley, Gaussian in uence diagrams, Management Science 5(35) (1989), 527-550.\n[62] L. Torgo and J. Gama, Search-Based Class Discretization, In European Conference on Machine Learning, Volume 1224 of Lecture Notes in Computer Science, pages 266-273, Prague (Czech Republic), 1997, Springer-Verlag.\n[63] J.J. Valdes, L.C. Molina and N. Peris, An Evolution Strategies Approach to the Simultaneous Discretization of Numeric Attributes in Data Mining, In Proceedings of the World Congress on Evolutionary Computation, pages 1957-1964, Canberra (Australia), 2003.\n\n[64] J.J. Valdes, L.C. Molina and N. Peris, An Evolution Strategies Approach to the Simultaneous Discretization of Numeric Attributes in Data Mining, In Proceedings of the Congress on Evolutionary Computation, 2003.\n[65] M. Wang and S. Geisser, Optimal dichotomization of screening test variables, Journal of Statistical Planning and Inference 131(1) (2005), 191-206.\n[66] I.H. Witten and E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, (Second Edition), Morgan Kauffman, 2005.\n[67] Q. Wu, G. Prasad, T.M. McGinnity, D. Bell, S. Zhong and J. Guan, A Novel Discretizer for Knowledge Discovery Based on Multiknowledge Approaches, In Proceedings of the International Conference on Intelligent Computing, Volume 344 of Lecture Notes in Computer Science, pages 778-783. Morgan Kauffman, August 2006.\n[68] Y. Yang, Discretization for Naive-Bayes Learning, Master's thesis, Monash University, 2003.\n[69] Y. Yang and G.I. Webb, Discretization for naive-Bayes learning: managing discretization bias and variance, Technical report, School of Computer Science and Software Engineering Monash University, Melbourne, VIC 3800, Australia, 2003.\n[70] Y. Yang and G.I. Webb, On Why Discretization Works for Naive-Bayes Classifiers, In Australian Conference on Artificial Intelligence 2003, Volume 2903 of Lecture Notes in Computer Science, pages 440-452, Perth (Australia), 2003, SpringerVerlag.\n[71] Q. Zhang, On stability of fixed points of limit models of univariate marginal distribution algorithm and factorized distribution algorithm, IEEE Transactions on Evolutionary Computation 8(1) (February 2004), 80-93.\n[72] Q. Zhang, On the convergence of a class of estimation of distribution algorithms, IEEE Transactions on Evolutionary Computation 8(2) (April 2004), 127-136.\n[73] A.A. Zhigljavsky, Theory of Global Random Search, Kluwer Academic Publisher, 1991.",
    "references": [
      {
        "ref_id": "1",
        "text": "J. Bacardit and J.M. Garrell, Evolving Multiple Discretizations with Adaptive Intervals for a Pittsburgh Rule-Based Learning Classifier System, In Proceedings of the Genetic and Evolutionary Computation Conference, volume 2724 of Lecture Notes in Computer Science, pages 1818-1831, Chicago (Illinois,USA), 2003, Springer-Verlag."
      },
      {
        "ref_id": "2",
        "text": "S.D. Bay, Multivariate Discretization of Continuous Variables for Set Mining, In Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 315-319, Boston (Massachusetts, USA), 2000. ACM Press."
      },
      {
        "ref_id": "3",
        "text": "R. Blanco, I. Inza and P. Larrañaga, Learning Bayesian networks in the space of structures by estimation of distribution algorithms, International Journal of Intelligent Systems 18(2) (2003), 205-220."
      },
      {
        "ref_id": "4",
        "text": "J.S. De Bonet, C. Isbell and P. Viola, MIMIC: Finding optima by estimating probability densities, in: Advances in Neural Information Processing, (Vol. 9), M. Mozer M. Jordan and M. Perrone, eds, Denver 1996, MIT Press, Cambridge."
      },
      {
        "ref_id": "5",
        "text": "M. Boullé, Khiops: A Discretization Method of Continuous Attributes with Guaranteed Resistance to Noise, In Proceedings of the 3rd International Conference on Machine Learning and Data Mining, Volume 2734 of Lecture Notes in Computer Science, Leipzig (Germany), 2003, Springer-Verlag, 50-64."
      },
      {
        "ref_id": "6",
        "text": "M. Boullé, Khiops: A statistical discretization method of continuous attributes, Machine Learning 55(1) (2004), 53-69."
      },
      {
        "ref_id": "7",
        "text": "M. Boullé, A Grouping Method for Categorical Attributes having Very Large Number of Values, In Proceedings of the 4th International Conference on Machine Learning and Data Mining in Pattern Recognition, Volume 3587 of Lecture Notes in Computer Science, pages 228-242, Leipzig (Germany), 2005, Springer-Verlag."
      },
      {
        "ref_id": "8",
        "text": "M. Boullé, Optimal bin number for equal frequency discretizations in supervised learning, Intelligent Data Analysis 9(2) (2005), 175-188."
      },
      {
        "ref_id": "9",
        "text": "R. Butterworth, D.A. Simovici, G.S. Santos and L. Ohno-Machado, A greedy algorithm for supervised discretization, Journal of Biomedical Informatics 37(4) (2004), 285-292."
      },
      {
        "ref_id": "10",
        "text": "E. Castillo, J.M. Gutierrez and A.S. Hadi, Expert Systems and Probabilistic Network Models, Springer-Verlag, 1996."
      },
      {
        "ref_id": "11",
        "text": "J. Catlett, On Changing Continuous Attributes into Ordered Discrete Attributes, In Proceedings of the 5th European Working Session on Learning, Vol. 482 of Lecture Notes in Computer Science, pages 164-178, Porto (Portugal), 1991, Springer-Verlag."
      },
      {
        "ref_id": "12",
        "text": "J.Y. Ching, A.C. Wong and K.C.C. Chan, Class-dependent discretization for inductive learning from continuous and mixed-mode data, IEEE Transactions on Pattern Analysis and Machine Learning 17(7) (1995), 641-651."
      },
      {
        "ref_id": "13",
        "text": "B.S. Chlebus and S.H. Nguyen, Discretization Problem for Rough Set Methods, In Proceedings of the 1st International Conference on Rough Sets and Current Trend in Computing, Volume 1424 of Lecture Notes in Computer Science, pages 545-552, Warsaw (Poland), 1998, Springer-Verlag."
      },
      {
        "ref_id": "14",
        "text": "E. Consortium, Elvira: An Environment for Creating and using Probabilistic Graphical Models, In Proceedings of the 1st European Workshop on Probabilistic Graphical Models, Cuenca (Spain), 2002."
      },
      {
        "ref_id": "15",
        "text": "J. Demsar, Statistical comparisons of classifiers over multiple data sets, Journal of Machine Learning Research 7 (2006), $1-30$."
      },
      {
        "ref_id": "16",
        "text": "P. Domingos and M. Pazzani, On the optimality of the simple Bayesian classifier under zero-one loss, Machine Learning 29(2-3) (1997), 103-130."
      },
      {
        "ref_id": "17",
        "text": "J. Dougherty, R. Kohavi and M. Sahami, Supervised and Unsupervised Discretization of Continuous Features, In Proceedings of the 12th International Conference on Machine Learning, Lecture Notes in Artificial Intelligence, pages 194-202, San Francisco (CA, USA), 1995."
      },
      {
        "ref_id": "18",
        "text": "R.O. Duda and P.E. Hart, Pattern Classification and Scene Analysis, John Wiley and Sons, 1973."
      },
      {
        "ref_id": "19",
        "text": "R. Etxeberria and P. Larrañaga, Global Optimization using Bayesian Networks, In Proceedings of the 2nd Symposium on Articial Intelligence Adaptive Systems, Volume 9, pages 332-339, La Habana (Cuba), 1999."
      },
      {
        "ref_id": "20",
        "text": "U.M. Fayyad and K.B. Irani, On the handling of continuous-valued attributes in decision tree generation, Machine Learning 8(1) (1992), 87-102."
      },
      {
        "ref_id": "21",
        "text": "U.M. Fayyad and K.B. Irani, Multi-Interval Discretization of Continuousvalued Attributes for Classification Learning, In Proceedings of the 13th International Joint Conference on Artificial Intelligence, pages 1022-1027, Chambery (France), 1993. Morgan Kaufmann."
      },
      {
        "ref_id": "22",
        "text": "S. Ferrandiz and M. Boullé, Multivariate Discretization by Recursive Supervised Bipartition of Graph, In Machine Learning and Data Mining, Volume 3587 of Lecture Notes in Artificial Intelligence, pages 253-264, Leipzig (Germany), 2005, Springer-Verlag."
      },
      {
        "ref_id": "23",
        "text": "S. Ferrandiz and M. Boullé, Supervised Evaluation of Dataset Partitions: Advantages and Practice, In Machine Learning and Data Mining, Volume 3587 of Lecture Notes in Artificial Intelligence, pages 600-609, Leipzig (Germany), 2005, Springer-Verlag."
      },
      {
        "ref_id": "24",
        "text": "J. Gama, Dynamic Discretization of Continuous Attributes, In Proceedings of the 6th Iberoamerican Conference on Artificial Intelligence, Lecture Notes in Artificial Intelligence, pages 160-169, Lisbon (Portugal), 1998."
      },
      {
        "ref_id": "25",
        "text": "P. Geurts and L. Wehenkel, Investigation and Reduction of Discretization Variance in Decision tree Induction, In Proceedings of the 11th European Conference on Machine Learning, Volume 1810 of Lecture Notes in Computer Science, pages 162-170, Cataluña (Spain), 2000, Springer-Verlag."
      },
      {
        "ref_id": "26",
        "text": "R. Giráldez, J.S. Aguilar-Ruiz and J.C. Riquelme Santos, Natural Coding: A More Efficient Representation for Evolutionary Learning, In Proceedings of the Genetic and Evolutionary Computation Conference, Vol. 2723 of Lecture Notes in Computer Science, pages 979-990, Chicago (Illinois,USA), 2003, Springer-Verlag."
      },
      {
        "ref_id": "27",
        "text": "C. González, J.A. Lozano and P. Larrañaga. Mathematical modeling of UMDA algorithm with tournament selection, International Journal of Approximate Reasoning (31) (2002), 313-340."
      },
      {
        "ref_id": "28",
        "text": "K. Grabczewski, SSV Criterion Based Discretization for Naive Bayes Classifiers, In Proceedings of the International Conference on Artificial Intelligence and Soft Computing, Volume 3070 of Lecture Notes in Computer Science, pages 574-579, Zakopane (Poland), 2004, Springer-Verlag."
      },
      {
        "ref_id": "29",
        "text": "R.C. Holte, Very simple classification rules perform well on most commonly used datasets, Machine Learning 11(1) (1993), 63-91."
      },
      {
        "ref_id": "30",
        "text": "C.-N. Hsu, H-J. Huang and T.-T. Wong, Implications of the Dirichlet assumption for discretization of continuous variables in naive Bayesian classifiers, Machine Learning Journal 53(3) (2002)."
      },
      {
        "ref_id": "31",
        "text": "H. Huang and C. Hsu, Bayesian Classification for Set and Interval Data, In Proceedings 2000 International Computer Symposium, ChiaYi (Taiwan), 2000."
      },
      {
        "ref_id": "32",
        "text": "I. Inza, M. Merino, P. Larrañaga, J. Quiroga, B. Sierra and M. Giralá, Feature subset selection by genetic algorithms and estimation of distribution algorithms. A case study in the survival of cirrhotic patients treated with TIPS, Artificial Intelligence in Medicine 23(2) (2001), 187-205."
      },
      {
        "ref_id": "33",
        "text": "R. Kerber, Chimerge: Discretization of Numeric Attributes, In Proceedings of the 10th National Conference on Artificial Intelligence, pages 123-128, San Jose (California,USA), 1992."
      },
      {
        "ref_id": "34",
        "text": "P. Knotkanen, P. Myllymaki and H. Tirri, A Bayesian Approach to Discretization, In Proceedings of the European Symposium on Intelligent Techniques, pages 265-268, Bari (Italy), 1997."
      },
      {
        "ref_id": "35",
        "text": "R. Kohavi and M. Sahami, Error-Based and Entropy-Based Discretization of Continuous Features, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, pages 114-119, Portland (Oregon,USA), 1996."
      },
      {
        "ref_id": "36",
        "text": "R. Kohavi and D.H. Wolpert, Bias Plus Variance Decomposition for Zeroone Loss Functions, In Lorenza Saitta, editor, Proceedings of the 13th International Conference on Machine Learning, pages 275-283. Morgan Kaufmann, 1996."
      },
      {
        "ref_id": "37",
        "text": "A.V. Kozlov and D. Koller, Nonuniform Dynamic Discretization in Hybrid Networks, In Proceedings of the 13th Conference on Uncertainy in Artificial Intelligence, pages 314-325, Providence (Rhode Island, USA), 1997."
      },
      {
        "ref_id": "38",
        "text": "L. Kurgan, Discretization algorithm that uses class attribute interdependence maximization, IEEE Transactions on Knowledge and Data Engineering 16(2) (February 2004), 145-163."
      },
      {
        "ref_id": "39",
        "text": "W. Kwedlo and M. Kretowski, An Evolutionary Algorithm using Multivariate Discretization for Decision Rule Induction, In Proceedings of the Third European Conference on Principles of Data Mining and Knowledge Discovery, Volume 1704 of Lecture Notes in Artificial Intelligence, pages 392-397, Prague (Czech Republic), 1999, Springer-Verlag."
      },
      {
        "ref_id": "40",
        "text": "P. Langley, W. Iba and K. Thompson, An analysis of Bayesian Classifiers, In Proceedings of the 10th International Conference on Artificial Intelligence, pages 223-228, 1992."
      },
      {
        "ref_id": "41",
        "text": "P. Larrañaga, R. Etxeberria, J.A. Lozano and J.M. Peña, Combinatorial optimization by learning and simulation of Bayesian networks. In Proceedings of the 16th Annual Conference on Uncertainty in Artificial Intelligence (UAI-00), pages 343-352, San Francisco (USA), 2000. Morgan Kaufmann."
      },
      {
        "ref_id": "42",
        "text": "P. Larrañaga, R. Etxeberria, J.A. Lozano and J.M. Peña, Optimization in Continuous Domains by Learning and Simulation of Gaussian Networks, In Proceedings of the 2000 Genetic and Evolutionary Computation Conference Workshop Program, pages 201-204, 2000."
      },
      {
        "ref_id": "43",
        "text": "P. Larrañaga and J.A. Lozano, Estimation of Distribution Algorithms, A New Tool for Evolutionary Computation. Kluwer Academic Publisher. 2002."
      },
      {
        "ref_id": "44",
        "text": "P. Larrañaga, J.A. Lozano and J.M. Peña, Optimization by Learning and Simulation of Bayesian and Gaussian Networks, Technical Report Technical Report KZZA-IK-4-99, Department of Computer Science and Artificial Intelligence, University of the Basque Country, 1999."
      },
      {
        "ref_id": "45",
        "text": "L. Liu, A.K.C. Wong and Y. Wang, A global optimal algorithm for class-dependent discretization of continuous data, Intelligent Data Analysis 8(2) (2004), 151-170."
      },
      {
        "ref_id": "46",
        "text": "H. Mühlenbein and T. Mahnig, FDA - a scalable evolutionary algorithm for the optimization of additively decomposed functions, Evolutionary Computation 7(4) (1999), 353-376."
      },
      {
        "ref_id": "47",
        "text": "H. Mühlenbein, The equation for the response to selection and its use for prediction, Evolutionary Computation 5(3) (1998), 303-346."
      },
      {
        "ref_id": "48",
        "text": "H. Mühlenbein and G. Paass, From Recombination of Genes to the Estimation of Distributions I. Binary Parameters, In Proceedings of the 4th International Conference on Parallel Problem Solving, Volume 1141 of Nature Lecture Notes in Computer Science, pages 178-187, Berlin (Germany), 1996."
      },
      {
        "ref_id": "49",
        "text": "M. Minsky, Steps toward artificial intelligence, Transactions on Institute of Radio Engineers 49 (1961), 8-30."
      },
      {
        "ref_id": "50",
        "text": "M.J. Pazzani, Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier, In Proceedings of the 13th International Conference on Machine Learning, Volume 29, pages 105-112, Bari (Italy), 1996."
      },
      {
        "ref_id": "51",
        "text": "M. Pelikan and D.E. Goldberg, Genetic Algorithms, Clustering, and the Breaking of Symmetry, In Proceedings of the 6th International Conference on Parallel Problem Solving from Nature, Volume 1917 of Lecture Notes In Computer Science, pages 385-394, London (UK), 2000, Springer-Verlag."
      },
      {
        "ref_id": "52",
        "text": "M. Pelikan and D.E. Goldberg, Hierarchical Problem Solving by the Bayesian Optimization Algorithm, In Proceedings of the Genetic and Evolutionary Computation Conference, Lecture Notes in Computer Science, pages 267-274, Las Vegas, Nevada (USA), 2000. Morgan Kauffman."
      },
      {
        "ref_id": "53",
        "text": "M. Pelikan and D.E. Goldberg, Research on the Bayesian Optimization Algorithm, In Proceedings of the Genetic and Evolutionary Computation Conference, Volume 1 of Lecture Notes in Computer Science, pages 212-215. Morgan Kauffman, 2000."
      },
      {
        "ref_id": "54",
        "text": "M. Pelikan, D.E. Goldberg and F.G. Lobo, A survey of optimization by building and using probabilistic models, Computational Optimization and Applications 21(1) (January 2002), 5-20."
      },
      {
        "ref_id": "55",
        "text": "B. Pfahringer, Compression-Based Discretization of Continuous Attributes, In International Conference on Machine Learning, pages 4560C 463, Tahoe City (California, USA), 1995."
      },
      {
        "ref_id": "56",
        "text": "J.R. Quinlan, Induction of decision trees, Machine Learning 1(1) (1986), 81-106."
      },
      {
        "ref_id": "57",
        "text": "K. Revoredo and G. Zaverucha, Search-Based Class Discretization for Hidden Markov Model for Regression, In Proceedings of the Brazilian Symposium on Artificial Intelligence, Volume 3171 of Lecture Notes in Computer Science, pages 317-325, São Luis (Maranhão, Brazil), 2004."
      },
      {
        "ref_id": "58",
        "text": "H. Scheffe, The Analysis of Variance, John Wiley, 1959."
      },
      {
        "ref_id": "59",
        "text": "M. Sebag and A. Ducoulombier, Extending Population-Based Incremental Learning to Continuous Search Spaces, In Proceedings of the 5th International Conference on Parallel Problem Solving from Nature, Volume 1498 of Lecture Notes in Computer Science, pages 418-427, London (UK), 1998, Springer-Verlag."
      },
      {
        "ref_id": "60",
        "text": "R. Settono and H. Liu, Chi2: Feature Selection and Discretization of Numeric Attributes, In Proceedings of the 7th International Conference on Tool with Artificial Intelligence, pages 3880C391, Whashington DC (USA), 1995."
      },
      {
        "ref_id": "61",
        "text": "R. Shachter and C.R. Kenley, Gaussian in uence diagrams, Management Science 5(35) (1989), 527-550."
      },
      {
        "ref_id": "62",
        "text": "L. Torgo and J. Gama, Search-Based Class Discretization, In European Conference on Machine Learning, Volume 1224 of Lecture Notes in Computer Science, pages 266-273, Prague (Czech Republic), 1997, Springer-Verlag."
      },
      {
        "ref_id": "63",
        "text": "J.J. Valdes, L.C. Molina and N. Peris, An Evolution Strategies Approach to the Simultaneous Discretization of Numeric Attributes in Data Mining, In Proceedings of the World Congress on Evolutionary Computation, pages 1957-1964, Canberra (Australia), 2003."
      },
      {
        "ref_id": "64",
        "text": "J.J. Valdes, L.C. Molina and N. Peris, An Evolution Strategies Approach to the Simultaneous Discretization of Numeric Attributes in Data Mining, In Proceedings of the Congress on Evolutionary Computation, 2003."
      },
      {
        "ref_id": "65",
        "text": "M. Wang and S. Geisser, Optimal dichotomization of screening test variables, Journal of Statistical Planning and Inference 131(1) (2005), 191-206."
      },
      {
        "ref_id": "66",
        "text": "I.H. Witten and E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, (Second Edition), Morgan Kauffman, 2005."
      },
      {
        "ref_id": "67",
        "text": "Q. Wu, G. Prasad, T.M. McGinnity, D. Bell, S. Zhong and J. Guan, A Novel Discretizer for Knowledge Discovery Based on Multiknowledge Approaches, In Proceedings of the International Conference on Intelligent Computing, Volume 344 of Lecture Notes in Computer Science, pages 778-783. Morgan Kauffman, August 2006."
      },
      {
        "ref_id": "68",
        "text": "Y. Yang, Discretization for Naive-Bayes Learning, Master's thesis, Monash University, 2003."
      },
      {
        "ref_id": "69",
        "text": "Y. Yang and G.I. Webb, Discretization for naive-Bayes learning: managing discretization bias and variance, Technical report, School of Computer Science and Software Engineering Monash University, Melbourne, VIC 3800, Australia, 2003."
      },
      {
        "ref_id": "70",
        "text": "Y. Yang and G.I. Webb, On Why Discretization Works for Naive-Bayes Classifiers, In Australian Conference on Artificial Intelligence 2003, Volume 2903 of Lecture Notes in Computer Science, pages 440-452, Perth (Australia), 2003, SpringerVerlag."
      },
      {
        "ref_id": "71",
        "text": "Q. Zhang, On stability of fixed points of limit models of univariate marginal distribution algorithm and factorized distribution algorithm, IEEE Transactions on Evolutionary Computation 8(1) (February 2004), 80-93."
      },
      {
        "ref_id": "72",
        "text": "Q. Zhang, On the convergence of a class of estimation of distribution algorithms, IEEE Transactions on Evolutionary Computation 8(2) (April 2004), 127-136."
      },
      {
        "ref_id": "73",
        "text": "A.A. Zhigljavsky, Theory of Global Random Search, Kluwer Academic Publisher, 1991."
      }
    ],
    "reference_count": 73,
    "pattern_matched": "(?:^|\\n)#+\\s*References?\\s*\\n"
  },
  "tables": [
    {
      "table_number": "1",
      "table_title": "Table 1 ANOVA Results",
      "headers": [
        "Source",
        "Type III Sum of Squares",
        "df",
        "Mean Square",
        "F",
        "Sig."
      ],
      "rows": [
        [
          "Corrected Model",
          17009148,
          26,
          654198,
          1652463,
          0
        ],
        [
          "Intercept",
          1925586675,
          1,
          19255867,
          4863912,
          0
        ],
        [
          "$\\alpha$",
          8954943,
          2,
          4477471,
          11309813,
          0
        ],
        [
          "$\\beta$",
          4814908,
          2,
          2407454,
          6081079,
          0
        ],
        [
          "$\\gamma$",
          1824063,
          2,
          912032,
          2303735,
          0
        ],
        [
          "$\\alpha \\beta$",
          872673,
          4,
          218168,
          551079,
          0
        ],
        [
          "$\\alpha \\gamma$",
          137746,
          4,
          34437,
          86985,
          0
        ],
        [
          "$\\beta \\gamma$",
          196554,
          4,
          49138,
          124121,
          0
        ],
        [
          "$\\alpha \\beta \\gamma$",
          208261,
          8,
          26033,
          65757,
          0
        ],
        [
          "Error",
          96202,
          243,
          396,
          "",
          ""
        ],
        [
          "Total",
          1942692025,
          270,
          "",
          "",
          ""
        ],
        [
          "Corrected Total",
          17105350,
          269,
          "",
          "",
          ""
        ]
      ],
      "row_count": 12,
      "column_count": 6
    },
    {
      "table_number": "2",
      "table_title": "UCI Datasets",
      "headers": [
        "\\#",
        "Data Set",
        "Num. Class Values",
        "Num. Variables",
        "Num. Instances"
      ],
      "rows": [
        [
          1,
          "Balance",
          3,
          4,
          625
        ],
        [
          2,
          "Bupa",
          2,
          6,
          246
        ],
        [
          3,
          "Hayes",
          3,
          4,
          160
        ],
        [
          4,
          "Image",
          7,
          20,
          2310
        ],
        [
          5,
          "Ionosphere",
          2,
          35,
          351
        ],
        [
          6,
          "Iris",
          3,
          4,
          150
        ],
        [
          7,
          "Liver",
          2,
          6,
          345
        ],
        [
          8,
          "Pima",
          2,
          8,
          768
        ],
        [
          9,
          "Vehicle",
          4,
          19,
          846
        ],
        [
          10,
          "Wine",
          3,
          13,
          179
        ]
      ],
      "row_count": 10,
      "column_count": 5
    },
    {
      "table_number": "3",
      "table_title": "Summary of the estimated predictive accuracy with 2 bins. Classical unsupervised approaches compared with WEDA",
      "headers": [
        "",
        "Equal <br> Frequency",
        "Equal <br> Width",
        "K Means",
        "Unsupervised <br> Monothetic <br> Contrast",
        "WEDA <br> +C4.5",
        "WEDA <br> + k-NN",
        "WEDA + <br> Naive <br> Bayes"
      ],
      "rows": [
        [
          1,
          "$73.02 \\pm 2.99$",
          "$73.02 \\pm 2.99$",
          "$77.50 \\pm 3.37$",
          "$\\mathbf{7 7 . 8 2} \\pm \\mathbf{4 . 0 7}$",
          "$77.64 \\pm 2.58$",
          "$77.75 \\pm 0.41$",
          "$77.40 \\pm 1.07$"
        ],
        [
          2,
          "$\\mathbf{6 6 . 3 8} \\pm \\mathbf{3 . 1 4}$",
          "$56.23 \\pm 3.75$",
          "$57.68 \\pm 1.21$",
          "$58.84 \\pm 2.63$",
          "$57.10 \\pm 0.70$",
          "$58.14 \\pm 1.33$",
          "$58.44 \\pm 0.59$"
        ],
        [
          3,
          "$56.45 \\pm 8.06$",
          "$37.73 \\pm 5.79$",
          "$51.85 \\pm 6.05$",
          "$31.73 \\pm 5.80$",
          "$\\mathbf{6 0 . 2 4} \\pm \\mathbf{2 . 3 9}$",
          "$46.94 \\pm 4.11$",
          "$57.60 \\pm 1.24$"
        ],
        [
          4,
          "$67.75 \\pm 0.83$",
          "$71.17 \\pm 2.29$",
          "$72.77 \\pm 1.26$",
          "$72.42 \\pm 1.66$",
          "$\\mathbf{9 2 . 1 5} \\pm \\mathbf{0 . 4 1}$",
          "$91.38 \\pm 0.57$",
          "$88.08 \\pm 0.27$"
        ],
        [
          5,
          "$66.49 \\pm 22.81$",
          "$68.23 \\pm 26.54$",
          "$63.08 \\pm 22.83$",
          "$65.64 \\pm 22.93$",
          "$90.52 \\pm 0.85$",
          "$87.35 \\pm 0.52$",
          "$\\mathbf{9 6 . 0 2} \\pm \\mathbf{0 . 6 1}$"
        ],
        [
          6,
          "$76.00 \\pm 2.81$",
          "$73.99 \\pm 5.96$",
          "$78.67 \\pm 3.80$",
          "$78.67 \\pm 5.05$",
          "$\\mathbf{9 5 . 4 7} \\pm \\mathbf{0 . 2 7}$",
          "$95.20 \\pm 0.42$",
          "$94.80 \\pm 0.00$"
        ],
        [
          7,
          "$\\mathbf{6 8 . 1 2} \\pm \\mathbf{8 . 3 9}$",
          "$55.36 \\pm 4.15$",
          "$56.23 \\pm 6.59$",
          "$57.10 \\pm 4.53$",
          "$57.71 \\pm 1.13$",
          "$58.32 \\pm 1.64$",
          "$60.33 \\pm 4.13$"
        ],
        [
          8,
          "$72.26 \\pm 3.39$",
          "$68.49 \\pm 4.74$",
          "$72.53 \\pm 1.83$",
          "$74.15 \\pm 1.57$",
          "$\\mathbf{7 4 . 5 3} \\pm \\mathbf{0 . 4 0}$",
          "$73.62 \\pm 0.41$",
          "$73.22 \\pm 0.53$"
        ],
        [
          9,
          "$45.68 \\pm 3.94$",
          "$39.78 \\pm 2.87$",
          "$43.28 \\pm 3.87$",
          "$42.51 \\pm 2.66$",
          "$66.36 \\pm 1.37$",
          "$\\mathbf{6 6 . 3 8} \\pm \\mathbf{1 . 0 0}$",
          "$61.34 \\pm 1.02$"
        ],
        [
          10,
          "$96.11 \\pm 3.85$",
          "$88.55 \\pm 4.43$",
          "$93.75 \\pm 6.75$",
          "$93.17 \\pm 6.84$",
          "$94.05 \\pm 1.03$",
          "$94.87 \\pm 0.98$",
          "$\\mathbf{9 6 . 3 8} \\pm \\mathbf{1 . 4 5}$"
        ]
      ],
      "row_count": 10,
      "column_count": 8
    },
    {
      "table_number": "4",
      "table_title": "Summary of the estimated predictive accuracy. Supervised approaches compared with WEDA",
      "headers": [
        "",
        "Fayyad Irani <br> Entropy",
        "CAIM",
        "WEDA",
        "WEDA",
        "WEDA +"
      ],
      "rows": [
        [
          "",
          "",
          "",
          4.5,
          "+ k-NN",
          "Naive Bayes"
        ],
        [
          1,
          "$73.02 \\pm 2.99$",
          "$73.02 \\pm 2.99$",
          "$77.64 \\pm 2.58$",
          "$\\mathbf{7 7 . 7 5} \\pm \\mathbf{0 . 4 1}$",
          "$77.40 \\pm 1.07$"
        ],
        [
          2,
          "$52.17 \\pm 8.20$",
          "$\\mathbf{6 2 . 9 0} \\pm \\mathbf{6 . 8 4}$",
          "$57.10 \\pm 0.70$",
          "$58.14 \\pm 1.33$",
          "$58.44 \\pm 0.59$"
        ],
        [
          3,
          "$60.14 \\pm 2.56$",
          "$53.87 \\pm 4.50$",
          "$\\mathbf{6 0 . 2 4} \\pm \\mathbf{2 . 3 9}$",
          "$46.94 \\pm 4.11$",
          "$57.60 \\pm 1.24$"
        ],
        [
          4,
          "$88.48 \\pm 0.18$",
          "$80.35 \\pm 3.31$",
          "$\\mathbf{9 2 . 1 5} \\pm \\mathbf{0 . 4 1}$",
          "$91.38 \\pm 0.57$",
          "$88.08 \\pm 0.27$"
        ],
        [
          5,
          "$88.91 \\pm 3.87$",
          "$89.75 \\pm 4.74$",
          "$90.52 \\pm 0.85$",
          "$87.35 \\pm 0.52$",
          "$\\mathbf{9 6 . 0 2} \\pm \\mathbf{0 . 6 1}$"
        ],
        [
          6,
          "$93.33 \\pm 2.36$",
          "$69.53 \\pm 2.58$",
          "$\\mathbf{9 5 . 4 7} \\pm \\mathbf{0 . 2 7}$",
          "$95.20 \\pm 0.42$",
          "$94.80 \\pm 0.00$"
        ],
        [
          7,
          "$58.28 \\pm 0.68$",
          "$\\mathbf{6 1 . 1 9} \\pm \\mathbf{4 . 3 0}$",
          "$57.71 \\pm 1.13$",
          "$58.32 \\pm 1.64$",
          "$60.33 \\pm 4.13$"
        ],
        [
          8,
          "$\\mathbf{7 5 . 7 6} \\pm \\mathbf{3 . 5 8}$",
          "$72.91 \\pm 5.02$",
          "$74.53 \\pm 0.40$",
          "$73.62 \\pm 0.41$",
          "$73.22 \\pm 0.53$"
        ],
        [
          9,
          "$61.26 \\pm 4.02$",
          "$46.55 \\pm 1.61$",
          "$66.36 \\pm 1.37$",
          "$\\mathbf{6 6 . 3 8} \\pm \\mathbf{1 . 0 0}$",
          "$61.34 \\pm 1.02$"
        ],
        [
          10,
          "$98.24 \\pm 2.63$",
          "$\\mathbf{9 8 . 2 4} \\pm \\mathbf{2 . 6 3}$",
          "$94.05 \\pm 1.03$",
          "$94.87 \\pm 0.98$",
          "$96.38 \\pm 1.45$"
        ]
      ],
      "row_count": 11,
      "column_count": 6
    }
  ]
}