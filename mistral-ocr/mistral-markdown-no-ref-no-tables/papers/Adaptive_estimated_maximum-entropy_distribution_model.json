{
  "metadata": {
    "file_path": "/Users/id05309/Documents/tfm/mistral/mistral-markdown/2007/Adaptive estimated maximum-entropy distribution model.md",
    "filename": "Adaptive estimated maximum-entropy distribution model.md",
    "title": "Adaptive estimated maximum-entropy distribution model",
    "year": "2007"
  },
  "references": {
    "header": "## References",
    "content": "[1] R. Agrawal, R. Srikant, Fast algorithm for mining association rules, Proceedings of VLDB Conference, September 1994, pp. 487-499.\n[2] M.Z. Ashrafi, D. Taniar, K.A. Smith, A new approach of eliminating redundant association rules, in: Database and Expert Systems Applications, Lecture Notes in Computer Science, vol. 3180, Springer-Verlag, 2004, pp. 65-474.\n[3] M.Z. Ashrafi, D. Taniar, K.A. Smith, An efficient compression technique for frequent itemset generation in association rule mining, in: Advances in Knowledge Discovery and Data Mining, PAKDD 2005, Lecture Notes in Computer Science, vol. 3518, SpringerVerlag, 2005, pp. 125-135.\n[4] M.Z. Ashrafi, D. Taniar, K.A. Smith, Redundant association rules reduction techniques, in: AI 2006: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 3809, Springer-Verlag, 2005, pp. 254-263.\n[5] S. Baluja, S. Davies, Using optimal dependency tree for combinatorial optimization: learning the structure of search space, Technical Report No. CMU-CS-97-107, Carnegie Mellon University, Pittsburgh, PA, 1997.\n[6] S. Baluja, Population based incremental learning: a method for integrating genetic search based function optimization and competitive learning, Technical Report No. CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, PA, 1994.\n[7] E. Bengoetxea, P. Larrañaga, I. Bloch, A. Perchant, C. Boeres, Learning and simulation of Bayesian networks applied to inexact graph matching, Pattern Recognition 35 (12) (2002) 2867-2880.\n[8] P.A.N. Bosman, D. Thierens, An algorithmic framework for density estimation based evolutionary algorithms, Technical Report UU-CS-1999-46, the Department of Information and Computing Sciences, Utrecht University, 1999.\n[9] E. Cantú-Paz, Efficient and Accurate Parallel Genetic Algorithms, Kluwer Academic Publishers, Boston, MA, 2000, pp. 68-70.\n[10] O. Daly, D. Taniar, Exception rules mining based on negative association rules, in: Computational Science and Applications, Lecture Notes in Computer Science, Part IV, vol. 3046, Springer-Verlag, 2004, pp. 543-552.\n[11] J. Darroch, D. Ratcliff, Generalized iterative scaling for log-linear models, Annals of Mathematical Statistics 43 (1972) 1470-1480.\n[12] J.S. De Bonet, C.L. Isbell, P. Viola, MIMIC: Finding optima by estimating probability densities, in: M. Mozer, M. Jordan, T. Petsche (Eds.), Advances in Neural Information Processing Systems, vol. 9, MIT Press, Cambridge, MA, 1997, pp. 424-431.\n[13] S. Della Pietra, V. Della Pietra, J. Lafferty, Inducing features of random fields, IEEE Transactions on Pattern Analysis and Machine Intelligence 19 (4) (1997) 380-393.\n[14] R.O. Duda, P.E. Hart, Pattern Classification and Scene Analysis, John Wiley \\& Sons, New York, NY, 1973.\n[15] A.A. Freitas, Data Mining and Knowledge Discovery with Evolutionary Algorithms, Springer-Verlag, Berlin, 2002, pp. 108-109.\n[16] D.E. Goldberg, P. Segrest, Finite Markov chain analysis of genetic algorithms, in: J. Grefenstette (Ed.), Genetic Algorithms and Their Applications: Proceedings of the Second International Conference on Genetic Algorithms, Lawrence Erlbaum Associates, Hillsdale, NJ, 1987, pp. 1-8.\n[17] D.E. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learning, Addison-Wesley Longman Publishing Co., Boston, MA, 1989.\n[18] J. Goodman, Classes for fast maximum entropy training, in: Proceedings of International Conference on Acoustics, Speech, and Signal Processing, IEEE CS Press, 2001, pp. 557-560.\n\n[19] J. Goodman, Sequential conditional generalized iterative scaling, in: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, July 2002, pp. 9-16.\n[20] G.R. Harik, Linkage learning via probabilistic modelling in the ECGA, Illegal Report No. 99010, Illinois Genetic Algorithm Laboratory, University of Illinois, Urbana, IL, 1999.\n[21] G.R. Harik, F.G. Lobo, D.E. Goldberg, The Compact genetic algorithm, in: Proceedings of IEEE Conference on Evolutionary Computation, IEEE Press, 1998, pp. 523-528.\n[22] F.S. Hillier, G.J. Lieberman, Introduction to Operations Research, seventh ed., McGraw-Hill, Boston, MA, 2001.\n[23] Y. Huang, H. Xiong, W. Wu, P. Deng, Z. Zhang, Mining maximal hyperclique pattern: a hybrid search strategy, Information Sciences 177 (3) (2007) 703-721.\n[24] I. Inza, P. Larrañaga, B. Sierra, Estimation of distribution algorithms for feature subset selection in large dimensionality domains, in: H.A. Abbas, R.A. Sarker, C.S. Newton (Eds.), Data Mining: A Heuristic Approach, Idea Group Publishing, Hershey, PA, 2002, pp. $97-116$.\n[25] M. Iosifescu, Finite Markov Processes and Their Applications, John Wiley, New York, NY, 1980.\n[26] A.K. Jain, R.P.W. Duin, J. Mao, Statistical pattern recognition: a review, IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (1) (2000) 4-37.\n[27] E.T. Jaynes, Information theory and statistical mechanics, Physical Reviews 106 (1957) 620-630.\n[28] J.N. Kapur, H.K. Kesavan, Entropy Optimization Principles with Applications, Academic Press, Boston, MA, 1992.\n[29] P. Larrañaga, A review on estimation of distribution algorithms, in: P. Larrañaga, J.A. Lozano (Eds.), Estimation of Distribution Algorithms: A New Tool for Evolutionary Optimization, Kluwer Academic Publishers, Boston, MA, 2001, pp. 57-100.\n[30] P. Larrañaga, R. Etxeberria, J.A. Lozano, J.M. Peña, Combinatorial optimization by learning and simulation of Bayesian networks, in: The Proceeding of the Sixteenth Conference on Uncertainty in Artificial Intelligence, Standford, 2000, pp. 343-352.\n[31] P. Larrañaga, R. Etxeberria, J.A. Lozano, J.M. Peña, Optimization by learning and simulation of Bayesian and Gaussian networks, Technical Report KZZA-IK-4-99, Department of Computer Science and Artificial Intelligence, University of the Basque Country, 1999 .\n[32] P. Larrañaga, R. Etxeberria, J.A. Lozano, J.M. Peña, Optimization in continuous domain by learning and simulation of Gaussian networks, in: The Proceeding of the 2000 Genetic and Evolutionary Computation Conference Workshop Program, Las Vegas, Nevada, 2000, pp. 201-204.\n[33] R. Malouf, A Comparison of algorithms for maximum entropy parameter estimation, in: Proceedings of Sixth Conference on Natural Language Learning, 2002, pp. 49-55.\n[34] H. Mühlenbein, T. Mahnig, The Factorized distribution algorithm for additively decomposed functions, in: Proceedings of Congress on Evolutionary Computation, IEEE Press, 1999, pp. 752-759.\n[35] H. Mühlenbein, G. Paaß, From combination of genes to the estimation of distributions: binary parameters, in: H.M. Voigt (Ed.), Parallel Problem Solving from Nature - PPSN IV, Lecture Notes in Computer Science, vol. 1411, Springer-Verlag, Berlin, 1996, pp. $178-187$.\n[36] R.M. Neal, Probabilistic inference using Markov chain Monte Carlo methods, Technical Report CRG-TR-93-1, Department of Computer Science, University of Toronto, 1993.\n[37] D.J. Newman, S. Hettich, C.L. Blake, C.J. Merz, UCI Repository of machine learning databases, University of California, Department of Information and Computer Science, Irvine, CA, 1998. <http://www.ics.uci.edu/ mlearn/MLRepository.html>.\n[38] A. Nix, M.D. Vose, Modeling genetic algorithms with Markov chains, Annals of Mathematics and Artificial Intelligence 5 (1992) 2734 .\n[39] T.K. Paul, H. Iba, Linear and combinatorial optimizations by estimation of distribution algorithms, in: The Proceeding of the 9th MPS Symposium on Evolutionary Computation, Japan, 2002, pp. 99-106.\n[40] M. Pelikan, D.E. Goldberg, E. Cantú-Paz, Linkage problem, distribution estimation and Bayesian networks, Evolutionary Computation 8 (3) (2000) 311-340.\n[41] G. Rudolph, Convergence analysis of canonical genetic algorithms, IEEE Transactions on Neural Networks 5 (1) (1994) 96-101.\n[42] L. Tan, D. Taniar, K.A. Smith, A clustering algorithm based on an estimated distribution model, International Journal of Business Intelligence and Data Mining 1 (2) (2006) 229-245.\n[43] L. Tan, D. Taniar, K.A. Smith, Maximum-entropy estimated distribution classification model, International Journal of Hybrid Intelligence Systems 3 (1) (2006) 1-10.\n[44] H.C. Tjioe, D. Taniar, Mining association rules in data warehouses, International Journal of Data Warehousing and Mining 1 (3) (2005) $28-62$.\n[45] Y-J. Tsay, Y-W. Chang-Chien, An efficient cluster and decomposition algorithm for mining association rules, Information Sciences $160(1-4)(2004) 161-171$.\n[46] J. Wang, X. Wu, C. Zhang, Support vector machines based on $K$-means clustering for real-time business intelligence systems, International Journal of Business Intelligence and Data Mining 1 (1) (2005) 54-64.\n[47] A. Wright, R. Poli, C. Stephens, W.B. Landgon, S. Pulavarty, An estimation of distribution algorithm based on maximum entropy, in: Proceedings of GECCO 2004, Lecture Notes in Computer Science, LNCS, vol. 3103, Springer-Verlag, 2004, pp. 343-354.\n[48] L. Yan, D.J. Miller, General statistical inference for discrete and mixed spaces by an approximate application of the maximum entropy principle, IEEE Transactions on Neural Networks 11 (3) (2000) 558-573.\n[49] S.X. Yu, P. Scheunders, Feature selection for high-dimensional remote sensing data by maximum entropy principle based optimization, Proceedings of Geoscience \\& Remote Sensing Symposium, vol. 7, IEEE Press, 2001, pp. 3303-3305.",
    "references": [
      {
        "ref_id": "1",
        "text": "R. Agrawal, R. Srikant, Fast algorithm for mining association rules, Proceedings of VLDB Conference, September 1994, pp. 487-499."
      },
      {
        "ref_id": "2",
        "text": "M.Z. Ashrafi, D. Taniar, K.A. Smith, A new approach of eliminating redundant association rules, in: Database and Expert Systems Applications, Lecture Notes in Computer Science, vol. 3180, Springer-Verlag, 2004, pp. 65-474."
      },
      {
        "ref_id": "3",
        "text": "M.Z. Ashrafi, D. Taniar, K.A. Smith, An efficient compression technique for frequent itemset generation in association rule mining, in: Advances in Knowledge Discovery and Data Mining, PAKDD 2005, Lecture Notes in Computer Science, vol. 3518, SpringerVerlag, 2005, pp. 125-135."
      },
      {
        "ref_id": "4",
        "text": "M.Z. Ashrafi, D. Taniar, K.A. Smith, Redundant association rules reduction techniques, in: AI 2006: Advances in Artificial Intelligence, Lecture Notes in Computer Science, vol. 3809, Springer-Verlag, 2005, pp. 254-263."
      },
      {
        "ref_id": "5",
        "text": "S. Baluja, S. Davies, Using optimal dependency tree for combinatorial optimization: learning the structure of search space, Technical Report No. CMU-CS-97-107, Carnegie Mellon University, Pittsburgh, PA, 1997."
      },
      {
        "ref_id": "6",
        "text": "S. Baluja, Population based incremental learning: a method for integrating genetic search based function optimization and competitive learning, Technical Report No. CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, PA, 1994."
      },
      {
        "ref_id": "7",
        "text": "E. Bengoetxea, P. Larrañaga, I. Bloch, A. Perchant, C. Boeres, Learning and simulation of Bayesian networks applied to inexact graph matching, Pattern Recognition 35 (12) (2002) 2867-2880."
      },
      {
        "ref_id": "8",
        "text": "P.A.N. Bosman, D. Thierens, An algorithmic framework for density estimation based evolutionary algorithms, Technical Report UU-CS-1999-46, the Department of Information and Computing Sciences, Utrecht University, 1999."
      },
      {
        "ref_id": "9",
        "text": "E. Cantú-Paz, Efficient and Accurate Parallel Genetic Algorithms, Kluwer Academic Publishers, Boston, MA, 2000, pp. 68-70."
      },
      {
        "ref_id": "10",
        "text": "O. Daly, D. Taniar, Exception rules mining based on negative association rules, in: Computational Science and Applications, Lecture Notes in Computer Science, Part IV, vol. 3046, Springer-Verlag, 2004, pp. 543-552."
      },
      {
        "ref_id": "11",
        "text": "J. Darroch, D. Ratcliff, Generalized iterative scaling for log-linear models, Annals of Mathematical Statistics 43 (1972) 1470-1480."
      },
      {
        "ref_id": "12",
        "text": "J.S. De Bonet, C.L. Isbell, P. Viola, MIMIC: Finding optima by estimating probability densities, in: M. Mozer, M. Jordan, T. Petsche (Eds.), Advances in Neural Information Processing Systems, vol. 9, MIT Press, Cambridge, MA, 1997, pp. 424-431."
      },
      {
        "ref_id": "13",
        "text": "S. Della Pietra, V. Della Pietra, J. Lafferty, Inducing features of random fields, IEEE Transactions on Pattern Analysis and Machine Intelligence 19 (4) (1997) 380-393."
      },
      {
        "ref_id": "14",
        "text": "R.O. Duda, P.E. Hart, Pattern Classification and Scene Analysis, John Wiley \\& Sons, New York, NY, 1973."
      },
      {
        "ref_id": "15",
        "text": "A.A. Freitas, Data Mining and Knowledge Discovery with Evolutionary Algorithms, Springer-Verlag, Berlin, 2002, pp. 108-109."
      },
      {
        "ref_id": "16",
        "text": "D.E. Goldberg, P. Segrest, Finite Markov chain analysis of genetic algorithms, in: J. Grefenstette (Ed.), Genetic Algorithms and Their Applications: Proceedings of the Second International Conference on Genetic Algorithms, Lawrence Erlbaum Associates, Hillsdale, NJ, 1987, pp. 1-8."
      },
      {
        "ref_id": "17",
        "text": "D.E. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learning, Addison-Wesley Longman Publishing Co., Boston, MA, 1989."
      },
      {
        "ref_id": "18",
        "text": "J. Goodman, Classes for fast maximum entropy training, in: Proceedings of International Conference on Acoustics, Speech, and Signal Processing, IEEE CS Press, 2001, pp. 557-560."
      },
      {
        "ref_id": "19",
        "text": "J. Goodman, Sequential conditional generalized iterative scaling, in: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, July 2002, pp. 9-16."
      },
      {
        "ref_id": "20",
        "text": "G.R. Harik, Linkage learning via probabilistic modelling in the ECGA, Illegal Report No. 99010, Illinois Genetic Algorithm Laboratory, University of Illinois, Urbana, IL, 1999."
      },
      {
        "ref_id": "21",
        "text": "G.R. Harik, F.G. Lobo, D.E. Goldberg, The Compact genetic algorithm, in: Proceedings of IEEE Conference on Evolutionary Computation, IEEE Press, 1998, pp. 523-528."
      },
      {
        "ref_id": "22",
        "text": "F.S. Hillier, G.J. Lieberman, Introduction to Operations Research, seventh ed., McGraw-Hill, Boston, MA, 2001."
      },
      {
        "ref_id": "23",
        "text": "Y. Huang, H. Xiong, W. Wu, P. Deng, Z. Zhang, Mining maximal hyperclique pattern: a hybrid search strategy, Information Sciences 177 (3) (2007) 703-721."
      },
      {
        "ref_id": "24",
        "text": "I. Inza, P. Larrañaga, B. Sierra, Estimation of distribution algorithms for feature subset selection in large dimensionality domains, in: H.A. Abbas, R.A. Sarker, C.S. Newton (Eds.), Data Mining: A Heuristic Approach, Idea Group Publishing, Hershey, PA, 2002, pp. $97-116$."
      },
      {
        "ref_id": "25",
        "text": "M. Iosifescu, Finite Markov Processes and Their Applications, John Wiley, New York, NY, 1980."
      },
      {
        "ref_id": "26",
        "text": "A.K. Jain, R.P.W. Duin, J. Mao, Statistical pattern recognition: a review, IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (1) (2000) 4-37."
      },
      {
        "ref_id": "27",
        "text": "E.T. Jaynes, Information theory and statistical mechanics, Physical Reviews 106 (1957) 620-630."
      },
      {
        "ref_id": "28",
        "text": "J.N. Kapur, H.K. Kesavan, Entropy Optimization Principles with Applications, Academic Press, Boston, MA, 1992."
      },
      {
        "ref_id": "29",
        "text": "P. Larrañaga, A review on estimation of distribution algorithms, in: P. Larrañaga, J.A. Lozano (Eds.), Estimation of Distribution Algorithms: A New Tool for Evolutionary Optimization, Kluwer Academic Publishers, Boston, MA, 2001, pp. 57-100."
      },
      {
        "ref_id": "30",
        "text": "P. Larrañaga, R. Etxeberria, J.A. Lozano, J.M. Peña, Combinatorial optimization by learning and simulation of Bayesian networks, in: The Proceeding of the Sixteenth Conference on Uncertainty in Artificial Intelligence, Standford, 2000, pp. 343-352."
      },
      {
        "ref_id": "31",
        "text": "P. Larrañaga, R. Etxeberria, J.A. Lozano, J.M. Peña, Optimization by learning and simulation of Bayesian and Gaussian networks, Technical Report KZZA-IK-4-99, Department of Computer Science and Artificial Intelligence, University of the Basque Country, 1999 ."
      },
      {
        "ref_id": "32",
        "text": "P. Larrañaga, R. Etxeberria, J.A. Lozano, J.M. Peña, Optimization in continuous domain by learning and simulation of Gaussian networks, in: The Proceeding of the 2000 Genetic and Evolutionary Computation Conference Workshop Program, Las Vegas, Nevada, 2000, pp. 201-204."
      },
      {
        "ref_id": "33",
        "text": "R. Malouf, A Comparison of algorithms for maximum entropy parameter estimation, in: Proceedings of Sixth Conference on Natural Language Learning, 2002, pp. 49-55."
      },
      {
        "ref_id": "34",
        "text": "H. Mühlenbein, T. Mahnig, The Factorized distribution algorithm for additively decomposed functions, in: Proceedings of Congress on Evolutionary Computation, IEEE Press, 1999, pp. 752-759."
      },
      {
        "ref_id": "35",
        "text": "H. Mühlenbein, G. Paaß, From combination of genes to the estimation of distributions: binary parameters, in: H.M. Voigt (Ed.), Parallel Problem Solving from Nature - PPSN IV, Lecture Notes in Computer Science, vol. 1411, Springer-Verlag, Berlin, 1996, pp. $178-187$."
      },
      {
        "ref_id": "36",
        "text": "R.M. Neal, Probabilistic inference using Markov chain Monte Carlo methods, Technical Report CRG-TR-93-1, Department of Computer Science, University of Toronto, 1993."
      },
      {
        "ref_id": "37",
        "text": "D.J. Newman, S. Hettich, C.L. Blake, C.J. Merz, UCI Repository of machine learning databases, University of California, Department of Information and Computer Science, Irvine, CA, 1998. <http://www.ics.uci.edu/ mlearn/MLRepository.html>."
      },
      {
        "ref_id": "38",
        "text": "A. Nix, M.D. Vose, Modeling genetic algorithms with Markov chains, Annals of Mathematics and Artificial Intelligence 5 (1992) 2734 ."
      },
      {
        "ref_id": "39",
        "text": "T.K. Paul, H. Iba, Linear and combinatorial optimizations by estimation of distribution algorithms, in: The Proceeding of the 9th MPS Symposium on Evolutionary Computation, Japan, 2002, pp. 99-106."
      },
      {
        "ref_id": "40",
        "text": "M. Pelikan, D.E. Goldberg, E. Cantú-Paz, Linkage problem, distribution estimation and Bayesian networks, Evolutionary Computation 8 (3) (2000) 311-340."
      },
      {
        "ref_id": "41",
        "text": "G. Rudolph, Convergence analysis of canonical genetic algorithms, IEEE Transactions on Neural Networks 5 (1) (1994) 96-101."
      },
      {
        "ref_id": "42",
        "text": "L. Tan, D. Taniar, K.A. Smith, A clustering algorithm based on an estimated distribution model, International Journal of Business Intelligence and Data Mining 1 (2) (2006) 229-245."
      },
      {
        "ref_id": "43",
        "text": "L. Tan, D. Taniar, K.A. Smith, Maximum-entropy estimated distribution classification model, International Journal of Hybrid Intelligence Systems 3 (1) (2006) 1-10."
      },
      {
        "ref_id": "44",
        "text": "H.C. Tjioe, D. Taniar, Mining association rules in data warehouses, International Journal of Data Warehousing and Mining 1 (3) (2005) $28-62$."
      },
      {
        "ref_id": "45",
        "text": "Y-J. Tsay, Y-W. Chang-Chien, An efficient cluster and decomposition algorithm for mining association rules, Information Sciences $160(1-4)(2004) 161-171$."
      },
      {
        "ref_id": "46",
        "text": "J. Wang, X. Wu, C. Zhang, Support vector machines based on $K$-means clustering for real-time business intelligence systems, International Journal of Business Intelligence and Data Mining 1 (1) (2005) 54-64."
      },
      {
        "ref_id": "47",
        "text": "A. Wright, R. Poli, C. Stephens, W.B. Landgon, S. Pulavarty, An estimation of distribution algorithm based on maximum entropy, in: Proceedings of GECCO 2004, Lecture Notes in Computer Science, LNCS, vol. 3103, Springer-Verlag, 2004, pp. 343-354."
      },
      {
        "ref_id": "48",
        "text": "L. Yan, D.J. Miller, General statistical inference for discrete and mixed spaces by an approximate application of the maximum entropy principle, IEEE Transactions on Neural Networks 11 (3) (2000) 558-573."
      },
      {
        "ref_id": "49",
        "text": "S.X. Yu, P. Scheunders, Feature selection for high-dimensional remote sensing data by maximum entropy principle based optimization, Proceedings of Geoscience \\& Remote Sensing Symposium, vol. 7, IEEE Press, 2001, pp. 3303-3305."
      }
    ],
    "reference_count": 49,
    "pattern_matched": "(?:^|\\n)#+\\s*References?\\s*\\n"
  },
  "tables": [
    {
      "table_number": "1",
      "table_title": "The adaptive functions notation",
      "headers": [
        "Notation",
        "Description"
      ],
      "rows": [
        [
          "$f_{\\text {max }}$",
          "Maximum scaled fitness value"
        ],
        [
          "$f_{\\text {min }}$",
          "Minimum scaled fitness value"
        ],
        [
          "$f_{t}$",
          "Average fitness at generation $t$"
        ],
        [
          "$f_{s}, t$",
          "Average fitness of selected population at generation $t$"
        ],
        [
          "$f_{\\text {max }}^{\\prime}$",
          "Maximum scaled fitness in selected population at generation $t$"
        ],
        [
          "$f_{\\text {min }}^{\\prime}$",
          "Minimum scaled fitness in selected population at generation $t$"
        ],
        [
          "$p_{z}$",
          "Threshold weight of constraints"
        ],
        [
          "$p_{\\eta}$",
          "Threshold weight of selected population"
        ],
        [
          "$p_{z \\text { max }}$",
          "Maximum threshold value of $p_{z}$"
        ],
        [
          "$p_{z \\text { min }}$",
          "Minimum threshold value of $p_{z}$"
        ],
        [
          "$p_{\\eta \\text { max }}$",
          "Maximum threshold value of $p_{\\eta}$"
        ],
        [
          "$p_{\\eta \\text { min }}$",
          "Minimum threshold value of $p_{\\eta}$"
        ]
      ],
      "row_count": 12,
      "column_count": 2
    },
    {
      "table_number": "2",
      "table_title": "The UCI data sets",
      "headers": [
        "Data sets",
        "Instances",
        "Attributes",
        "Classes"
      ],
      "rows": [
        [
          "contact-lenses",
          24,
          4,
          3
        ],
        [
          "credit-a",
          690,
          15,
          2
        ],
        [
          "diabetes",
          768,
          8,
          2
        ],
        [
          "glass",
          214,
          9,
          7
        ],
        [
          "iris",
          150,
          4,
          3
        ],
        [
          "labour",
          57,
          16,
          2
        ],
        [
          "mushroom",
          8124,
          22,
          2
        ],
        [
          "segment",
          2310,
          19,
          7
        ],
        [
          "sonar",
          208,
          60,
          2
        ],
        [
          "soybean",
          683,
          35,
          19
        ],
        [
          "splice",
          3190,
          62,
          3
        ],
        [
          "vote",
          435,
          16,
          2
        ],
        [
          "vowel*",
          990,
          10,
          11
        ],
        [
          "weather",
          14,
          4,
          2
        ],
        [
          "zoo*",
          101,
          17,
          7
        ]
      ],
      "row_count": 15,
      "column_count": 4
    },
    {
      "table_number": "3",
      "table_title": "The best fitness and the run-time on 15 data sets",
      "headers": [
        "Data sets",
        "GA",
        "",
        "GauED",
        "",
        "MEED",
        ""
      ],
      "rows": [
        [
          "",
          "Fitness",
          "Time (s)",
          "Fitness",
          "Time (s)",
          "Fitness",
          "Time (s)"
        ],
        [
          "contact-lenses",
          22.94,
          1.89,
          24.67,
          2.16,
          24.81,
          1.85
        ],
        [
          "credit-a",
          562.44,
          104.7,
          693.18,
          119.57,
          695.94,
          92.91
        ],
        [
          "diabetes",
          354.43,
          76.02,
          387.06,
          85.95,
          394.85,
          67.08
        ],
        [
          "glass",
          81.5,
          64.39,
          80.8,
          71.37,
          85.92,
          51.33
        ],
        [
          "iris",
          355.18,
          11.23,
          361.9,
          12.8,
          363.19,
          9.94
        ],
        [
          "labour",
          33.47,
          9.35,
          41.44,
          9.98,
          41.44,
          6.46
        ],
        [
          "mushroom",
          7314.78,
          1546.86,
          9016.59,
          1745.88,
          9113.99,
          1403.09
        ],
        [
          "segment",
          1428.47,
          1235.3,
          1307.66,
          1287.73,
          1384.82,
          916.87
        ],
        [
          "sonar",
          113.97,
          113.41,
          129.41,
          123.23,
          129.25,
          94.62
        ],
        [
          "soybean",
          65.37,
          1261.37,
          63.73,
          1329.66,
          69.11,
          907.45
        ],
        [
          "splice",
          1762.93,
          1973.88,
          1859.11,
          2292.89,
          1903.68,
          2107.8
        ],
        [
          "vote",
          914.35,
          59.22,
          914.97,
          73.03,
          914.74,
          50.87
        ],
        [
          "vowel",
          140.74,
          500.57,
          122.24,
          567.34,
          154.77,
          383.02
        ],
        [
          "weather",
          20.13,
          0.86,
          20.02,
          0.83,
          20.13,
          0.12
        ],
        [
          "zoo",
          55.68,
          39.08,
          55.28,
          39.34,
          55.62,
          28.68
        ]
      ],
      "row_count": 16,
      "column_count": 7
    },
    {
      "table_number": "4",
      "table_title": "The error rate on 15 data sets",
      "headers": [
        "Data sets",
        "$k$-means",
        "",
        "GA",
        "",
        "GauED",
        "",
        "MEED",
        ""
      ],
      "rows": [
        [
          "",
          "Mean",
          "Std. dev.",
          "Mean",
          "Std. dev.",
          "Mean",
          "Std. dev.",
          "Mean",
          "Std. dev."
        ],
        [
          "contact-lenses",
          0.27,
          0.14,
          0.38,
          0.08,
          0.43,
          0.02,
          0.38,
          0.04
        ],
        [
          "credit-a",
          0.68,
          0.06,
          0.36,
          0.07,
          0.31,
          0.0,
          0.31,
          0.01
        ],
        [
          "diabetes",
          0.95,
          0.0,
          0.38,
          0.08,
          0.35,
          0.0,
          0.41,
          0.13
        ],
        [
          "glass",
          0.31,
          0.0,
          0.6,
          0.06,
          0.55,
          0.09,
          0.51,
          0.08
        ],
        [
          "iris",
          0.69,
          0.0,
          0.18,
          0.1,
          0.12,
          0.01,
          0.12,
          0.0
        ],
        [
          "labour",
          0.29,
          0.02,
          0.29,
          0.22,
          0.09,
          0.0,
          0.09,
          0.0
        ],
        [
          "mushroom",
          0.6,
          0.02,
          0.4,
          0.14,
          0.29,
          0.13,
          0.29,
          0.13
        ],
        [
          "segment",
          0.99,
          0.01,
          0.55,
          0.13,
          0.37,
          0.06,
          0.4,
          0.05
        ],
        [
          "sonar",
          0.98,
          0.01,
          0.53,
          0.02,
          0.49,
          0.04,
          0.5,
          0.04
        ],
        [
          "soybean",
          0.06,
          0.05,
          0.54,
          0.1,
          0.38,
          0.04,
          0.34,
          0.04
        ],
        [
          "splice",
          0.73,
          0.03,
          0.56,
          0.07,
          0.51,
          0.07,
          0.52,
          0.06
        ],
        [
          "vote",
          0.14,
          0.0,
          0.35,
          0.18,
          0.18,
          0.11,
          0.13,
          0.0
        ],
        [
          "vowel",
          0.99,
          0.0,
          0.75,
          0.03,
          0.72,
          0.03,
          0.73,
          0.04
        ],
        [
          "weather",
          0.17,
          0.13,
          0.37,
          0.08,
          0.36,
          0.0,
          0.36,
          0.0
        ],
        [
          "zoo",
          0.2,
          0.02,
          0.36,
          0.2,
          0.12,
          0.03,
          0.12,
          0.05
        ]
      ],
      "row_count": 16,
      "column_count": 9
    }
  ]
}