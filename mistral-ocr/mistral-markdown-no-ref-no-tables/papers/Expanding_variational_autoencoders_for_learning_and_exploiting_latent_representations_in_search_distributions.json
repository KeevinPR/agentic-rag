{
  "metadata": {
    "file_path": "/Users/id05309/Documents/tfm/mistral/mistral-markdown/2018/Expanding variational autoencoders for learning and exploiting latent representations in search distributions.md",
    "filename": "Expanding variational autoencoders for learning and exploiting latent representations in search distributions.md",
    "title": "Expanding variational autoencoders for learning and exploiting latent representations in search distributions",
    "year": "2018"
  },
  "references": {
    "header": "## REFERENCES",
    "content": "[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citto, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, and others. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR abs/1603.04467 (2016). http://arxiv.org/abs/1603.04467\n[2] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. 1987. A learning algorithm for Boltzmann machines. In Readings in Computer Vision. Elsevier, $522-533$.\n[3] Shumeet Baluja. 2017. Deep Learning for Explicitly Modeling Optimization Landscapes. CoRR abs/1703.07394 (2017). http://arxiv.org/abs/1703.07394\n[4] A. E. I. Brownlee, J. A. McCall, and S. K. Shakya. 2012. The Markov Network Fitness Model. In Markov Networks in Evolutionary Computation, S. Shakya and R. Santana (Eds.). Springer, 125-140.\n[5] D. Cho and B. Zhang. 2004. Evolutionary Continuous Optimization by Distribution Estimation with Variational Bayesian Independent Component Analyzers Mixture Model. In Parallel Problem Solving from Nature (PPSN VIII), Vol. 3242. Springer, 212-221.\n[6] Alexander W Churchill, Siddharth Sigtia, and Christantha Fernando. 2014. A denoising autoencoder that guides stochastic search. CoRR abs/1404.1614 (2014). http://arxiv.org/abs/1404.1614\n[7] C.A.C. Corllo, G.B. Lamont, and D.A. Van Veldhuizen. 2007. Evolutionary Algorithms for Solving Multi-objective Problems. Springer-Verlag New York Inc.\n[8] Marcus Gallagher, Marcus Frean, and Tom Downs. 1999. Real-valued Evolutionary Optimization using a Flexible Probability Density Estimator. In Proceedings of the Genetic and Evolutionary Computation Conference GECCO-1999, Vol. I. Morgan Kaufmann Publishers, San Francisco, CA, Orlando, FL, 840-846.\n[9] Unai Garciarena, Roberto Santana, and Alexander Mendiburu. 2018. Evolved GANs for generating Pareto set approximations. In Proceedings of the 2018 on Genetic and Evolutionary Computation Conference Companion. ACM.. Accepted for publication.\n[10] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 249-256.\n[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Wankt-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems, 2672-2680.\n[12] Nikolaus Hansen. 2006. The CMA evolution strategy: a comparing review. In Towards a new evolutionary computation. Springer, 75-102.\n[13] Hsiao-Ping Hsu, Vishal Mehra, and Peter Grassberger. 2003. Structure optimization in an off-lattice protein model. Physical Review E 68, 2 (2003), 4 pages. article number 037703 .\n[14] Daniel Jiswong Im, Sungjin Ahn, Roland Memisevic, Yoshua Bengio, and others. 2017. Denoising Criterion for Variational Auto-Encoding Framework. In Proceedings of 31st National Conference on Artificial Intelligence AAAI-2017, 2059-2065.\n[15] H. Karshman, R. Santana, C. Bielea, and P. Larrahaga. 2014. Multiobjective Estimation of Distribution Algorithm Based on Joint Modeling of Objectives and Variables. IEEE Transactions on Evolutionary Computation 18, 4 (2014), 519-542.\n[16] Tseksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. 2017. Learning to discover cross-domain relations with generative adversarial networks. CoRR abs/1703.0519 (2017). http://arxiv.org/abs/1703.05192\n[17] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational Bayes. CoRR abs/1312.6114 (2013). http://arxiv.org/abs/1312.6114\n[18] Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathematical statistics 22, 1 (1951), 79-86.\n[19] P. Larrahaga and J. A. Lozano (Eds.). 2002. Estimation of Distribution Algorithms. A New Tool for Evolutionary Computation. Kluwer Academic Publishers.\n[20] H. Li and Q. Zhang. 2008. Multiobjective Optimization Problems with Complicated Pareto Sets, MOEA/D and NSGA-II. IEEE Transactions on Evolutionary Computation 13, 2 (2000), 284-302.\n[21] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. 2015. Adversarial autoencoders. CoRR abs/1511.05644 (2015). http://arxiv. org/abs/1511.05644\n[22] Marcella SR Martins, Mohamed El Yafrani, Myriam RBS Delgado, Markus Wagner, Belald Ahiod, and Ricardo Lüders. 2017. HSEDA: a heuristic selection approach based on estimation of distribution algorithm for the travelling thief problem. In Proceedings of the Genetic and Evolutionary Computation Conference. ACM, $361-368$.\n[23] H. Mühlenbein and G. Paaß. 1996. From recombination of genes to the estimation of distributions I. Binary parameters. In Parallel Problem Solving from Nature PPSN IV (Lectures Notes in Computer Science), Vol. 1141. Springer, Berlin, 178-187.\n[24] Malte Probst. 2015. Denoising autoencoders for fast combinatorial black box optimization. In Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation. ACM, 1459-1460.\n[25] Malte Probst. 2015. Generative Adversarial Networks in Estimation of Distribution Algorithms for Combinatorial Optimization. CoRR abs/1509.09235 (2015). http://arxiv.org/abs/1509.09235\n[26] Malte Probst, Franz Rothlauf, and Jörn Grahl. 2017. Scalability of using Restricted Boltzmann Machines for combinatorial optimization. European Journal of Operational Research 256, 2 (2017), 368-383.\n[27] Jason Tyler Rolfe. 2016. Discrete variational autoencoders. CoRR abs/1609.02200 (2016). http://arxiv.org/abs/1609.02200\n[28] Ruslan R Salakhutdinov and Geoffrey E Hinton. 2008. Using deep belief nets to learn covariance kernels for Gaussian processes. In Advances in neural information processing systems, 1249-1256.\n[29] Roberto Santana. 2017. Gray-box optimization and factorized distribution algorithms: where two worlds collide. CoRR abs/1707.03093 (2017). https: //arxiv.org/abs/1707.03093\n[30] S. Shakya and R. Santana (Eds.). 2012. Markov Networks in Evolutionary Computation. Springer.\n[31] S. K. Shakya, A. E. I. Brownlee, J. McCall, W. Fournier, and G. Owusu. 2009. A fully multivariate DEUM algorithm. In Proceedings of the 2009 Congress on Evolutionary Computation CEC-2009. IEEE Press, Norway, 479-486.\n[32] Paul Smolensky. 1986. Information processing in dynamical systems: Foundations of harmony theory. Technical Report CU-CS-321-86. Colorado University at Boulder, Dept. of Computer Science.\n[33] F.H. Stillinger, T. Head-Gordon, and C. Hirshfeld. 1993. Toy Model for Protein Folding. Physical Review E 48 (1993), 1469-1477.\n[34] H. Tang, V.A. Shim, K.C. Tan, and J.Y. Chia. 2010. Restricted Boltzmann machine based algorithm for multi-objective optimization. In Evolutionary Computation (CEC), 2010 IEEE Congress on. IEEE, 1-8.\n[35] Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics bulletin 1, 6 (1945), 80-85.",
    "references": [
      {
        "ref_id": "1",
        "text": "Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citto, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, and others. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR abs/1603.04467 (2016). http://arxiv.org/abs/1603.04467"
      },
      {
        "ref_id": "2",
        "text": "David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. 1987. A learning algorithm for Boltzmann machines. In Readings in Computer Vision. Elsevier, $522-533$."
      },
      {
        "ref_id": "3",
        "text": "Shumeet Baluja. 2017. Deep Learning for Explicitly Modeling Optimization Landscapes. CoRR abs/1703.07394 (2017). http://arxiv.org/abs/1703.07394"
      },
      {
        "ref_id": "4",
        "text": "A. E. I. Brownlee, J. A. McCall, and S. K. Shakya. 2012. The Markov Network Fitness Model. In Markov Networks in Evolutionary Computation, S. Shakya and R. Santana (Eds.). Springer, 125-140."
      },
      {
        "ref_id": "5",
        "text": "D. Cho and B. Zhang. 2004. Evolutionary Continuous Optimization by Distribution Estimation with Variational Bayesian Independent Component Analyzers Mixture Model. In Parallel Problem Solving from Nature (PPSN VIII), Vol. 3242. Springer, 212-221."
      },
      {
        "ref_id": "6",
        "text": "Alexander W Churchill, Siddharth Sigtia, and Christantha Fernando. 2014. A denoising autoencoder that guides stochastic search. CoRR abs/1404.1614 (2014). http://arxiv.org/abs/1404.1614"
      },
      {
        "ref_id": "7",
        "text": "C.A.C. Corllo, G.B. Lamont, and D.A. Van Veldhuizen. 2007. Evolutionary Algorithms for Solving Multi-objective Problems. Springer-Verlag New York Inc."
      },
      {
        "ref_id": "8",
        "text": "Marcus Gallagher, Marcus Frean, and Tom Downs. 1999. Real-valued Evolutionary Optimization using a Flexible Probability Density Estimator. In Proceedings of the Genetic and Evolutionary Computation Conference GECCO-1999, Vol. I. Morgan Kaufmann Publishers, San Francisco, CA, Orlando, FL, 840-846."
      },
      {
        "ref_id": "9",
        "text": "Unai Garciarena, Roberto Santana, and Alexander Mendiburu. 2018. Evolved GANs for generating Pareto set approximations. In Proceedings of the 2018 on Genetic and Evolutionary Computation Conference Companion. ACM.. Accepted for publication."
      },
      {
        "ref_id": "10",
        "text": "Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 249-256."
      },
      {
        "ref_id": "11",
        "text": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Wankt-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems, 2672-2680."
      },
      {
        "ref_id": "12",
        "text": "Nikolaus Hansen. 2006. The CMA evolution strategy: a comparing review. In Towards a new evolutionary computation. Springer, 75-102."
      },
      {
        "ref_id": "13",
        "text": "Hsiao-Ping Hsu, Vishal Mehra, and Peter Grassberger. 2003. Structure optimization in an off-lattice protein model. Physical Review E 68, 2 (2003), 4 pages. article number 037703 ."
      },
      {
        "ref_id": "14",
        "text": "Daniel Jiswong Im, Sungjin Ahn, Roland Memisevic, Yoshua Bengio, and others. 2017. Denoising Criterion for Variational Auto-Encoding Framework. In Proceedings of 31st National Conference on Artificial Intelligence AAAI-2017, 2059-2065."
      },
      {
        "ref_id": "15",
        "text": "H. Karshman, R. Santana, C. Bielea, and P. Larrahaga. 2014. Multiobjective Estimation of Distribution Algorithm Based on Joint Modeling of Objectives and Variables. IEEE Transactions on Evolutionary Computation 18, 4 (2014), 519-542."
      },
      {
        "ref_id": "16",
        "text": "Tseksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. 2017. Learning to discover cross-domain relations with generative adversarial networks. CoRR abs/1703.0519 (2017). http://arxiv.org/abs/1703.05192"
      },
      {
        "ref_id": "17",
        "text": "Diederik P Kingma and Max Welling. 2013. Auto-encoding variational Bayes. CoRR abs/1312.6114 (2013). http://arxiv.org/abs/1312.6114"
      },
      {
        "ref_id": "18",
        "text": "Solomon Kullback and Richard A Leibler. 1951. On information and sufficiency. The annals of mathematical statistics 22, 1 (1951), 79-86."
      },
      {
        "ref_id": "19",
        "text": "P. Larrahaga and J. A. Lozano (Eds.). 2002. Estimation of Distribution Algorithms. A New Tool for Evolutionary Computation. Kluwer Academic Publishers."
      },
      {
        "ref_id": "20",
        "text": "H. Li and Q. Zhang. 2008. Multiobjective Optimization Problems with Complicated Pareto Sets, MOEA/D and NSGA-II. IEEE Transactions on Evolutionary Computation 13, 2 (2000), 284-302."
      },
      {
        "ref_id": "21",
        "text": "Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. 2015. Adversarial autoencoders. CoRR abs/1511.05644 (2015). http://arxiv. org/abs/1511.05644"
      },
      {
        "ref_id": "22",
        "text": "Marcella SR Martins, Mohamed El Yafrani, Myriam RBS Delgado, Markus Wagner, Belald Ahiod, and Ricardo Lüders. 2017. HSEDA: a heuristic selection approach based on estimation of distribution algorithm for the travelling thief problem. In Proceedings of the Genetic and Evolutionary Computation Conference. ACM, $361-368$."
      },
      {
        "ref_id": "23",
        "text": "H. Mühlenbein and G. Paaß. 1996. From recombination of genes to the estimation of distributions I. Binary parameters. In Parallel Problem Solving from Nature PPSN IV (Lectures Notes in Computer Science), Vol. 1141. Springer, Berlin, 178-187."
      },
      {
        "ref_id": "24",
        "text": "Malte Probst. 2015. Denoising autoencoders for fast combinatorial black box optimization. In Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation. ACM, 1459-1460."
      },
      {
        "ref_id": "25",
        "text": "Malte Probst. 2015. Generative Adversarial Networks in Estimation of Distribution Algorithms for Combinatorial Optimization. CoRR abs/1509.09235 (2015). http://arxiv.org/abs/1509.09235"
      },
      {
        "ref_id": "26",
        "text": "Malte Probst, Franz Rothlauf, and Jörn Grahl. 2017. Scalability of using Restricted Boltzmann Machines for combinatorial optimization. European Journal of Operational Research 256, 2 (2017), 368-383."
      },
      {
        "ref_id": "27",
        "text": "Jason Tyler Rolfe. 2016. Discrete variational autoencoders. CoRR abs/1609.02200 (2016). http://arxiv.org/abs/1609.02200"
      },
      {
        "ref_id": "28",
        "text": "Ruslan R Salakhutdinov and Geoffrey E Hinton. 2008. Using deep belief nets to learn covariance kernels for Gaussian processes. In Advances in neural information processing systems, 1249-1256."
      },
      {
        "ref_id": "29",
        "text": "Roberto Santana. 2017. Gray-box optimization and factorized distribution algorithms: where two worlds collide. CoRR abs/1707.03093 (2017). https: //arxiv.org/abs/1707.03093"
      },
      {
        "ref_id": "30",
        "text": "S. Shakya and R. Santana (Eds.). 2012. Markov Networks in Evolutionary Computation. Springer."
      },
      {
        "ref_id": "31",
        "text": "S. K. Shakya, A. E. I. Brownlee, J. McCall, W. Fournier, and G. Owusu. 2009. A fully multivariate DEUM algorithm. In Proceedings of the 2009 Congress on Evolutionary Computation CEC-2009. IEEE Press, Norway, 479-486."
      },
      {
        "ref_id": "32",
        "text": "Paul Smolensky. 1986. Information processing in dynamical systems: Foundations of harmony theory. Technical Report CU-CS-321-86. Colorado University at Boulder, Dept. of Computer Science."
      },
      {
        "ref_id": "33",
        "text": "F.H. Stillinger, T. Head-Gordon, and C. Hirshfeld. 1993. Toy Model for Protein Folding. Physical Review E 48 (1993), 1469-1477."
      },
      {
        "ref_id": "34",
        "text": "H. Tang, V.A. Shim, K.C. Tan, and J.Y. Chia. 2010. Restricted Boltzmann machine based algorithm for multi-objective optimization. In Evolutionary Computation (CEC), 2010 IEEE Congress on. IEEE, 1-8."
      },
      {
        "ref_id": "35",
        "text": "Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics bulletin 1, 6 (1945), 80-85."
      }
    ],
    "reference_count": 35,
    "pattern_matched": "(?:^|\\n)#+\\s*References?\\s*\\n"
  },
  "tables": [
    {
      "table_number": null,
      "table_title": null,
      "headers": [
        "Structure",
        "VAE",
        "E-VAE",
        "CE-VAE"
      ],
      "rows": [
        [
          "VAE",
          0,
          247,
          206
        ],
        [
          "E-VAE",
          253,
          0,
          196
        ],
        [
          "CE-VAE",
          294,
          304,
          0
        ]
      ],
      "row_count": 3,
      "column_count": 4
    },
    {
      "table_number": null,
      "table_title": null,
      "headers": [
        "Str.",
        "F1",
        "F2",
        "F3",
        "F4",
        "F5",
        "F7",
        "F8",
        "F9"
      ],
      "rows": [
        [
          "VAE",
          403,
          502,
          461,
          462,
          480,
          449,
          488,
          453
        ],
        [
          "E-VAE",
          413,
          437,
          465,
          414,
          427,
          411,
          508,
          449
        ],
        [
          "CE-VAE",
          678,
          560,
          568,
          621,
          589,
          635,
          499,
          598
        ]
      ],
      "row_count": 3,
      "column_count": 9
    },
    {
      "table_number": "2",
      "table_title": "Scores for all functions. These scores were computed by adding all the scores in each row of the score matrix for each function. See an example of these matrices in Table 1. These numbers were rounded to integers. A larger value denotes a better performance.",
      "headers": [
        "Structure",
        "VAE",
        "E-VAE",
        "CE-VAE"
      ],
      "rows": [
        [
          "VAE",
          0,
          45.2308,
          -89.7701
        ],
        [
          "E-VAE",
          -45.2308,
          0,
          -135.001
        ],
        [
          "CE-VAE",
          89.7701,
          135.001,
          0
        ]
      ],
      "row_count": 3,
      "column_count": 4
    },
    {
      "table_number": "3",
      "table_title": "Performance index example for $F 9$. The lower the number, the better the performance by the model",
      "headers": [
        "Str.",
        "F1",
        "F2",
        "F3",
        "F4",
        "F5",
        "F7",
        "F8",
        "F9"
      ],
      "rows": [
        [
          "VAE",
          -274,
          43,
          25,
          14,
          54,
          -272,
          -129,
          -44
        ],
        [
          "E-VAE",
          -648,
          -231,
          -169,
          -272,
          -224,
          -378,
          -324,
          -180
        ],
        [
          "CE-VAE",
          923,
          187,
          143,
          257,
          169,
          651,
          453,
          224
        ]
      ],
      "row_count": 3,
      "column_count": 9
    },
    {
      "table_number": "5",
      "table_title": "Overall mean results for the 5 search algorithms, for 3 problem dimensions, with 3 different population sizes.",
      "headers": [
        "Dim.",
        "21",
        "",
        "",
        "",
        "",
        "34",
        "",
        "",
        "",
        ""
      ],
      "rows": [
        [
          "Pop.",
          "Rnd",
          "$U_{C}$",
          "V",
          "E-V",
          "CE-V",
          "Rnd",
          "$U_{C}$",
          "V",
          "E-V",
          "CE-V"
        ],
        [
          50,
          1,
          22,
          1,
          2,
          18,
          30,
          30,
          29,
          29,
          30
        ],
        [
          100,
          0,
          18,
          0,
          0,
          2,
          30,
          27,
          28,
          29,
          30
        ],
        [
          200,
          0,
          14,
          0,
          0,
          0,
          28,
          27,
          6,
          14,
          30
        ],
        [
          300,
          0,
          9,
          0,
          0,
          0,
          0,
          28,
          25,
          0,
          2
        ]
      ],
      "row_count": 5,
      "column_count": 11
    }
  ]
}