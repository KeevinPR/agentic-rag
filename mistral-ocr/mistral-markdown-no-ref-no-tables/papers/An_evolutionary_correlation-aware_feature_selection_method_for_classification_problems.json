{
  "metadata": {
    "file_path": "/Users/id05309/Documents/tfm/mistral/mistral-markdown/2022/An evolutionary correlation-aware feature selection method for classification problems.md",
    "filename": "An evolutionary correlation-aware feature selection method for classification problems.md",
    "title": "An evolutionary correlation-aware feature selection method for classification problems",
    "year": "2022"
  },
  "references": {
    "header": "## References",
    "content": "[1] Y. Cao, T.A. Geddes, J.Y.H. Yang, P. Yang, Ensemble deep learning in bioinformatics, Nat. Mach. Intell. 2 (9) (2020) 500-508, https://doi.org/10.1038/ s42256-020-0217-y\n[2] I.P. Kilincer, F. Ertan, A. Sengur, Machine learning methods for cyber security intrusion detection: datasets and comparative study, Comput. Netw. 188 (Apr. 2021), 107940, https://doi.org/10.1016/j.comnet.2021.107940.\n[3] M. Dong, L. Yao, X. Wang, B. Benazallah, C. Huang, X. Ning, Opinion fraud detection via neural autoencoder decision forest, Pattern Recognit. Lett. 132 (Apr. 2020) 21-29, https://doi.org/10.1016/j.patres.2018.07.013.\n[4] Z.K. Sonturk, Early diagnosis of Parkinson's disease using machine learning algorithms, Med. Hypotheses 138 (2020), 109603, https://doi.org/10.1016/j. mrhy.2020.109603.\n[5] N. Maleki, Y. Zeinali, S.T.A. Niaki, A k-NN method for lung cancer prognosis with the use of a genetic algorithm for feature selection, Expert Syst. Appl. 164 (2021), 113981, https://doi.org/10.1016/j.eswa.2020.113981.\n[6] S. Lalmuanawma, J. Hussain, L. Chhakchhuak, Applications of machine learning and artificial intelligence for Covid-19 (SARS-CoV-2) pandemic: a review, Chaos Solitons Fractals 139 (2020), 110059, https://doi.org/10.1016/j. chaos.2020.110059.\n[7] J. Kim, J. Kang, M. Sohn, Ensemble learning-based filter-centric hybrid feature selection framework for high-dimensional imbalanced data, Knowl. Based Syst. 220 (2021), 106901, https://doi.org/10.1016/j.knosys.2021.106901.\n[8] B. Xue, M. Zhang, W.N. Browne, X. Yao, A survey on evolutionary computation approaches to feature selection, IEEE Trans. Evol. Comput. 20 (4) (2016) 606-626, https://doi.org/10.1109/TEVC.2015.2504420.\n[9] B. Tran, B. Xue, M. Zhang, Variable-length particle swarm optimization for feature selection on high-dimensional classification, IEEE Trans. Evol. Comput. 23 (3) (2019) 473-487, https://doi.org/10.1109/TEVC.2018.2869405.\n[10] S. Sayed, M. Naseef, A. Badr, I. Farag, A Nested Genetic Algorithm for feature selection in high-dimensional cancer Microarray datasets, Expert Syst. Appl. 121 (2019) 233-243, https://doi.org/10.1016/j.eswa.2018.12.022.\n[11] C.B. Gokulnath, S.P. Shantharajah, An optimized feature selection based on genetic approach and support vector machine for heart disease, Clust. Comput. 22 (6) (2019) 14777-14787, https://doi.org/10.1007/s10586-019-2416-4.\n[12] F. Amini, G. Hu, A two-layer feature selection method using Genetic Algorithm and Elastic Net, Expert Syst. Appl. 166 (2021), 114072, https://doi.org/10.1016/j. eswa.2020.114072.\n[13] M. Amoszegar, B. Minaei-Bidgoli, Optimizing multi-objective PSO based feature selection method using a feature elitism mechanism, Expert Syst. Appl. 113 (2018) 499-514, https://doi.org/10.1016/j.eswa.2018.07.013.\n[14] R.K. Huda, H. Banka, Efficient feature selection and classification algorithm based on PSO and rough sets, Neural Comput. Appl. 31 (8) (2019) 4287-4303, https:// doi.org/10.1007/s00521-017-3317-9.\n[15] Y. Xue, T. Tang, W. Pang, A.X. Liu, Self-adaptive parameter and strategy based particle swarm optimization for large-scale feature selection problems with multiple classifiers, Appl. Soft. Comput. J. 88 (Mar. 2020), 106031, https://doi. org/10.1016/j.asoc.2019.106031.\n[16] X. fang Song, Y. Zhang, D. wei Gong, X. yan Sun, Feature selection using barebones particle swarm optimization with mutual information, Pattern Recognit. 112 (Apr. 2021), 107804, https://doi.org/10.1016/j.patrog.2020.107804.\n[17] H. Peng, C. Ying, S. Tan, B. Hu, Z. Sun, An improved feature selection algorithm based on ant colony optimization, IEEE Access 6 (2018) 69203-69209, https://doi. org/10.1109/ACE255.2018.2879503.\n[18] R. Joseph Manoj, M.D. Anto Pravenna, K. Vijayakumar, An ACO-ANN based feature selection algorithm for big data, Clust. Comput. 22 (2) (2019) 3953-3960, https://doi.org/10.1007/s10586-018-2550-z.\n[19] W. Ma, X. Zhou, H. Zhu, L. Li, L. Jiao, A two-stage hybrid ant colony optimization for high-dimensional feature selection, Pattern Recognit. 116 (Aug. 2021), 107933, https://doi.org/10.1016/j.patrog.2021.107933.\n[20] R.J. Kuo, S.R.L. Huang, F.E. Zalvia, T.W. Liao, Artificial bee colony-based support vector machines with feature selection and parameter optimization for rule extraction, Knowl. Inf. Syst. 55 (1) (2018) 253-274, https://doi.org/10.1007/ s10115-017-1083-8.\n[21] Y. Zhang, S. Cheng, Y. Shi, D. Gong, X. Zhao, Cost-sensitive feature selection using two-archive multi-objective artificial bee colony algorithm, Expert Syst. Appl. 137 (2019) 46-58, https://doi.org/10.1016/j.eswa.2019.06.044.\n[22] X. Wang, Y. Zhang, X. Sun, Y. Wang, C. Du, Multi-objective feature selection based on artificial bee colony: an acceleration approach with variable sample size, Appl. Soft Comput. 88 (2020), 106041, https://doi.org/10.1016/j.asoc.2019.106041.\n[23] M.A. El Aziz, A.E. Hassanien, Modified cuckoo search algorithm with rough sets for feature selection, Neural Comput. Appl. 29 (4) (2018) 925-934, https://doi.org/ 10.1007/s00521-016-2473-7.\n[24] A.C. Pandey, D.S. Rajpoot, M. Saraswat, Feature selection method based on hybrid data transformation and binary binomial cuckoo search, J. Ambient Intell. Humanis. Comput. 11 (2) (2020) 719-738, https://doi.org/10.1007/s12652-019- 01330-1.\n[25] M. Hauschild, M. Pelikan, An introduction and survey of estimation of distribution algorithms, Swarm Evol. Comput. 1 (3) (2011) 111-128, https://doi.org/10.1016/ j.sawvo.2011.08.003.\n[26] H. Karobenas, R. Santana, C. Bielza, P. Larrataga, Multiobjective estimation of distribution algorithm based on joint modeling of objectives and variables, IEEE Trans. Evol. Comput. 18 (4) (2014) 519-542, https://doi.org/10.1109/ TEXC.2013.2281524.\n[27] F. Tan, X. Fu, Y. Zhang, A.G. Bourgeois, A genetic algorithm-based method for feature subset selection, Soft Comput. 12 (2) (2008) 111-120, https://doi.org/ 10.1007/s00509-007-0193-8.\n[28] S. Oreski, G. Oreski, Genetic algorithm-based heuristic for feature selection in credit risk assessment, Expert Syst. Appl. 41 (4) (2014) 2052-2064, https://doi. org/10.1016/j.eswa.2013.09.004. Part 2.\n[29] F. Modehi, A. Haeri, A novel hybrid wrapper-filter approach based on genetic algorithm, particle swarm optimization for feature subset selection, J. Ambient Intell. Humanis. Comput. 11 (3) (2020) 1105-1127, https://doi.org/10.1007. s12652-019-01364-5.\n[30] A.K. Das, S. Das, A. Ghosh, Ensemble feature selection using bi-objective genetic algorithm, Knowl. Based Syst. 123 (May 2017) 116-127, https://doi.org/10.1016/ j.knosys.2017.02.013.\n[31] J. Kennedy, R. Eberhart, Particle swarm optimization 4 (1995) 1942-1948, https://doi.org/10.1109/8305.1995.488968.\n[32] P. Moradi, M. Gholampour, A hybrid particle swarm optimization for feature subset selection by integrating a novel local search strategy, Appl. Soft. Comput. 43 (2016) 117-130, https://doi.org/10.1016/j.asoc.2016.01.044.\n[33] M. Dorigo and G. Di Caro, \"Ant colony optimization: a new meta-heuristic,\" in Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 997H8406), 1999, vol. 2, pp. 1470-1477, doi: 10.1109/CEC.1999.782657.\n[34] M. Ghosh, R. Guba, R. Sarkar, A. Abraham, A wrapper-filter feature selection technique based on ant colony optimization, Neural Comput. Appl. 32 (12) (2020) 7839-7857, https://doi.org/10.1007/s00521-019-04171-3.\n[35] D. Karaboga, An idea based on how to use swarm for numerical optimization, in: Technical report-pt06, Enziyes University, Engineering Faculty, Computer Engineering Department, 2005.\n[36] E. Hauser, B. Xue, M. Zhang, D. Karaboga, B. Akay, Pareto front feature selection based on artificial bee colony optimization, Inf. Sci. (Ny). 422 (2018) 462-479, https://doi.org/10.1016/j.iso.2017.09.026.\n[37] S. Maza, M. Tsoudiris, Feature selection for intrusion detection using new multiobjective estimation of distribution algorithms, Appl. Intell. 49 (12) (2019) 4237-4257, https://doi.org/10.1007/s10489-019-01503-7.\n[38] G.K. Hardi, F.G. Lobo, D.E. Goldberg, The compact genetic algorithm, IEEE Trans. Evol. Comput. 3 (4) (1999) 287-297.\n[39] I. Inas, P. Larrataga, B. Sierra, Feature Subset Selection by Estimation of Distribution Algorithms BT - Estimation of Distribution Algorithms: a New Tool for Evolutionary Computation, Springer US, Boston, MA, 2002, pp. 269-293. P. Larrataga and J. A. Lutano, Eds.\n[40] L. Zhang, K. Mistry, C.P. Lim, S.C. Neoh, Feature selection using firefly optimization for classification and regression models, Decis. Support Syst. 106 (2018) 64-85, https://doi.org/10.1016/j.dss.2017.12.001.\n[41] M.A. Laamari, N. Kamel, A Hybrid Bat Based Feature Selection Approach for Intrusion Detection, BT: Bio-Inspired Computing - Theories and Applications (2014) 230-238, https://doi.org/10.1007/978-3-662-45049-9_38.\n[42] M. Subarkar, Z. Aydin, A noise-aware feature selection approach for classification, Soft. Comput. 25 (8) (2021) 6391-6400, https://doi.org/10.1007/s00500-021-05630-7.\n\n[43] R. Katuwal, P.N. Suganthan, L. Zhang, Heterogeneous oblique random forest, Pattern Recognit. 99 (2020), 107078, https://doi.org/10.1016/j. patcog.2019.107078.\n[44] D. Dua, C. Graff, Machine Learning Repository, Universityof California, Irvine, School of Information and Computer Sciences (2017). http://archive.ics.uci.edu /mi.\n[45] F. Wilcoxon, Individual Comparisons by Ranking Methods, in: S. Kotz, N. L. Johnson (Eds.), Breakthroughs in Statistics, Springer Series in Statistics, Springer, New York, NY, 1992, pp. 196-202, https://doi.org/10.1007/978-1-4612-4380-9_16.\n[46] M. Friedman, The use of ranks to avoid the assumption of normality implicit in the analysis of variance, J. Am. Stat. Assoc. 32 (200) (1937) 675-701, https://doi.org/ 10.1080/01621459.1937.10503522.\n[47] A. Thakkar, R. Lohiya, Attack classification using feature selection techniques: a comparative study, J. Ambient Intell. Humanis. Comput. 12 (1) (2021) 1249-1266, https://doi.org/10.1007/s12652-020-02167-9.\n[48] M. Tavallaee, E. Bagheri, W. Lu, A.A. Ghorbani, A detailed analysis of the KDD CUP 99 data set, in: Proceedings of the IEEE Symposium on Computational Intelligence for Security and Defense Applications, IEEE, 2009, pp. 1-6.\n[49] K. Michalak, H. Kwasnicka, Correlation based feature selection method, Int. J. BioInspired Comput. 2 (5) (2010) 319-332, https://doi.org/10.1504/ IJBIC.2010.036158.\n[50] H. Tian, S.-C. Chen, M.-L. Shyu, Evolutionary programming based deep learning feature selection and network construction for visual data classification, Inf. Syst. Front. 22 (5) (2020) 1053-1066, https://doi.org/10.1007/s10796-020-10023-6.",
    "references": [
      {
        "ref_id": "1",
        "text": "Y. Cao, T.A. Geddes, J.Y.H. Yang, P. Yang, Ensemble deep learning in bioinformatics, Nat. Mach. Intell. 2 (9) (2020) 500-508, https://doi.org/10.1038/ s42256-020-0217-y"
      },
      {
        "ref_id": "2",
        "text": "I.P. Kilincer, F. Ertan, A. Sengur, Machine learning methods for cyber security intrusion detection: datasets and comparative study, Comput. Netw. 188 (Apr. 2021), 107940, https://doi.org/10.1016/j.comnet.2021.107940."
      },
      {
        "ref_id": "3",
        "text": "M. Dong, L. Yao, X. Wang, B. Benazallah, C. Huang, X. Ning, Opinion fraud detection via neural autoencoder decision forest, Pattern Recognit. Lett. 132 (Apr. 2020) 21-29, https://doi.org/10.1016/j.patres.2018.07.013."
      },
      {
        "ref_id": "4",
        "text": "Z.K. Sonturk, Early diagnosis of Parkinson's disease using machine learning algorithms, Med. Hypotheses 138 (2020), 109603, https://doi.org/10.1016/j. mrhy.2020.109603."
      },
      {
        "ref_id": "5",
        "text": "N. Maleki, Y. Zeinali, S.T.A. Niaki, A k-NN method for lung cancer prognosis with the use of a genetic algorithm for feature selection, Expert Syst. Appl. 164 (2021), 113981, https://doi.org/10.1016/j.eswa.2020.113981."
      },
      {
        "ref_id": "6",
        "text": "S. Lalmuanawma, J. Hussain, L. Chhakchhuak, Applications of machine learning and artificial intelligence for Covid-19 (SARS-CoV-2) pandemic: a review, Chaos Solitons Fractals 139 (2020), 110059, https://doi.org/10.1016/j. chaos.2020.110059."
      },
      {
        "ref_id": "7",
        "text": "J. Kim, J. Kang, M. Sohn, Ensemble learning-based filter-centric hybrid feature selection framework for high-dimensional imbalanced data, Knowl. Based Syst. 220 (2021), 106901, https://doi.org/10.1016/j.knosys.2021.106901."
      },
      {
        "ref_id": "8",
        "text": "B. Xue, M. Zhang, W.N. Browne, X. Yao, A survey on evolutionary computation approaches to feature selection, IEEE Trans. Evol. Comput. 20 (4) (2016) 606-626, https://doi.org/10.1109/TEVC.2015.2504420."
      },
      {
        "ref_id": "9",
        "text": "B. Tran, B. Xue, M. Zhang, Variable-length particle swarm optimization for feature selection on high-dimensional classification, IEEE Trans. Evol. Comput. 23 (3) (2019) 473-487, https://doi.org/10.1109/TEVC.2018.2869405."
      },
      {
        "ref_id": "10",
        "text": "S. Sayed, M. Naseef, A. Badr, I. Farag, A Nested Genetic Algorithm for feature selection in high-dimensional cancer Microarray datasets, Expert Syst. Appl. 121 (2019) 233-243, https://doi.org/10.1016/j.eswa.2018.12.022."
      },
      {
        "ref_id": "11",
        "text": "C.B. Gokulnath, S.P. Shantharajah, An optimized feature selection based on genetic approach and support vector machine for heart disease, Clust. Comput. 22 (6) (2019) 14777-14787, https://doi.org/10.1007/s10586-019-2416-4."
      },
      {
        "ref_id": "12",
        "text": "F. Amini, G. Hu, A two-layer feature selection method using Genetic Algorithm and Elastic Net, Expert Syst. Appl. 166 (2021), 114072, https://doi.org/10.1016/j. eswa.2020.114072."
      },
      {
        "ref_id": "13",
        "text": "M. Amoszegar, B. Minaei-Bidgoli, Optimizing multi-objective PSO based feature selection method using a feature elitism mechanism, Expert Syst. Appl. 113 (2018) 499-514, https://doi.org/10.1016/j.eswa.2018.07.013."
      },
      {
        "ref_id": "14",
        "text": "R.K. Huda, H. Banka, Efficient feature selection and classification algorithm based on PSO and rough sets, Neural Comput. Appl. 31 (8) (2019) 4287-4303, https:// doi.org/10.1007/s00521-017-3317-9."
      },
      {
        "ref_id": "15",
        "text": "Y. Xue, T. Tang, W. Pang, A.X. Liu, Self-adaptive parameter and strategy based particle swarm optimization for large-scale feature selection problems with multiple classifiers, Appl. Soft. Comput. J. 88 (Mar. 2020), 106031, https://doi. org/10.1016/j.asoc.2019.106031."
      },
      {
        "ref_id": "16",
        "text": "X. fang Song, Y. Zhang, D. wei Gong, X. yan Sun, Feature selection using barebones particle swarm optimization with mutual information, Pattern Recognit. 112 (Apr. 2021), 107804, https://doi.org/10.1016/j.patrog.2020.107804."
      },
      {
        "ref_id": "17",
        "text": "H. Peng, C. Ying, S. Tan, B. Hu, Z. Sun, An improved feature selection algorithm based on ant colony optimization, IEEE Access 6 (2018) 69203-69209, https://doi. org/10.1109/ACE255.2018.2879503."
      },
      {
        "ref_id": "18",
        "text": "R. Joseph Manoj, M.D. Anto Pravenna, K. Vijayakumar, An ACO-ANN based feature selection algorithm for big data, Clust. Comput. 22 (2) (2019) 3953-3960, https://doi.org/10.1007/s10586-018-2550-z."
      },
      {
        "ref_id": "19",
        "text": "W. Ma, X. Zhou, H. Zhu, L. Li, L. Jiao, A two-stage hybrid ant colony optimization for high-dimensional feature selection, Pattern Recognit. 116 (Aug. 2021), 107933, https://doi.org/10.1016/j.patrog.2021.107933."
      },
      {
        "ref_id": "20",
        "text": "R.J. Kuo, S.R.L. Huang, F.E. Zalvia, T.W. Liao, Artificial bee colony-based support vector machines with feature selection and parameter optimization for rule extraction, Knowl. Inf. Syst. 55 (1) (2018) 253-274, https://doi.org/10.1007/ s10115-017-1083-8."
      },
      {
        "ref_id": "21",
        "text": "Y. Zhang, S. Cheng, Y. Shi, D. Gong, X. Zhao, Cost-sensitive feature selection using two-archive multi-objective artificial bee colony algorithm, Expert Syst. Appl. 137 (2019) 46-58, https://doi.org/10.1016/j.eswa.2019.06.044."
      },
      {
        "ref_id": "22",
        "text": "X. Wang, Y. Zhang, X. Sun, Y. Wang, C. Du, Multi-objective feature selection based on artificial bee colony: an acceleration approach with variable sample size, Appl. Soft Comput. 88 (2020), 106041, https://doi.org/10.1016/j.asoc.2019.106041."
      },
      {
        "ref_id": "23",
        "text": "M.A. El Aziz, A.E. Hassanien, Modified cuckoo search algorithm with rough sets for feature selection, Neural Comput. Appl. 29 (4) (2018) 925-934, https://doi.org/ 10.1007/s00521-016-2473-7."
      },
      {
        "ref_id": "24",
        "text": "A.C. Pandey, D.S. Rajpoot, M. Saraswat, Feature selection method based on hybrid data transformation and binary binomial cuckoo search, J. Ambient Intell. Humanis. Comput. 11 (2) (2020) 719-738, https://doi.org/10.1007/s12652-019- 01330-1."
      },
      {
        "ref_id": "25",
        "text": "M. Hauschild, M. Pelikan, An introduction and survey of estimation of distribution algorithms, Swarm Evol. Comput. 1 (3) (2011) 111-128, https://doi.org/10.1016/ j.sawvo.2011.08.003."
      },
      {
        "ref_id": "26",
        "text": "H. Karobenas, R. Santana, C. Bielza, P. Larrataga, Multiobjective estimation of distribution algorithm based on joint modeling of objectives and variables, IEEE Trans. Evol. Comput. 18 (4) (2014) 519-542, https://doi.org/10.1109/ TEXC.2013.2281524."
      },
      {
        "ref_id": "27",
        "text": "F. Tan, X. Fu, Y. Zhang, A.G. Bourgeois, A genetic algorithm-based method for feature subset selection, Soft Comput. 12 (2) (2008) 111-120, https://doi.org/ 10.1007/s00509-007-0193-8."
      },
      {
        "ref_id": "28",
        "text": "S. Oreski, G. Oreski, Genetic algorithm-based heuristic for feature selection in credit risk assessment, Expert Syst. Appl. 41 (4) (2014) 2052-2064, https://doi. org/10.1016/j.eswa.2013.09.004. Part 2."
      },
      {
        "ref_id": "29",
        "text": "F. Modehi, A. Haeri, A novel hybrid wrapper-filter approach based on genetic algorithm, particle swarm optimization for feature subset selection, J. Ambient Intell. Humanis. Comput. 11 (3) (2020) 1105-1127, https://doi.org/10.1007. s12652-019-01364-5."
      },
      {
        "ref_id": "30",
        "text": "A.K. Das, S. Das, A. Ghosh, Ensemble feature selection using bi-objective genetic algorithm, Knowl. Based Syst. 123 (May 2017) 116-127, https://doi.org/10.1016/ j.knosys.2017.02.013."
      },
      {
        "ref_id": "31",
        "text": "J. Kennedy, R. Eberhart, Particle swarm optimization 4 (1995) 1942-1948, https://doi.org/10.1109/8305.1995.488968."
      },
      {
        "ref_id": "32",
        "text": "P. Moradi, M. Gholampour, A hybrid particle swarm optimization for feature subset selection by integrating a novel local search strategy, Appl. Soft. Comput. 43 (2016) 117-130, https://doi.org/10.1016/j.asoc.2016.01.044."
      },
      {
        "ref_id": "33",
        "text": "M. Dorigo and G. Di Caro, \"Ant colony optimization: a new meta-heuristic,\" in Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 997H8406), 1999, vol. 2, pp. 1470-1477, doi: 10.1109/CEC.1999.782657."
      },
      {
        "ref_id": "34",
        "text": "M. Ghosh, R. Guba, R. Sarkar, A. Abraham, A wrapper-filter feature selection technique based on ant colony optimization, Neural Comput. Appl. 32 (12) (2020) 7839-7857, https://doi.org/10.1007/s00521-019-04171-3."
      },
      {
        "ref_id": "35",
        "text": "D. Karaboga, An idea based on how to use swarm for numerical optimization, in: Technical report-pt06, Enziyes University, Engineering Faculty, Computer Engineering Department, 2005."
      },
      {
        "ref_id": "36",
        "text": "E. Hauser, B. Xue, M. Zhang, D. Karaboga, B. Akay, Pareto front feature selection based on artificial bee colony optimization, Inf. Sci. (Ny). 422 (2018) 462-479, https://doi.org/10.1016/j.iso.2017.09.026."
      },
      {
        "ref_id": "37",
        "text": "S. Maza, M. Tsoudiris, Feature selection for intrusion detection using new multiobjective estimation of distribution algorithms, Appl. Intell. 49 (12) (2019) 4237-4257, https://doi.org/10.1007/s10489-019-01503-7."
      },
      {
        "ref_id": "38",
        "text": "G.K. Hardi, F.G. Lobo, D.E. Goldberg, The compact genetic algorithm, IEEE Trans. Evol. Comput. 3 (4) (1999) 287-297."
      },
      {
        "ref_id": "39",
        "text": "I. Inas, P. Larrataga, B. Sierra, Feature Subset Selection by Estimation of Distribution Algorithms BT - Estimation of Distribution Algorithms: a New Tool for Evolutionary Computation, Springer US, Boston, MA, 2002, pp. 269-293. P. Larrataga and J. A. Lutano, Eds."
      },
      {
        "ref_id": "40",
        "text": "L. Zhang, K. Mistry, C.P. Lim, S.C. Neoh, Feature selection using firefly optimization for classification and regression models, Decis. Support Syst. 106 (2018) 64-85, https://doi.org/10.1016/j.dss.2017.12.001."
      },
      {
        "ref_id": "41",
        "text": "M.A. Laamari, N. Kamel, A Hybrid Bat Based Feature Selection Approach for Intrusion Detection, BT: Bio-Inspired Computing - Theories and Applications (2014) 230-238, https://doi.org/10.1007/978-3-662-45049-9_38."
      },
      {
        "ref_id": "42",
        "text": "M. Subarkar, Z. Aydin, A noise-aware feature selection approach for classification, Soft. Comput. 25 (8) (2021) 6391-6400, https://doi.org/10.1007/s00500-021-05630-7."
      },
      {
        "ref_id": "43",
        "text": "R. Katuwal, P.N. Suganthan, L. Zhang, Heterogeneous oblique random forest, Pattern Recognit. 99 (2020), 107078, https://doi.org/10.1016/j. patcog.2019.107078."
      },
      {
        "ref_id": "44",
        "text": "D. Dua, C. Graff, Machine Learning Repository, Universityof California, Irvine, School of Information and Computer Sciences (2017). http://archive.ics.uci.edu /mi."
      },
      {
        "ref_id": "45",
        "text": "F. Wilcoxon, Individual Comparisons by Ranking Methods, in: S. Kotz, N. L. Johnson (Eds.), Breakthroughs in Statistics, Springer Series in Statistics, Springer, New York, NY, 1992, pp. 196-202, https://doi.org/10.1007/978-1-4612-4380-9_16."
      },
      {
        "ref_id": "46",
        "text": "M. Friedman, The use of ranks to avoid the assumption of normality implicit in the analysis of variance, J. Am. Stat. Assoc. 32 (200) (1937) 675-701, https://doi.org/ 10.1080/01621459.1937.10503522."
      },
      {
        "ref_id": "47",
        "text": "A. Thakkar, R. Lohiya, Attack classification using feature selection techniques: a comparative study, J. Ambient Intell. Humanis. Comput. 12 (1) (2021) 1249-1266, https://doi.org/10.1007/s12652-020-02167-9."
      },
      {
        "ref_id": "48",
        "text": "M. Tavallaee, E. Bagheri, W. Lu, A.A. Ghorbani, A detailed analysis of the KDD CUP 99 data set, in: Proceedings of the IEEE Symposium on Computational Intelligence for Security and Defense Applications, IEEE, 2009, pp. 1-6."
      },
      {
        "ref_id": "49",
        "text": "K. Michalak, H. Kwasnicka, Correlation based feature selection method, Int. J. BioInspired Comput. 2 (5) (2010) 319-332, https://doi.org/10.1504/ IJBIC.2010.036158."
      },
      {
        "ref_id": "50",
        "text": "H. Tian, S.-C. Chen, M.-L. Shyu, Evolutionary programming based deep learning feature selection and network construction for visual data classification, Inf. Syst. Front. 22 (5) (2020) 1053-1066, https://doi.org/10.1007/s10796-020-10023-6."
      }
    ],
    "reference_count": 50,
    "pattern_matched": "(?:^|\\n)#+\\s*References?\\s*\\n"
  },
  "tables": [
    {
      "table_number": "1",
      "table_title": "Comparison of the state-of-the-art FS methods that utilized population-based optimization.",
      "headers": [
        "Method",
        "Population-based algorithm",
        "Type",
        "Representation method",
        "The number of objectives",
        "Classifier"
      ],
      "rows": [
        [
          -28,
          "GA",
          "Wrapper",
          "Binary",
          "SO",
          "ANN"
        ],
        [
          -29,
          "GA, PSO",
          "Hybrid",
          "Continues",
          "SO",
          "ANN"
        ],
        [
          -30,
          "GA",
          "Filter",
          "Binary",
          "MO",
          "-"
        ],
        [
          -32,
          "PSO",
          "Hybrid",
          "Continues",
          "SO",
          "KNN"
        ],
        [
          13,
          "PSO",
          "Wrapper",
          "Continues",
          "MO",
          "KNN"
        ],
        [
          9,
          "PSO",
          "Hybrid",
          "Continues",
          "SO",
          "KNN"
        ],
        [
          -18,
          "ACO",
          "Wrapper",
          "Continues",
          "SO",
          "ANN"
        ],
        [
          34,
          "ACO",
          "Hybrid",
          "Continues",
          "SO",
          "KNN/ANN"
        ],
        [
          0.36,
          "ABC",
          "Wrapper",
          "Binary/ Continues",
          "MO",
          "KNN"
        ],
        [
          "ABC-SVM-DT [20]",
          "ABC",
          "Wrapper",
          "Continues",
          "SO",
          "SVM/DT"
        ],
        [
          -21,
          "ABC",
          "Wrapper",
          "Continues",
          "MO",
          "KNN"
        ],
        [
          37,
          "EDA",
          "Wrapper",
          "Binary",
          "MO",
          "-"
        ],
        [
          -39,
          "cGA",
          "Wrapper",
          "Continues",
          "SO",
          "Naive Bayes"
        ]
      ],
      "row_count": 13,
      "column_count": 6
    },
    {
      "table_number": "2",
      "table_title": "The update procedure of $S V$ based on the winner and the loser vectors.",
      "headers": [
        "loser",
        "$l_{i}=0$",
        "$l_{i}=1$"
      ],
      "rows": [
        [
          "winner",
          "$S V$",
          ""
        ],
        [
          "$w_{i}=0$",
          "$S V+$",
          "$S V-$"
        ]
      ],
      "row_count": 2,
      "column_count": 3
    },
    {
      "table_number": "3",
      "table_title": "The update procedure of IM based on the winner and the loser vectors.",
      "headers": [
        "loser",
        "$l_{i}, l_{j}=0,0$",
        "$l_{i}, l_{j}=0,1$",
        "$l_{i}, l_{j}=1,0$",
        "$l_{i}, l_{j}=1,1$"
      ],
      "rows": [
        [
          "winner",
          "",
          "",
          "",
          ""
        ],
        [
          "$w_{i}, w_{j}=0,0$",
          "IM",
          "IM",
          "IM",
          "IM -"
        ],
        [
          "$w_{i}, w_{j}=0,1$",
          "IM",
          "IM",
          "IM",
          "IM -"
        ],
        [
          "$w_{i}, w_{j}=1,0$",
          "IM",
          "IM",
          "IM",
          "IM -"
        ],
        [
          "$w_{i}, w_{j}=1,1$",
          "IM +",
          "IM ++",
          "IM ++",
          "IM"
        ]
      ],
      "row_count": 5,
      "column_count": 5
    },
    {
      "table_number": "4",
      "table_title": "Parameter setting.",
      "headers": [
        "Method",
        "Parameters",
        "Settings"
      ],
      "rows": [
        [
          "WFACOFS",
          "Exploitation balance factor",
          1
        ],
        [
          "",
          "Exploration balance factor",
          1
        ],
        [
          "",
          "Weight of accuracy",
          100
        ],
        [
          "",
          "Weight of number of features",
          1
        ],
        [
          "",
          "Pheromone evaporation factor",
          0.15
        ],
        [
          "",
          "Pheromone evaluation factor",
          0.8
        ],
        [
          "RSVM-SBS",
          "Kernel function",
          "RBF"
        ],
        [
          "",
          "C",
          100
        ],
        [
          "",
          "$\\sigma$",
          0.5
        ],
        [
          "GA-SVM",
          "Crossover rate",
          0.7
        ],
        [
          "",
          "Mutation rate",
          0.01
        ],
        [
          "",
          "Selection mechanism",
          "Roulette wheel"
        ],
        [
          "cGA-FS",
          "Np",
          0.6
        ],
        [
          4.5,
          "Confidence factor",
          0.25
        ],
        [
          "",
          "Min. instance per leaf",
          2
        ],
        [
          "Random forest",
          "The number of trees",
          200
        ],
        [
          "",
          "$\\mathrm{mtry}^{\\mathrm{n}}$",
          "$\\sqrt{n}$"
        ],
        [
          -43,
          "The number of trees",
          500
        ],
        [
          "",
          "Mtry",
          "$\\sqrt{n}$"
        ],
        [
          "The proposed Method",
          "Change factor",
          0.01
        ]
      ],
      "row_count": 20,
      "column_count": 3
    },
    {
      "table_number": "5",
      "table_title": "Details of datasets.",
      "headers": [
        "Dataset",
        "Size",
        "Number of <br> samples",
        "Number of <br> features",
        "Number of <br> classes"
      ],
      "rows": [
        [
          "Breast Cancer",
          "Small",
          699,
          9,
          2
        ],
        [
          "Glass",
          "Small",
          214,
          9,
          6
        ],
        [
          "Heart",
          "Medium",
          270,
          13,
          2
        ],
        [
          "Wine",
          "Medium",
          178,
          13,
          3
        ],
        [
          "Segmentation",
          "Medium",
          2310,
          19,
          7
        ],
        [
          "German",
          "Medium",
          1000,
          24,
          2
        ],
        [
          "Ionosphere",
          "Medium",
          351,
          34,
          2
        ],
        [
          "Soybean- <br> small",
          "Medium",
          47,
          35,
          4
        ],
        [
          "Sonar",
          "Medium",
          208,
          60,
          2
        ],
        [
          "Hill-valley",
          "Large",
          1212,
          100,
          2
        ],
        [
          1,
          "Large",
          476,
          167,
          2
        ],
        [
          "Arrhythmia",
          "Large",
          452,
          279,
          16
        ],
        [
          5,
          "Large",
          1559,
          617,
          26
        ]
      ],
      "row_count": 13,
      "column_count": 5
    },
    {
      "table_number": "6",
      "table_title": "The accuracy (\\%) for different methods on the synthetic dataset.",
      "headers": [
        "Method",
        "Accuracy",
        "Selected features"
      ],
      "rows": [
        [
          "GA-SVM",
          98.38,
          "$\\left(f_{1}, f_{2}, f_{3}, f_{7}, f_{4}, f_{5}\\right)$"
        ],
        [
          "cGA-FS",
          80.76,
          "$\\left(f_{1}, f_{2}, f_{3}, f_{4}, f_{6}, f_{5}\\right)$"
        ],
        [
          "WFACOFS",
          79.03,
          "$\\left(f_{1}, f_{2}, f_{3}, f_{4}, f_{7}, f_{5}, f_{10}\\right)$"
        ],
        [
          "MODEAFS",
          83.87,
          "$\\left(f_{1}, f_{2}, f_{3}, f_{7}, f_{8}, f_{5}, f_{10}\\right)$"
        ],
        [
          "RSVM-SBS",
          81.69,
          "$\\left(f_{1}, f_{2}, f_{3}, f_{4}, f_{7}, f_{6}, f_{10}\\right)$"
        ],
        [
          "The proposed Method",
          98.39,
          "$\\left(f_{1}, f_{2}, f_{3}, f_{4}, f_{10}\\right)$"
        ]
      ],
      "row_count": 6,
      "column_count": 3
    },
    {
      "table_number": "7",
      "table_title": "The average accuracy (\\%) metric for different methods on different datasets.",
      "headers": [
        "",
        "C4.5",
        "Random <br> forest",
        "$\\begin{aligned} & \\text { OblRF-H } \\\\ & \\text { (without } \\\\ & \\text { FS) } \\end{aligned}$",
        "OblRF-H <br> (with FS)",
        "The proposed method"
      ],
      "rows": [
        [
          "Breast Cancer",
          96.57,
          97.14,
          97.14,
          98.23,
          98.85
        ],
        [
          "Glass",
          66.04,
          75.47,
          70.26,
          71.04,
          71.7
        ],
        [
          "Heart",
          83.58,
          79.1,
          84.51,
          86.48,
          91.04
        ],
        [
          "Wine",
          97.73,
          93.18,
          97.32,
          98.62,
          99.36
        ],
        [
          "Segmentation",
          95.49,
          96.17,
          95.05,
          95.36,
          94.8
        ],
        [
          "German",
          75.2,
          76.8,
          75.15,
          79.31,
          82.8
        ],
        [
          "Ionosphere",
          93.18,
          94.32,
          94.16,
          95.58,
          95.45
        ],
        [
          "Soybean- <br> small",
          100.0,
          100.0,
          100.0,
          100.0,
          100.0
        ],
        [
          "Sonar",
          75.0,
          88.46,
          85.63,
          89.14,
          92.3
        ],
        [
          "Hill-valley",
          52.15,
          54.13,
          59.46,
          61.74,
          71.61
        ],
        [
          1,
          79.83,
          82.35,
          85.24,
          86.62,
          84.03
        ],
        [
          "Arrhythmia",
          66.37,
          71.45,
          71.08,
          72.4,
          72.66
        ],
        [
          5,
          76.41,
          88.79,
          87.96,
          89.15,
          90.0
        ]
      ],
      "row_count": 13,
      "column_count": 6
    },
    {
      "table_number": "8",
      "table_title": "The average F1-score (\\%) metric for different methods on different datasets.",
      "headers": [
        "",
        "C4.5",
        "Random <br> forest",
        "OblRF-H <br> (without <br> FS)",
        "OblRF-H <br> (with FS)",
        "The <br> proposed <br> method"
      ],
      "rows": [
        [
          "Breast Cancer",
          96.3,
          97.05,
          97.63,
          98.06,
          "$\\mathbf{9 8 . 8 9}$"
        ],
        [
          "Glass",
          55.56,
          65.8,
          66.14,
          "$\\mathbf{6 7 . 2 1}$",
          64.78
        ],
        [
          "Heart",
          82.85,
          78.29,
          83.47,
          86.76,
          "$\\mathbf{9 2 . 2 5}$"
        ],
        [
          "Wine",
          98.09,
          93.74,
          96.35,
          96.64,
          "$\\mathbf{9 9 . 4 6}$"
        ],
        [
          "Segmentation",
          95.58,
          97.61,
          "$\\mathbf{9 8 . 5 1}$",
          97.2,
          95.28
        ],
        [
          "German",
          70.79,
          69.39,
          74.2,
          73.41,
          "$\\mathbf{7 7 . 9 1}$"
        ],
        [
          "Ionosphere",
          92.84,
          93.12,
          93.62,
          95.0,
          "$\\mathbf{9 5 . 7 2}$"
        ],
        [
          "Soybean- <br> small",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$"
        ],
        [
          "Sonar",
          74.8,
          88.44,
          82.39,
          90.64,
          "$\\mathbf{9 3 . 1 2}$"
        ],
        [
          "Hill-valley",
          52.17,
          54.08,
          60.24,
          69.1,
          "$\\mathbf{7 5 . 3 9}$"
        ],
        [
          1,
          78.96,
          81.33,
          82.86,
          82.53,
          "$\\mathbf{8 3 . 1 4}$"
        ],
        [
          "Arrhythmia",
          39.87,
          40.01,
          38.24,
          39.79,
          "$\\mathbf{4 0 . 1 0}$"
        ],
        [
          5,
          77.2,
          90.19,
          88.28,
          90.21,
          "$\\mathbf{9 0 . 3 6}$"
        ]
      ],
      "row_count": 13,
      "column_count": 6
    },
    {
      "table_number": "9",
      "table_title": "$P$-value of Wilcoxon signed-rank test between the proposed approach and each other methods in terms of accuracy and F1-score.",
      "headers": [
        "Method",
        "$p$-value (accuracy)",
        "$p$-value (F1-score)"
      ],
      "rows": [
        [
          4.5,
          0.00097,
          0.00146
        ],
        [
          "Random forest",
          0.02539,
          0.01611
        ],
        [
          "OblRF-H (without FS)",
          0.00244,
          0.02685
        ],
        [
          "OblRF-H (with FS)",
          0.03417,
          0.04248
        ]
      ],
      "row_count": 4,
      "column_count": 3
    },
    {
      "table_number": "10",
      "table_title": "The best feature subsets and their performances.",
      "headers": [
        "",
        "Random forest",
        "The proposed method",
        "",
        ""
      ],
      "rows": [
        [
          "",
          "Best features",
          "Accuracy",
          "Best features",
          "Accuracy"
        ],
        [
          "Breast Cancer",
          "$(3,4,6,7)$",
          96.0,
          "$(2,4,7,9)$",
          97.71
        ],
        [
          "Glass",
          "$(1,2,3,4,6,7)$",
          67.92,
          "$(1,3,4,5,6,8)$",
          67.92
        ],
        [
          "Heart",
          "$(8,10,12,13)$",
          88.06,
          "$(2,3,12,13)$",
          94.03
        ],
        [
          "Wine",
          "$(7,10,12,13)$",
          97.73,
          "$(1,10,12,13)$",
          100.0
        ],
        [
          "Segmentation",
          "$(2,10,11,12,16,17,19)$",
          90.12,
          "$(1,2,10,12,16,18,19)$",
          92.2
        ],
        [
          "German",
          "$(1,3,4,5,8,9,10,11,12,13)$",
          76.8,
          "$(1,2,3,4,5,9,16,17,19,20)$",
          78.8
        ],
        [
          "Ionosphere",
          "$(3,4,5,6,7,8,16,27)$",
          87.5,
          "$(1,4,5,7,8,12,15,31)$",
          92.05
        ],
        [
          "Soybean-small",
          "$(21,22)$",
          100.0,
          "$(21,22)$",
          100.0
        ],
        [
          "Sonar",
          "$(9,10,11,12,13,51)$",
          86.54,
          "$(1,10,11,12,37,48)$",
          92.3
        ],
        [
          "Hill-valley",
          1,
          52.15,
          2,
          60.73
        ],
        [
          1,
          1,
          73.95,
          2,
          73.95
        ],
        [
          "Arrhythmia",
          1,
          68.14,
          2,
          69.91
        ],
        [
          5,
          1,
          89.23,
          2,
          90.26
        ]
      ],
      "row_count": 14,
      "column_count": 5
    },
    {
      "table_number": "11",
      "table_title": "The average accuracy (Acc), F1-score (F1), and the number of selected features (SF) for different methods.",
      "headers": [
        "",
        "IG-FS",
        "",
        "",
        "CS-FS",
        "",
        "",
        "RFE-FS",
        "",
        "",
        "The proposed method",
        "",
        ""
      ],
      "rows": [
        [
          "",
          "Acc",
          1,
          "SF",
          "Acc",
          1,
          "SF",
          "Acc",
          1,
          "SF",
          "Acc",
          1,
          "SF"
        ],
        [
          "Breast Cancer",
          95.43,
          94.71,
          7.2,
          94.86,
          94.18,
          7.4,
          94.29,
          93.37,
          5.3,
          98.85,
          98.89,
          4.3
        ],
        [
          "Glass",
          60.38,
          54.43,
          5.3,
          54.72,
          50.99,
          4.7,
          62.26,
          57.76,
          8.6,
          71.7,
          64.78,
          6.5
        ],
        [
          "Heart",
          82.09,
          83.27,
          6.7,
          82.09,
          80.86,
          7.3,
          83.54,
          84.5,
          10.2,
          91.04,
          92.25,
          4.6
        ],
        [
          "Wine",
          88.64,
          89.95,
          7.2,
          97.73,
          97.97,
          6.4,
          90.91,
          91.83,
          7.8,
          99.36,
          99.46,
          4.3
        ],
        [
          "Segmentation",
          82.84,
          81.86,
          7.8,
          84.24,
          86.54,
          8.9,
          84.23,
          83.58,
          5.6,
          94.8,
          95.28,
          8.7
        ],
        [
          "German",
          72.4,
          65.57,
          9.2,
          74.4,
          65.59,
          9.1,
          75.6,
          70.6,
          16.4,
          82.8,
          77.91,
          14.8
        ],
        [
          "Ionosphere",
          81.82,
          81.03,
          15.6,
          87.5,
          86.67,
          19.4,
          88.64,
          88.35,
          28.1,
          95.45,
          95.72,
          12.0
        ],
        [
          "Soybean-small",
          100.0,
          100.0,
          12.3,
          100.0,
          100.0,
          13.2,
          100.0,
          100.0,
          6.5,
          100.0,
          100.0,
          3.1
        ],
        [
          "Sonar",
          76.92,
          76.48,
          28.1,
          71.15,
          71.3,
          22.6,
          78.85,
          78.65,
          52.4,
          92.3,
          93.12,
          13.7
        ],
        [
          "Hill-valley",
          60.07,
          65.79,
          52.4,
          52.84,
          56.08,
          43.2,
          50.83,
          33.7,
          79.3,
          71.61,
          75.39,
          33.4
        ],
        [
          1,
          72.63,
          75.52,
          90.6,
          70.59,
          70.56,
          75.1,
          82.35,
          82.16,
          95.1,
          84.03,
          83.14,
          34.4
        ],
        [
          "Arrhythmia",
          61.06,
          28.45,
          121.4,
          59.29,
          39.65,
          101.5,
          55.75,
          30.4,
          98.2,
          72.66,
          40.1,
          22.0
        ],
        [
          5,
          84.12,
          80.24,
          305.1,
          79.45,
          80.64,
          276.8,
          87.18,
          85.42,
          241.4,
          90.0,
          90.36,
          141.3
        ]
      ],
      "row_count": 14,
      "column_count": 13
    },
    {
      "table_number": "12",
      "table_title": "P-value of Wilcoxon signed-rank test between the proposed approach and each other methods in terms of accuracy and F1-score.",
      "headers": [
        "Method",
        "$p$-value (accuracy)",
        "$p$-value (F1-score)"
      ],
      "rows": [
        [
          "IG-FS",
          0.0005,
          0.0005
        ],
        [
          "CS-FS",
          0.0005,
          0.0005
        ],
        [
          "RFE-FS",
          0.0005,
          0.0005
        ]
      ],
      "row_count": 3,
      "column_count": 3
    },
    {
      "table_number": "13",
      "table_title": "The average accuracy (Acc) and F1-score (F1) for different FS methods on NSL-KDD dataset.",
      "headers": [
        "",
        "DoS",
        "",
        "Probe",
        "",
        "R2L",
        "",
        "U2R",
        ""
      ],
      "rows": [
        [
          "",
          "Acc",
          1,
          "Acc",
          1,
          "Acc",
          1,
          "Acc",
          1
        ],
        [
          "IG-FS",
          98.88,
          98.4,
          99.2,
          98.78,
          98.65,
          98.41,
          97.68,
          92.72
        ],
        [
          "CS-FS",
          97.31,
          97.12,
          97.31,
          96.52,
          93.63,
          96.39,
          90.34,
          91.45
        ],
        [
          "RFE-FS",
          98.76,
          98.42,
          98.75,
          98.72,
          98.45,
          98.68,
          99.12,
          99.01
        ],
        [
          "The proposed method",
          98.88,
          99.24,
          99.35,
          99.39,
          99.12,
          99.32,
          98.27,
          98.87
        ]
      ],
      "row_count": 5,
      "column_count": 9
    },
    {
      "table_number": "14",
      "table_title": "The average accuracy (\\%) metric for different methods on different datasets.",
      "headers": [
        "",
        "All-features",
        "GA-SVM",
        "cGA-FS",
        "WFACOFS",
        "MOEDAFS",
        "RSVM-SBS",
        "The proposed Method"
      ],
      "rows": [
        [
          "Breast Cancer",
          70.29,
          96.57,
          94.85,
          97.71,
          96.0,
          96.58,
          "$\\mathbf{9 8 . 8 5}$"
        ],
        [
          "Glass",
          56.6,
          62.26,
          67.92,
          69.81,
          69.81,
          "$\\mathbf{7 2 . 5 6}$",
          71.7
        ],
        [
          "Heart",
          82.09,
          88.06,
          85.07,
          83.58,
          89.55,
          83.75,
          "$\\mathbf{9 1 . 0 4}$"
        ],
        [
          "Wine",
          93.18,
          97.89,
          97.22,
          97.72,
          98.83,
          98.04,
          "$\\mathbf{9 9 . 3 6}$"
        ],
        [
          "Segmentation",
          93.07,
          93.58,
          87.69,
          "$\\mathbf{9 5 . 3 2}$",
          94.28,
          91.76,
          94.8
        ],
        [
          "German",
          73.6,
          82.4,
          79.2,
          77.6,
          80.4,
          78.91,
          "$\\mathbf{8 2 . 8 0}$"
        ],
        [
          "Ionosphere",
          86.36,
          92.05,
          88.63,
          94.31,
          89.77,
          90.69,
          "$\\mathbf{9 5 . 4 5}$"
        ],
        [
          "Soybean-small",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          90.0,
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$"
        ],
        [
          "Sonar",
          73.08,
          78.84,
          82.69,
          90.38,
          90.38,
          89.03,
          "$\\mathbf{9 2 . 3 0}$"
        ],
        [
          "Hill-valley",
          48.18,
          "$\\mathbf{9 2 . 7 3}$",
          59.07,
          51.48,
          53.46,
          61.79,
          71.61
        ],
        [
          1,
          82.35,
          "$\\mathbf{9 2 . 4 3}$",
          68.06,
          86.55,
          86.55,
          82.74,
          84.03
        ],
        [
          "Arrhythmia",
          51.32,
          "$\\mathbf{7 4 . 3 3}$",
          61.94,
          60.17,
          61.06,
          63.97,
          72.66
        ],
        [
          "Isolett",
          76.4,
          "$\\mathbf{9 5 . 3 8}$",
          90.76,
          82.05,
          84.1,
          88.45,
          90.0
        ]
      ],
      "row_count": 13,
      "column_count": 8
    },
    {
      "table_number": "15",
      "table_title": "The average precision (\\%) metric for different methods on different datasets.",
      "headers": [
        "",
        "All-features",
        "GA-SVM",
        "cGA-FS",
        "WFACOFS",
        "MOEDAFS",
        "RSVM-SBS",
        "The proposed Method"
      ],
      "rows": [
        [
          "Breast Cancer",
          71.53,
          96.67,
          94.56,
          97.81,
          95.98,
          96.8,
          "$\\mathbf{9 8 . 8 0}$"
        ],
        [
          "Glass",
          43.53,
          63.3,
          54.19,
          61.42,
          59.59,
          66.73,
          "$\\mathbf{6 8 . 7 5}$"
        ],
        [
          "Heart",
          81.91,
          86.22,
          84.85,
          83.88,
          88.51,
          86.46,
          "$\\mathbf{9 1 . 7 0}$"
        ],
        [
          "Wine",
          93.89,
          97.87,
          97.43,
          98.14,
          98.43,
          97.52,
          "$\\mathbf{9 9 . 8 6}$"
        ],
        [
          "Segmentation",
          93.28,
          93.35,
          88.03,
          "$\\mathbf{9 5 . 4 0}$",
          94.46,
          73.19,
          94.81
        ],
        [
          "German",
          76.26,
          72.89,
          70.8,
          70.29,
          69.23,
          71.37,
          "$\\mathbf{7 7 . 2 4}$"
        ],
        [
          "Ionosphere",
          82.38,
          89.45,
          77.27,
          93.73,
          85.61,
          90.07,
          "$\\mathbf{9 4 . 2 9}$"
        ],
        [
          "Soybean-small",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          88.12,
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$"
        ],
        [
          "Sonar",
          76.67,
          80.0,
          82.68,
          90.29,
          90.74,
          91.61,
          "$\\mathbf{9 2 . 1 5}$"
        ],
        [
          "Hill-valley",
          48.13,
          "$\\mathbf{9 2 . 7 9}$",
          56.25,
          52.72,
          54.43,
          63.71,
          70.43
        ],
        [
          1,
          82.44,
          "$\\mathbf{9 2 . 1 0}$",
          68.43,
          86.5,
          87.0,
          87.11,
          83.75
        ],
        [
          "Arrhythmia",
          29.87,
          "$\\mathbf{5 5 . 1 4}$",
          38.76,
          13.61,
          18.18,
          33.08,
          36.2
        ],
        [
          "Isolett",
          67.96,
          "$\\mathbf{9 5 . 8 2}$",
          89.73,
          81.75,
          83.83,
          80.57,
          90.07
        ]
      ],
      "row_count": 13,
      "column_count": 8
    },
    {
      "table_number": "16",
      "table_title": "The average recall (\\%) metric for different methods on different datasets.",
      "headers": [
        "",
        "All-features",
        "GA-SVM",
        "cGA-FS",
        "WFACOFS",
        "MOEDAFS",
        "RSVM-SBS",
        "The proposed Method"
      ],
      "rows": [
        [
          "Breast Cancer",
          68.88,
          96.31,
          94.24,
          96.8,
          94.38,
          95.35,
          "$\\mathbf{9 8 . 8 0}$"
        ],
        [
          "Glass",
          33.85,
          "$\\mathbf{7 0 . 5 8}$",
          63.31,
          67.6,
          63.4,
          70.35,
          59.58
        ],
        [
          "Heart",
          84.1,
          89.95,
          83.88,
          84.18,
          90.05,
          89.06,
          "$\\mathbf{9 0 . 9 5}$"
        ],
        [
          "Wine",
          92.9,
          96.94,
          98.61,
          96.96,
          97.66,
          98.68,
          "$\\mathbf{9 8 . 8 1}$"
        ],
        [
          "Segmentation",
          93.93,
          93.43,
          88.45,
          95.76,
          94.89,
          81.5,
          "$\\mathbf{9 5 . 9 0}$"
        ],
        [
          "German",
          70.01,
          75.5,
          81.25,
          75.4,
          "$\\mathbf{8 2 . 7 7}$",
          75.96,
          80.3
        ],
        [
          "Ionosphere",
          85.42,
          93.24,
          93.42,
          94.24,
          86.74,
          94.85,
          "$\\mathbf{9 6 . 4 9}$"
        ],
        [
          "Soybean-small",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          86.22,
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$"
        ],
        [
          "Sonar",
          80.56,
          78.44,
          82.44,
          90.47,
          91.67,
          89.91,
          "$\\mathbf{9 2 . 6 5}$"
        ],
        [
          "Hill-valley",
          47.91,
          "$\\mathbf{9 2 . 8 1}$",
          60.59,
          53.21,
          54.65,
          66.76,
          80.26
        ],
        [
          1,
          82.33,
          "$\\mathbf{9 2 . 3 5}$",
          68.88,
          86.5,
          87.32,
          90.31,
          83.58
        ],
        [
          "Arrhythmia",
          24.12,
          "$\\mathbf{5 4 . 2 7}$",
          40.08,
          16.16,
          38.77,
          43.34,
          46.78
        ],
        [
          "Isolett",
          71.81,
          "$\\mathbf{9 5 . 4 8}$",
          89.72,
          83.36,
          85.58,
          89.1,
          89.9
        ]
      ],
      "row_count": 13,
      "column_count": 8
    },
    {
      "table_number": "17",
      "table_title": "The average F1-score (\\%) metric for different methods on different datasets.",
      "headers": [
        "",
        "All-features",
        "GA-SVM",
        "cGA-FS",
        "WFACOFS",
        "MOEDAFS",
        "RSVM-SBS",
        "The proposed Method"
      ],
      "rows": [
        [
          "Breast Cancer",
          70.58,
          96.79,
          93.87,
          97.58,
          95.47,
          96.24,
          "$\\mathbf{9 8 . 8 9}$"
        ],
        [
          "Glass",
          36.48,
          66.42,
          56.91,
          64.32,
          60.84,
          "$\\mathbf{6 7 . 5 8}$",
          64.78
        ],
        [
          "Heart",
          83.09,
          88.34,
          83.97,
          84.03,
          89.07,
          86.72,
          "$\\mathbf{9 2 . 2 5}$"
        ],
        [
          "Wine",
          92.83,
          97.82,
          96.67,
          97.04,
          98.65,
          98.29,
          "$\\mathbf{9 9 . 4 6}$"
        ],
        [
          "Segmentation",
          93.91,
          92.89,
          88.39,
          95.16,
          94.28,
          78.13,
          "$\\mathbf{9 5 . 2 8}$"
        ],
        [
          "German",
          73.26,
          73.97,
          75.58,
          72.59,
          75.21,
          73.39,
          "$\\mathbf{7 7 . 9 1}$"
        ],
        [
          "Ionosphere",
          83.13,
          90.43,
          84.15,
          93.21,
          86.73,
          93.05,
          "$\\mathbf{9 5 . 7 2}$"
        ],
        [
          "Soybean-small",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          87.08,
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$",
          "$\\mathbf{1 0 0 . 0 0}$"
        ],
        [
          "Sonar",
          79.28,
          79.93,
          83.28,
          91.1,
          91.92,
          91.47,
          "$\\mathbf{9 3 . 1 2}$"
        ],
        [
          "Hill-valley",
          47.37,
          "$\\mathbf{9 3 . 0 2}$",
          58.71,
          53.33,
          54.91,
          65.57,
          75.39
        ],
        [
          1,
          81.85,
          "$\\mathbf{9 1 . 6 9}$",
          68.12,
          85.97,
          86.79,
          88.15,
          83.14
        ],
        [
          "Arrhythmia",
          25.98,
          "$\\mathbf{5 4 . 3 1}$",
          38.7,
          14.07,
          24.04,
          36.81,
          40.1
        ],
        [
          "Isolett",
          70.21,
          "$\\mathbf{9 6 . 0 2}$",
          90.1,
          82.92,
          85.07,
          84.99,
          90.36
        ]
      ],
      "row_count": 13,
      "column_count": 8
    },
    {
      "table_number": "18",
      "table_title": "The average and the best (in parentheses) number of selected features for different methods on different datasets.",
      "headers": [
        "",
        "All-features",
        "GA-SVM",
        "cGA-FS",
        "WFACOFS",
        "MOEDAFS",
        "RSVM-SBS",
        "The proposed Method"
      ],
      "rows": [
        [
          "Breast Cancer",
          99,
          "$7.0(5)$",
          "$6.9(6)$",
          "$8.0(7)$",
          "$7.3(6)$",
          "$6.5(6)$",
          "$\\mathbf{4 . 3 ( 4 )}$"
        ],
        [
          "Glass",
          99,
          "$7.5(7)$",
          "$8.0(7)$",
          "$6.6(4)$",
          "$6.5(6)$",
          "$6.8(6)$",
          "$\\mathbf{6 . 5 ( 6 )}$"
        ],
        [
          "Heart",
          1313,
          "$11.8(11)$",
          "$7.0(6)$",
          "$9.2(8)$",
          "$6.6(5)$",
          "$11.0(10)$",
          "$\\mathbf{4 . 6 ( 4 )}$"
        ],
        [
          "Wine",
          1313,
          "$11.5(10)$",
          "$8.0(6)$",
          "$7.1(6)$",
          "$9.7(9)$",
          "$10.3(9)$",
          "$\\mathbf{4 . 3 ( 4 )}$"
        ],
        [
          "Segmentation",
          1919,
          "$10.7(9)$",
          "$10.7(7)$",
          "$12.4(9)$",
          "$8.9(7)$",
          "$15.8(14)$",
          "$\\mathbf{8 . 7 ( 8 )}$"
        ],
        [
          "German",
          2424,
          "$17.9(12)$",
          "$17.1(13)$",
          "$16.9(13)$",
          "$18.2(15)$",
          "$18.9(16)$",
          "$\\mathbf{1 4 . 8 ( 1 0 )}$"
        ],
        [
          "Ionosphere",
          3434,
          "$26.3(21)$",
          "$12.7(5)$",
          "$\\mathbf{1 0 . 8 ( 6 )}$",
          "$16.0(6)$",
          "$18.5(12)$",
          "$12.0(8)$"
        ],
        [
          "Soybean-small",
          3535,
          "$17.2(10)$",
          "$5.5(4)$",
          "$16.8(12)$",
          "$15.5(14)$",
          "$16.3(10)$",
          "$\\mathbf{3 . 1 ( 2 )}$"
        ],
        [
          "Sonar",
          6060,
          "$54.3(44)$",
          "$42.0(27)$",
          "$39.1(33)$",
          "$19.3(8)$",
          "$30.7(17)$",
          "$\\mathbf{1 3 . 7 ( 6 )}$"
        ],
        [
          "Hill-valley",
          100100,
          "$68.1(56)$",
          "$39.7(26)$",
          "$56.9(45)$",
          "$65.3(46)$",
          "$63.4(53)$",
          "$\\mathbf{3 3 . 4 ( 2 4 )}$"
        ],
        [
          1,
          167167,
          "$143.2(122)$",
          "$59.0(47)$",
          "$42.5(29)$",
          "$82.5(71)$",
          "$56.0(41)$",
          "$\\mathbf{3 4 . 4 ( 2 2 )}$"
        ],
        [
          "Arrhythmia",
          279279,
          "$188.9(171)$",
          "$120.9(95)$",
          "$44.7(24)$",
          "$134.9(115)$",
          "$87.2(70)$",
          "$\\mathbf{2 2 . 0 ( 1 5 )}$"
        ],
        [
          "Isolest",
          617617,
          "$326.6(273)$",
          "$297.2(247)$",
          "$142.4(97)$",
          "$392.8(327)$",
          "$236.3(181)$",
          "$\\mathbf{1 4 1 . 3 ( 9 4 )}$"
        ]
      ],
      "row_count": 13,
      "column_count": 8
    },
    {
      "table_number": "19",
      "table_title": "The average ACC $\\times$ PDF (\\%) metric for different methods on different datasets.",
      "headers": [
        "",
        "All-features",
        "GA-SVM",
        "cGA-FS",
        "WFACOFS",
        "MOEDAFS",
        "RSVM-SBS",
        "The proposed Method"
      ],
      "rows": [
        [
          "Breast Cancer",
          0.0,
          32.19,
          26.87,
          16.28,
          25.06,
          29.5,
          "$\\mathbf{5 3 . 2 6}$"
        ],
        [
          "Glass",
          0.0,
          12.1,
          11.31,
          "$\\mathbf{2 8 . 6 9}$",
          21.33,
          20.95,
          21.9
        ],
        [
          "Heart",
          0.0,
          10.83,
          42.53,
          28.28,
          49.59,
          16.1,
          "$\\mathbf{5 3 . 2 2}$"
        ],
        [
          "Wine",
          0.0,
          17.18,
          45.0,
          48.47,
          27.92,
          25.56,
          "$\\mathbf{6 7 . 8 6}$"
        ],
        [
          "Segmentation",
          0.0,
          45.06,
          46.84,
          41.63,
          54.82,
          19.8,
          "$\\mathbf{5 5 . 1 3}$"
        ],
        [
          "German",
          0.0,
          31.07,
          29.53,
          29.25,
          24.79,
          21.53,
          "$\\mathbf{4 0 . 0 2}$"
        ],
        [
          "Ionosphere",
          0.0,
          28.01,
          65.55,
          66.0,
          60.72,
          50.01,
          "$\\mathbf{6 7 . 3 7}$"
        ],
        [
          "Soybean-small",
          0.0,
          45.42,
          75.06,
          58.85,
          53.56,
          62.42,
          "$\\mathbf{9 2 . 7 1}$"
        ],
        [
          "Sonar",
          0.0,
          14.25,
          35.13,
          36.07,
          70.56,
          53.63,
          "$\\mathbf{7 7 . 1 4}$"
        ],
        [
          "Hill-valley",
          0.0,
          35.19,
          39.66,
          25.24,
          23.7,
          25.82,
          "$\\mathbf{5 1 . 0 5}$"
        ],
        [
          1,
          0.0,
          19.03,
          46.45,
          68.02,
          46.77,
          58.7,
          "$\\mathbf{6 9 . 8 4}$"
        ],
        [
          "Arrhythmia",
          0.0,
          26.38,
          37.96,
          52.75,
          33.71,
          45.94,
          "$\\mathbf{6 7 . 8 4}$"
        ],
        [
          5,
          0.0,
          49.03,
          50.73,
          66.13,
          35.03,
          58.53,
          "$\\mathbf{7 2 . 8 3}$"
        ]
      ],
      "row_count": 13,
      "column_count": 8
    },
    {
      "table_number": "20",
      "table_title": "The number of reported best results for different methods in terms of each performance metric on different datasets.",
      "headers": [
        "",
        "All-features",
        "GA-SVM",
        "cGA-FS",
        "WFACOFS",
        "MOEDAFS",
        "RSVM-SBS",
        "The proposed Method"
      ],
      "rows": [
        [
          "Accuracy",
          1,
          5,
          0,
          2,
          1,
          2,
          "$\\mathbf{8}$"
        ],
        [
          "Precision",
          1,
          5,
          0,
          2,
          1,
          2,
          "$\\mathbf{8}$"
        ],
        [
          "Recall",
          1,
          6,
          0,
          1,
          3,
          1,
          "$\\mathbf{7}$"
        ],
        [
          "F1-score",
          1,
          5,
          0,
          1,
          2,
          2,
          "$\\mathbf{8}$"
        ],
        [
          "ACC $>$ PDF",
          0,
          0,
          0,
          1,
          1,
          0,
          "$\\mathbf{1 2}$"
        ],
        [
          "\\# of Selected Features",
          0,
          0,
          0,
          1,
          2,
          0,
          "$\\mathbf{1 3}$"
        ]
      ],
      "row_count": 6,
      "column_count": 8
    },
    {
      "table_number": "21",
      "table_title": "$P$-value of Wilcoxon signed-rank test between the proposed approach and each other methods in terms of ACC $\\times$ PDF.",
      "headers": [
        "Method",
        "$p$-value"
      ],
      "rows": [
        [
          "All-features",
          0.0002
        ],
        [
          "GA-SVM",
          0.0002
        ],
        [
          "cGA-FS",
          0.0002
        ],
        [
          "WFACOFS",
          0.0017
        ],
        [
          "MOEDAFS",
          0.0002
        ],
        [
          "RSVM-SBS",
          0.0002
        ]
      ],
      "row_count": 6,
      "column_count": 2
    }
  ]
}