{
  "metadata": {
    "file_path": "/Users/id05309/Documents/tfm/mistral/mistral-markdown/2021/Acceleration Control Design of HEVs with Comfortability Evaluation based on IRL.md",
    "filename": "Acceleration Control Design of HEVs with Comfortability Evaluation based on IRL.md",
    "title": "Acceleration Control Design of HEVs with Comfortability Evaluation based on IRL",
    "year": "2021"
  },
  "references": {
    "header": "## REFERENCES",
    "content": "H. Lee, C. Song, N. Kim, et al. Comparative analysis of energy management strategies for HEV: dynamic programming and reinforcement learning. IEEE Access, 8: 67112-67123, 2020.\nX. Hu, T. Liu, X. Qi, et al. Reinforcement learning for hybrid and plug-in hybrid electric vehicle energy management: recent advances and prospects. IEEE Industrial Electronics Magazine, 13(3): 16-25, 2019.\nQ. Jiang et al., Comparative study of real-time HEV energy management strategies. IEEE Trans. on Vehicular Technology, 66(12):10875-10888, 2017.\nF. Zhang, X. Hu, R. Langari, et al. Adaptive energy management in automated hybrid electric vehicles with flexible torque request. Energy, 214: 118873, 2021.\nF. Xu and T. Shen. Decentralized optimal merging control with optimization of energy consumption for connected hybrid electric vehicles. IEEE Trans. on Intelligent Transportation Systems, DOI:10.1109/TITS.2021.3054903, 2021.\nJ. Zhang, T. Shen and J. Kako. Short-term optimal energy management of power-split hybrid electric vehicles under velocity tracking control. IEEE Transactions on Vehicular Technology, 69(1): 182-193, 2020\nX. Shen, X. Zhang, T. Ouyang, et al. Cooperative Comfortable-driving at signalized intersections for connected and automated vehicles. IEEE Robotics and Automation Letters, 5(4): 6247-6254, 2020.\nM. Zhu, Y. Wang, Z. Pu, et al. Safe, efficient, and comfortable velocity control based on reinforcement learning for autonomous driving. Transportation Research Part C: Emerging Technologies, 117:102662, 2020.\nB. Burchfiel, C. Tomasi, and R. Parr, et al. Distance Minimization for Reward Learning from Scored Trajectories.AAAI Conf. on Artificial Intelligence,30(1),2016.\nA. Y. Ng and S. Russell, et al. Algorithms for Inverse Reinforcement Learning. the 17th International Confernce on Machine Learning, 663-670, 2000.\nP. Abbeel and A. Y. Ng, et al. Apprenticeship Learning via Inverse Reinforcement Learning. the 21st International Confernce on Machine Learning, 1-8, 2004.\nGuang-Bin Huang and Qin-Yu Zhu and Chee-Kheong Siew Extreme Lerning Machine: A New Learning Scheme of Feedforward Neural Networks. Neurocomputing,vol. 70, pp. 489-501 2006.\nP Larrañaga and R Etxeberria and J A Lozano and J M Peña et al. Optimization by learning and simulation of Bayesian and Gaussian networks. Servicio de Publicaciones de la Facultad de Informática, UPV-EHU. EHU-KZAA-IK-4-99, 1999.",
    "references": [
      {
        "ref_id": "1",
        "text": "H. Lee, C. Song, N. Kim, et al. Comparative analysis of energy management strategies for HEV: dynamic programming and reinforcement learning. IEEE Access, 8: 67112-67123, 2020."
      },
      {
        "ref_id": "2",
        "text": "X. Hu, T. Liu, X. Qi, et al. Reinforcement learning for hybrid and plug-in hybrid electric vehicle energy management: recent advances and prospects. IEEE Industrial Electronics Magazine, 13(3): 16-25, 2019."
      },
      {
        "ref_id": "3",
        "text": "Q. Jiang et al., Comparative study of real-time HEV energy management strategies. IEEE Trans. on Vehicular Technology, 66(12):10875-10888, 2017."
      },
      {
        "ref_id": "4",
        "text": "F. Zhang, X. Hu, R. Langari, et al. Adaptive energy management in automated hybrid electric vehicles with flexible torque request. Energy, 214: 118873, 2021."
      },
      {
        "ref_id": "5",
        "text": "F. Xu and T. Shen. Decentralized optimal merging control with optimization of energy consumption for connected hybrid electric vehicles. IEEE Trans. on Intelligent Transportation Systems, DOI:10.1109/TITS.2021.3054903, 2021."
      },
      {
        "ref_id": "6",
        "text": "J. Zhang, T. Shen and J. Kako. Short-term optimal energy management of power-split hybrid electric vehicles under velocity tracking control. IEEE Transactions on Vehicular Technology, 69(1): 182-193, 2020"
      },
      {
        "ref_id": "7",
        "text": "X. Shen, X. Zhang, T. Ouyang, et al. Cooperative Comfortable-driving at signalized intersections for connected and automated vehicles. IEEE Robotics and Automation Letters, 5(4): 6247-6254, 2020."
      },
      {
        "ref_id": "8",
        "text": "M. Zhu, Y. Wang, Z. Pu, et al. Safe, efficient, and comfortable velocity control based on reinforcement learning for autonomous driving. Transportation Research Part C: Emerging Technologies, 117:102662, 2020."
      },
      {
        "ref_id": "9",
        "text": "B. Burchfiel, C. Tomasi, and R. Parr, et al. Distance Minimization for Reward Learning from Scored Trajectories.AAAI Conf. on Artificial Intelligence,30(1),2016."
      },
      {
        "ref_id": "10",
        "text": "A. Y. Ng and S. Russell, et al. Algorithms for Inverse Reinforcement Learning. the 17th International Confernce on Machine Learning, 663-670, 2000."
      },
      {
        "ref_id": "11",
        "text": "P. Abbeel and A. Y. Ng, et al. Apprenticeship Learning via Inverse Reinforcement Learning. the 21st International Confernce on Machine Learning, 1-8, 2004."
      },
      {
        "ref_id": "12",
        "text": "Guang-Bin Huang and Qin-Yu Zhu and Chee-Kheong Siew Extreme Lerning Machine: A New Learning Scheme of Feedforward Neural Networks. Neurocomputing,vol. 70, pp. 489-501 2006."
      },
      {
        "ref_id": "13",
        "text": "P Larrañaga and R Etxeberria and J A Lozano and J M Peña et al. Optimization by learning and simulation of Bayesian and Gaussian networks. Servicio de Publicaciones de la Facultad de Informática, UPV-EHU. EHU-KZAA-IK-4-99, 1999."
      }
    ],
    "reference_count": 13,
    "pattern_matched": "(?:^|\\n)#+\\s*References?\\s*\\n"
  },
  "tables": [
    {
      "table_number": "1",
      "table_title": "Test conditions and comfortability scores from expert model",
      "headers": [
        "",
        "Assistant motor torques",
        "Score"
      ],
      "rows": [
        [
          "Test $1\\left(a_{1}, v_{1}\\right)$",
          "$\\tau_{m_{1}}=\\tau_{m_{2}}=0$",
          27.9
        ],
        [
          "Test $2\\left(a_{2}, v_{2}\\right)$",
          "$\\tau_{m_{1}}=0, \\tau_{m_{2}}=20(\\mathrm{Nm})$",
          25.7
        ],
        [
          "Test $3\\left(a_{3}, v_{3}\\right)$",
          "$\\tau_{m_{1}}=0, \\tau_{m_{2}}=\\tau_{m_{2}}^{m a x}$",
          26.1
        ]
      ],
      "row_count": 3,
      "column_count": 3
    },
    {
      "table_number": "2",
      "table_title": "Physical parameter of powertrain",
      "headers": [
        "symbol",
        "value",
        "symbol",
        "value"
      ],
      "rows": [
        [
          "$I_{c}$",
          "$0.22\\left(\\mathrm{~kg} \\cdot \\mathrm{~m}^{2}\\right)$",
          "$I_{m 1}$",
          "$0.06\\left(\\mathrm{~kg} \\cdot \\mathrm{~m}^{2}\\right)$"
        ],
        [
          "$I_{1}$",
          "$2.975 \\times 10^{3}$",
          "$I_{2}$",
          17.52
        ],
        [
          "$C_{1}$",
          41.42,
          "$C_{2}$",
          7.38
        ],
        [
          "$c_{m 1}$",
          0.05,
          "$c_{m 2}$",
          0.05
        ],
        [
          "$G_{m}$",
          5.75,
          "$G_{f}$",
          3.307
        ],
        [
          "$R_{w}$",
          "$0.39(\\mathrm{~m})$",
          "",
          ""
        ]
      ],
      "row_count": 6,
      "column_count": 4
    }
  ]
}