{
  "metadata": {
    "generation_date": "2025-06-06 10:04:29",
    "total_questions": 30,
    "generation_method": "chunk_based",
    "statistics": {
      "successful": 30,
      "failed_generation": 0,
      "total_questions": 30
    }
  },
  "questions": [
    {
      "question": "What are the fundamental differences between how Estimation of Distribution Algorithms (EDAs) and local search methods utilize information during optimization?",
      "answer": "EDAs mainly rely on global information extracted from the search space to build a probabilistic model, while local search is an exploration method based on local information of solutions found so far. Local search combines global statistical information and the location information of solutions.",
      "chunk_id": 8,
      "paper_id": "A Hybrid EDA for Protein Folding Based on HP Model. IEEJ",
      "paper_title": "A Hybrid EDA for Protein Folding Based on HP Model",
      "paper_year": "2010",
      "chunk_content": "This chunk details the local search component of the hybrid Estimation of Distribution Algorithm (EDA) proposed for protein structure prediction using the HP model. It explains the rationale for incorporating local search to enhance EDA's performance by exploiting local information and introduces \"guided operators\" under specific conditions to refine solutions. The method utilizes a composite fitness function to select individuals for local search, combining global statistical information with location-specific data. An efficient EA should make use of both the local information of solutions found so far and the global information about the search space. The local information of solutions found so far can be helpful for exploitation, while the global information can guide the search for exploring promising areas. The search in EDAs is mainly based on the global information, but local search is an exploration method based on local information. Therefore, it is worthwhile investigating whether combining local search with EDA could improve the performance of the EDA.  \nLocal search with a set of guided operators is implemented in the proposed hybrid EDA. Some of these operations have been utilized as mutations in the previous GA and ant colony optimizations studies of protein folding [7]. But in this paper, we call them as 'guided operators', meaning that those operations are implemented only under some special conditions.  \nTaking the 2-D HP model as example, the special conditions are defined as follows: (i) The guided operation should grantee the validity of the individual, i.e. it cannot produce position collision in the lattice. If we want to change some residues to other positions in the lattice, the object positions must be empty. (ii) Guided operation should follow a basic principle that make H's as near as possible to the HCC and P's far away from the HCC according to the relative position in lattice, as shown in Fig. 3.  \nThe method of choosing individuals to implement local search is described as follows: In each iteration procedure of EDAs, use the composite fitness function (described by (3)) to sort the selection individuals. According the distribution of individuals' fitness, randomly select some individuals (the number is a certain percentage of the population) in each fitness domain to implement the local search with guided operators.  \nEDAs extract globally statistical information from the previous search and then build a probabilistic model for modeling the distribution of the best solutions visited in the search space. However, the information of the locations of the best individual solutions\n[IMAGE_2]  \nFig. 3. The guided operators for local search\nfound so far is not directly used for guiding further search. Local search with a guided operator generates offspring through a combination of global statistical information and the location information of solutions found so far. The resultant solution can (hopefully) fall in or close to a promising area, which is characterized by the probabilistic model.",
      "question_type": "definition",
      "complexity": "basic",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:04:31",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "What practical steps are involved in rebuilding the probabilistic model of the Mutual-Information-Maximizing Input Clustering (MIMIC) algorithm during each iteration?",
      "answer": "First, a number of individuals are generated according to the current model, and a number of individuals are selected according to some selection mechanism. Then, a path is constructed greedily based on the entropy of the distribution of the bits at the different positions. The first node of the new path is a position with the lowest entropy, and each subsequent node is chosen with respect to the lowest entropy conditional on the distribution of the current last node in the path.",
      "chunk_id": 6,
      "paper_id": "Bivariate estimation-of-distribution algorithms can find an exponential number of optima",
      "paper_title": "Bivariate estimation-of-distribution algorithms can find an exponential number of optima",
      "paper_year": "2023",
      "chunk_content": "This section introduces the mutual-information-maximizing input clustering (MIMIC) algorithm, a bivariate estimation-of-distribution algorithm (EDA), detailing its probabilistic model, parameters, and the step-by-step process (Algorithm 1) for implementation, including model initialization, individual generation, selection, and path construction based on entropy. It is used to optimize the EQUALBLOCKSONEMAX (EBOM) test function which has an exponential number of optimal solutions. Mutual-information-maximizing input clustering (MIMIC; [16]) is a bivariate estimation-of-distribution algorithm (EDA). The Bayesian network of the probabilistic model of MIMIC can be represented as a directed path over [FORMULA_32] nodes, where each of the nodes corresponds to one of the [FORMULA_33] bit positions of [FORMULA_34]. Further, MIMIC has two parameters, [FORMULA_35] with [FORMULA_36], that represent how many individuals are generated and selected each iteration, respectively.  \nAlgorithm 1: MIMIC [16] with parameters [FORMULA_37] and [FORMULA_38], and a selection scheme select [FORMULA_39], optimizing a fitness function [FORMULA_40] with [FORMULA_41].\n[FORMULA_42]\n[FORMULA_43]\n[FORMULA_44];\nrepeat\n[FORMULA_45]\nfor [FORMULA_46] do\n[FORMULA_47]\n[FORMULA_48]\n[FORMULA_49]\n[FORMULA_50];\n[FORMULA_51];\n[FORMULA_52]\nfor [FORMULA_53] do [FORMULA_54];\nfor [FORMULA_55] do\n[FORMULA_56];\n[FORMULA_57]\nfor [FORMULA_58] do [FORMULA_59];\nrestrict all values of [FORMULA_60] to the interval [FORMULA_61];\n[FORMULA_62]\nuntil termination criterion met;  \nInitially, the model represents the uniform distribution. It is rebuilt each iteration in the following way: first, [FORMULA_63] individuals are generated according to the current model, and [FORMULA_64] individuals are selected according to some selection mechanism. We call the resulting (multi-)set 5 . A path is constructed greedily based on the entropy of the distribution of the bits at the different positions in [FORMULA_65].  \nThe first node of the new path is a position with the lowest entropy, that is, a position with the largest number of 1 s or 0 s . Each subsequent node is chosen with respect to the lowest entropy conditional on the distribution of the current last node in the path. This way, the new path represents a model that best reflects the distributions of pairs of positions observed in [FORMULA_66]. We now go into detail about our implementation of MIMIC (Algorithm 1).",
      "question_type": "practical application",
      "complexity": "medium",
      "topics": [
        "termination"
      ],
      "generated_at": "2025-06-06 10:04:33",
      "generation_style": "practical_application"
    },
    {
      "question": "What mathematical properties define when an operator is in the class of epsilon-equivalent selection operators?",
      "answer": "An operator is in the class of epsilon-equivalent selection operators if FORMULA_255, i.e., FORMULA_256 and FORMULA_257.",
      "chunk_id": 11,
      "paper_id": "On the model updating operators in univariate estimation of distribution algorithms",
      "paper_title": "On the model updating operators in univariate estimation of distribution algorithms",
      "paper_year": "2016",
      "chunk_content": "This section of the paper describes and compares different model updating operators used in univariate estimation of distribution algorithms (EDAs). It introduces two novel operators, $\\varphi_4$ and $\\varphi_5$, alongside previously defined operators like cGA. The section details the mathematical definitions of each operator, their characteristics, and how they can be tuned to achieve $(\\epsilon, N)$-equivalence. It also introduces convergence reliability, maximum likelihood estimation (MLE) and briefly discusses an alternative approach for assessing the merits of a model updating operator $\\varphi$. 4. [FORMULA_242], where [FORMULA_243]. In this case it is not possible to find the analytical solution of the equation [FORMULA_244] but it can be solved numerically. See Fig. 2d for the characteristic of [FORMULA_245] for the admissible range of [FORMULA_246] values.\n5. [FORMULA_247], where [FORMULA_248] is the cdf of the standard normal distribution, i.e.,  \n[FORMULA_249]  \n[FORMULA_250] is the normal inverse cdf (quantile) function, and [FORMULA_251]. In this case [FORMULA_252] and [FORMULA_253] is in the class of [FORMULA_254]-equivalent selection operators if [FORMULA_255] i.e., [FORMULA_256] and [FORMULA_257]. See Fig. 2e for the characteristic of [FORMULA_258] for the admissible range of [FORMULA_259] values.  \n[IMAGE1]  \nFig. 2 Characteristics of the proposed operators as function of their parameters a operator [FORMULA_260], i.e., cGA, [FORMULA_261] operator [FORMULA_262], [FORMULA_263] operator [FORMULA_264], [FORMULA_265] operator [FORMULA_266], e operator [FORMULA_267]. (Color figure online)  \nSee Sect. 4 for a theoretical justification of the above proposed operators. See Fig. 2 for the different characteristics of these operators as function of their parameters.  \nAn alternative way of assessing the merits of a model updating operator [FORMULA_268] on a given fitness function (optimization problem) is to establish a criterium for its\nconvergence reliability and computes the required effort for convergence. A possibility is to compare the number of generations required by a given operator for which the maximum likelihood estimation (MLE) of the probability of success (reaching a global maximum) is greater or equal than a given confidence level. This approach is also used in  \nSastry and Goldberg (2001) and followed in the example below.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:04:35",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What advantages does combining PBILA with VNS provide compared to using PBILA alone for solving QAP?",
      "answer": "Hybridizing PBILA with VNS significantly improves the ability to find best-known solutions compared to PBILA alone, which fails to obtain the best-known solution for most test instances.",
      "chunk_id": 13,
      "paper_id": "A variable neighbourhood search enhanced estimation of distribution algorithm for quadratic assignment problems. OPSEARCH",
      "paper_title": "A variable neighbourhood search enhanced estimation of distribution algorithm for quadratic assignment problems",
      "paper_year": "2021",
      "chunk_content": "This chunk is the conclusion of a paper proposing a hybrid algorithm (PBILA-VNS) to solve Quadratic Assignment Problems (QAP). It summarizes the algorithm's performance, demonstrating that the hybridization of Population Based Incremental Learning Algorithm (PBILA) with Variable Neighbourhood Search (VNS) significantly improves the ability to find best-known solutions compared to PBILA alone. The conclusion also compares PBILA-VNS to state-of-the-art algorithms, noting its competitive performance, and suggests future research directions. It also includes statements on author contributions, data availability, conflict of interest, and code availability. In this study, a hybrid algorithm combining PBILA with VNS is proposed to solve one of the most complex combinatorial optimization problems, namely QAP. This study reveals that PBILA on its own fails to obtain the BKS for most of the test instances considered. Out of the total 101 test instances solved PBILA is able to obtain BKS for only six problems. But, when hybridized with VNS, the performance of the algorithm improved drastically. The proposed hybrid algorithm performs well by obtaining the BKS for most of the QAP test instances considered. PBILA-VNS is able to obtain the optimal solution at least once for 95 problems and for 49 test instances the PBILA-VNS is able to get the known best solution in all the ten trials. The results obtained for the PBILA-VNS are then compared with those of the recent state-of-the-art algorithms published in the literature.  \nFrom the comparison tables it is found that the proposed algorithm performs better or equally with the algorithms considered for comparison. Thus, this paper opens up a new direction in terms of the design of algorithms for solving complex combinatorial optimization problems. Applying the proposed PBILA-VNS for solving other combinatorial optimization problems can be an extension of this work. Hybridizing PBILA with other local search methods may improve the performance of the algorithm, which can provide a further scope for research.  \nAuthors' contributions All authors contributed to the development of the solution methodologies and the preparation of the manuscript. The first draft of the manuscript and revised manuscript was written by Pradeepmon T. G.; R. Sridharan and Vinay V. Panicker did the editing of the manuscript. All authors read and approved the final manuscript.  \nAvailability of data and material All data used in the work are available in the internet or are adapted from other published materials.\nConflict of interest On behalf of all authors, the corresponding author states that there is no conflict of interest.  \nCode availability The code used in the work are developed by the authors and can be made available as and when required.",
      "question_type": "advantage",
      "complexity": "basic",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:04:37",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What algorithmic steps are involved in the vertex elimination method for triangulating a graph?",
      "answer": "When a vertex is selected to be removed from the graph, all arcs needed to make the subgraph of its adjacent nodes complete are added. Then, the vertex and all arcs related to it are removed. The graph that incorporates all the new arcs to its set of original arcs is a triangulated graph.",
      "chunk_id": 3,
      "paper_id": "Triangulation of Bayesian networks with recursive estimation of distribution algorithms",
      "paper_title": "Triangulation of Bayesian networks with recursive estimation of distribution algorithms",
      "paper_year": "2009",
      "chunk_content": "This chunk from a paper on Bayesian network triangulation using Estimation of Distribution Algorithms (EDAs) explains the inference problem in Bayesian networks and the need for efficient triangulation. It describes the evidence propagation algorithm, moralization, and triangulation using vertex elimination and minimum weight criteria. It introduces UMDA and MIMIC as EDAs for finding optimal triangulations. [FORMULA_29]  \nWe can rewrite Eq. (1) as  \n[FORMULA_30]  \nHere, each factor is not a conditional probability, but a potential function. The potential functions are, at first, the conditional probabilities, and their parameters are the variables that are connected in the graph. An useful procedure to discover which variables are related to each potential function is to moralize the graph, that is, to add an arc between the parent nodes of each node (see Fig. 2). Nevertheless, when we sum over [FORMULA_31] in expression [FORMULA_32] for all the values of [FORMULA_33], we obtain a new factor (a new auxiliary potential function) depending on [FORMULA_34] and [FORMULA_35], and between these nodes there is no arc. This is a problem due to the prior definition of potential function. We can solve this problem with the triangulation of the graph (see [26] for a more extensive explanation). A graph is triangulated if it has no cycles with a length greater than three without a cord. In the Asia network, it is enough to add [FORMULA_36] arc in order to triangulate it (see Fig. 3), but in more complex networks it is not so easy. A good triangulation could make it possible to obtain a solution to some problems related to graphs in polynomial time instead of exponential time [3].\n[IMAGE0]  \nFig. 1. The Asia Bayesian network.\n[IMAGE1]  \nFig. 2. The moralized Asia network.  \n[IMAGE2]  \nFig. 3. A triangulation of Asia network.  \nIn our experiments, the algorithm that searches for the best triangulation will use the known method of the vertex elimination. Basically, when a vertex is selected to be removed from the graph, we add all the arcs needed to make the subgraph of their adjacent nodes complete. Then, we remove the vertex and all the arcs related to it. The graph that incorporates all the new arcs to its set of original arcs is a triangulated graph. The sequence of removed vertices determines the efficiency of the resulting triangulated graph, given by the size of its cliques. [FORMULA_37] There are several criteria to evaluate the quality of a triangulation [34]. One of the most known criterions is the minimum weight, used in the experiments as the evaluation function of the EDAs. This method associates a weight, [FORMULA_38], to each clique:  \n[FORMULA_39]  \nwhere [FORMULA_40] is the number of possible states of node [FORMULA_41] that belongs to clique [FORMULA_42]. So, we can define a weight of the triangulated graph:  \n[FORMULA_43]  \nSo, our aim is to minimize [FORMULA_44].\nObtaining the best triangulation for a Bayesian network is an NP-hard problem [2,36]. In this work, we will use the univariate marginal distribution algorithm (UMDA) [28] and the mutual information maximization for input clustering (MIMIC) [12], both of them estimation of distribution algorithms (EDAs), to find the best possible triangulation. EDAs are a new metaheuristic approach belonging to evolutionary computation that are founded in probability theory.",
      "question_type": "algorithm",
      "complexity": "medium",
      "topics": [
        "graph triangulation",
        "vertex elimination"
      ],
      "generated_at": "2025-06-06 10:04:39",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What metrics are most appropriate for assessing the quality of solutions generated by the Team of Bayesian Optimization Algorithms (TBOA) when applied to task assignment problems in heterogeneous computing systems?",
      "answer": "The cost of a solution, as described in Eq. (1), is used as its fitness value to assess the quality of solutions generated by the Team of Bayesian Optimization Algorithms (TBOA).",
      "chunk_id": 8,
      "paper_id": "Team of Bayesian optimization algorithms to solve task assignment problems in heterogeneous computing systems",
      "paper_title": "Team of Bayesian Optimization Algorithms to Solve Task Assignment Problems in Heterogeneous Computing Systems",
      "paper_year": "2014",
      "chunk_content": "This chunk details the application of the proposed TBOA to the task assignment problem in heterogeneous computing (HC) systems, covering solution representation, initial population generation, Bayesian network structure, and the complete TBOA algorithm (Algorithm 2). It describes how TBOA uses multiple Bayesian networks to efficiently explore the solution space for assigning tasks to processors. We now apply the proposed TBOA to solve a task assignment problem in HC systems. First, solution representation and how to generate initial population are given. Next, the structure of a Bayesian network is described. At last, a complete TBOA algorithm is represented.\nSolution representation is important for TBOA. A welldesigned representation can make problem-solving easy. A potential solution for solving task assignment in HC systems is an integer vector [FORMULA_110] with [FORMULA_111] elements, [FORMULA_112] denotes that processor [FORMULA_113] is assigned to the [FORMULA_114]-th task. [FORMULA_115] is the number of tasks and [FORMULA_116] is the number of processors. Initially, a population [FORMULA_117] is randomly generated by a uniform distribution, the [FORMULA_118]-th dimension of the [FORMULA_119]-th solution is described as [FORMULA_120], [FORMULA_121] and [FORMULA_122]. randint is a function that generates an integer uniformly distributed in the integer range [FORMULA_123]. Then, the cost of a solution described in Eq. (1) is used as its fitness value.\nWe use Eqs. (4) and (5) to construct the structures of multiple Bayesian networks for solving this task assignment problem. First, some promising solutions are selected from the current population according to their fitness values via a selection procedure. Next, the structures of multiple Bayesian networks in this task assignment problem are constructed according to Eqs. (4) and (5), where [FORMULA_124] is the number of promising solutions, [FORMULA_125] and [FORMULA_126]. Each matrix in Eq. (5) is used to estimate and construct a Bayesian network that searches a task assignment domain. Thus, each element in matrix [FORMULA_127] can be \" 1 \" meaning that the [FORMULA_128]-th task is assigned on the [FORMULA_129]-th processor in the [FORMULA_130]-th promising solution, and\" 0 \" means \"not\".\nAt each iteration, some new values (processors) are sampled in each dimension (task) according to the joint probability distribution encoded by the constructed Bayesian network. Note that there may be many 1's are sampled by a Bayesian network in a dimension. Implying that a task is assigned to different processors. This is unlikely to happen because a task can only be assigned to one processor in every assignment. This work adopts the first \" 1 \" sampled by a Bayesian network. After all tasks are assigned to their processors, a new offspring is generated. Thus, some new offspring are generated by all tasks sample their Bayesian networks repeatedly, then they replace some worse solutions in the previous population. The process of TBOA for solving a task assignment problem in HC systems is presented in Algorithm 2.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "topics": [
        "design"
      ],
      "generated_at": "2025-06-06 10:04:42",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What are the fundamental components used to model Networked Control Systems with time delays and packet losses?",
      "answer": "Networked Control Systems with time delays and packet losses are modeled as a discrete-time switched system, using the concept of 'effective sensor packets' based on Round-Trip Time (RTT) delay, and analyzing packet loss.",
      "chunk_id": 6,
      "paper_id": "Optimal Stabilizing Gain Selection for Networked Control Systems With Time Delays and Packet Losses",
      "paper_title": "Optimal Stabilizing Gain Selection for Networked Control Systems With Time Delays and Packet Losses",
      "paper_year": "2009",
      "chunk_content": "This section models Networked Control Systems (NCSs) with time delays and packet losses as a discrete-time switched system. It defines \"effective sensor packets\" based on Round-Trip Time (RTT) delay, analyzes packet loss, and derives NCS dynamics for different delay scenarios using equations (7)-(12). The goal is to develop a general NCS model without assumptions on delay or loss models, leading to stability conditions in Theorem 1. This section will present a discrete-time switched model for NCSs. To address this problem, we introduce the definition of an effective sensor packet, and it will be used throughout this brief. A packet from the sensor is called an effective sensor packet for controller (4), if its RTT delay is no longer than [FORMULA_37]. Let [FORMULA_38], a subsequence of [FORMULA_39], denotes the sequence of time index of effective sensor packets. Since only effective sensor packets are used to control the plant, the sensor packets between two effective sensor packets can all be considered as dropped packets. To avoid confusion, we used the following illustration to explain and demonstrate the used nota-  \n[IMAGE1]  \nFig. 2. Illustration of effective packets in NCSs.\ntions. Let us assume that [FORMULA_40] in an NCS is [FORMULA_41] (see Fig. 2). It means that the 2 nd, 4 th, 8 th, and 11th sensor packets are effective sensor packets, while the [FORMULA_42] th sensor packets [FORMULA_43] are not used to control the plant, and they are considered as dropped packets.  \nBased on the earlier notations, the packet-loss process in the NCSs can be defined as  \n[FORMULA_44]  \nwhich means that, from [FORMULA_45] to [FORMULA_46], the number of dropped packets is [FORMULA_47]. For the sake of analysis, we define [FORMULA_48]. Then, we can conclude that [FORMULA_49] takes values in a finite set [FORMULA_50].  \nLet [FORMULA_51] express the RTT delay encountered by the [FORMULA_52] th effective sensor packet. As shown in Fig. 3, for NCSs during the time interval between two effective packets, i.e., [FORMULA_53], four cases may arise and are discussed as follows.\nCase 1) [FORMULA_54], i.e., the RTT delay encountered by the [FORMULA_55] th effective sensor packet is zero. Since the sensor packets between two effective sensor packets are lost, the control input for [FORMULA_56] will be [FORMULA_57] because of ZOH . Therefore, the dynamics of the NCS can be described as follows:  \n[FORMULA_58]  \nCorrespondingly, for the time instant [FORMULA_59], we have  \n[FORMULA_60]  \nCase 2) [FORMULA_61], i.e., the RTT delay encountered by the [FORMULA_62] th effective sensor packet ranges within [FORMULA_63]. In this case, the last control signal [FORMULA_64] and the new control signal [FORMULA_65] are used to control the plant during [FORMULA_66] and [FORMULA_67] [FORMULA_68], respectively. Therefore, the dynamics of the NCS can be described as follows:  \n[FORMULA_69]  \n[IMAGE2]  \nFig. 3. Illustration of time delays and packet losses in NCS.  \nCorrespondingly, for the time instant [FORMULA_70], we have  \n[FORMULA_71]  \nwhere the notation [FORMULA_72] satisfies the following property:  \n[FORMULA_73]  \nNote that property (10) is used to ensure that (9) is a general expression. For example, when [FORMULA_74] is one, (9) can be reduced to the first equation in (8) with property (10).",
      "question_type": "theory",
      "complexity": "basic",
      "topics": [
        "control"
      ],
      "generated_at": "2025-06-06 10:04:44",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "How can practitioners apply the Random Embedding in Estimation of Distribution Algorithm (REMEDA) to optimize high-dimensional problems where only a few variables significantly impact the function value?",
      "answer": "REMEDA employs the idea of random embedding to exploit intrinsic dimension without needing to know the influential subspace of the input space or its dimension. This allows it to optimize very large dimensional problems as long as their intrinsic dimension is low.",
      "chunk_id": 1,
      "paper_id": "REMEDA- Random Embedding EDA for Optimising Functions with Intrinsic Dimension",
      "paper_title": "REMEDA: Random Embedding EDA for Optimising Functions with Intrinsic Dimension",
      "paper_year": "2016",
      "chunk_content": "This chunk contains the paper's title, authors, affiliations, abstract, and keywords, introducing REMEDA, an Estimation of Distribution Algorithm (EDA) that uses random embedding to optimize functions with low intrinsic dimension in high-dimensional spaces. It addresses the limitations of traditional EDAs in high-dimensional problems. Momodou L. Sanyang [FORMULA_0] and Ata Kaban [FORMULA_1][FORMULA_2] School of Computer Science, University of Birmingham, Edgbaston B15 2TT, UK\\{M.L. Sanyang, A.Kaban\\}@cs.bham.ac.uk[FORMULA_3] School of Information Technolgy and Communication, University of the Gambia, Brikama Campus, P.O. Box 3530, Serekunda, The GambiaMLSanyang@utg.edu.gm\nIt has been observed that in many real-world large scale problems only few variables have a major impact on the function value: While there are many inputs to the function, there are just few degrees of freedom. We refer to such functions as having a low intrinsic dimension. In this paper we devise an Estimation of Distribution Algorithm (EDA) for continuous optimisation that exploits intrinsic dimension without knowing the influential subspace of the input space, or its dimension, by employing the idea of random embedding. While the idea is applicable to any optimiser, EDA is known to be remarkably successful in low dimensional problems but prone to the curse of dimensionality in larger problems because its model building step requires large population sizes. Our method, Random Embedding in Estimation of Distribution Algorithm (REMEDA) remedies this weakness and is able to optimise very large dimensional problems as long as their intrinsic dimension is low.  \nKeywords: Estimation of distribution algorithm [FORMULA_4] Black-box optimization [FORMULA_5] Intrinsic dimension",
      "question_type": "implementation",
      "complexity": "medium",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:04:46",
      "generation_style": "practical_application"
    },
    {
      "question": "What mathematical properties of the pheromone positive feedback mechanism in ant colony algorithms lead to the algorithm's susceptibility to local optima?",
      "answer": "Large pheromone accumulation due to the positive feedback mechanism makes ant colony algorithms easy to fall into local optimum.",
      "chunk_id": 1,
      "paper_id": "An Improved Ant Colony Algorithm Based on Distribution Estimation",
      "paper_title": "An Improved Ant Colony Algorithm based on Distribution Estimation",
      "paper_year": "2014",
      "chunk_content": "This chunk introduces an improved ant colony algorithm using distribution estimation to address limitations like local optima. It discusses the algorithm's foundation in pheromone-based positive feedback, its applications, and the integration of distribution estimation algorithms to enhance global search capabilities. The paper then outlines the structure, including sections on the binary ant colony algorithm, the proposed improved algorithm, experiments, and conclusions. Fang BeiSuZhou Polytechnic Institute of Agriculture, JiangSu SuZhou 215008, Chinahoushuai2014cn@163.com\nIn last two decades, Ant colony algorithm got extensive application in combinatorial optimization, function optimization and other fields. Ant colony algorithm is easy to fall into local optimum. A novel estimation of distribution algorithm by fusion improvement on ant colony algorithm and PBIL estimation of distribution algorithm is proposed. The algorithm introduce probability distribution model of PBIL algorithm to guide route choice, which greatly improves the faults that positive feedback mechanism of pheromone. Although the hybrid ant colony algorithm has achieved good results, this is just the preliminary attempt of distributed estimation algorithm combined with ant colony algorithm. Probability distribution model of other distribution estimation algorithm can also be used to guide the choice of ant colony optimal path.  \nKeywords-Ant colony algorithm; distribution estimation; the binary ant colony algorithm\nDuring the 1990s, the Italian scholar M.Dorigo, V.Maniezzo proposed a heuristic evolutionary bionic algorithm based on population by simulation the collective behavior of ant routing, and found the whole ant colony make use of pheromone to collaborate with each other to form a positive feedback that each ant follow the shortest path[1-2]. In last two decades, Ant colony algorithm got extensive application in combinatorial optimization, function optimization, system identification, network route, robot path planning, data mining and cabling design of large scale integrated circuit[3-8]. But many scholars realized the limitation of traditional ant colony algorithm to improve. The improvement of ant colony algorithm have two handles, one aspect is the improvement of itself, such as improvement mode of pheromone release, probability of selection method, and the other is fusion improvement on ant colony algorithm and other intelligent optimization algorithms [FORMULA_0].  \nBecause the ant colony algorithm itself is a kind of optimization algorithm based on pheromone positive feedback mechanism, large pheromone accumulation is easy to fall into local optimum, and application effect of the basic ant colony algorithm in the continuous domain is not obvious. In order to solve this problem, people explore other evolutionary algorithms[11,12], which are introduced to solve the problem of pheromone accumulation of positive feedback, such as the ant colony algorithm and artificial immune algorithm, ant colony algorithm with neural network fusion, the fusion of ant colony algorithm and particle swarm algorithm, the mutation operator of genetic algorithm introduced into ant colony algorithm, etc. The\nhybrid algorithms relieve local optimal problem of the ant colony algorithm to some extent, but there is still space for further research. This paper introduced distribution estimation algorithm into ant colony algorithm. Distribution estimation algorithm is a kind of evolutionary algorithm based on probability model, which is based on statistical feature. The paper is organized as follows. In the next section, the binary ant colony algorithm is given. In Section 3, a novel ant colony algorithm based on distribution estimation algorithm and traditional ant colony algorithm is presented In Section 4, experiments are done. Finally, we conclude our paper in section 5.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "topics": [
        "genetic"
      ],
      "generated_at": "2025-06-06 10:04:48",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What distinguishes the PPCA-based EDA from other methods like FDA, BOA, and IDEA in terms of structure search?",
      "answer": "The PPCA-based EDA does not have an explicit search procedure for the probability density structure, allowing for rapid distribution estimation and easy sampling of new individuals, unlike methods like FDA, BOA, and IDEA that explicitly model the relationship between variables.",
      "chunk_id": 2,
      "paper_id": "Continuous Estimation of Distribution Algorithms with Probabilistic Principal",
      "paper_title": "Continuous estimation of distribution algorithms with probabilistic principal component analysis",
      "paper_year": "2001",
      "chunk_content": "This chunk from a paper on Continuous Estimation of Distribution Algorithms with Probabilistic Principal Component Analysis (PPCA) introduces the problem of capturing high-order dependencies in evolutionary algorithms and reviews existing Estimation of Distribution Algorithms (EDAs) like FDA, BOA, and IDEA. It highlights the computational cost of these methods and introduces the authors' PPCA-based EDA as a more efficient alternative for continuous spaces, capable of handling complex interactions through latent variables without explicit structure search. The chunk concludes by outlining the structure of the paper. Artificial Intelligence Lab (SCAI)\nSchool of Computer Science and Engineering\nSeoul National University\nSeoul 151-742, Korea\nbtzhang@scai.snu.ac.kr\ntively.\nCovering some pairwise interactions is still insufficient to solve problems with high-order dependencies. To capture more complex dependencies, Muhlenbein and Mahnig [12] present the factorized distribution algorithm (FDA). Here, the distribution is decomposed into various factors or conditional probabilities, and then this factorized distribution is used as a fixed model. FDA can be extended to an algorithm, LFDA, which computes a good factorization from the data with Bayesian networks. Pelikan et al. [14] propose the Bayesian Optimization Algorithm (BOA) which uses the techniques for modeling multivariate data by Bayesian networks in order to estimate the distribution of promising solutions.  \nTo search good probability density models in continuous spaces, integrated density estimation evolutionary algorithm (IDEA) [4] used the Kullback-Leibler divergence as a distance metric to the full joint probability density structure and tested the various probability density functions for each element in the probability density structure. Larranaga et al. [9] replace the Bayesian network with the Gaussian network for the continuous domain and employed four score metrics to construct the networks for the selected individuals.  \nAll these methods except the simplest ones use distribution models to represent explicitly the relationship between variables in the problem. However, the problem of determining the best model with respect to a given score metric is usually very hard. For example, finding the optimal Bayesian network for a given dataset is NP-complete [5]. That is, enormous time is required for building the models when the problem size is large. Thus most researchers adopted greedy version of the original algorithm to prevent this ill-behavior although the exact distributions cannot be estimated by using the realistic methods. Recently, Zhang and Shin [17] developed another type of EDA, where the Helmholtz machines are used to model and sample from the distribution of selected individuals without explicit expression of multivariate interactions. They empirically showed that the learning time tends to grow linearly as the problem complexity or size increase.  \nIn this paper, we propose new estimation of distribution algorithm with probabilistic principal component analysis (PPCA) [16] which can also cover higher order interactions with latent variables like the Helmholtz machines. Since there is no explicit search procedure for the probability density struc-  \nture, it is possible to rapidly estimate the distribution and easily sample the new individuals from it. This method can also be applied to the discrete space, however we focus on the continuous domain for demonstration purposes.  \nThe paper is organized as follows. In section 2 we explain the basic concept of PPCA. Section 3 presents the EDA with PPCA algorithms for the continuous domain. Section 4 reports the results of experiments for some benchmark functions and Section 5 summarizes our findings in this study.",
      "question_type": "relationship",
      "complexity": "basic",
      "topics": [
        "search"
      ],
      "generated_at": "2025-06-06 10:04:50",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the key implementation limitations of using Bayesian networks within Estimation of Distribution Algorithms (EDA) for job scheduling problems?",
      "answer": "Bayesian networks, when used as directed acyclic graphs in EDA, may not provide reliable information about the interactions among each part of the problem. The actual correlation is more complex than what the Bayesian network can describe explicitly. They have a disability in detecting latent connections underlying objects.",
      "chunk_id": 5,
      "paper_id": "Estimation_of_Distribution_Algorithms_For_Job_Schedule_Problem",
      "paper_title": "Estimation of Distribution Algorithms For Job Schedule Problem",
      "paper_year": "2009",
      "chunk_content": "This chunk concludes the paper on applying Estimation of Distribution Algorithms (EDA) to the job scheduling problem. It summarizes the findings from experiments with UMDA, EBNA_BIC, and MIMIC, noting UMDA's surprisingly better performance despite the problem's variable dependencies. It discusses the limitations of Bayesian networks in capturing complex interactions and suggests future work on designing more informative graph models for EDA. In this paper we present EDA as a novel approach for job scheduling problem. Three types of EDA with different complexity are adopted and the simulation results are satisfactory.  \nAlthough the results obtain the acceptable solution, it also uncovers the drawback of EDA.  \nIt is clearly that each variable in the job scheduling problem is dependent with others, but surprisingly UMDA return the best results comparing with [FORMULA_65] especially MIMIC. Intuitive, more than one parent should be involved instead of scattered and isolated nodes. It also means that the relationship rooted in the problem can not be simply exposed by the arrow from parents to node. The internal complication interactions beyond the competence of bayesian network.  \nIn a great number of cases bayesian network as a directly acyclic graph adapted to EDA can not give us reliable information about the interactions among each part of problem. The actual correlation is more complex than what the explicit of bayesian network can describe.  \nDespite of disability of detecting latent connection underlying objects, EDA is reasonably well than traditional GA. The further task is to design applicable graph model which can give us more information in details for the characters of problems and at the same time decompose special optimization problem to simple parts in order to improve the quality of graph model.",
      "question_type": "definition",
      "complexity": "medium",
      "topics": [
        "algorithms",
        "data structures"
      ],
      "generated_at": "2025-06-06 10:04:52",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What metrics can be used to evaluate the effectiveness of the Bayesian network construction within each BOA of the TBOA algorithm, beyond just the final fitness of the solutions generated?",
      "answer": "The Bayesian network's effectiveness can be evaluated by examining its ability to estimate the interdependent relationships between different values within a dimension's value region and how well it constructs the joint probability distribution of those values. The interdependent relationships indicate whether the selection of a value is influenced by the selection states of that value's parents.",
      "chunk_id": 5,
      "paper_id": "Team of Bayesian optimization algorithms to solve task assignment problems in heterogeneous computing systems",
      "paper_title": "Team of Bayesian Optimization Algorithms to Solve Task Assignment Problems in Heterogeneous Computing Systems",
      "paper_year": "2014",
      "chunk_content": "This section introduces the Team of Bayesian Optimization Algorithms (TBOA), a novel approach designed to address the limitations of standard BOA in high-dimensional optimization problems, particularly task assignment in heterogeneous computing systems. It details the structure of TBOA, emphasizing its use of multiple BOAs, each responsible for a single dimension, and explains how TBOA searches and learns dimensionality by constructing Bayesian networks and sampling new values. The significant characteristic of BOA is that it owns an excellent estimation characteristic through a Bayesian network. However, it is difficult to estimate the interdependent relations between different dimensions as problem dimensionality increases. Inspired by swarm intelligence approaches [15] and clustering [16], this work proposes an effective algorithm to search and learn dimensionality, called a team of BOAs (TBOA). It consists of multiple BOAs, in which the number of BOAs used is equal to problem dimension count. Each BOA is responsible for searching in a dimension. It is used to search this dimension's value region, then a Bayesian network is constructed by estimating the interdependent relationships between different values and building the joint probability distribution of different values. Next, each BOA uses this Bayesian network to sample some new values. In the end, new offspring of problem are composed by all BOAs' sampling values.  \n[IMAGE0]  \nFig. 1. The structure of TBOA where Q is current population.\nTBOA consists of multiple BOAs. Each BOA is responsible for searching in a dimension, and dimensional values in this dimension are sampled by its Bayesian network. Fig. 1 illustrates its structure. It consists of [FORMULA_47] BOAs. [FORMULA_48] is an optimization problem dimension count. Each BOA is considered as an independent entity and responsible for searching in a dimension. The Bayesian network of the [FORMULA_49]-th BOA can be defined as [FORMULA_50], in which [FORMULA_51] is the [FORMULA_52]-th Bayesian network structure, and [FORMULA_53] is as conditional probability table. [FORMULA_54] denote the values in the [FORMULA_55]-th Bayesian network, [FORMULA_56] is the number of values in the [FORMULA_57]-th dimension's value region. Each value corresponds to a continuous interval if the value region is continuous or corresponds to an integer value if it is discrete. [FORMULA_58] denotes the selection states of all values according to those selected promising solutions that correspond to the [FORMULA_59]-th dimension, [FORMULA_60] is the new values sampled from the [FORMULA_61]-th Bayesian network. [FORMULA_62] is the new offspring set that sampled from all Bayesian networks. \"Select\" denotes some promising solutions are selected from the current population according to their fitness values via a certain selection procedure. \"Replace\" denotes some worse solutions in the population are replaced by new offspring.  \nNext, we describe the process of TBOA. First, a promising set [FORMULA_63] of solutions is selected from the current population [FORMULA_64]. Then [FORMULA_65] is mapped into [FORMULA_66] that denotes selection states of all values in the [FORMULA_67]-th dimension according to those selected promising solutions. Next, BOA [FORMULA_68] uses its [FORMULA_69] to estimate the joint probability distribution of different values and construct the [FORMULA_70]-th Bayesian network, and then obtains the interdependent relationships between different values. The interdependent relationships indicate that whether to select a value is influenced by the selection states of this value's parents. Third, each Bayesian network samples the joint probability distribution to obtain a certain number of new candidate\nvalues [FORMULA_71], which are integrated into the new offspring set [FORMULA_72]. At last, [FORMULA_73] is used to replace some worse solutions in population [FORMULA_74], as shown in Fig. 1.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:04:54",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "Why is the concept of constraint handling important when using BBO algorithms for real-world problems?",
      "answer": "Many real-world problems have constraints, making the performance of BBO algorithms under different constraint-handling techniques important.",
      "chunk_id": 3,
      "paper_id": "A novel population-based multi-objective CMA-ES and the impact of different constraint handling techniques",
      "paper_title": "A Novel Population-based Multi-Objective CMA-ES and the Impact of Different Constraint Handling Techniques",
      "paper_year": "2014",
      "chunk_content": "This introductory section of a GECCO 2014 paper outlines the research on multi-objective Covariance Matrix Adaptation Evolutionary Strategy (MO-CMA-ES) optimization with constraint handling. It addresses limitations of existing MO-CMA-ES variants, specifically their population size and constraint handling, and introduces a novel population-based MO-CMA-ES. The paper aims to evaluate the performance of different MO-CMA-ES variants and the iMAMaLGaM algorithm under various constraint-handling techniques, using benchmark problems. It also describes the paper's structure. GECCO'14, July 12-16, 2014, Vancouver, BC, Canada.\nCopyright 2014 ACM 978-1-4503-2662-9/14/07 ...S15.00.\nhttp://dx.doi.org/10.1145/2576768.2598329.  \nMany real-world problems have constraints, making the performance of BBO algorithms under different constrainthandling techniques important. All MO-CMA-ES variants previously introduced use a penalty term to handle box constraints. This approach lead to fast convergence speeds for certain benchmark problems [FORMULA_3]. However, this approach has drawbacks. For example, it only performs well with box constraints since these, differently from general problem constraints, allow an easy mapping to the feasible space. Furthermore, infeasible solutions may be accepted in the elitist archive with this constraint handling technique.  \nThe main objectives of this paper stem from the fact that all existing MO-CMA-ES variants use populations of size one and that only the penalty approach was used to handle constraints. Our first goal is to study the benefits of having a population-based MO-CMA-ES. To do so, we study a combination between a multi-objective optimization framework that was recently introduced [6] with the most general SO version of CMA-ES [5]. Our second goal is to assess the performance and robustness of the previously introduced MO-CMA-ES variants, the novel population-based MO-CMAES and the iMAMaLGaM algorithm [6] under different and more general constraint handling techniques.  \nThe remainder of this paper is organized as follows: Section 2 briefly introduces the key concepts in MO optimization. Then, Section 3 describes the most popular MO-CMAES variants. Subsequently, in Section 4 a recently introduced general framework for extending population-based algorithms from single- to multi-objective optimization is described. Thereafter, iMAMaLGaM is described in Section 5. The novel population-based MO-CMA-ES is introduced in Section 6. Subsequently, the different constraint-handling techniques are briefly discussed in Section 7. The performance of the algorithms is tested in Section 9 on 6 benchmark problems and with different constraint-handling techniques. A discussion of the results is given in Section 10, while final conclusions are presented in Section 11.",
      "question_type": "process",
      "complexity": "basic",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:04:56",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "In what scenarios might practitioners choose a fixed-height histogram (FHH) over a fixed-width histogram (FWH) when implementing histogram-based EDAs?",
      "answer": "A fixed-height histogram (FHH) might be chosen when there is a need for more accurate modeling in denser regions of the variable's domain, as FHH allocates more bins to these areas compared to FWH which divides the domain into bins of fixed width.",
      "chunk_id": 16,
      "paper_id": "A review on probabilistic graphical models in evolutionary computation",
      "paper_title": "A review on probabilistic graphical models in evolutionary computation",
      "paper_year": "2012",
      "chunk_content": "This chunk discusses non-parametric probabilistic models used in continuous Estimation of Distribution Algorithms (EDAs), including normal kernel distributions, mixtures of factor analyzers, Parzen estimators, and histogram-based methods. It details specific implementations like IDEA, MOPEDA, KEDA, and HEDAs, highlighting their approaches to density estimation and optimization in continuous spaces. Other probabilistic models that estimate a non-parametric distribution for the variables have also been used in continuous EDAs. IDEA, for example, has em-  \nployed other models, apart from MND, like normal kernel distribution (a Gaussian kernel for each sample) or histograms in its framework (Bosman and Thierens 2000a, 2000b).  \nCho and Zhang (2002) proposed a continuous EDA that learns a mixture of factor analyzers using the EM algorithm. They also employed a more complicated mixture of variational Bayesian independent component analyzers in a later study (Cho and Zhang 2004). MOPEDA (Costa and Minisci 2003) applies a Parzen estimator that convolves the empirical estimation obtained from a finite data set with a squared integrable kernel function in order to reduce the variance of the probability distribution estimation. Both Gaussian and Cauchy kernels are used alternatively during evolution to utilize their intrinsic complementary characteristics. In KEDA (Luo and Qian 2009), the width of each kernel is dynamically computed during the optimization.  \nHistogram-based EDAs (HEDAs) discretize each variable's values by dividing their range to a number of bins. Tsutsui et al. (2001) proposed two types of marginal histogram models: (i) a fixed-width histogram (FWH) where the domain of each variable is divided into a fixed number of bins whose height may differ depending on the variable values; (ii) a fixed-height histogram (FHH) where all bins have an equal value generation probability but can have different widths. Consequently, there will be more bins in denser regions and thus modeling will be more accurate.  \nDing et al. (2008) proposed two improvements to this histogram modeling in their HEDA. They introduced a surrounding effect, where the values of each bin can affect the values of its surrounding bins using a special surrounding factor. They also employed a shrinkage strategy whereby the height of the bin containing the best value of the variable can exceed a predefined threshold. [FORMULA_171] is also extended with histograms (Xiao et al. 2009), combining the original updating rule with bin updating, where the bins reaching a predefined height are divided.  \nHistogram modeling has also been applied to optimization in permutation domains (Tsutsui 2002; Tsutsui et al. 2006) using two different types of models. The first is an edge histogram matrix where each entry indicates the frequency of two permutation values occurring adjacent to each other in the population. The second is a node histogram matrix that encodes the frequency at which a special value in the permutation occurs at a specific location in the solution. Specific sampling algorithms are developed for these models where a new value is generated according to the value of adjacent permutation locations or the position for which the value is going to be generated.",
      "question_type": "advantage",
      "complexity": "medium",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:04:58",
      "generation_style": "practical_application"
    },
    {
      "question": "What mathematical properties differentiate the Gaussian distribution model approach from the histogram model approach within the framework of Estimation of Distribution Algorithms applied to continuous optimization?",
      "answer": "The primary difference between EDAs is the aspect of building the probabilistic model. Generally, in continuous optimization there are two considerable branches: one is based on the Gaussian distribution model and the other on the histogram model. The first is the most widely used and has been studied extensively.",
      "chunk_id": 4,
      "paper_id": "Estimation of Distribution Algorithms with Fuzzy Sampling for Stochastic Programming Problems",
      "paper_title": "Estimation of Distribution Algorithms with Fuzzy Sampling for Stochastic Programming Problems",
      "paper_year": "2020",
      "chunk_content": "This chunk is from a paper on using Estimation of Distribution Algorithms (EDAs) with fuzzy sampling for stochastic programming problems. It introduces EDAs, highlighting their use in global optimization and continuous optimization, and contrasting Gaussian and histogram models. This section provides background for the paper's proposed EDA-based methods for handling noisy, nonlinear optimization problems. EDAs were firstly introduced in [44] as a new population-based method, and have been extensively studied in the field of global optimization [26,44]. Despite the fact that EDAs were firstly proposed for combinatorial optimization, many studies have been performed applying them to continuous optimization. The primary difference between EDAs is the aspect of building the probabilistic model. Generally, in continuous optimization there are two considerable branches: one is based on the Gaussian distribution model [25,26,45-51], and the other on the histogram model [47,52-58]. The first is the most widely used and has been studied extensively. The main steps of general EDAs are stated in Algorithm 1.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "topics": [
        "optimization",
        "algorithms"
      ],
      "generated_at": "2025-06-06 10:05:00",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does AEDA-SS perform compared to the original AEDA?",
      "answer": "AEDA-SS outperforms AEDA in most scenarios, or performs equivalently. There are far more scenarios where AEDA-SS performs better than where AEDA performs better.",
      "chunk_id": 16,
      "paper_id": "Handling Uncertainty in Financial Decision Making A Clustering Estimation of Distribution Algorithm With Simplified Simulation",
      "paper_title": "Handling Uncertainty in Financial Decision Making: A Clustering Estimation of Distribution Algorithm With Simplified Simulation",
      "paper_year": "2021",
      "chunk_content": "This chunk is from a paper on handling uncertainty in financial decision-making, specifically a group insurance portfolio problem. It discusses the effectiveness of using Monte Carlo simulation to estimate the expectation reward versus using statistical averages, finding that AEDA-SS, which uses simulation, outperforms the original AEDA. This validates the modification of the objective function to use simulation rewards for a more accurate approximation. Existing work considered the statistical average value of each parameter in the model to approximate the expectation reward of the problem [25]. By considering this approximated expectation reward as the objective function, the fitness value of the algorithm is deterministic. However, it cannot estimate the expectation reward accurately. By considering the reward of the portfolio plan in the MC simulation of the investment, a more accurate value of the expectation reward is approximated. The modification of the objective function makes the algorithm more practical.  \nTo validate the effectiveness of the modification of the objective function, we compare the performance of the original AEDA with AEDA-SS. The comparison results are shown in Table VI. It can be discovered that in the first and second situation, AEDA-SS outperforms AEDA in most of the scenarios. In the third situation, the two algorithms perform equivalently in most of the scenarios. However, in the rest scenarios, the scenarios that AEDA-SS performs better are far more than the scenarios that AEDA performs better. By considering simulation reward as the fitness value, a more accurate approximation of the real expectation reward can be gain. Better solutions that perform well in most of the scenarios in all situations can be optimized  \nTABLE VI\nCOMPARISON OF AEDA-SS WITH AEDA  \nb-number: scenarios that AEDA performs better than AEDA-SS.\nw-number: scenarios that AEDA performs worse than AEDA-SS.\ne-number: scenarios that AEDA perform equivalent to AEDA-SS.  \nTABLE VII\nCOMPARISON OF AEDA-SS WITH AEDA-D AND AEDA-G  \nb-number: scenarios that AEDA-D(G) perform better than AEDA-SS.\nw-number: scenarios that AEDA-D(G) perform worse than AEDA-SS.\ne-number: scenarios that AEDA-D(G) perform equivalent to AEDA-SS.\nby AEDA-SS. Therefore, the rationality of the objective function has been validated.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "topics": [
        "fitness"
      ],
      "generated_at": "2025-06-06 10:05:02",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What algorithmic steps are involved in the diversity-preserving multiobjective real-coded Bayesian optimization algorithm to maintain solution diversity?",
      "answer": "The diversity-preserving multiobjective real-coded Bayesian optimization algorithm uses a dynamic crowding and adaptive sharing approach as a diversity-preserving selection mechanism.",
      "chunk_id": 5,
      "paper_id": "An incremental-learning model-based multiobjective estimation of distribution algorithm",
      "paper_title": "An incremental-learning model-based multiobjective estimation of distribution algorithm",
      "paper_year": "2021",
      "chunk_content": "This section provides background on Multiobjective Estimation of Distribution Algorithms (MOEDAs), the class of algorithm to which the proposed ILME belongs. It details the two major components of MOEDAs: the modeling method (mixture of distributions or graphical model) and the fitness assignment function. It then reviews existing MOEDAs based on graphical models like Bayesian networks and mixture of distributions, highlighting their approaches to approximating the Pareto Set (PS). The proposed algorithm is type of a multiobjective estimation of distribution algorithm (MOEDA), which is a modified form of the estimation of distribution algorithm (EDA). The modeling method and the fitness assignment function are the two major components of a MOEDA. Its fitness assignment function usually comes from a MOEA, and most often in current research, a Pareto dominance-based approach is used. The modeling method adopted in MOEDA is based either on a mixture of distributions or on a graphical model.  \nMOEDAs based on graphical models lean mostly upon Bayesian network, which is a graphical expression of the conditional relationships among the stochastic variables. One of the most widely used algorithms is the multiobjective Bayesian optimization algorithm [24], which exploits the fitness assignment of NSGA-II. A hierarchical Bayesian optimization algorithm [25] uses a strength criterion from SPEA, but combines [FORMULA_25]-means in the objective space to improve scalability. By estimating a Gaussian Bayesian network to cluster the solutions, the diversity-preserving multiobjective real-coded Bayesian optimization algorithm [26] transforms the problem variables and utilizes a diversity-preserving selection mechanism, which uses a dynamic crowding and adaptive sharing approach. Martins et al. [27] developed a hybrid multiobjective Bayesian estimation of distribution algorithm (HMOBEDA), which scrutinizes a probabilistic graphic model based on a Bayesian network. The probabilistic graphic model gives the joint probability of decisions, objectives, variables, and configuration parameters to assess the influence of both diversity and convergence along the approximate PF. Martins et al. [28] improved HMOBEDA using the information in the final structure of the probabilistic graphic model. Garrido et al. [29] designed an information-based predictive entropy search for the simultaneous optimization of multiple expensive-to-evaluate blackbox functions under the presence of several constraints.  \nMOEDAs based on a mixture of distributions explicitly employ a mixture of probability distributions to directly approximate the PS. Bosman et al. [30] proposed multiobjective iterated density estimation algorithms (MIDEAs) as a paradigm for MOEDA, in which a probabilistic model is learned to stimulate the desirable parallel exploration along the PF. Li et al. [31] proposed a hybrid EDA with a joint probability distribution for multiobjective [FORMULA_26] knapsack problems. The adaptive variance scaling was enhanced by introducing a standard deviation ratio trigger embedded within the normal mixture distribution in MIDEA [32]. This approach avoids the loss of diversity and premature convergence. Further, adaptive variance scaling has been conjointly employed with an anticipated mean shift in a multiobjective adapted maximum-likelihood Gaussian mixture model (GMM) [33]. Shim et al. [34] introduced a restricted Boltzmann machine, which is an energy-based stochastic neural network, into MOEDA. The energy function of the network is used to construct a probabilistic model. Mohagheghi et al. [35] adopted a Voronoi mesh to construct the stochastic model. Maza et al. [36] integrated the mutual information between random variables as a probabilistic model to guide the search by modeling the redundancy and relevance relations between features in intrusion detection systems.",
      "question_type": "theory",
      "complexity": "medium",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:05:04",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How should researchers evaluate the effectiveness of the neighborhood search operator properties derived for the Unrelated Parallel Machine Scheduling Problem with Sequence-Dependent Setup Times?",
      "answer": "The effectiveness of the properties can be demonstrated by numerical comparisons.",
      "chunk_id": 1,
      "paper_id": "A Hybrid Estimation of Distribution Algorithm for Unrelated Parallel Machine Scheduling with Sequence-Dependent Setup Times",
      "paper_title": "A Hybrid Estimation of Distribution Algorithm for Unrelated Parallel Machine Scheduling with Sequence-Dependent Setup Times",
      "paper_year": "2016",
      "chunk_content": "This chunk introduces a hybrid Estimation of Distribution Algorithm with Iterated Greedy search (EDA-IG) for solving the Unrelated Parallel Machine Scheduling Problem with Sequence-Dependent Setup Times (UPMSP-SDST) to minimize makespan. It discusses neighborhood search operator properties, a probability model based on job neighbor relations, and iterated greedy search. It provides context on parallel machine scheduling problems (PMSP) and the relevance of sequence-dependent setup times (SDST) in UPMSP. Ling Wang, Shengyao Wang, and Xiaolong Zheng\nA hybrid estimation of distribution algorithm (EDA) with iterated greedy (IG) search (EDA-IG) is proposed for solving the unrelated parallel machine scheduling problem with sequence-dependent setup times (UPMSP-SDST). For makespan criterion, some properties about neighborhood search operators to avoid invalid search are derived. A probability model based on neighbor relations of jobs is built in the EDA-based exploration phase to generate new solutions by sampling the promising search region. Two types of deconstruction and reconstruction as well as an IG search are designed in the IG-based exploitation phase. Computational complexity of the algorithm is analyzed, and the effect of parameters is investigated by using the Taguchi method of design-of-experiment. Numerical tests on 1640 benchmark instances are carried out. The results and comparisons demonstrate the effectiveness of the EDA-IG. Especially, the bestknown solutions of 531 instances are updated. In addition, the effectiveness of the properties is also demonstrated by numerical comparisons.  \nIndex Terms-Unrelated parallel machine scheduling, sequence-dependent setup time (SDST), estimation of distribution algorithm (EDA), iterated greedy search.\nPARALLEL machine scheduling problem (PMSP) [FORMULA_0] is a typical combinatorial optimization problem existing in many manufacturing systems [FORMULA_1]. According to the characteristic of machines, PMSP can be classified into three types: identical PMSP [FORMULA_2], where each job has an identical processing time on every machine; uniform PMSP [FORMULA_3], where the processing time of a job is inversely proportional to the processing speed of a machine; and unrelated PMSP (UPMSP) [FORMULA_4], where the processing times of a job on different machines are unrelated. Identical PMSP and uniform PMSP can be regarded as the special cases of UPMSP. As a complex scheduling problem, PMSP is NP-hard even in a two-machine case [FORMULA_5]. Therefore, it is important to study PMSP and to develop effective solution algorithms, especially for the most general one, UPMSP.  \nManuscript received February 29, 2016; accepted May 17, 2016. This work was supported by the National Science Fund for Distinguished Young Scholars of China (61525304). Recommended by Associate Editor Yanjun Liu.\nCitation: Ling Wang, Shengyao Wang, Xiaolong Zheng. A hybrid estimation of distribution algorithm for unrelated parallel machine scheduling with sequence-dependent setup times. IEEE/CAA Journal of Automatica Sinica, 2016, 3(3): [FORMULA_6]  \nLing Wang, Shengyao Wang, and Xiaolong Zheng are with the Department of Automation, Tsinghua University, Beijing 100084, China (e-mail: wangling@mail.tsinghua.edu.cn; wangshengyao@tsinghua.org.cn; zhengxl11@mails.tsinghua.edu.cn).  \nIn the literature about the UPMSP, it is common to assume that there is no additional time between any two processing operations [FORMULA_7]. However, setup times widely exist in practice when preparing machines for the coming jobs [FORMULA_8]. Setup times are relevant to the job sequences on machines. During recent years, the UPMSP with sequence-dependent setup times (UPMSP-SDST) has been a focus in the area of PMSP. According to the reported research work before 2008, a brief review of the UPMSP-SDST was presented in [10]. Here, we focus on the typical literature after 2008.",
      "question_type": "implementation",
      "complexity": "advanced",
      "topics": [
        "complexity"
      ],
      "generated_at": "2025-06-06 10:05:06",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "Why is the concept of encoding signal cycles into a vector of integers important for traffic signal optimization?",
      "answer": "Encoding signal cycles into a vector of integers, where each integer denotes the duration time of a phase, allows for a straightforward representation of candidate solutions. This is important because the duration times of real traffic lights are always integers, making the encoding practical and easily implementable.",
      "chunk_id": 5,
      "paper_id": "Traffic Signal Optimization with Estimation of Distribution Algorithms and VISSIM-MATLAB Integrated Simulation Platform",
      "paper_title": "Traffic Signal Optimization with Estimation of Distribution Algorithms and VISSIM-MATLAB Integrated Simulation Platform",
      "paper_year": "2020",
      "chunk_content": "This section details the methodology for traffic signal optimization using modified Estimation of Distribution Algorithms (EDAs). It explains solution encoding as a vector of integers representing phase durations, the optimization criterion of minimizing mean vehicle delay time using VISSIM simulation, and introduces two modified EDAs: EDA with variance enlargement strategy (EDAve) and EDA with variable-width histogram model (EDA-VWH). In this section, the details of our approaches for traffic signal optimization are presented. Firstly, the solution encoding and the optimization criterion are described. Then the modified [FORMULA_8] and EDA-VWH are introduced for finding high-quality solutions for the traffic signal optimization problem.\nIn many traffic networks, traffic lights are designated with fixed signal cycles over a period of time and they simultaneously perform their own cycles repeatedly to control the flow of all vehicles. The traffic lights located in the same intersection are harmoniously ruled by a common signal cycle. A signal cycle is a sequence of basic signal phases, where each phase has a separate duration time and indicates the color states of all signal lights allowing compatible vehicles to pass the intersection securely. Fig. 2 presents a signal cycle with four phases in an intersection, and we assume that each cycle has a fixed order of phases, i.e., phases are repeated based on the order \" 1 \", \" 2 \", \" 3 \" and \" 4 \". This is an acceptable cycle program and is adopted by many cities like Xi'an. The main focus of this paper is to optimize the duration time of each phase with the aim of improving the service capability of a traffic network.\n[IMAGE1]  \nFig. 2. Four phases of a signal cycle.\nBased on the above consideration, we can encode the signal cycles into a vector of integers, where each integer denotes the duration time of a phase of a given signal cycle. The reason of using integer value lies in that the duration times of real traffic lights are always integers. Table I presents a simple example to explain the solution encoding process.  \nThere are two intersections and each of them has its own cycle with fixed order of phases. Each phase is assigned with a separate duration time specified in seconds. In this way, a candidate solution can be easily obtained and its length will be four times of the intersection number.  \nTABLE I. Solution Encoding of Two Intersections\nTo identify high-quality solutions, each candidate solution is evaluated by simulating it in the traffic networks implemented in VISSIM. The information obtained from the simulation can be employed as optimization criterion, i.e. fitness value. There are many useful information can be extracted from VISSIM through its evaluation module, such as the global mean speed of vehicles, mean travel time and mean delay time. The main objective of this study is to minimize the mean delay time of all vehicles traveling in a traffic network. The delay time of a vehicle is the time difference between its real travel time and the ideal travel time (no other vehicles, no signal control). By minimizing the mean delay time of all vehicles, the drivers can travel through a local area as soon as possible and thus get higher degree of satisfaction.\n[FORMULA_9] and EDA-VWH [15] are two efficient variants of estimation of distribution algorithms for solving continuous optimization problems. They will be introduced and modified to tackle the traffic signal optimization problem in this subsection.",
      "question_type": "evaluation",
      "complexity": "basic",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:05:08",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "How should developers practically configure the constriction factor in the canonical Particle Swarm Optimization (PSO) algorithm, given the typical parameter settings?",
      "answer": "Developers should set the cognitive and social coefficients (FORMULA_29 and FORMULA_30) to 2.05 and FORMULA_28 to 1. This results in the constriction factor (FORMULA_31) being equal to FORMULA_32.",
      "chunk_id": 2,
      "paper_id": "An Estimation of Distribution Particle Swarm Optimization Algorithm",
      "paper_title": "An estimation of distribution particle swarm optimization algorithm",
      "paper_year": "2006",
      "chunk_content": "This section introduces the canonical Particle Swarm Optimization (PSO) algorithm, detailing its initialization, update rules for particle velocity and position, and the concept of constriction. It highlights the algorithm's reliance on individual particle memory and its limitation in swarm-level learning, which leads to inefficient re-exploration of the search space. The authors motivate their EDPSO algorithm as a means to address this limitation. The first Particle Swarm Optimization (PSO) algorithm was introduced by Kennedy and Eberhart [12]. It is a population-based optimization algorithm inspired by the social behavior of birds and, like other algorithms of its kind, it is initialized with a population of complete solutions (called particles) randomly located in a [FORMULA_4]-dimensional solution space. An objective function [FORMULA_5] where [FORMULA_6], determines the quality of a particle's position, that is, a particle's position represents a solution to the problem being solved. A particle [FORMULA_7] at time step [FORMULA_8] has a position vector [FORMULA_9] and a velocity vector [FORMULA_10]. Another vector [FORMULA_11] stores the position in which it has received the best evaluation of the objective function. This vector is updated every time the particle finds a better position. Finally, the best vector [FORMULA_12] (i.e., the one with the best objective function value) of any particle belonging to the \"neighborhood\" of particle [FORMULA_13] is stored in vector [FORMULA_14]. If the neighborhood of particle [FORMULA_15] is the whole swarm, then [FORMULA_16] is the best solution found so far.  \nThe algorithm iterates updating particles' velocity and position until a stopping criterion is met, usually a sufficiently good solution value or a maximum number of iterations or function evaluations. The update rules are:  \n[FORMULA_17]  \nwhere [FORMULA_18] and [FORMULA_19] are two constants called the cognitive and social coefficients respectively, [FORMULA_20] and [FORMULA_21] are two [FORMULA_22]-dimensional uniformly distributed random vectors (generated every iteration) in which each component goes from zero to one, and [FORMULA_23] is an element-by-element vector multiplication operator.  \nClerc and Kennedy [3] introduced the concept of constriction in PSO. Since it is based on a rigorous analysis of the dynamics of a simplified model of the original PSO, it became highly influential in the field to the point that it is now referred to as the canonical PSO. The difference with respect to the original PSO is the addition of a constriction factor in Equation 1. The modified velocity update rule becomes  \n[FORMULA_24]  \nwith  \n[FORMULA_25]  \nwhere [FORMULA_26] and [FORMULA_27]. Usually, [FORMULA_28] is set to 1 and both [FORMULA_29] and [FORMULA_30] are set to 2.05 , giving as a result [FORMULA_31] equal to [FORMULA_32]. This is the PSO version we use in our comparisons [FORMULA_33].  \nFrom Equations 1 and 3, it is clear that the behavior of every particle is partially determined by its previous experience (through vector [FORMULA_34] ). This memory allows a particle to search somewhere around its own previous best position and the best position ever found by a particle in its neighborhood. However, during a search different particles move and test (i.e., evaluate the objective function) over and over again the same, or approximately the same, region in the search space without any individual improvement. While this is part of the search process and allows the swarm to explore the search space, it is also a waste of computing power when the explored regions have been visited before by the swarm without success. This happens because the swarm as a single entity does not learn.",
      "question_type": "relationship",
      "complexity": "medium",
      "topics": [
        "search"
      ],
      "generated_at": "2025-06-06 10:05:11",
      "generation_style": "practical_application"
    },
    {
      "question": "What mathematical properties characterize the probabilistic dependency model used by the Estimation of Distribution Algorithm (EDA) to produce a solution set of discriminative methylation sites?",
      "answer": "The method builds a probabilistic dependency model to produce a solution that is a set of discriminative methylation sites.",
      "chunk_id": 1,
      "paper_id": "Identifying DNA Methylation Modules Associated with a Cancer by Probabilistic Evolutionary Learning",
      "paper_title": "Identifying DNA Methylation Modules Associated with a Cancer by Probabilistic Evolutionary Learning",
      "paper_year": "2018",
      "chunk_content": "This chunk contains the title, authors, affiliations, and abstract of a research paper focused on identifying DNA methylation modules associated with cancer. It introduces an Estimation of Distribution Algorithm (EDA) to find high-order interactions of DNA methylation sites relevant to cancer, using a probabilistic dependency model. The paper applies the algorithm to DNA methylation profiling datasets and demonstrates its ability to identify cancer-related DNA methylation modules. Key topics include DNA methylation, cancer, Estimation of Distribution Algorithm, and high-throughput data analysis. [IMAGE_0]\nCancer Research Institute, College of Medicine, Catholic University of Korea, Seoul, KOREA\nResearch Institute of Agriculture and Life Sciences, College of Agriculture and Life Sciences, Seoul National University, Seoul, KOREA\nSchool of Computer Science \\& Engineering, Seoul National University, Seoul, KOREA  \nAbstract-DNA methylation leads to inhibition of downstream gene expression. Recently, considerable studies have been made to determine the effects of DNA methylation on complex disease. However, further studies are necessary to find the multiple interactions of many DNA methylation sites and their association with cancer. Here, to assess DNA methylation modules potentially relevant to disease, we use an Estimation of Distribution Algorithm (EDA) to identify high-order interaction of DNA methylated sites (or modules) that are potentially relevant to disease. The method builds a probabilistic dependency model to produce a solution that is a set of discriminative methylation sites. The algorithm is applied to array- and sequencing-based high-throughput DNA methylation profiling datasets. The experimental results show that it is able to identify DNA methylation modules for cancer.",
      "question_type": "definition",
      "complexity": "advanced",
      "topics": [
        "analysis"
      ],
      "generated_at": "2025-06-06 10:05:13",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What advantage do Estimation of Distribution Algorithms have over other population-based algorithms?",
      "answer": "The EDA advantage is that it relies on the construction and maintenance of a probability model that generates satisfactory solutions for the problem solved.",
      "chunk_id": 2,
      "paper_id": "A Bio Inspired Estimation of Distribution Algorithm for Global Optimization",
      "paper_title": "A Bio Inspired Estimation of Distribution Algorithm for Global Optimization",
      "paper_year": "2012",
      "chunk_content": "This section provides background on Estimation of Distribution Algorithms (EDAs) and Quantum-inspired Immune Clonal Algorithms (QICA), key components of the proposed hybrid optimization algorithm. It details EDA's probabilistic modeling approach and QICA's integration of quantum computing principles with immune clonal selection. iterated density estimation evolutionary algorithms (IDEAs) are EAs that apply an explicit sampling procedure through using probabilistic models rep- resenting the solutions characteristics. Estimation of Distribution Algorithms (EDAs) are types of the IDEA and population based algorithms with a theoretical foundation of probability theory. They can extract the global statistical information about the search space from the search so far and builds a probability model of promising solutions [FORMULA_2]. The general procedure of EDA is described in algorithm 1 .  \nThe EDA advantage is that it relies on the construction and maintenance of a probability model that generates satisfactory solutions for the problem solved. An estimated probabilistic model, to capture the joint probailties between variables, is constructed from selecting the current best solutions and then it is simulated for producing samples to guide the search process and update the induced model. Estimating the joint probability distribution associated with the data constitutes the bottleneck of EDA. Based on the complexity of the model used, EDAs are classified into different categories, without interdependencies,  \n  \npair wise dependencies and multiply dependencies algorithms where detailed description is shown in [15].\nQuantum-Inspired Artificial Immune System algorithms had been applied extensively in virous real applications [FORMULA_3]. The vaccine operator was also used in many works with the AIS algorithms to enhance their exploration ability and increase their detection efficiency [FORMULA_4]. Quantum inspired ICA (QICA), is the hybridization between QC and classical ICA to enhance the perfrmonace of the ICA and helpe in solving the problem of its ineffective performance in high dimensional problems. Inspired quantum concepts used in QICA include quantum bit (q-bit), quantum mutation gate and observation process [FORMULA_5].",
      "question_type": "comparison",
      "complexity": "basic",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:05:15",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the three types of time delays designed in the PMBGNP algorithm's graphical structure, and what is the purpose of these time delays?",
      "answer": "The three types of time delays are for judgment nodes, node transition, and processing nodes. The purpose of these time delays is to simulate the interaction between agents and environments explicitly, reflecting the time spent on judging the environment, preparing for actions, and taking actions.",
      "chunk_id": 4,
      "paper_id": "Probabilistic model building Genetic Network Programming using multiple probability vectors",
      "paper_title": "Probabilistic Model Building Genetic Network Programming Using Multiple Probability Vectors",
      "paper_year": "2010",
      "chunk_content": "This section details the structure of the proposed PMBGNP${_M}$ algorithm, an extension of GNP, using directed graphs with judgment and processing nodes. It explains node transitions, \"if-then\" decision-making, action execution, and the implementation of time delays to simulate agent-environment interaction in dynamic environments. [FORMULA_7] is an extension of GNP, whose structure is expressed by a directed graph which is composed of a number of nodes. Fig. 1 shows the basic structure of PMBGNP [FORMULA_8], as well as GNP. One start node, fixed number of judgment nodes and processing nodes compose of the graphical structure. The transition starts from the start node, and each judgment node which works as \"if-then\" type decision making functions judges the information from environments to make a decision to transit to the next node. Processing nodes preserve the action functions to determine the actions. Moreover, judgment nodes have conditional branches connecting to different nodes due to the different judging results, while processing nodes have no conditional branch.  \nThere are time delays on [FORMULA_9]. Three types of time delays are designed in the graphical structure. One is devoted to the judgment nodes, another is designed for the\n[IMAGE0]  \nFig. 1. Basic structure of [FORMULA_10].\nnode transition and the last is for the processing nodes. The motivation of time delays is to simulate the interaction between agents and environments explicitly. For example, when an agent moves in an environment, some time it is needed to be spent on judging the environment, preparing for actions and taking actions. The values of time delays can be predefined. In this paper, the time delay of judgment nodes is set at 1 time unit, that of node transition is set at 0 and that of processing node is set at 5 time units. The robot will take one step of action when 10 or more time units are used. For example, after executing four judgments and one processing, if another one processing is executed, the total time units becomes 14 , which lead to end one step. On the other hand, the simulated agents end when the end condition is satisfied, that is the time step exceeds its threshold or the given task finishes by [FORMULA_11]. Therefore, with the design of time delays, [FORMULA_12] can create flexible programs for different dynamic environments.",
      "question_type": "process",
      "complexity": "medium",
      "topics": [
        "algorithms"
      ],
      "generated_at": "2025-06-06 10:05:17",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What metrics are most appropriate for assessing the impact of the chromosome repair heuristic within the hybrid Restricted Boltzmann Machine based Estimation of Distribution Algorithm, especially concerning the penalty applied when a salesman is unassigned?",
      "answer": "The impact of the chromosome repair heuristic can be assessed by observing the frequency with which the penalty constant is applied, which occurs when a salesman is not assigned any city. This frequency indicates how often the repair mechanism is invoked to satisfy routing conditions.",
      "chunk_id": 10,
      "paper_id": "A hybrid estimation of distribution algorithm for solving the multi-objective multiple traveling salesman problem",
      "paper_title": "A Hybrid Estimation of Distribution Algorithm for Solving the Multi-objective Multiple Traveling Salesman Problem",
      "paper_year": "2012",
      "chunk_content": "This excerpt details the core steps of the hybrid Restricted Boltzmann Machine based Estimation of Distribution Algorithm (hREDA) proposed for solving the Multi-objective Multiple Traveling Salesman Problem (MmTSP). It covers probabilistic modeling using RBM, solution sampling, chromosome repair, and solution updating within a decomposition framework, including local search via Evolutionary Gradient Search (EGS). The algorithm balances total traveling cost and workload among salesmen. a) Decode the integer representation of the cities into the binary representation. Train the network. Compute the [FORMULA_107] as shown in Step 4 of Fig. 1. Encode the binary representation of [FORMULA_108] into integer representation. Construct the probabilistic model [FORMULA_109] by computing the marginal probability of each city [FORMULA_110], where [FORMULA_111], in each permutation location as follows.  \n[FORMULA_112]  \nwhere [FORMULA_113] is the probability distribution of the cities at generation [FORMULA_114], [FORMULA_115] is the probability of city [FORMULA_116] to be located at the [FORMULA_117] position of the chromosome, [FORMULA_118] is the city [FORMULA_119].) and [FORMULA_120] is the normalizing constant as shown in Step 4 of Fig. 1.\nb) Sample [FORMULA_121] to generate [FORMULA_122] children solutions as follows.  \n[FORMULA_123]  \nwhere [FORMULA_124] is a newly generated city at [FORMULA_125] position of a chromosome.\nc) Improvement: Apply specific heuristic approach to repair the chromosomes to ensure that the conditions of routing are satisfied. Penalize the solution if any salesman is not assigned any city by multiplying the objective value with a constant [FORMULA_126].\nStep 3: Update solution:\nFor [FORMULA_127], do\na) Update of [FORMULA_128] : For [FORMULA_129], if [FORMULA_130], then set [FORMULA_131]\nb) Update of neighboring solutions: For [FORMULA_132], if [FORMULA_133] [FORMULA_134], then set [FORMULA_135] and [FORMULA_136].\nEnd do\nStep 4: Local search:\na) Perform local search if local search is activated. Next, apply Step 3 to update the created children solutions.\nStep 5: Stopping criterion: If stopping criterion is met, then stop. Else, go to Step 2.\nFig. 4. Pseudo code of hREDA\nsolutions, and the superior solutions will replace the inferior ones.  \nLocal search is performed if it is activated and it is applied every generation thereafter. The same procedure is carried out until the stopping criterion is met. The pseudo code of the EGS is presented in Fig 2. However, some modifications are required to adapt the EGS for the permutation-based problem. Firstly, each subproblem that undergoes local search will be perturbed to generate [FORMULA_137] local neighbors, which are created by simply swapping two genes in a chromosome. For each local solution, Step 3 is carried out to update the [FORMULA_138] and [FORMULA_139]. The fitness of the solutions are aggregated according to the  \nTABLE I\nParameNTER SEtTINGS FOR EXPERIMENTS  \nTABLE II\nInDICES OF DIPHERENT WELGHT SEtTINGS",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "topics": [
        "evolutionary"
      ],
      "generated_at": "2025-06-06 10:05:19",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What are the fundamental research interests of Josef Slezk?",
      "answer": "Josef Slezk's research interests are circuit theory and evolutionary synthesis of analog circuits.",
      "chunk_id": 16,
      "paper_id": "Evolutionary Synthesis of Cube Root Computational Circuit Using Graph Hybrid Estimation of Distribution Algorithm",
      "paper_title": "Evolutionary Synthesis of Cube Root Computational Circuit Using Graph Hybrid Estimation of Distribution Algorithm",
      "paper_year": "2014",
      "chunk_content": "This chunk provides biographical information about the authors, Josef Slezk and Ji Petrela, including their education, research interests (circuit theory, evolutionary synthesis, nonlinear dynamics, analog circuit design), and current positions at Brno University of Technology. This information appears at the end of the paper. Josef SLEZAK was born in Zlin, Czech Republic, in 1982. His research interest is circuit theory and evolutionary synthesis of analog circuits. He received the MSc. degree from the Brno University of Technology in 2007. Now he is working towards a PhD. degree at Department of Radio Electronics, Brno University of Technology\nJiri PETRZELA was born in Brno, Czech Republic, in 1978. He received the MSc. and PhD. degrees from the Brno University of Technology in 2003 and 2007 respectively. His research interest covers the nonlinear dynamics, chaos theory and analog circuit design. Currently he is an Associate Professor at the Department of Radio Electronics.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "topics": [
        "design"
      ],
      "generated_at": "2025-06-06 10:05:21",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "What practical steps should be taken when applying the Nash-Sutcliffe efficiency (NSE) to evaluate hydrologic model performance, especially when dealing with computationally intensive models and limited resources?",
      "answer": "Practitioners should limit the maximum number of function evaluations due to time and computing resource constraints. They should also implement each algorithm multiple times to account for randomness and record the best function value reached after each evaluation to assess performance based on efficiency, variability, reliability, and accuracy.",
      "chunk_id": 9,
      "paper_id": "Evaluation of Estimation of Distribution Algorithm to Calibrate Computationally Intensive Hydrologic Model",
      "paper_title": "Evaluation of Estimation of Distribution Algorithm to Calibrate Computationally Intensive Hydrologic Model",
      "paper_year": "2016",
      "chunk_content": "This chunk defines the Nash-Sutcliffe efficiency (NSE) as the primary objective function for calibrating the SWAT hydrologic model using the Estimation of Distribution Algorithm (EDA) and three other optimization algorithms. It also outlines the experimental setup, including computational constraints, the number of trials, and the metrics used to compare algorithm performance: efficiency, variability, reliability, and accuracy (NSE, volume error (R) and peak percent threshold statistics (PPTS)). The simulation performance of a model is usually determined by pairwise comparisons of model simulation results with observations.  \nThe Nash-Sutcliffe efficiency (NSE) is a commonly used indicator to evaluate the performance of hydrologic models (Legates and McCabe 1999; Nash and Sutcliffe 1970). It is a normalized statistic that determines the relative magnitude of the residual variance compared with the measured data variance. Therefore, the NSE evaluates how well the simulated hydrograph fits the observations.  \nThe NSE is defined as follows:  \n[FORMULA_51]  \nwhere [FORMULA_52] and [FORMULA_53] observed and simulated discharges at time step [FORMULA_54], respectively; [FORMULA_55] mean discharge for the entire time period of the evaluation; and [FORMULA_56] total number of time steps in the evaluation period. The NSE ranges from minus infinity to 1.0 , with higher values indicating better agreement. Because it is a minimization problem, the objective function is selected as [FORMULA_57] NSE.\nGenerally, algorithm efficacy can be measured by the number of function evaluations required to find the global optimum. However, because of limited time and computing resources, it is impossible to perform large numbers of simulations for a computationally intensive hydrologic model. In this study, on average, one execution of the SWAT model required 63 s on a Core i5 3GHZ processor (Intel, China). Furthermore, previous investigations (Duan et al. 1992; Fu et al. 2006; Goldberg 1989; Mugunthan et al. 2005; Paul and Iba 2002; Vose 1999) have proven that the EDA, MLMSRBF algorithm, SCE algorithm, and GA can converge to the global optimum if adequate computational effort can be implemented. Therefore, in this study the maximum number of function evaluations was limited to 500. As a result, the time consumed by one execution was 6.6 h for EDA, 6.8 h for MLMSRBF, 6.5 h for SCE, and 6.3 h for GA when calibrating the SWAT model for the Xunhe basin. Meanwhile, in consideration of the randomness of these algorithms, each algorithm was implemented 10 times. In each trial, the best function value reached by the algorithm after every function evaluation was recorded.  \nTo assess the performance of the algorithms, the following were compared: (1) efficiency, which is indicated by the average best objective function value; (2) variability, which can be evaluated by the standard variance and interquartile range; (3) reliability, which is shown by the means of the empirical cumulative distribution function; and (4) accuracy, which is estimated by the NSE, overall\n[IMAGE2]  \nFig. 3. Comparison of algorithm performance  \nTable 2. Average and Standard Deviation of the Objective Function Values for Each Algorithm  \nvolume error (R) and peak percent threshold statistics (PPTS) (Lohani et al. 2014).",
      "question_type": "application",
      "complexity": "medium",
      "topics": [
        "metrics"
      ],
      "generated_at": "2025-06-06 10:05:23",
      "generation_style": "practical_application"
    },
    {
      "question": "What mathematical properties of the Goldstein elimination criterion allow it to be used as a sufficient condition for rotamer configuration elimination within the dead-end elimination step of the EDAs?",
      "answer": "The Goldstein elimination criterion establishes a sufficient condition for a rotamer configuration to be absent from the optimal solution. The iterative use of this criterion allows for the elimination of rotamers that cannot be part of the optimal solution, contributing to a reduction in the dimension of the search space.",
      "chunk_id": 5,
      "paper_id": "Adding Probabilistic Dependencies to the Search of Protein Side Chain Configurations Using EDAs",
      "paper_title": "Adding Probabilistic Dependencies to the Search of Protein Side Chain Configurations Using EDAs",
      "paper_year": "2008",
      "chunk_content": "This chunk, from the \"Experiments\" section, details the protein dataset used to test the Estimation of Distribution Algorithms (EDAs) for side chain placement, explaining the data selection criteria (X-ray crystal structures, resolution, R factor, sequence identity), preprocessing steps (Goldstein criterion, SPRINT convergence), and the resulting 50 difficult protein instances used as a benchmark. It also specifies the parameter settings for the EDAs (population size, generations, truncation selection, elitism) and the problem reduction step using dead-end elimination based on the Goldstein criterion. To test our algorithms, we started with a protein dataset of 493 X-ray crystal structures [FORMULA_42] with a resolution better than or equal to [FORMULA_43], an [FORMULA_44] factor below [FORMULA_45], and a mutual sequence identity lower than [FORMULA_46]. Each protein consisted of 1-4 chains and up to 1000 residues. As a pre-processing step, we determined the instances in which the Goldstein criterion [5] eliminated all configurations but one, and those instances in which the inference-based algorithm for structure prediction (SPRINT) 23] converged.  \nSPRINT is one of the state-of-the-art algorithms for protein side chain placement. In 23], the energies obtained by SCWRL 36] (version 2.9) were reported to be strictly higher than those found by SPRINT in the small class of instances. Unfortunately, the SCWRL (version 3.0) implementation does not provide the energy values corresponding to solutions calculated by the algorithm. Proteins that were solved using the Goldstein criterion and those for which SPRINT converged were removed from the original database. The number of the remaining instances, which were used for our experiments, was 50 . They serve as an appropriate testbed to focus the investigation of EDAs on a representative set of difficult instances were other efficient algorithms have failed.\nTo work, EDAs require the definition of several parameters. We have used the same settings for all instances of the problems treated. The quality of the results achieved by the algorithms will depend on these settings. Since, in this paper, we focus on the role played by dependencies, no attempt has been made to tune the parameters to achieve an optimal performance. The parameters of the EDAs have been set as follows. The population size was set at 5000 . The maximum number of generations is 500 . Truncation selection with parameter [FORMULA_47] has been used. In this selection scheme, the best [FORMULA_48] individuals of the population are selected to construct the probabilistic model. By setting a rather low value of truncation, a strong selection pressure is induced, forcing the algorithm to discard poor solutions as a faster pace.  \nWe apply a replacement strategy called best elitism in which the selected population at generation [FORMULA_49] is incorporated into the population of generation [FORMULA_50], keeping the best individuals found so far and avoiding to revaluate their fitness function. The algorithm stops when the maximum number of generations  \n[^0]\n[^0]:    [FORMULA_51] These instances have been obtained from Chen Yanover's page: http://www.cs.huji.ac.il/ cheny/proteinsMRF.html  \nis reached or the selected population has become too homogeneous (no more than 10 different individuals).  \nEDAs incorporate an additional problem reduction step to decrease the number of variables and their number of values. This step starts from the application of a dead-end elimination step [5], based on the iterative use of the Goldstein elimination criterion, which establishes a sufficient condition for rotamer configuration [FORMULA_52] to be absent from the optimal solution. When no condition that further eliminates rotamers can be established, the algorithm stops. This step considerably contributes to reduce the dimension of the search space, but the search space remains huge.",
      "question_type": "theory",
      "complexity": "advanced",
      "topics": [
        "convergence"
      ],
      "generated_at": "2025-06-06 10:05:26",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the local search in the memetic NSGA-II with EDA compare to the local search in the hybrid NSGA-II and MOEA/D approach?",
      "answer": "The local search in the hybrid NSGA-II and MOEA/D approach is considered less effective and efficient because it is randomly applied to a predefined large number of subproblems. The memetic NSGA-II with EDA employs a clustering technique to select suitable Pareto solutions for local search, and learns a distribution model to sample new solutions for local improvements.",
      "chunk_id": 1,
      "paper_id": "Using EDA-Based Local Search to Improve the Performance of NSGA-II for Multiobjective Semantic Web Service Composition",
      "paper_title": "Using EDA-Based Local Search to Improve the Performance of NSGA-II for Multiobjective Semantic Web Service Composition",
      "paper_year": "2019",
      "chunk_content": "This chunk is the abstract and introduction to a paper on improving Web service composition using a memetic NSGA-II algorithm with EDA-based local search. It addresses limitations in existing hybrid NSGA-II and MOEA/D approaches and aims to optimize both functional (QoSM) and non-functional (QoS) quality. The paper introduces a clustering technique for Pareto solution selection and distribution model-guided local search, demonstrating improved performance compared to state-of-the-art methods. Keywords include Web service composition, QoS optimization, and EDA. Chen Wang [FORMULA_0], Hui Ma, and Gang ChenSchool of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand\\{chen.wang,hui.ma, aaron.chen\\}@ecs.vuw.ac.nz\nService-oriented computing is a computing paradigm that creates reusable modules over the Internet, often known as Web services. Web service composition aims to accomplish more complex functions by loosely coupling web services. Researchers have been proposing evolutionary computation (EC) techniques for efficiently building up composite services with optimized non-functional quality (i.e., QoS). Some of these techniques employ multi-objective EC algorithms to handle conflict qualities in QoS for fully automated service composition. One recent state-of-art work hybridizes NSGA-II and MOEA/D, which allows the multi-objective service composition problem to be decomposed into many scalar optimization subproblems, where a simple form of local search can be easily applied. However, their local search is considered to be less effective and efficient because it is randomly applied to a predefined large number of subproblems without focusing on the most suitable candidate solutions. In this paper, we propose a memetic NSGA-II with probabilistic model-based local search based on Estimation of Distribution Algorithm (EDA). In particular, a clustering technique is employed to select suitable Pareto solutions for local search. Each selected solution and its belonged cluster members are used to learn a distribution model that samples new solutions for local improvements. Besides that, a more challenging service composition problem that optimizes both functional and non-functional quality is considered. Experiments have shown that our method can effectively and efficiently produce better Pareto optimal solutions compared to other state-of-art methods in the literature.  \nKeywords: Web service composition [FORMULA_1] QoS optimisation [FORMULA_2] EDA",
      "question_type": "implementation",
      "complexity": "basic",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:05:28",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "How should developers distinguish between 1st-class and 2nd-class individuals during the selection process in UMDA?",
      "answer": "1st-class individuals are those that are always chosen during selection, regardless of the value of a particular bit. 2nd-class individuals are those that compete with other individuals for selection, meaning their bit value is biased toward 1 compared to 1st-class individuals.",
      "chunk_id": 8,
      "paper_id": "Lower bounds on the run time of the Univariate Marginal Distribution Algorithm on OneMax",
      "paper_title": "Lower bounds on the run time of the Univariate Marginal Distribution Algorithm on OneMax",
      "paper_year": "2020",
      "chunk_content": "This chunk from a theoretical computer science paper on the Univariate Marginal Distribution Algorithm (UMDA) introduces key concepts for bounding UMDA's runtime on the OneMax problem. It defines 1st-class and 2nd-class individuals based on their likelihood of selection and explains how the number of 2nd-class individuals ($C^*$) influences frequency updates. Figure 1 illustrates these definitions in relation to fitness levels. The chunk sets up the analysis of UMDA's selection bias and its impact on optimization, leading to a lower bound on the algorithm's runtime. [IMAGE1]  \nFig. 1. An exemplary visualization of the different definitions we need. The bones depict all of the [FORMULA_105] levels, the numbers above show their respective fitness, and the dots symbolize individuals in these levels. The line cutting through level [FORMULA_106] marks the point where more than [FORMULA_107] individuals have been sampled when starting from the top. In that level, not all individuals are going to be selected. Further, the individuals from the level below can be selected (as their fitness can still increase by one when sampling the last bit), and individuals from the level above can be not selected. Hence, the individuals in those levels are 2nd-class candidates. The individuals in higher levels will always be selected, thus they are 1st-class individuals. Out of the 2nd-class candidates, those individuals that are chosen during selection are the 2nd-class individuals (in this example, those would be two individuals, i.e., [FORMULA_108] ). Last, [FORMULA_109] depicts the cut level, i.e., the topmost level such that the number of sampled individuals is greater than [FORMULA_110] when including the next (lower) level.\nas individuals from level [FORMULA_111], once bit [FORMULA_112] has been sampled. Thus, individuals from level [FORMULA_113]were still prone to selection. This means that the outcome of bis [FORMULA_114] can influence whether the individual is being selected or not.  \nAmong the [FORMULA_115] individuals chosen during selection, we distinguish between two different types: 1st-class and 2nd-class individuals. 1st-class individuals are those which are chosen during selection no matter which value bit [FORMULA_116] has. The remaining of the [FORMULA_117] individuals are the 2nd-class individuals; they had to compete with other individuals for selection. Therefore, their bit value [FORMULA_118] is biased toward 1 compared to 1st-class individuals. Note that 2nd-class individuals can only exist if [FORMULA_119], since in this case, individuals from level [FORMULA_120]can still be as good as individuals from level [FORMULA_121]after sampling bit [FORMULA_122].  \nLet [FORMULA_123] be the number of 1 s at position [FORMULA_124] of the [FORMULA_125] selected individuals in iteration [FORMULA_126] of UMDA, and let [FORMULA_127] denote the number of 2nd-class individuals in iteration [FORMULA_128]. Note that the number of 1 s of 1 st-class individuals during iteration [FORMULA_129] follows a binomial distribution with success probability [FORMULA_130]. Since we have [FORMULA_131] 1st-class individuals, the distribution of the number of 1 s of these follows [FORMULA_132]. Note that the actual frequency in iteration [FORMULA_133] might be set to either [FORMULA_134] or [FORMULA_135] if the number of 1 s in the [FORMULA_136] selected individuals is too close to 0 or [FORMULA_137], respectively. We will be able to ignore this fact in our forthcoming analyses since all considerations are stopped when a frequency drops below [FORMULA_138] or exceeds [FORMULA_139].",
      "question_type": "implementation",
      "complexity": "medium",
      "topics": [
        "optimization",
        "algorithms"
      ],
      "generated_at": "2025-06-06 10:05:30",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What criteria should be used to evaluate the performance of the Estimation of Distribution Algorithm (EDA) relative to other optimization algorithms like MLMSRBF, SCE, and GA when applied to hydrologic model calibration, beyond just the final objective function value?",
      "answer": "The performance of EDA, MLMSRBF, SCE, and GA can be compared on the basis of their performance in addressing the global minimization problem, considering that these search methods can be generalized because no problem-dependent information is needed.",
      "chunk_id": 4,
      "paper_id": "Evaluation of Estimation of Distribution Algorithm to Calibrate Computationally Intensive Hydrologic Model",
      "paper_title": "Evaluation of Estimation of Distribution Algorithm to Calibrate Computationally Intensive Hydrologic Model",
      "paper_year": "2016",
      "chunk_content": "This chunk, from a study evaluating the Estimation of Distribution Algorithm (EDA) for hydrologic model calibration, introduces the mathematical formulation of parameter calibration as a global minimization problem. It sets the stage for comparing EDA with other optimization algorithms (MLMSRBF, SCE, GA) for Soil and Water Assessment Tool (SWAT) model calibration in the Xunhe River Basin, highlighting EDA's advantage of fewer parameters compared to Genetic Algorithms (GA). The parameter calibration of hydrologic models can be generalized as a global minimization problem with box constraints  \n[FORMULA_12]  \nsubject to  \n[FORMULA_13]  \nwhere [FORMULA_14] model parameter value that needs to be calibrated, varying from [FORMULA_15] to [FORMULA_16]. The bounds [FORMULA_17] and [FORMULA_18] should be on the basis of a physically feasible range of parameters. The [FORMULA_19] is the objective function on the basis of the error between the observed and simulated streamflow of the basin outlet. In most cases, [FORMULA_20] is a multimodal function without prior information.  \nFour automatic search methods, the EDA, MLMSRBF algorithm, SCE algorithm, and GA, were tailored for this minimization problem and compared on the basis of their performance. These search methods can be generalized because no problem-dependent information is needed.\nThe EDA was developed as a novel class of evolutionary optimization algorithm (Muhlenbein et al. 1996). The main advantage of\nthe EDA over the GA is the absence of genetic operator parameters to be tuned (Armananzas et al. 2008) (Fig. 1).",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "topics": [
        "optimization"
      ],
      "generated_at": "2025-06-06 10:05:33",
      "generation_style": "evaluation_metrics"
    }
  ]
}