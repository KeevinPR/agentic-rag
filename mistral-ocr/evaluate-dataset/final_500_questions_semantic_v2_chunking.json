{
  "metadata": {
    "generation_date": "2025-06-28 23:51:40",
    "target_questions": 500,
    "total_questions": 500,
    "generation_method": "chunk_based",
    "statistics": {
      "successful": 500,
      "failed_generation": 1,
      "duplicate_skipped": 13,
      "total_attempts": 514
    }
  },
  "questions": [
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of solution representation work within the OBEDA (Online Batching Estimation of Distribution Algorithm) for the online order-batching problem?",
      "topics": [
        "initialization"
      ],
      "answer": "The OBEDA represents a solution to the online batching problem by assigning customer orders to batches. This is done through an order assignment vector, where the number of elements in the vector equals the total number of orders to pick up. Each element in the vector contains an integer value that represents the batch to which the corresponding order is assigned. For example, with 10 orders and 3 batches, the first batch might consist of orders {5, 7, 9}, the second batch includes orders {1, 3, 4, 6}, and the third batch incorporates orders {2, 8, 10}.",
      "chunk_id": 24637,
      "paper_id": "Finding Interactions or Relationships Between Customer Orders for Building Better Batches by Means of an Estimation of Distribution Algorithm-based approach for the Online Order Batching Problem",
      "paper_title": "Finding Interactions or Relationships Between Customer Orders for Building Better Batches by Means of an Estimation of Distribution Algorithm-based approach for the Online Order Batching Problem",
      "paper_year": "2016",
      "chunk_content": "Any algorithm for the online order-batching problem has to form and release batches without having complete information on the characteristics and arrival times of future orders. [IMAGE_0] Figure 1. Example of an S-Shape route in a single-block warehouse. When a set of unprocessed orders arrives and an order picker becomes available, those unprocessed orders can be grouped into one or more batches that should either be released directly or whose start should be postponed until a later point in time, as defined previously. ## 4. OBEDA - FOR THE ONLINE BATCHING PROBLEM In this section, we propose the OBEDA to solve the online orderbatching problem. We also introduce the solution representation, population initialization, and a probability model. ### 4.1 Solution representation (chromosomes codification) Any solution to the online batching problem mentioned should be expressed by the assignment of customer orders to batches. An order assignment vector represents a solution where the number of elements equals the total number of orders to pick up, where each element contains an integer value that represents the batch to be assigned. This representation is depicted in Figure 2 with 10 orders and 3 batches. Please note that the first batch consists of orders $\\{5 ; 7 ; 9\\}$, the second batch includes orders $\\{1 ; 3 ; 4 ; 6\\}$, and the third batch incorporates orders $\\{2 ; 8 ; 10\\}$.",
      "question_type": "definition",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:51:42",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "When deploying the HMRF-CSA/DE/EDA algorithm for image segmentation, what practical considerations should be taken into account regarding its computational complexity compared to a variant without region-based processing (PHMRF-CSA/DE/EDA)?",
      "topics": [
        "complexity"
      ],
      "answer": "When deploying HMRF-CSA/DE/EDA, practitioners should be aware that it exhibits relatively high computational complexity due to the time-consuming nature of bio-inspired algorithms, specifically the Differential Evolution (DE) and Estimation of Distribution Algorithm (EDA) components. While HMRF-CSA/DE/EDA outperforms PHMRF-CSA/DE/EDA (the variant without region-based HMRF), the region-based HMRF component contributes to the increased time cost. Therefore, consider the trade-off between segmentation accuracy and computational time, especially when processing large datasets or requiring real-time performance. The original implementation was tested with an Intel Core i7-4710HQ CPU and 16 GB of memory, using Matlab R2015a, which provides a baseline for expected performance.",
      "chunk_id": 23117,
      "paper_id": "CSA-DE-EDA- A Clonal Selection Algorithm Using Differential Evolution and Estimation of Distribution Algorithm",
      "paper_title": "CSA-DE/EDA: A Clonal Selection Algorithm Using Differential Evolution and Estimation of Distribution Algorithm",
      "paper_year": "2018",
      "chunk_content": "The accuracy of these six algorithms in the segmentation of gray matter, white matter and overall brain volume on each image is depicted in Fig. 3, and the average segmentation accuracies of these algorithms are compared in Table 2. As it can be noticed, the proposed HMRFCSA/DE/EDA algorithm can produce more accurate segmentation of gray matter and overall brain volume than the other five algorithms on the IBSRV2.0 dataset. We conducted the ablation experiment with HMRF-CSA/DE/EDA and HMRFCSA/DE/EDA without region-based part (PHMRF-CSA/DE/EDA). The average accuracy and time cost of the two algorithms on one 3D MR image of IBSRV2.0 are shown in Table 3 (Intel Core i7-4710HQ CPU, 16 GB memory and Matlab R2015a). It shows that HMRF-CSA/DE/EDA outperforms PHMRF-CSA/DE/EDA, which means that the region-based HMRF can indeed improve the pixel-based HMRF as an auxiliary. However, HMRF-CSA/DE/EDA still has a relatively high computational complexity due to the time-consuming nature of bio-inspired algorithms.",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:51:44",
      "generation_style": "practical_application"
    },
    {
      "question": "In the Scatter Search (SS) algorithm described, how does the management of diversification and intensification, specifically through the RefSet update mechanism, contribute to the search for solutions, and what role do the parameters $b_1$ and $b_2$ play in balancing these two aspects?",
      "topics": [
        "design"
      ],
      "answer": "The Scatter Search algorithm manages diversification and intensification through the RefSet. Initially, $b_1$ solutions with the best objective function values are added to the RefSet. Subsequently, $b_2$ most diverse solutions in relation to the solutions already in the RefSet are added. The RefSet is updated by generating new subsets of solutions from it, combining and improving them. If the RefSet changes, the process repeats. Otherwise, the $b_2$ solutions with the worst objective function values in the RefSet are deleted. $b_1$ and $b_2$ control the size of the RefSet and the balance between selecting high-quality solutions (intensification) and maintaining diversity within the RefSet (diversification).",
      "chunk_id": 12750,
      "paper_id": "Scatter Search in software testing",
      "paper_title": "Scatter Search in software testing, comparison and collaboration with Estimation of Distribution Algorithms",
      "paper_year": "2006",
      "chunk_content": "4.1. SS in software testing As pointed out in Section 2, when tackling the generation of a test input covering a branch, the associated search space is usually large and complex. A well-known conjecture in Operations Research is that an appropriate management of the diversification and intensification concepts during the search in such spaces yields to good solutions. These are the principles on which SS is based. $$\n\\begin{aligned}\n& P \\leftarrow \\emptyset \\\\\n& P \\leftarrow \\operatorname{Add}\\|P\\| \\text { distinct solutions obtained by diversification and improvement } \\\\\n& \\text { RefSet } \\leftarrow \\text { Add the } b_{1} \\text { solutions in } P \\text { with best objective function value and delete them from } P \\\\\n& \\text { Repeat for } l=1,2, \\ldots, \\text { MaximumIteration } \\\\\n& \\text { RefSet } \\leftarrow \\text { Add the } b_{2} \\text { most diverse solutions in } P \\text { in relation to the solutions in RefSet } \\\\\n& \\text { NewSolutions } \\leftarrow T R U E \\\\\n& \\text { Repeat while NewSolutions }=\\text { TRUE } \\\\\n& \\text { NewSolutions } \\leftarrow F A L S E \\\\\n& \\text { Generate all new subsets of solutions from RefSet } \\\\\n& \\text { Obtain new solutions by combination and improvement } \\\\\n& \\text { RefSet } \\leftarrow \\text { Update RefSet with new solutions } \\\\\n& \\text { If RefSet changed then } \\\\\n& \\text { NewSolutions } \\leftarrow \\text { TRUE } \\\\\n& \\text { else } \\\\\n& \\text { RefSet } \\leftarrow \\text { Delete the } b_{2} \\text { solutions with worst objective function value in RefSet } \\\\\n& P \\leftarrow \\emptyset \\\\\n& P \\leftarrow \\text { Add }\\|P\\| \\text { distinct solutions obtained by diversification and improvement }\n\\end{aligned}\n$$ Fig. 8. Pseudocode of basic SS. [IMAGE_3] Fig. 9. Schematic of a basic SS design. This, together with the flexibility of the SS methodology, makes it suitable for solving the test case generation problem. Moreover, in Laguna and Marti (2000) several SS designs were used to solve a set of nonlinear function minimization problems, obtaining encouraging results. Since the test case generation is tackled as the resolution of a number of function optimization problems, SS seems to be a promising technique to be studied. The SS test case generator proposed here is based on the general framework described in Section 2. The optimization problem is tackled by means of an SS algorithm, where a solution is a [FORMULA162] string representing an input and its objective function value is obtained according to Eq.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:51:46",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "According to the details in the chunk, what advantages does the decentralized coordination learning automaton (DCLA) have over decentralized learning automata (DLAs) in coordinating learning?",
      "topics": [
        "scheduling"
      ],
      "answer": "The decentralized coordination learning automaton (DCLA) uses a new selection probability vector combined with probability vectors from both learning automata (LA) and estimation of distribution algorithm (EDA). This allows DCLA to incorporate both the individual learning experience of each LA and the estimation of the distribution of promising individuals from the whole swarm, providing a central coordination mechanism not present in standard decentralized learning automata (DLAs).",
      "chunk_id": 1619,
      "paper_id": "Using estimation of distribution algorithm to coordinate decentralized learning automata for meta-task scheduling",
      "paper_title": "Using Estimation of Distribution Algorithm to Coordinate Decentralized Learning Automata for Meta-task Scheduling",
      "paper_year": "2014",
      "chunk_content": "Using Estimation of Distribution Algorithm to Coordinate Decentralized Learning Automata for Meta-task Scheduling Jie Li and Junqi Zhang ${ }^{\\dagger}$Department of Computer Science and TechnologyKey Laboratory of Embedded System and Service Computing, Ministry of EducationTongji University, Shanghai, 200092, China.$M$ Corresponding author (E-mail: lijietjsh@gmail.com, zhangjunqi@tongji.edu.cn) #### Abstract Learning automaton (LA) is a reinforcement learning model that aims to determine the optimal action out of a set of actions. It is characterized by updating a selection probability vector through a sequence of repetitive feedback cycles interacting with an environment. Decentralized learning automata (DLAs) consists of many learning automata (LAs) that learn at the same time. Each LA independently selects an action based on its own selection probability vector. In order to provide an appropriate central coordination mechanism in DLAs, this paper proposes a novel decentralized coordination learning automaton (DCLA) using a new selection probability vector which is combined with the probability vectors derived from both LA and estimation of distribution algorithm (EDA). LA contributes to the own learning experience of each LA while EDA estimates the distribution of the whole swarm's promising individuals.",
      "question_type": "advantage",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:51:48",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What programming considerations should be taken into account when implementing the calculation of the level of processing time variation (LPTV) within an Estimation of Distribution Algorithm (EDA) designed for scheduling problems with stochastic processing times?",
      "topics": [
        "performance"
      ],
      "answer": "When implementing the LPTV calculation, a developer must accurately compute both the expected value E(P) and the standard deviation σ of the processing time. The LPTV is then calculated as σ / E(P). Code should handle potential division-by-zero errors if E(P) is zero. Furthermore, given that a larger LPTV indicates a greater deviation between expected and actual processing times, the EDA implementation should incorporate mechanisms to manage this uncertainty, such as robust statistical sampling or adaptive parameter tuning within the probabilistic model.",
      "chunk_id": 9730,
      "paper_id": "A hybrid estimation of distribution algorithm for simulation-based scheduling in a stochastic permutation flowshop",
      "paper_title": "A hybrid estimation of distribution algorithm for simulation-based scheduling in a stochastic permutation flowshop",
      "paper_year": "2015",
      "chunk_content": "Section 3 describes the proposed TSSB-HEDA in details. To validate the performance of TSSB-HEDA under stochastic processing times, simulations are conducted and the computation results are analysed in Section 4. Finally, in Section 5, we conclude the paper and discuss some topics for future research. ## 2. Problem description The PFSP is a well-known combinatorial optimisation problem. In the classical PFSP, a finite set $j=\\{1,2, \\ldots, n\\}$ of n jobs are firstly released simultaneously to the shop floor, and then are processed on a finite set $M=\\left\\{m_{1}, m_{2}, \\ldots, m_{m}\\right\\}$ of m machines with no preemption allowed. Each job $j, j \\in J$, consists of m operations that have to be processed on the machines in the order of $m_{1}, m_{2}, \\ldots$, $m_{m}$. All the jobs have deterministic processing times and follow the same processing order on each machine. In the real-world manufacturing environments, however, a variety of unexpected events, such as tool wear, equipment failure, operator unavailability, and quality issues, may lead to uncertain processing times (Lawrence \\& Sewell, 1997). This paper describes the processing time uncertainty using the level of processing time variation (LPTV), which is described as follows: $L P T V=\\sigma / E(P)$ where $E(P)$ and $\\sigma$ indicate the expected value and the standard deviation of processing time, respectively. According to formula (1), a larger LPTV may result in a large deviation between the expected and the actual processing times. For example, suppose $E[P]$ of a job equals 15 time units, LPTV values of 0.2 and 0.4 lead the standard deviation of actual processing time from $\\min \\left\\{E\\left[C\\left(\\pi_{n}, m\\right)\\right]\\right\\}$ to be 3 and 6 times units respectively.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:51:50",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of the Improved Estimation of Distribution Algorithm (IEDA) for solving HW/SW partitioning problems, how is the size of the clone elite population, denoted as s'(g), determined, and what two factors influence the clone size, denoted as nc_i, of the ith elite solution?",
      "topics": [
        "search"
      ],
      "answer": "The size of the clone elite population s'(g) is determined by selecting Ms best fitness individuals from pop(g), where Ms is calculated as [α*M], with M representing the population size and 0 < α < 1. The clone size nc_i of the ith elite solution X_i is influenced by two factors: the objective function value cost(X_i) and the similarity of X_i with the rest of the solutions in the population. Specifically, a smaller cost(X_i) and a smaller similarity indicate that X_i is more promising, thus requiring a strengthened search.",
      "chunk_id": 1721,
      "paper_id": "Application of estimation of distribution algorithm in HW SW partition",
      "paper_title": "Application of Estimation of Distribution Algorithm in HW/SW Partition",
      "paper_year": "2014",
      "chunk_content": "Based on this theory, the proposed method strengthens the local search ability of original algorithm by cloning and searching good solutions, and alleviates diversity loss by correcting probability model. The improved algorithm based on EDA is named as IEDA, which is applied to solving HW/SW partitioning problem. ## A. Representation of Solution The solution of HW/SW partitioning problem is represented as a binary string with length of n , i.e., $\\boldsymbol{X}=\\left(x_{l} \\ldots x_{l} \\ldots x_{n}\\right)$. B. Elite Clone Selection Operation The elite clone selection operation is applied before Step 2 of EDA algorithm. ## 1.) Size of Clone Elite population $\\mathrm{s}^{\\prime}(\\mathrm{g})$ which consists of Ms the best fitnesses individuals in $\\operatorname{pop}(\\mathrm{g})$ is selected and cloned, $\\mathrm{Ms}=\\left[\\alpha^{*} \\mathrm{M}\\right], \\mathrm{M}$ is population size, $0<\\alpha<1 . \\mathrm{nc}_{\\mathrm{i}}$ is the clone size of the ith elite solution $\\boldsymbol{X}_{i}$, when $\\mathrm{nc}_{\\mathrm{i}}$ is determined, two factors should be considered:objective function value $\\operatorname{cost}\\left(\\boldsymbol{X}_{i}\\right)$ and the similarity of $T$ with the rest solutions of the population. In the problem represented by Eq.(2), the smaller $X_{i}^{j}$ and the smaller the similarity of $\\boldsymbol{X}_{i}, \\boldsymbol{X}_{i}$ is more promising and should strengthen the search for it.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:51:52",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs using the linkage difference ($L_D$)?",
      "topics": [
        "algorithm"
      ],
      "answer": "The algorithmic approach aims to enhance the Replacement Tournament Replacement (RTR) method in Estimation of Distribution Algorithms (EDAs) by replacing the Hamming distance with a new method based on linkage similarity. The linkage difference, $L_D$, accumulates $1-I(i, j)$ to represent the distance between individuals by measuring their linkage similarity, where $I(i, j)$ represents the relation between two variables and Diff represents the difference of two individuals at loci.",
      "chunk_id": 10677,
      "paper_id": "A Linkage-Learning Niching in Estimation of Distribution Algorithm",
      "paper_title": "A Linkage-Learning Niching in Estimation of Distribution Algorithm",
      "paper_year": "2012",
      "chunk_content": "The value of $L_{D}$ is between 1 and 0 , and the higher value means the higher relation between two variables $1-I(i, j)$ and [FORMULA24]. [IMAGE_1] Figure 2: The number of function evaluation on SGA, hBOA with linkage-learning niching, and hBOA with RTR. The [FORMULA25] computation time increases the number of function evaluations. However, we only calculus the matrix once in each generation, so the computation time does not affect the total too much. In the next step, we utilize the evaluated matrix to calculate the similarity of two individuals. We define the linkage difference as follows: $$\nL_{D}=\\sum_{i=1}^{l}\\left(\\text { Diff }_{i} \\times \\sum_{j=1}^{l}(1-I(i, j))\\right)\n$$ where $i$ and $j$ are the loci of the problem and Diff is the difference of two individuals at loci [FORMULA29]. $$\n\\operatorname{Diff} f_{i}= \\begin{cases}1 & \\text { if the bits are different } \\\\ 0 & \\text { otherwise }\\end{cases}\n$$ $L_{D}$ accumulates the $1-I(i, j)$ and presents the distance of two individuals by measuring their linkage similarity. In the final step, we utilize the method to enhance RTR with replacing Hamming distance by our new method. The results show in Figure 2. ## 4.",
      "question_type": "synthesis summary",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:51:54",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of using a probability matrix, with elements denoted as $p_{ji}$, influence the generation of new individuals, and what specific steps are taken when a batch reaches its capacity?",
      "topics": [
        "representation"
      ],
      "answer": "Within the described EDA approach, a probability matrix is designed where the element $p_{ji}$ represents the probability that batch $N$ is used for customer order $N$. Sampling from this matrix generates new individuals. Specifically, for every position $S(g+1)$, batch $g+1$ is selected with probability $g < GN$. If a batch has been filled according to the device capacity, the entire column $p_{j1}, p_{j2}, ..., p_{ji}$ of the probability matrix is set to zero, effectively removing that batch from consideration for future customer orders.",
      "chunk_id": 9628,
      "paper_id": "A continuous estimation of distribution algorithm for the online order-batching problem",
      "paper_title": "A continuous estimation of distribution algorithm for the online order-batching problem",
      "paper_year": "2015",
      "chunk_content": "This representation is shown in Fig. 2 with 10 orders. This continuous vector does not have a real meaning on the solution that it represents. ### 4.2 Generation of the population Initial population members are generated randomly in order to enable a wide range of solutions [9]. ### 4.3 A weakness approach of the probability model A primary approach for the probability model is to design a probability matrix, i.e., a batch probability matrix. The element $p_{j i}$ of the probability matrix would represent the probability that batch $N$ were used for the $N \\leq$ customer order. For all $j(j=1, \\ldots, m)$ and for all $i(i=1, \\ldots, n)$. The value of $K \\cup S(g)$ would indicate the opportunity of a customer order on a certain batch. Via sampling according to the probability matrix, new individuals could be generated. For every position $S(g+1)$, batch $g=g+1$ would be selected with the probability $g<\\mathrm{GN}$. If batch ${ }^{\\circledR}$ has already been filled according to the device capacity, it would mean the assignment of batch $^{\\circledR} 3500$ has been finished. Then, the whole column $p_{j 1}, p_{j 2}, \\ldots, p_{j i}$ of the probability matrix would be set as zero.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:51:55",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners configure the Adjacent model (EDP model) to leverage the EPT, given its tree structure for parent and child relationships?",
      "topics": [
        "search",
        "Estimation of Distribution Algorithms"
      ],
      "answer": "When implementing the Adjacent model (EDP model), practitioners should use the EPT (Extended Path Tester) to construct parent and child relationships in the tree structure. This differs from traditional EDP implementations, as it specifically uses the EPT for relationship construction. The provided information does not specify exact parameter values or thresholds for the EPT, but highlights its crucial role in defining the tree structure within the EDP model.",
      "chunk_id": 12252,
      "paper_id": "A Bayesian Network Approach to Program Generation",
      "paper_title": "A Bayesian Network Approach to Program Generation",
      "paper_year": "2008",
      "chunk_content": "[IMAGE_3] Fig. 4. Network structures of (a) the PIPE model and (b) the EDP model. ## V. COMPARATIVE EXPERIMENTS In order to investigate the search performance of our approach, we applied POLE to three problems: the MAX problem, the deceptive MAX (DMAX) problem and the royal tree problem. The main parameters are listed in Table I. In the experiments, we compared the performance of the four models listed below to compare and contrast the performance of the present POLE method with the other three methods. ## - POLE. This is the proposed method of this study. To estimate interactions between nodes, a Bayesian network is constructed. The maximum number of incoming edges per node is unlimited. Grow is used to initialize individuals. - Univariate model (PIPE model). This algorithm is a univariate case of POLE. No networks are constructed in this algorithm, and no interactions between the nodes are estimated [Fig. 4(a)]. The search mechanism resembles that of PIPE. Grow is again used to initialize individuals. - Adjacent model (EDP model). This model constructs parent and child relationships in the tree structure according to EDP. However, in contrast to traditional EDP, this algorithm uses the EPT.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:51:57",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the UMDA (Univariate Marginal Distribution Algorithm) discussed, how does its performance compare to Lasso in terms of statistical significance on AUC (Area Under the Curve) and accuracy measures, based on the symbols used to indicate statistical superiority?",
      "topics": [
        "performance"
      ],
      "answer": "The UMDA exhibits statistically significant better behavior compared to Lasso on both AUC and accuracy measures for all data sets, as indicated by the symbol $\\odot$. However, in some specific instances, Lasso outperforms UMDA, as shown by the symbol [FORMULA224]. These instances include Breast data with 2 genes on accuracy, Colon data with 2, 8 and 9 genes on AUC, and Leukemia data with 13, 14 and 17 to 20 genes on AUC.",
      "chunk_id": 23636,
      "paper_id": "Regularized logistic regression without a penalty term- An application to cancer classification with microarray data",
      "paper_title": "Regularized logistic regression without a penalty term: An application to cancer classification with microarray data",
      "paper_year": "2011",
      "chunk_content": "For comparing UMDA [FORMULA217]with R_GE and Lasso, the symbols used are $\\odot$ and $\\bullet$, respectively. The symbols mean that UMDA [FORMULA220]reveals a statistically significant better behavior when [^0]compared to the other algorithm with respect to the performance measure, accuracy or AUC, depending on the column, with a [FORMULA221]-value [FORMULA222]. When R_GE or Lasso is statistically superior to UMDA [FORMULA223], the symbol used is [FORMULA224], meaning that UMDA [FORMULA225]is beaten. The conclusions are as follows. First, when compared to Lasso, UMDA [FORMULA226]is statistically superior both on AUC and accuracy measures for all data sets (see symbols in columns 2 and 3). For some isolated cases, Lasso is superior to UMDA [FORMULA227]; for Breast, with 2 genes on accuracy (see column 6 in Table 1); for Colon, with 2, 8 and 9 genes on AUC (see column 7 in Table 2); and for Leukemia, with 13, 14 and 17 to 20 genes on AUC (see column 7 in Table 3).",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:51:59",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "According to the details in the chunk, what advantages does incorporating the differential evolution (DE) algorithm and the estimation of distribution algorithm (EDA) into the clonal selection algorithm (CSA) provide?",
      "topics": [
        "optimization"
      ],
      "answer": "Incorporating the differential evolution (DE) algorithm and the estimation of distribution algorithm (EDA) into the clonal selection algorithm (CSA) improves both local and global search ability. Specifically, DE is used to implement hypermutation and EDA is used to implement receptor editing, providing improved local and global search, respectively.",
      "chunk_id": 17556,
      "paper_id": "CSA-DE:EDA- a Novel Bio-inspired Algorithm for Function Optimization and Segmentation of Brain MR Images",
      "paper_title": "CSA-DE/EDA: a Novel Bio-inspired Algorithm for Function Optimization and Segmentation of Brain MR Images",
      "paper_year": "2019",
      "chunk_content": "CSA-DE/EDA: a Novel Bio-inspired Algorithm for Function Optimization and Segmentation of Brain MR Images Zhe $\\mathbf{L i}^{1} \\cdot$ Yong Xia ${ }^{1,2}$ _ Hichem Sahli ${ }^{2,3,4}$Received: 2 October 2018 / Accepted: 3 July 2019 / Published online: 7 August 2019(C) Springer Science+Business Media, LLC, part of Springer Nature 2019 #### Abstract The clonal selection algorithm (CSA), which describes the basic features of an immune response to an antigenic stimulus, has drawn a lot of attention in the biologically inspired computing community, due to its highly adaptive and easy-to-implement nature. Despite many successful applications, CSA still suffers from limited ability to explore the solution space. In this paper, we incorporate the differential evolution (DE) algorithm and the estimation of distribution algorithm (EDA) into CSA, and thus propose a novel bio-inspired algorithm referred to as CSA-DE/EDA. In the proposed algorithm, the hypermutation and receptor editing processes are implemented based on DE and EDA, which provide improved local and global search ability, respectively. We have applied the proposed algorithm to five commonly used benchmark functions for optimization and brain magnetic resonance (MR) image segmentation.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:00",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model described, including the roles of the parameters S, L, and NP in population updating, and how does the algorithm address premature convergence in UMDA?",
      "topics": [
        "selection"
      ],
      "answer": "The EDA model execution involves these steps: Selection of 'S' advantage individuals using truncation selection based on fitness values; Modeling a probability distribution among these 'S' individuals; Sampling 'L' new individuals based on this probability distribution; Updating the population by combining 'S' selected individuals, 'L' sampled individuals, and 'NP-S-L' randomly generated individuals at each iteration; and finally, checking for a terminal condition (maximum iterations). To mitigate premature convergence in UMDA, which stems from the lack of variable relationships and marginal probability distributions being treated as joint probabilities, a variable scale variance strategy should be considered, though the chunk does not provide specific details on how to implement this strategy.",
      "chunk_id": 25927,
      "paper_id": "An Operation Optimization Method Based on Improved EDA for BOF End-point Control",
      "paper_title": "An Operation Optimization Method Based on Improved EDA for BOF End-point Control",
      "paper_year": "2016",
      "chunk_content": "Step3: Selection. According to the ordering of fitness values, $S$ advantage individuals are selected by truncation selection strategy. Step4: Modeling. For the formula (7), the probability distribution model is established among advantage individuals. Step5: Sampling. According to the probability distribution, the $L$ individuals are sampled as a part of new individuals. Step6: Updating. The new individuals are composed of selected advantage individuals $S, L$ sampling individuals and $N P-S-L$ random individuals at the $x_{i, j}^{g}$ th iteration. Simultaneously, $j$ iteration is regarded as $g+1$ iteration. Step7: Stopping. If the algorithm satisfies the terminal condition (Maximum iteration), the running is end. Otherwise, go to Step 2. ## C. Variable Scale Variance Strategy Due to the lack relationship of UMDA $F$ between variables, and the marginal probability distributions of sampling individuals are considered as the joint probability distribution of the new population, thus the premature convergence of UMDA $\\zeta_{i, j}^{g}$ often appears in the search process, and leads to the poor search ability of global optimal solution, the population often obtains the local optimal solution.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:02",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What constitutes a better individual and corresponding portfolio in the context of the PBIL-CCPS algorithm, according to the fitness criteria?",
      "topics": [
        "fitness",
        "PBIL-CCPS algorithm"
      ],
      "answer": "Within the PBIL-CCPS algorithm, a smaller fitness value indicates a better individual and a more desirable portfolio.",
      "chunk_id": 15684,
      "paper_id": "An Estimation of Distribution Algorithm Based Portfolio Selection Approach",
      "paper_title": "An Estimation of Distribution Algorithm Based Portfolio Selection Approach",
      "paper_year": "2010",
      "chunk_content": "Obviously, the smaller the fitness is, the better the individual and the corresponding portfolio are. The stepwise procedures of PBIL-CCPS are organized as follows. ## Step 1) - Initialization In the step, the population, probability vector, mean vector, and standard deviation vector are initialized. Firstly, for each individual, randomly choose $_{i}^{(1)}, \\ldots$ assets to be invested. The proportion of capital invested in each chosen asset is initialized to be a random real number between 0 and 1 . For the unselected assets, the proportions are initialized to be 0 . The portfolio that each constructed individual presents may not satisfy the constraints of the problem. Hence, we need to amend the individual by adjusting the values of the selection vector and the proportion vector. The detail about how to amend an individual will be given in Part B. Secondly, initialize each dimension of the probability vector to be 0.5 . Finally, initialize the mean vector and the standard deviation vector. Each dimension of the mean vector $X_{i}$ is initialized as $X_{i j}=\\left(\\delta_{i}+\\varepsilon_{i}\\right) / 2$. For the standard deviation vector, each dimension $\\sigma_{i}$ is initialized as (10). $$\n\\sigma_{i}:=\\sqrt{\\sum_{j=1}^{N c P}\\left(x_{j i}-X_{i}\\right)^{2} / P O P}\n$$ where $P O P$ is the size of the population, $x_{j i}$ denotes the $N\\left(X_{i}, \\sigma_{i}^{2}\\right)$-th dimension of the proportion vector in individual $0 \\leq x_{i} \\leq 1$.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:03",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs, specifically regarding the Univariate Marginal Distribution Algorithm (UMDA) on the LeadingOnes problem?",
      "topics": [
        "proof"
      ],
      "answer": "The primary objective is to provide a runtime estimate for the UMDA on the LeadingOnes problem that is tight up to constant factors and holds with high probability, achieved through a proof method that avoids level-based analysis. The analysis aims to quantify the iterations required for the UMDA to optimize LeadingOnes, considering frequencies that remain above constant values and those that reach a lower border, expressed in terms of parameters like \\$\\lambda\\$ (population size) and \\$\\mu\\$ (selection size).",
      "chunk_id": 17872,
      "paper_id": "A simplified run time analysis of the univariate marginal distribution algorithm on LeadingOnes",
      "paper_title": "A simplified run time analysis of the univariate marginal distribution algorithm on LeadingOnes",
      "paper_year": "2021",
      "chunk_content": "We obtained these results via a different proof method that avoids the technically deeper level-based method. Our arguments can also be employed for lower bounds. Overall, we provide a run time estimate for the UMDA on LeAdingONes that is tight up to constant factors and that holds with high probability. We note that the general proof idea can be extended also to the parameter regime of $\\mu \\in o(n \\log n)$ for the UMDA. We conjecture that a more general upper bound of the UMDA (with $\\lambda \\in \\Omega(\\log n)$ ) on LeAdingONes is $$\nO\\left(\\lambda\\left(n+\\frac{n}{e^{\\mu / n}}\\left(\\frac{n}{\\lambda}+\\log \\min \\{\\mu, n\\})\\right)\\right)\n$$ Speaking in terms of iterations and thus ignoring the factor of [FORMULA402], this expression can be explained as follows: the first term of [FORMULA403] considers $O(n)$ frequencies that do not drop below constant values. Each of these frequencies is set to [FORMULA405] within a constant number of iterations with high probability. Since [FORMULA406], frequencies at [FORMULA407] do not drop until the optimum is sampled with high probability. The second, more complicated term deals with frequencies that, pessimistically, reached the lower border [FORMULA408]. There are $O\\left(n / e^{\\mu / n}\\right)$ of these frequencies, by the same argument as used in the proof of Lemma 1.",
      "question_type": "advantage",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:05",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of solving the 0-1 Quadratic Knapsack Problem, what is the significance of considering the profit achieved when both items *k* and [9,10] are selected as 99% instead of calculating it as  *p<sub>ij</sub>* + *p<sub>ji</sub>*, and how does this simplification affect the algorithmic search for maximizing the objective function: Maximize: ∑<sub>j=1</sub><sup>n</sup> ∑<sub>j=1</sub><sup>n</sup> *p<sub>ij</sub>* *x<sub>i</sub>* *x<sub>j</sub>*?",
      "topics": [
        "algorithm"
      ],
      "answer": "The simplification of considering the profit as 99% instead of *p<sub>ij</sub>* + *p<sub>ji</sub>* when both items *k* and [9,10] are selected, where *j* > *i*, arises from the assumption that the matrix *P* is symmetric, meaning *p<sub>ij</sub>* = *p<sub>ji</sub>* for all *i* ≠ *j*. This assumption is made without loss of generality. By using 99%, the formulation simplifies the calculation of the overall profit, especially in scenarios where both items *i* and *j* are selected (i.e., *x<sub>i</sub>* = 1 and *x<sub>j</sub>* = 1). This simplification impacts the algorithmic search by potentially altering the landscape of the objective function, making it smoother or introducing different local optima compared to the original formulation using *p<sub>ij</sub>* + *p<sub>ji</sub>*. The goal remains to maximize ∑<sub>j=1</sub><sup>n</sup> ∑<sub>j=1</sub><sup>n</sup> *p<sub>ij</sub>* *x<sub>i</sub>* *x<sub>j</sub>* subject to the constraint ∑<sub>j=1</sub><sup>n</sup> *w<sub>j</sub>* *x<sub>j</sub>* ≤ *c*, where *x<sub>j</sub>* is a binary variable indicating whether the *j*-th item is selected, and *c* is the knapsack capacity.",
      "chunk_id": 9231,
      "paper_id": "Solving the 0–1 Quadratic Knapsack Problem with a competitive Quantum Inspired Evolutionary Algorithm",
      "paper_title": "Solving the 0-1 Quadratic Knapsack Problem with a competitive Quantum Inspired Evolutionary Algorithm",
      "paper_year": "2015",
      "chunk_content": "QKP is to find a subset of items whose total weight is not more than Knapsack capacity c such that the overall profit is maximized. $x_{j}$ is a binary variable which is equal to 1 if $p_{i j}+p_{j i}$ th item is selected and 0 otherwise, the problem is formulated as follows. $$\n\\begin{array}{ll}\n\\text { Maximize: } & \\sum_{j=1}^{n} \\sum_{j=1}^{n} p_{i j} x_{i} x_{j} \\\\\n\\text { Subject to: } & \\sum_{j=1}^{n} w_{j} x_{j} \\leq c \\\\\nx_{j} \\in\\{0,1\\} & j \\in\\{1, \\ldots, n\\}\n\\end{array}\n$$ [^0] [^0]: * Corresponding author. E-mail addresses: cpatvardhan@gmail.com (C. Patvardhan), sulabh.bansal.78@gmail.com (S. Bansal), asr@informatik.uni-kiel.de (A. Srivastav). 1 Permanent Residential Address: 1/25, Hazuri Bhawan, Peepal Mandi, Agra 282003, India. Without loss of generality matrix $P$ is considered symmetric such that $p_{i j}=p_{j i}$ for all $i \\neq j$ and $G=(V, E)$. To simplify, another formulation of problem due to the above assumption is popular where additional profit achieved if both items $k$ and $[9,10]$ are selected is considered as $99 \\%$ rather than $p_{i j}+p_{j i}$, for $j>i$.",
      "question_type": "conceptual deep",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:08",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm that utilizes clustering, how should practitioners calculate the probability, denoted as alpha ($\\alpha_{i}$), for selecting elements from each cluster?",
      "topics": [
        "performance"
      ],
      "answer": "Practitioners should calculate alpha for each cluster ($i$) by dividing the number of elements in that cluster ($c_{i}$) by the total number of elements across all clusters (${\\sum_{j=1}^{k} c_{j}}$), where $k$ is the total number of clusters. The formula is: $\\alpha_{i}=\\frac{c_{i}}{\\sum_{j=1}^{k} c_{j}}$.",
      "chunk_id": 14707,
      "paper_id": "GAIS- A Gaussian Artificial Immune System for Continuous Optimization",
      "paper_title": "GAIS: A Gaussian Artificial Immune System for Continuous Optimization",
      "paper_year": "2010",
      "chunk_content": "The value of each $\\dagger$ is proportional to the number of elements in the ${ }_{M}$-th cluster: $$\n\\alpha_{i}=\\frac{c_{i}}{\\sum_{j=1}^{k} c_{j}}, \\quad i=1, \\ldots, k\n$$ where $c_{i}$ is the number of elements in cluster [FORMULA70]. # 4 Experiments This section describes the experiments carried out to evaluate the proposed algorithm. We have applied GAIS to eight well-known problems and compared the performance with other optimization tools reported in the literature. ### 4.1 Test Functions Eight functions often utilized in the literature were tested during the experiments. In what follows, we provide a description of each function for a generalized dimensionality $d$. - Sphere: This function is probably the most standard unimodal benchmark problem for optimization. It involves the minimization of a single hyperparabola and the minimum value for any dimensionality is 0 which is obtained if all $x_{i}$ take the value of 0 . $$\nF_{1}(x)=\\sum_{i=1}^{d} x_{i}^{2}, \\quad x_{i} \\in[-100 ; 100]^{d}\n$$ - Summation Cancellation (SumCan): This problem has multivariate linear interactions between the variables. So, algorithms that are capable of modelling these dependencies are supposed to outperform algorithms that are not capable of doing so. The optimum is located at a very sharp peak, which implies that the optimization algorithm needs to be able to prevent premature convergence in order to reach the global optimum.",
      "question_type": "application",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:09",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the BUMDA algorithm discussed, how does the variance influence the search process, and what mechanism is triggered when the variance reaches a certain threshold?",
      "topics": [
        "optimization"
      ],
      "answer": "The variance in the BUMDA algorithm is used as an activator for a reset mechanism. Specifically, the reset mechanism is activated when the variance becomes lower than a pre-established value. This activation leads to an intensive exploration of the search space, using a defined number of function evaluations as a reference point for exploration intensity.",
      "chunk_id": 1015,
      "paper_id": "Design and Optimization of Heat-Integrated Distillation Column Schemes through a New Robust Methodology Coupled with a Boltzmann-Based Estimation of Distribution Algorithm",
      "paper_title": "Design and Optimization of Heat-Integrated Distillation Column Schemes through a New Robust Methodology Coupled with a Boltzmann-Based Estimation of Distribution Algorithm",
      "paper_year": "2014",
      "chunk_content": "In this work, 60 individuals and 50 generations were used, resulting in 3000 function evaluations for each case study. The number of function evaluations used in this study represents the number in which a suitable value of the fitness is obtained using a reasonable computing time. This election took place after doing some evaluations considering several values (1800, 3000 , and 3600 ). On the other hand, this number of evaluations was considered to be the value of reference to intensively explore the search space. This intensive exploration is given by the reset mechanism, which is activated when the variance becomes lower than the value established. In general terms, this fact means that the variance was used as an activator of the reset mechanism instead of being used as the stop criterion. This election leads to an intensive exploration of the search space, taking as a reference an appropriate number of function evaluations. The simulations were made with a PC with an i5 processor core, clock frequency at 2.8 GHz , and 16 GB of RAM. On average, for each case study, 145 s were used for evaluation of the function. ## 4. RESULTS First, in agreement with the design and optimization goals established, it is important to highlight that the BUMDA algorithm has been able to successfully deal with complex problems (nonlinear and multivariable) constrained with efficacy and robustness through an intensified search using a reset mechanism.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:10",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of different meta classifiers in the stacked generalization method compare, based on the mean accuracy percentages when applied to emotion recognition?",
      "topics": [
        "performance"
      ],
      "answer": "The stacked generalization method, when using single classifiers as meta classifiers, shows relatively small differences in mean accuracy. Four classifiers range from 50.16% to 51.43%, indicating no single meta classifier significantly outperforms the others in this configuration.",
      "chunk_id": 25796,
      "paper_id": "Classifier Subset Selection for the Stacked Generalization Method Applied to Emotion Recognition in Speech",
      "paper_title": "Classifier Subset Selection for the Stacked Generalization Method Applied to Emotion Recognition in Speech",
      "paper_year": "2016",
      "chunk_content": "Table 2. First phase. Accuracy percentages for each person using single classifiers. Mean and SD rows denote the average and standard deviation for each classifier considering all of the actors. BN, Bayesian network; NBT, naive Bayes tree; OneR, one rule; RIPPER, repeated incremental pruning to produce error reduction; RandomF, random forest. Table 3. First phase. Accuracy percentages for each person using stacking and bagging and boosting multi-classifiers. Mean and SD rows denote the average and standard deviation for each standard multi-classifier considering all of the actors. The performance of the standard multi-classifiers systems for all of the actors in the first phase is presented in Table 3, with mean and SD values in the last two rows. In the first 10 columns, the results obtained by the stacked generalization method with the single classifiers as meta classifiers are presented. In addition, the accuracy achieved by the bagging and boosting multi-classifiers are shown in the last two columns. The best results per actor are marked in bold. In contrast to single classifiers, there is no meta classifier that performs much better than the others. This is evident looking at their mean values, with four classifiers in the range from $50.16 \\%$ to $51.43 \\%$, showing low differences between them.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:12",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model, specifically the univariate marginal distribution algorithm (UMDA), as it relates to runtime analysis?",
      "topics": [
        "search"
      ],
      "answer": "The provided text mentions analyzing the expected runtime of the univariate marginal distribution algorithm (UMDA) on benchmark functions like LEADINGONES and OneMax. While it doesn't detail the precise algorithmic steps, it focuses on the analysis of the runtime performance of UMDA on these problems. The core idea is to evaluate how the UMDA performs in terms of runtime when solving these optimization problems.",
      "chunk_id": 9211,
      "paper_id": "Novel Absorber Based on Pixelated Frequency Selective Surface Using Estimation of Distribution Algor",
      "paper_title": "Novel Absorber Based on Pixelated Frequency Selective Surface Using Estimation of Distribution Algorithm",
      "paper_year": "2015",
      "chunk_content": "We demonstrate this new approach by analysing the expected runtime of the univariate marginal distribution algorithm (UMDA) on standard benchmark functions. In particular, for the LEADINGONES problem, we show a significantly improved upper bound on the runtime. Furthermore, we analyse the expected runtime on OneMax, a problem that has been remained open for a very long time. The significance of this work lies in the relatively ease with which these results were obtained. Previous runtime analyses of EDAs have been highly involved, even in simple settings, preventing studies of more interesting scenarios. We believe that the new technique could make it possible to analyse the runtime of more complex EDAs and more challenging fitness landscapes. ## Acknowledgements The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no 618091 (SAGE).",
      "question_type": "implementation focused",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:13",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of power service be measured using PMU data, according to the criteria of analysis, automated correction, and disturbance recording?",
      "topics": [
        "analysis"
      ],
      "answer": "PMUs increase power service quality by providing precise analysis, automated correction, and disturbance recording of sources of system degradation.",
      "chunk_id": 22878,
      "paper_id": "Multiple Power Line Outage Detection in Smart Grids Probabilistic Bayesian Approach",
      "paper_title": "Multiple Power Line Outage Detection in Smart Grids: Probabilistic Bayesian Approach",
      "paper_year": "2018",
      "chunk_content": "However, PMUs have demonstrated a significant advantage over SCADA in a number of ways including measurement resolution, state observation capability and coverage area [8]. Some key differences between SCADA and PMU technologies are highlighted in Table 1. In addition to power system protection, PMUs are also used for implementing a load shedding method that consists of simultaneous reactive and active power to address the frequency and voltage stability issues respectively [9]. Moreover, PMUs increase the power service TABLE 1. SCADA vs. PMU [8]. quality by precise analysis, automated correction and disturbance recording of sources of system degradation [10]. ## B. RELATED WORK The state-of-the-art protection techniques for power grids can be classified into two broad categories. Firstly, it includes strategies which determine the optimal location and number of PMUs to achieve a grid-wide observability. Secondly, the techniques which exploit rich PMU data for efficient identification of power lines in outage. Some important works are discussed as follows. The work in [11] proposes a two step PMU placement technique. At first, linear programming is used to obtain an optimum upper bound on the number of PMUs. Then, a branch and bound algorithm is used to obtain a near optimal placement solution.",
      "question_type": "relationship",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:14",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs when hybridizing Differential Evolution (DE) with other Evolutionary Algorithms (EAs), as illustrated by LSHADE-SPACMA and its variants?",
      "topics": [
        "control"
      ],
      "answer": "The primary purpose of hybridizing Differential Evolution with other Evolutionary Algorithms, such as in LSHADE-SPACMA and ELSHADE-SPACMA, is to leverage the complementary strengths of different algorithms. This involves combining DE variants like LSHADE with techniques such as covariance matrix adaptation (CMA-ES) to enhance exploration and exploitation. For example, ELSHADE-SPACMA dynamically adjusts the $NP$ value to control the greediness of the mutation strategy, aiming to balance exploration and exploitation throughout the evolutionary process, thereby improving overall performance and achieving state-of-the-art results in competitions like IEEE CEC.",
      "chunk_id": 3218,
      "paper_id": "An improved differential evolution by hybridizing with estimation-of-distribution algorithm",
      "paper_title": "An improved differential evolution by hybridizing with estimation-of-distribution algorithm",
      "paper_year": "2022",
      "chunk_content": "In 2022, Cao et al. [27] proposed an adaptive differential evolution algorithm by using the standard deviation of fitness value and the sum of standard deviation of each dimension of population to fully extract and efficiently utilize the population feature information. (c) Hybrid DE with other EAs. In 2017, Awad et al. [28] proposed LSHADE-cnEpSin based on LSHADE-EpSin [15] by adopting an adaptation scheme based on earlier success for the ensemble of sinusoidal approaches, and a new crossover operator based on covariance matrix adaptation with Euclidean neighborhood for non-variance dealing capability. Besides, Awad et al.[29] also proposed a hybrid algorithm named LSHADE-SPACMA, ranked fourth place in the IEEE CEC 2017 competition, by combining LSHADE-SPA [29] and the modified CMA-ES to make full use of each advantage and character of the two algorithms. In 2018, Hadi et al. [30] proposed an improved version of LSHADE-SPACMA named ELSHADE-SPACMA, ranked third in the IEEE CEC 2018 competition, in which $N P$ value that controls the greediness of the mutation strategy is dynamic to balance exploration and exploitation for different stages of the evolutionary process. Additionally, the ELSHADE-SPACMA was further enhanced by integrating another directed mutation strategy within the hybridization framework [31] in 2019.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:16",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the mutation operator affect the time complexity, given that the average community scale of the variant chromosome is represented by [FORMULA202]?",
      "topics": [
        "mutation",
        "time complexity",
        "algorithmic analysis"
      ],
      "answer": "The mutation operator in the algorithm has a direct impact on the time complexity. Specifically, line 5 of the algorithm has at most $\\lambda L M$ variations, where $\\lambda$ represents a constant ([FORMULA209]), $L$ is related to the average community scale of the variant chromosome ([FORMULA202]), and $M$ is the executive algebra ([FORMULA211]). The execution time of the mutation operator is $O(c n)$, where $n$ is some measure and $c$ is a constant. Consequently, the time required for line 5 does not exceed $O(\\lambda L M c n)$, and the algorithm's overall time complexity is [FORMULA212].",
      "chunk_id": 22073,
      "paper_id": "The Improved Estimation of Distribution Algorithms for Community Detection",
      "paper_title": "The Improved Estimation of Distribution Algorithms for Community Detection",
      "paper_year": "2018",
      "chunk_content": "Let [FORMULA202] be the average community scale of the variant chromosome [FORMULA203], and it performs algebra with $L$. The highest time complexity in this algorithm is line 5, while the runtime of the other steps is less than or equal to $O(n)$. Line 5 in the algorithm has $\\lambda L M$ times variation at most; thus, the execution time of the mutation operator is $O(c n)$. This means that the time required for line 5 does not exceed $O(\\lambda L M c n)$, when [FORMULA209] is a constant, the population scale is [FORMULA210], the executive algebra is [FORMULA211], and the parameter g is also regarded as a constant. Therefore, the algorithm's time complexity is [FORMULA212]. ## V. EXPERIMENTAL CONCLUSION AND ANALYSIS In our quantitative analysis of the LSMEDA algorithm, we used a benchmark network and a real large-scale network for testing and verification. The experimental environment used to implement the algorithm was as follows: an Intel(R) Core(TM) M450 processor running at $2.40 \\mathrm{GHz}, 6.00 \\mathrm{~GB}$ of memory, a 320-GB hard disk, Microsoft Windows 764 -bit operating system, and the Matlab 7.10 programming environment.",
      "question_type": "algorithmic mechanism",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:18",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "When configuring an EDA such as EMNA global, EEDA, or EDA-MCC, for multimodal landscapes, what challenges arise in parameter estimation, and how might these challenges impact the algorithm's performance compared to algorithms like CMA-ES?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "Parameter Estimation",
        "Multimodal Landscapes",
        "Algorithm Comparison"
      ],
      "answer": "When using EDAs like EMNA global, EEDA, and EDA-MCC, which employ a single multivariate Gaussian model, estimating parameters for multimodal landscapes becomes difficult. The single Gaussian model struggles to capture the shape of such landscapes. Consequently, these algorithms may underperform compared to algorithms like CMA-ES, which can better handle multimodal landscapes. This is because EMNA global, EEDA, and EDA-MCC have trouble estimating their parameters for this type of function.",
      "chunk_id": 17269,
      "paper_id": "A latent space-based estimation of distribution algorithm for large-scale global optimization",
      "paper_title": "A latent space-based estimation of distribution algorithm for large-scale global optimization",
      "paper_year": "2019",
      "chunk_content": "The performance of EEDA, EDA-MCC, CMA-ES and LS-EDA is similar to each other. CMA-ES performs better than EMNA global, EEDA, EDA-MCC and LS-EDA on F13. Intuitively, it is hard to capture the shape by one Gaussian model due to multimodal landscape, and the experimental results coincide with this. EMNA global, EEDA, EDA-MCC and LS-EDA belong to a single multivariate model. They have trouble to estimate their parameters for this group of functions. Since UMDA [FORMULA222] employs multiple univariate Gaus- Table 4 Comparisons of 50-D functions For each problem, the best result is bolded. The result of EMNA $_{\\text {global }}$, EEDA and EDA-MCC was reported in Dong et al. (2013). Results of LS-EDA are compared with those of UMDA ${ }_{a}^{G}$. CMA-ES and RP-EDA, respectively, by Wilcoxon rank-sum test at the significance level of 0.05 . The marker \"-\" is worse than the results of LS-EDA, \" + \" is better than the results of LS-EDA, and \" $\\approx$ \" is equivalent to the results of LS-EDA Table 5 Comparisons of 100-D functions For each problem, the best result is bolded.",
      "question_type": "parameter selection and configuration guidance",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:20",
      "generation_style": "practical_application"
    },
    {
      "question": "Considering the HMRF-CSA/DE/EDA MR segmentation algorithm described, what specific step incorporates an Estimation of Distribution Algorithm (EDA) to refine the model parameters, and what role does this EDA play in updating the affinity values of the parameter set \\(\\boldsymbol{A} \\boldsymbol{b}^{\\boldsymbol{k}}\\)?",
      "topics": [
        "initialization"
      ],
      "answer": "Within the HMRF-CSA/DE/EDA MR segmentation algorithm, the EDA is used to replace the 'd' lowest affinity values from the parameter set \\(\\boldsymbol{A} \\boldsymbol{b}^{\\boldsymbol{k}}\\. Specifically, the step denoted as '\\(\\boldsymbol{A} \\boldsymbol{b}{(\\boldsymbol{d})}^{\\boldsymbol{k}}\\) replace the d lowest affinity from \\(\\boldsymbol{A} \\boldsymbol{b}^{\\boldsymbol{k}}\\) using EDA' indicates that the EDA is employed to identify and substitute the 'd' parameters in \\(\\boldsymbol{A} \\boldsymbol{b}^{\\boldsymbol{k}}\\) that exhibit the weakest affinity, thus guiding the search towards a more optimal parameter configuration for the Hidden Markov Random Field (HMRF) model.",
      "chunk_id": 17569,
      "paper_id": "CSA-DE:EDA- a Novel Bio-inspired Algorithm for Function Optimization and Segmentation of Brain MR Images",
      "paper_title": "CSA-DE/EDA: a Novel Bio-inspired Algorithm for Function Optimization and Segmentation of Brain MR Images",
      "paper_year": "2019",
      "chunk_content": "Considering an input brain MR image, we first apply the super pixel algorithm-TurboPixel [42] to generate an image segmentation into $m$ regions $\\boldsymbol{R}=\\left\\{R_{i} \\mid i=1,2, \\ldots, m\\right\\}$. Then we define a region adjacency graph (RAG) [43] composed of nodes, representing the regions $R_{i}$ of $\\boldsymbol{R}$, and edges representing their adjacency. To each node we associate the mean gray value of the pixels within the region. This coarse segmentation allows estimating the model parameters, which will be then used for a final pixel-based segmentation. The overall process is summarized in Algorithm 1. ``` Algorithm 1: HMRF-CSA/DE/EDA MR segmentation algorithm Input: observed brain MR image \\(\\boldsymbol{Y}\\) Output: optimal voxel class labels \\(\\boldsymbol{X}\\) and model parameters \\(\\boldsymbol{\\theta}\\). Initialization: segmentation by k-means, random model parameters and initial bias field \\(\\boldsymbol{b}\\) while the stopping criterion is not met (region-based) Evaluate the vector \\(\\boldsymbol{f}^{\\boldsymbol{k}}\\) through Eq. (11) Select \\(\\boldsymbol{A} \\boldsymbol{b}{(\\boldsymbol{x})}^{\\boldsymbol{k}}\\) Select \\(\\boldsymbol{A} \\boldsymbol{b}{(\\boldsymbol{\\theta})}^{\\boldsymbol{k}}\\) Clone \\(\\boldsymbol{A} \\boldsymbol{b}{(\\boldsymbol{x})}^{\\boldsymbol{k}} \\rightarrow \\boldsymbol{C}^{\\boldsymbol{k}}\\) Hypermutation using DE \\(\\boldsymbol{C}^{\\boldsymbol{k}} \\rightarrow \\boldsymbol{C}^{-\\boldsymbol{k}}\\) Reselect from \\(\\boldsymbol{C}^{-\\boldsymbol{k}}\\) to partly replace \\(\\boldsymbol{A} \\boldsymbol{b}^{\\boldsymbol{k}}\\) \\(\\boldsymbol{A} \\boldsymbol{b}{(\\boldsymbol{d})}^{\\boldsymbol{k}}\\) replace the d lowest affinity from \\(\\boldsymbol{A} \\boldsymbol{b}^{\\boldsymbol{k}}\\) using EDA Update class labels in Eq.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:22",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does incorporating a local entropy measure affect the fitness function in the described EDA approach, compared to using a random strategy?",
      "topics": [
        "fitness",
        "Estimation of Distribution Algorithms"
      ],
      "answer": "The EDA approach introduces a local entropy measure, denoted as s(c_guess), which is calculated based on the Shannon entropy of a combination concatenated with previously played combinations. This measure is then used to bias the fitness function, f_l(c_guess), defined as s(c_guess) / (1 + f(c_guess)). This biases the search towards guesses with higher local entropy, unlike the random strategy which does not incorporate such a bias. The local entropy calculation involves counting the occurrences of symbols 'g' from an alphabet within the combination, as reflected in the formula s(c_guess) = ∑_{g ∈{A, …, F}} (#g / ((n+1)ℓ)) log(((n+1)ℓ) / #g)), where '#g' represents the number of times the symbol 'g' appears.",
      "chunk_id": 15590,
      "paper_id": "Adapting Heuristic Mastermind Strategies to Evolutionary Algorithms",
      "paper_title": "Adapting Heuristic Mastermind Strategies to Evolutionary Algorithms",
      "paper_year": "2010",
      "chunk_content": "However, this approach is likely to perform as well as the random strategy discussed in the previous section. When finding a single consistent guess we cannot apply the heuristic strategies from the previous section. For this reason we introduce now a local entropy measure, which can be applied to non-consistent guesses and so bias our search. The local entropy assumes that the fact that some combinations are better than others depends on its informational content, and that in turn depends on the entropy of the combination along with the rest of the combinations played so far. To compute local entropy, the combination is concatenated with $1 / 10$ combinations played so far and its Shannon entropy computed: $$\ns\\left(c_{\\text {guess }}\\right)=\\sum_{g \\in\\{A, \\ldots, F\\}} \\frac{\\# g}{(n+1) \\ell} \\log \\left(\\frac{(n+1) \\ell}{\\# g}\\right)\n$$ with $g$ being a symbol in the alphabet and \\# denotes the number of them. Thus, the fitness function which includes the local entropy is defined as, $$\nf_{\\ell}\\left(c_{\\text {guess }}\\right)=\\frac{s\\left(c_{\\text {guess }}\\right)}{1+f\\left(c_{\\text {guess }}\\right)}\n$$ In this way a bias is introduced to the fitness to as to select the guess with the highest local entropy. When a consistent combination is found, the combination with the highest entropy found in the generation is played (which might be the only one or one among several; however, no special provision is done to generate several). The result of ten independent runs of the EDA over the whole search space are now compared with the results of the previous section.",
      "question_type": "COMPARATIVE EVALUATION",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:24",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model, specifically the Dynamic Wrapper Evolutionary (DWE) algorithm using a Gaussian EDA, as described for dynamic weight computation?",
      "topics": [
        "search"
      ],
      "answer": "The DWE algorithm, utilizing a Gaussian EDA, involves the following steps: First, divide both genetic ($Y_{R}$) and clinical ($N$) data into $k$ folds. Then, iterate through the data for $L$ iterations (STEP 1). In each iteration, select a $k^{th}$ test fold, and use the remaining $k-1$ folds to train both SVM Genetic and SVM Clinical models (STEP 2). The probability $P_{r}$ of a training subset being selected is calculated as $P_{r}=\\frac{(k-1) \\times \\eta}{N \\times L}$. The selected test fold is then classified using equation (1), where weights are computed using the algorithm detailed in Figure (4). The selection probability $P_{p}$ for each fold to be part of the validation data is $P_{r}=\\frac{\\eta}{N \\times L}$.",
      "chunk_id": 13158,
      "paper_id": "Ambient Cardiac Expert A Cardiac Patient Monitoring System using Genetic and Clinical Knowledge Fusion",
      "paper_title": "Ambient cardiac expert: A cardiac patient monitoring system using genetic and clinical knowledge fusion",
      "paper_year": "2007",
      "chunk_content": "In (6) $K\\left(Y_{R i}, Y_{R i}\\right)$ is the kernel function and in our experiments, a linear, polynomial and RBF functions are used. $C$ is the only adjustable parameter in the above calculations and can be computed using simple grid search method. Following Sub-Section will present algorithm to compute weights and used in (1) to predict the odds of heart failure. [IMAGE1] Figure 2: The ACE Framework ### 2.1 Dynamic Wrapper Evolutionary Algorithm for Adaptive Weight Calculation To compute weights and in (1) both genetic $Y_{R}$ and clinical $N=$ data are first divided into two sets of $k$ folds and then DWE processes the data for $L$-iterations (STEP 1, Figure 3). [IMAGE2] Figure 3: The DWE algorithm for dynamic computation of weights using Gaussian EDA. For every iteration a $k^{\\text {th }}$ test fold is selected, rest of the $\\eta=$ 1 folds in $P_{r}=\\frac{\\eta}{N \\times L}$ and $k-1$ are used for the training of SVMs (SVM Genetic and SVM Clinical, Figure 2) respectively (STEP 2) such that, the probability $P_{r}$ of the training subset being selected as a training data for a particular iteration is: $P_{r}=\\frac{(k-1) \\times \\eta}{N \\times L}$ where $N=$ total data items per class, $k=$ number of folds, $L$ $=$ number of classes and $\\eta=$ number of samples in each subset. The remaining one fold is used for testing (leave one out) such that the selection probability $P_{p}$ of each fold to become a part of validation data for a particular iteration is: $P_{r}=\\frac{\\eta}{N \\times L}$ After both Genetic SVM and Clinical SVM are trained on $k-1$ folds the $P_{n}$ fold is classified using (1) where the weights are computed using algorithm represented in Figure (4).",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:26",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the EDA's output be measured using the criteria of probabilistic relationships among problem variables, in the context of high-dimensional bioinformatic problems?",
      "topics": [
        "design"
      ],
      "answer": "The quality of an EDA's output, particularly in high-dimensional bioinformatic tasks, can be assessed by explicitly inspecting the probabilistic relationships among problem variables for each generation of the evolutionary process. This involves going beyond simple univariate models to analyze multivariate relationships, allowing bio-experts to propose novel biological hypotheses based on the induced probabilistic relationships from the evolutionary model.",
      "chunk_id": 12128,
      "paper_id": "A review of estimation of distribution algorithms in bioinformatics",
      "paper_title": "A review of estimation of distribution algorithms in bioinformatics",
      "paper_year": "2008",
      "chunk_content": "A group of interesting papers demonstrate the efficiency and the competitive accuracy of this novel search paradigm in a set of challenging NP-hard genomic and proteomic bioinformatic tasks. As the number of EDA application papers in bioinformatics is modest and the number and variety of problems is constantly growing, there is room for new EDA applications in the field. An interesting opportunity for future research is the adaptation and application of multivariate EDA models that can efficiently deal with the huge dimensionality of current bioinformatic problems. Going further than simple univariate models, bio-experts could explicitly inspect the probabilistic relationships among problem variables for each generation of the evolutionary process. This would create opportunities for improved accuracy. These probabilistic relationships induced from the evolutionary model are an attractive way of proposing novel biological hypotheses to be further tested by bio-experts. ## Competing interests The authors declare that they have no competing interests. ## Authors' contributions RA, II, and PL conceived of the manuscript. II, YS, JLF, RB, VR and CB participated in writing the genomics section. The proteomics section was designed and written by RS and JAL. The introduction to EDAs was carried out by RA, RS and YS. RA was in charge of the writing and coordination process.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:28",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for Estimation of Distribution Algorithms using IEDA, particularly focusing on the sampling strategy and selection operators?",
      "topics": [
        "convergence"
      ],
      "answer": "The primary purpose of the IEDA approach is to enhance population diversity and convergence within the Estimation of Distribution Algorithm framework. This is achieved through a combination of probabilistic sampling using the PBIL probability model and the application of IGA's crossover and mutation operators (random single-point crossover, parametric unified crossover, and gene position exchange) to generate new individuals. Additionally, an elitism criterion is used to replace the worst individual in the current generation with the best individual from the previous generation, aiming to improve solution quality.",
      "chunk_id": 22495,
      "paper_id": "Independent tasks scheduling in cloud computing via improved estimation of distribution algorithm",
      "paper_title": "Independent Tasks Scheduling in Cloud Computing via Improved Estimation of Distribution Algorithm",
      "paper_year": "2018",
      "chunk_content": "The rest of initial individuals are generated at random. As for the concrete operation operator design of our proposed IGA, firstly, roulette method is chosen as the selection operator; secondly, the random single-point crossover and the parametric unified crossover are combined as the crossover operator, and finally, mutation operator is that two different gene positions are randomly selected for exchange. Finally, our sampling strategy in IEDA is divided into two parts: one is probabilistic sampling by PBIL probability model; on the other hand, in order to increase the diversity of population, the crossover and mutation operation of IGA are applied to all individuals of this generation to generate new individuals. Furthermore, an improved and novel elitism criterion is employed, which converts the worst individual wg in this generation to the best individual bg-1 in the last generation after sampling. # Algorithm Presentation After elaborating on the improvements of PBIL algorithm, our proposed IEDA is presented as Algorithm 1. Simultaneously, the population size of IEDA is set to $\\mathrm{N}=\\mathrm{n} \\times \\mathrm{m}$, and the maximum iteration $\\mathrm{G}=5 \\times \\mathrm{n}$ is seen as a termination criterion. TABLE. 1 After elaborating on the improvements ## EXPERIMENT ANALYSIS As mentioned earlier, there is no state-of-the-art algorithm for solving the studied problem, so three different comparative approaches: EMM[4], MGA [5], and PBIL[7] are chosen to compare with our proposed IEDA from two aspects: solution quality and convergence speed of all comparative algorithms.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:29",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of a filter modeled with its transmission line equivalent model, what mathematical principles govern the relationship between the characteristic impedance of the equivalent transmission lines and the filter's transmission coefficient, specifically as expressed by the formula for $S_{21}$?",
      "topics": [
        "performance"
      ],
      "answer": "The transmission coefficient $S_{21}$ of the filter, which represents the transfer function, is mathematically determined by the chain matrix of the entire structure. This chain matrix is a product of single chain matrices, each dependent on the characteristic impedance $Z_{i}$ and the electrical length $P$ of individual transmission lines. The formula provided expresses $S_{21}$ as \\$\\frac{2 \\sqrt{Z_{\\text {out }} / Z_{\\text {in }}}}{A_{\\text {not }}+B_{\\text {not }} / Z_{\\text {in }}+\\left(Z_{\\text {out }} / Z_{\\text {in }}\\right)\\left(C_{\\text {not }} Z_{\\text {in }}+D_{\\text {not }}\\right)}\\$ where $Z_{\\text {out }}$ and $Z_{\\text {in }}$ are the reference impedances at the output and input ports, respectively, and $A_{\\text {not }}, B_{\\text {not }}, C_{\\text {not }}, D_{\\text {not }}$ are entries of the chain matrix. The performance of the filter, particularly its bandwidth, is influenced by the number of lines (represented by $P$) and the values of the characteristic impedance $Z_{i}$ of the equivalent transmission lines, demonstrating a direct mathematical relationship between these parameters and the filter's transmission characteristics.",
      "chunk_id": 10745,
      "paper_id": "Improved compact genetic algorithm for EM complex system design",
      "paper_title": "Improved Compact Genetic Algorithm for EM Complex System Design",
      "paper_year": "2012",
      "chunk_content": "The lines are disposed symmetrically with respect to the central one. [IMAGE_2] Figure 5: Geometry of a $\\mathrm{N}=2$ filter The filter can be easily modeled with its transmission line equivalent model, i.e. with a cascade of $Z_{\\text {out }}, Z_{\\text {in }}$ transmission lines, having all the same electrical length ( $P$ ), but different characteristic impedance $Z_{i}$, since this latter quantity depends on the line width $w_{i}$. The filter can be therefore seen as a sequence of two-port networks, each of which can be represented by its chain matrix [4], whose entries depend only on the characteristic impedance and on the electric length. The chain matrix of the entire structure is given by the product of $60 \\%$ single chain matrices and the transmission coefficient (i.e. the transfer function) of the filter is expressed in terms of the entries of the chain matrix of the whole structure: $$\nS_{21}=\\frac{2 \\sqrt{Z_{\\text {out }} / Z_{\\text {in }}}}{A_{n o t}+B_{n o t} / Z_{\\text {in }}+\\left(Z_{\\text {out }} / Z_{\\text {in }}\\right)\\left(C_{n o t} Z_{\\text {in }}+D_{n o t}\\right)}\n$$ where $Z_{\\text {out }}, Z_{\\text {in }}$ are the reference impedances at the output and input ports of the filter, respectively. The performances of the filter depend on the number of lines used for its realization (the greater is $P$, the larger is the bandwidth, but also the longer the filter is), and on the values of the characteristic impedance of the equivalent transmission lines.",
      "question_type": "conceptual deep",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:31",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to allocate charges to machines in a steelmaking process, how should practitioners initialize the first population POP(0) using the Longest Processing Time (LPT) and Earliest Available Machine (EAM) heuristics, specifically detailing the machine selection process in upstream and downstream stages?",
      "topics": [
        "scheduling"
      ],
      "answer": "To initialize the first population POP(0), the first individual POP(0,0) is generated as follows: In the last stage, select the machine with the Longest Processing Time (LPT) for each cast. In the upstream stages, select the Earliest Available Machine (EAM) for each machine, proceeding in a backward scheduling manner. After this initial individual is constructed, the remaining NP-1 solutions are generated using a random shuffling approach. Note that $J_{i,k}$ represents the charge set assigned on machine $M_{i, k}$ where $i<g$, and $C_{k}$ represents the sorted casts assigned on machine $M_{g,k}$.",
      "chunk_id": 14476,
      "paper_id": "An Adaptive Sampling-Estimation of Distribution Algorithm for Robust Scheduling in the Steelmaking-Continuous Casting Process",
      "paper_title": "An Adaptive Sampling-Estimation of Distribution Algorithm for Robust Scheduling in the Steelmaking-Continuous Casting Process",
      "paper_year": "2017",
      "chunk_content": "3. The charge set $\\{1,2,3,4,5,6\\}$ is allocated on $\\left\\{M_{1,2}, M_{1,1}, M_{1,3}, M_{1,1}, M_{1,3}, M_{1,2}\\right\\}$ and $\\left\\{M_{2,4}, M_{2,1}\\right.$, $\\left.M_{2,3}, M_{2,3}, M_{2,4}, M_{2,2}\\right\\}$. Charges in the first cast $\\{1,2,3\\}$ and the second cast $\\{4,5,6\\}$ are assigned on $M_{3,1}$ and $M_{3,2}$ respectively. For initializing the first population $P O P(0)$, we generate the first individual $P O P(0,0)$ with following steps. Step 1: In the last stage, select the machine with the longest processing time (LPT) for each cast. Step 2: In the upstream stages, select the earliest available machine (EAM) for each machine with backward scheduling manner. After $$\n\\begin{gathered}\nW_{1}-s_{1, j} \\leq-r_{j}, \\quad j \\in J \\\\\nW_{i}-s_{i, j}+s_{i-1, j} \\leq-p_{i-1, j}, \\quad j \\in J, i \\in\\{2, \\cdots, g\\} \\\\\ns_{i, j_{1}}-s_{i, j_{2}} \\leq-p_{i, j_{1}}, i \\in\\{1, \\cdots, g-1\\},\\left(j_{i}, j_{2}\\right) \\in J_{i, k} \\\\\ns_{i, j}-s_{i+1, j}+t r_{i, i+1} \\leq-p_{i, j}, i \\in\\{1, \\cdots, g-1\\}, j \\in J \\\\\ns_{g, j_{2}}-s_{g, j_{2}} \\leq p_{g, j_{2}, k}+s u_{j_{1}},\\left(b_{1}, b_{2}\\right) \\in C_{k} \\\\\ns_{g, j_{2}}=s_{g, j_{1}}+p_{g, k, j_{2}}, r=1, \\cdots, N_{b}-1 \\\\\nj_{1}=B_{b, r}, j_{2}=B_{b, r+1}, b \\in C_{k}\n\\end{gathered}\n$$ is constructed, the other $N P-1$ solutions are generated with a random shuffling manner. ## B. Evaluation by $R O$ After the machine allocation vector is determined, the decision variables $\\Omega$ and $\\Omega^{2}=1-\\exp \\left(-\\Omega^{2} / 2\\right)$ in MAP are known. To simplify TTP, we use $J_{i, k}$ to represent the charge set assigned on machine $M_{i, k}(i<g)$, and $C_{k}$ to represent the sorted casts assigned on machine $M_{g, k}$. Then, the TTP is reformulated as follows.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:33",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EHBSA algorithm discussed, what characteristic of its offspring generation process makes it suitable for the UCARP problem, and what limitation does the original EHBSA have in the context of UCARP?",
      "topics": [
        "mutation"
      ],
      "answer": "EHBSA generates new offspring by sampling a probabilistic model that learns the adjacency of elements in permutations, which is considered a good fit for UCARP. However, the original EHBSA does not handle uncertainties explicitly, making it inefficient for UCARP. The UCARP problem is defined using a complete and undirected graph $G(V, E)$ where $V$ and $E$ represent the sets of vertices and edges, respectively, and $T$ represents the set of edges where $d(\\tau) > 0$, with $d(\\tau)$ representing demand. $I_{j}$ is defined as a stochastic variable.",
      "chunk_id": 24722,
      "paper_id": "Estimation of the Distribution Algorithm With a Stochastic Local Search for Uncertain Capacitated Arc Routing Problems",
      "paper_title": "Estimation of the Distribution Algorithm With a Stochastic Local Search for Uncertain Capacitated Arc Routing Problems",
      "paper_year": "2016",
      "chunk_content": "EHBSA fits this characteristic well because it generates new offspring by sampling a probabilistic model that learns the adjacency of elements in permutations. Therefore, an intuitive idea is that EHBSA can generate high-quality solutions to UCARP. However, the original EHBSA does not handle uncertainties explicitly and thus is not efficient for UCARP. Inspired by the idea of Memetic Algorithms (MAs), the SLS procedure is developed to address this challenge. The remainder of this paper is organized as follows: Section II introduces the problem definition of UCARP and the corresponding notations. In Section III, details of EDASLS are presented. Section IV presents the computational experiments on two benchmark sets adapted from the UCARP literature. The results are compared to those obtained by two other state-of-the-art MAs. Finally, conclusions and future work is presented in Section V. ## II. Problem DEFINITION AND NOTATIONS Let $G(V, E)$ be a complete and undirected graph where $T=\\{\\tau \\in E \\mid d(\\tau)>0\\}$ and $v_{0} \\in V$ represent the sets of vertices and edges, respectively. Associated with each edge $\\Theta$ there is a $\\operatorname{cost} c(e)>0$ and a demand $d(e) \\geq 0$. Both $1 \\leq j \\leq N, I_{j}$ and $I_{j} \\in \\Theta$ are stochastic variables.",
      "question_type": "definition",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:35",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the search performance of XEDP compare to GMPE in solving the Max problem, specifically regarding the number of evaluations needed to achieve a certain success rate?",
      "topics": [
        "search"
      ],
      "answer": "For the Max problem, GMPE required 13590 evaluations to achieve a 60% success rate, while XEDP needed only 13200 evaluations to achieve a 100% success rate. This indicates that XEDP exhibits higher search performance than GMPE for this particular problem.",
      "chunk_id": 26655,
      "paper_id": "Probabilistic distribution models for EDA-based GP",
      "paper_title": "Probabilistic distribution models for EDA-based GP",
      "paper_year": "2005",
      "chunk_content": "GP with mutation has never produced the optimum solution till the 100th generation. Search with Type A $\\left(P_{r e p}=0\\right)$ was also successful. This means that program generation using the conditional probability tree is effective regarding the Max problem. [3] reports that GMPE required 13590 evaluations to achieve the success rate $60 \\%$. On the other hand, XEDP needed only 13200 evaluations $(=200 \\times 66)$ for $100 \\%$ success rate, which means XEDP has higher search performance than GMPE as for the Max problem. In the wall-following problem, the cumulative probabilities of success with XEDP got better during the latter half of evolution. Then probability of success of XEDP at the 50th generation is higher than that of GP. On the other hand, GP with crossover shows no fluctuation after the 40th generation. ## 4. DISCUSSION As for the Max problem, the Multiplexer problem, and the Wall-following problem, XEDP marked as high search performance as or higher than GP. PIPE [1] is available only for the search problem of real value function. Besides, eCGP [2] and GMPE do not show significant search performance regarding the problems for which GP search is effective.",
      "question_type": "comparison",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:37",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "In implementing an Estimation of Distribution Algorithm (EDA) for optimizing Plug-In Hybrid Electric Vehicle (PHEV) charging, how can the provided formula for \\(J(k)\\) be used to structure the cost function calculation within the algorithm's iterative process?",
      "topics": [
        "optimization"
      ],
      "answer": "The formula for \\(J(k)\\), which represents a cost function at time step \\(k\\), should be implemented as a core component of the EDA's fitness evaluation. Specifically, for each candidate solution (representing a charging schedule), the \\(SoC_i(k)\\) (State of Charge of vehicle \\(i\\) at time \\(k\\)), \\(P_i(k)\\) (charging power of vehicle \\(i\\) at time \\(k\\)), \\(Cap_i\\) (battery capacity of vehicle \\(i\\)), \\(\\Delta t\\) (time step duration), and \\(V_i(k)\\) (voltage of vehicle \\(i\\) at time \\(k\\)) values need to be calculated or retrieved. These values are then used to compute the cost associated with that particular charging schedule. The weights \\(w_i\\) allow for prioritization or penalization of specific vehicles or charging characteristics. The EDA would then use these cost values to update its probabilistic model and generate new candidate solutions in subsequent iterations.",
      "chunk_id": 10525,
      "paper_id": "Performance Evaluation of an EDA-Based Large-Scale Plug-In Hybrid Electric Vehicle Charging Algorithm",
      "paper_title": "Performance Evaluation of an EDA-Based Large-Scale Plug-In Hybrid Electric Vehicle Charging Algorithm",
      "paper_year": "2012",
      "chunk_content": "For the most part, the simulated number of $\\mathrm{PHEVs} / \\mathrm{PEVs}$ connected to the grid at any given time matches the actual data quite well, as shown in Fig. 2. The initial SOC can be modeled as a random variable under log-normal distribution as found by [15] and [16]. The average mileage per day per vehicle is assumed to be 32 miles. Accordingly, the daily distance is log-normally distributed with a mean of 32 miles and a standard deviation of 24 miles. EV battery range spans from 20 miles to 300 miles depending on the number and type of battery, weather, driving behavior, and terrain, which is out of scope in this paper. Thus, in the following simulation, the average battery range is simply assumed to be 100 miles. The relationship between initial SOC and typical daily travel distance was proposed in [17]. $X$ is the initial SOC $$\nJ(k)=\\sum_{i} w_{i} \\cdot\\left[\\operatorname{SoC}_{i}(k)+\\frac{2 P_{i}(k) \\Delta t}{\\operatorname{Cap}_{i} \\cdot\\left[\\sqrt{\\frac{2 P_{i}(k) \\Delta t}{C_{i}}+V_{i}^{2}(k)+V_{i}(k)\\right]}\\right]\n$$ [IMAGE1] Fig. 2. Typical parking utilization over a period of 24 h . TABLE I Typical Hourly Parking Utilization During Weekdays [IMAGE2] Fig. 3. Probability density function of a typical initial SOC.",
      "question_type": "process",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:39",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What benchmarking approach is outlined to assess the EDA when examining the effectiveness of mutation operation?",
      "topics": [
        "genetic"
      ],
      "answer": "Computational simulations are carried out on four-peaks problems, $F c_{4}$ function, and MAXSAT problems to confirm the effectiveness of the proposed approach.",
      "chunk_id": 13074,
      "paper_id": "The effectiveness of mutation operation in the case of Estimation of Distribution Algorithms",
      "paper_title": "The effectiveness of mutation operation in the case of Estimation of Distribution Algorithms",
      "paper_year": "2007",
      "chunk_content": "Introduction Recently, Estimation of Distribution Algorithms (EDAs) have attracted much attention by Evolutionary Algorithm researchers due to their search abilities (Larranaga and Lozano, 2003). Genetic operators such as crossover and mutation are not adopted in the EDAs. Instead of this, new populations are generated from the probabilistic model estimated in each generation. This probabilistic model is estimated from the genetic information of individuals, which are selected from the population in each generation. Such reproduction procedure by using the probabilistic model allows EDAs to search for optimal solutions effectively. However, it significantly decreases the diversity of the genetic infor- [^0]mation in the generated population when the population size is not large enough. This paper discusses the effectiveness of mutation operation in the case of EDAs. Although conventional EDAs do not employ mutation to reproduce individuals in the next generation as we mentioned above, mutation operators are incorporated into conventional EDAs in order to maintain the diversities in populations. Two kinds of mutation operators are examined: a bitwise mutation used in Simple Genetic Algorithm (SGA) (Goldberg, 1989), and a mutation operator taking account into the structure of probabilistic model estimated by EDAs. In order to confirm the effectiveness of the proposed approach, computational simulations on four-peaks problems, $F c_{4}$ function, and MAXSAT problems are carried out (Larranaga and Lozano, 2003; De Bonet et al., 1996).",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:41",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of the specific EDA discussed, considering the existing use of metaheuristic algorithms like ACO, PSO, GRASP, Simulated Annealing, and NSGA-II for similar optimization problems?",
      "topics": [
        "search"
      ],
      "answer": "The key takeaway is that despite the widespread use of metaheuristic optimization algorithms and some non-metaheuristic approaches such as integer programming for problems similar to the Next Release Problem, Estimation of Distribution Algorithms (EDAs) have received surprisingly little attention, even though they have shown promise in related areas like feature selection, both in single-objective and multi-objective contexts.",
      "chunk_id": 7217,
      "paper_id": "FEDA-NRP A fixed-structure multivariate estimation of distribution algorithm to solve the multi-objective Next Release Problem with requirements interactions",
      "paper_title": "FEDA-NRP: A fixed-structure multivariate estimation of distribution algorithm to solve the multi-objective Next Release Problem with requirements interactions",
      "paper_year": "2023",
      "chunk_content": "(2018), and so it has been commonly solved with metaheuristic optimisation search algorithms, such as ACO (del Sagrado et al., 2015), PSO (Hamdy and Mohamed, 2019), GRASP (Perez-Piqueras et al., 2022), Simulated Annealing (Baker et al., 2006) and NSGA-II (Durillo et al., 2011). Non metaheuristic approaches based on integer programming (Dong et al., 2022) and pursuing anytime behaviour (Dominguez-Rios et al., 2019) have also been proposed. However, almost no attention has been focused on the use of Estimation of Distribution Algorithms (EDAs) (Larranaga and Lozano, 2002). This is striking in view of the results obtained by this family of methods in a related problem such as feature selection (Guyon and Elisseeff, 2003; Abdollahzadeh and Gharehchopogh, 2022), both in its singleobjective (Bermejo et al., 2011) and multi-objective versions (Maza and Tosabria, 2019).",
      "question_type": "challenge",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:42",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of the presented scheduling algorithm, how are the time and energy consumption for load operation of an operation (denoted as [FORMULA632] in the text) calculated, specifically detailing the role of the parameters involved in determining the time component?",
      "topics": [
        "algorithm"
      ],
      "answer": "The time consumption for load operation involves several components including calculations related to x-axis and y-axis movements. The calculation of \\(T_{xlas,2}(O_{1,2})\\), which represents a time component related to x-axis movement, uses the formula \\(c \\cdot Vx / a_{xx}\\), where \\(c\\) is a coefficient calculated as \\((Ws + Wj + Wx + Wy) / (Qn + Wx + Wy)\\), \\(Vx\\) represents a velocity parameter, and \\(a_{xx}\\) represents an acceleration parameter. Similarly, \\(T_{xlak,2}(O_{1,2})\\) is calculated as \\(c \\cdot Vx / a_{xk}\\) with \\(a_{xk}\\) being another acceleration parameter.  Another time component, \\(T_{xlad,2}(O_{1,2})\\), is calculated using the maximum of an expression involving the absolute difference between positions \\(x_{sp}\\) and \\(x_{pp}\\) divided by \\(Vx\\), and a term involving \\(Vx\\), \\(c\\), \\(a_{xx}\\) and \\(a_{xk}\\). Analogous calculations are performed for the y-axis, including \\(T_{ylss,2}(O_{1,2}) = d \\cdot Vy / a_{ys}\\) and \\(T_{ylss,2}(O_{1,2}) = d \\cdot Vy / a_{yb}\\), where \\(d = (Ws + Wj + Wy) / (Qn + Wy)\\), \\(Vy\\) is a velocity parameter, and \\(a_{ys}\\) and \\(a_{yb}\\) are acceleration parameters. The total load standby time \\(T_{ls,2}(O_{1,2})\\) is then the sum of these individual time components. The energy consumption \\(E_{ls,2}(O_{1,2})\\) is calculated based on these time components and power consumption parameters such as \\(P_{ss}\\), \\(P_{ys}\\), \\(P_{sd}\\), and \\(P_{yd}\\), along with the coefficients \\(Ws\\), \\(Wj\\), and \\(Qn\\).",
      "chunk_id": 18097,
      "paper_id": "A hybrid estimation of distribution algorithm for distributed flexible job shop scheduling with crane transportations",
      "paper_title": "A hybrid estimation of distribution algorithm for distributed flexible job shop scheduling with crane transportations",
      "paper_year": "2021",
      "chunk_content": "(13) and (14), the time and energy consumption of no-load standby of $\\mathrm{O}_{1,2}$ is calculated in (92) and (93). $$\n\\begin{aligned}\nT_{n s, 2}\\left(O_{1,2}\\right) & =\\max \\left\\{T c\\left(O_{1,1}\\right)-T c_{m s, 2}\\left(O_{1,2}\\right), 0\\right\\} \\\\\n& =\\max \\{10-7.7418,0\\}=2.2582(\\mathrm{~min})\n\\end{aligned}\n$$ $E_{n s, 2}\\left(O_{1,2}\\right)=T_{n s, 2}\\left(O_{1,2}\\right) \\cdot P s=2.2582 \\cdot 750 \\cdot 60 / 1000=101.6190(\\mathrm{~kJ})$ According to Eqs. (15) and (16), the time and energy consumption of load standby of [FORMULA627] is calculated in (94) and (95). Specifically, in (94), [FORMULA628] is the immediate successor operation of [FORMULA629] being performed on the same machine. $$\n\\begin{aligned}\nT_{l s, 2}\\left(O_{1,2}\\right) & =\\max \\left\\{T c\\left(O_{3,2}\\right)-T c_{n s, 2}\\left(O_{1,2}\\right), 0\\right\\} \\\\\n& =\\max \\{12.3725-10,0\\}=2.3725(\\mathrm{~min})\n\\end{aligned}\n$$ $E_{l s, 2}\\left(O_{1,2}\\right)=T_{l s, 2}\\left(O_{1,2}\\right) \\cdot P s=1.0032 \\cdot 750 \\cdot 60 / 1000=45.1440(\\mathrm{~kJ})$ According to Eqs. (17) - (26), the time and energy consumption of load operation of [FORMULA632] is calculated in (96) - (105). $$\n\\begin{aligned}\nc & =(W s+W j+W x+W y) /(Q n+W x+W y) \\\\\n& =(900+2100+9700+5500) /(10000+9700+5500)=0.7222 \\\\\n& T_{x l a s, 2}\\left(O_{1,2}\\right)=c \\cdot V x / a_{x x}=0.7222 \\cdot 50 /\\left(0.5 \\cdot 60^{2}\\right)=0.0201(\\mathrm{~min}) \\\\\n& T_{x l a k, 2}\\left(O_{1,2}\\right)=c \\cdot V x / a_{x k}=0.7222 \\cdot 50 /\\left(0.8 \\cdot 60^{2}\\right)=0.0125(\\mathrm{~min}) \\\\\n& T_{x l a d, 2}\\left(O_{1,2}\\right)= \\max \\left\\{\\frac{\\left|x_{s p}-x_{p p}\\right|}{V x}-\\frac{V x}{2} \\cdot c \\cdot\\left(\\frac{1}{a_{x x}}+\\frac{1}{a_{x k}}\\right) \\cdot 0\\right\\} \\\\\n& =\\max \\left\\{\\frac{|160-160|}{50}-\\frac{50}{2} \\cdot 0.7222 \\cdot\\left(\\frac{1}{0.5 \\cdot 60^{2}}+\\frac{1}{0.8 \\cdot 60^{2}}\\right) \\cdot 0\\right\\}=0(\\mathrm{~min})\n\\end{aligned}\n$$ Table 10 start time and completion time of all operations in machine process and crane transportation (unit: min). $$\n\\begin{aligned}\nd & =(W s+W j+W y) /(Q n+W y) \\\\\n& =(900+2100+5500) /(10000+5500)=0.5484 \\\\\nT_{\\text {ylss }, 2}\\left(O_{1,2}\\right) & =d \\cdot V y / a_{y s}=0.5484 \\cdot 30 /\\left(0.4 \\cdot 60^{2}\\right)=0.0114(\\mathrm{~min}) \\\\\nT_{\\text {ylss }, 2}\\left(O_{1,2}\\right) & =d \\cdot V y / a_{y b}=0.5484 \\cdot 30 /\\left(0.6 \\cdot 60^{2}\\right)=0.0076(\\mathrm{~min}) \\\\\nT_{\\text {ylss }, 2}\\left(O_{1,2}\\right) & =\\max \\left\\{\\frac{\\left|v_{u r}-v_{r b}\\right|}{V y}-\\frac{V y}{2} d \\cdot\\left(\\frac{1}{a_{y s}}+\\frac{1}{a_{y b}}\\right) \\cdot 0\\right\\} \\\\\n& =\\max \\left\\{\\frac{|60-2 b|}{30}-\\frac{30}{2} \\cdot 0.5484 \\cdot\\left(\\frac{1}{0.4 \\cdot 60^{2}}+\\frac{1}{0.6 \\cdot 60^{2}}\\right) \\cdot 0\\right\\}=1.3238(\\mathrm{~min})\n\\end{aligned}\n$$ $$\n\\begin{aligned}\nT_{l s, 2}\\left(O_{1,2}\\right)= & T_{s l s s, 2}\\left(O_{1,2}\\right)+T_{s l s s, 2}\\left(O_{1,2}\\right)+T_{s l s s, 2}\\left(O_{1,2}\\right)+T_{y l s s, 2}\\left(O_{1,2}\\right) \\\\\n& +T_{y l s s, 2}\\left(O_{1,2}\\right)+T_{y l s s, 2}\\left(O_{1,2}\\right) \\\\\n= & 0.0201+0.0125+0+0.0114+0.0076+1.3238=1.3755(\\mathrm{~min}) \\\\\n& \\\\\nE_{l s, 2}\\left(O_{1,2}\\right)= & T_{s l s s, 2}\\left(O_{1,2}\\right) \\cdot P_{s s}+T_{y l s s, 2}\\left(O_{1,2}\\right) \\cdot P_{y s} \\\\\n& +(W s+W j) / Q n \\cdot\\left(T_{s l s s, 2}\\left(O_{1,2}\\right) \\cdot P_{s d}+T_{y l s d, 2}\\left(O_{1,2}\\right) \\cdot P_{y d}\\right) \\\\\n= & {[0.0201 \\cdot 20000+0.0114 \\cdot 12500+(900+2100) / 10000 } \\\\\n& \\cdot(0 \\cdot 8000+1.3238 \\cdot 5000)] \\cdot 60 / 1000=151.7858(\\mathrm{~kJ})\n\\end{aligned}\n$$ According to the time of machine process and crane transportation, the start and completion of these stages can be obtained. The start time and completion time of all operations in machine process and crane transportation are listed in Table 10, while the corresponding Gantt chart is demonstrated in Fig. 18. From Table 10 and Fig.",
      "question_type": "conceptual deep",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:46",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm that uses clonal selection to prevent deterioration, how should practitioners determine the size of the clone to copy bits from one individual to another?",
      "topics": [
        "fitness"
      ],
      "answer": "The size of the clone, denoted as D, can be determined dynamically using the formula  \\(D_{i}^{g}=\\operatorname{round}\\left(\\frac{n \\cdot fu\\left(n_{i}^{p}\\right)}{\\sum_{1}^{n} fu\\left(n_{i}^{p}\\right)}\\right)\\), where 'n' is the size of the population at generation 'g', and \\(fu(n_i^p)\\) represents a function related to the probability of selecting individuals from a population for immune operation.",
      "chunk_id": 1547,
      "paper_id": "MAX-SAT problem using evolutionary algorithms",
      "paper_title": "MAX-SAT Problem using Evolutionary Algorithms",
      "paper_year": "2014",
      "chunk_content": "These individuals are selected from [FORMULA158] population in [^0]: [FORMULA159] Bra-ket notation is a standard notation for describing quantum states in the theory of quantum computing. It is composed of angle brackets and vertical bars. $\\langle a|$ is known as bra and $|a\\rangle$ is known as ket. proportion of [FORMULA162], where [FORMULA163] is the probability of selecting individuals from population for immune operation. Clonal selection is used for preventing deterioration. Clonal selection states that antigens can selectively react to antibodies; if antibody matches the antigen sufficiently well, and antibody's B cell (i.e., number of bits from certain individual) is stimulated and can produce related clones. In the traditional artificial immune clone algorithms scale of the copy of clone (i.e. the number of bits copied from one individual to other individual) is fixed. However, we adopt the dynamic copy of the clone as follows [6]: $$\nD_{i}^{g}=\\operatorname{round}\\left(\\frac{n \\cdot f u\\left(n_{i}^{p}\\right)}{\\sum_{1}^{n} f u\\left(n_{i}^{p}\\right)}\\right)\n$$ Where, $D$ is the size of the clone, [FORMULA166] is the size of the population at generation [FORMULA167]. If fitness of the offspring is greater (in context of MAX-SAT problem) then the fitness of parent, the offspring will participate in the next competition or vice versa.",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:48",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Distance Minimized Inverse Reinforcement Learning (DMIRL) approach used for optimization, how are the unknown reward function \\(R\\) and the state of the system \\(x_k\\) mathematically represented, and what role do the feature vector \\(\\phi(x_k)\\) and weighting coefficients \\(w\\) play in defining \\(R\\)?",
      "topics": [
        "optimization"
      ],
      "answer": "The unknown reward function \\(R\\) at time step \\(k\\) is defined as \\(R(x_k) = w^\\top \\phi(x_k)\\), where \\(x_k = [a_k \\ k]^\\top\\) represents the state of the system, with \\(a_k\\) being a discretized signal of acceleration at time step \\(k\\). The feature vector \\(\\phi(x_k) = [\\phi_1(x_k) \\ \\phi_2(x_k) \\ \\cdots \\ \\phi_N(x_k)]^\\top\\) maps the state \\(x_k\\) onto an \\(N\\)-dimensional space, and \\(w = [w_1 \\ w_2 \\ \\cdots \\ w_N]^\\top\\) is a vector of weighting coefficients. Therefore, \\(R\\) is a linear combination of the features extracted from the state, weighted by the coefficients in \\(w\\). Each neuron function \\(\\phi_j(x_k)\\) within \\(\\phi(x_k)\\) is defined as \\(\\phi_j(x_k) = \\tanh(p_j a_k + h_j k + q_j)\\), where \\(p_j, h_j, q_j\\) are randomly assigned weighting factors.",
      "chunk_id": 17835,
      "paper_id": "Acceleration Control Design of HEVs with Comfortability Evaluation based on IRL",
      "paper_title": "Acceleration Control Design of HEVs with Comfortability Evaluation based on IRL",
      "paper_year": "2021",
      "chunk_content": "To this end, the Distance Minimized Inverse Reinforcement Learning (DMIRL) approach proposed by Burchfiel (2016) is used with a set of training data. Let $a_{k}^{i}\\left(k=0,1, \\cdots, M^{i} \\leq M\\right)$ denote a discreterized signal of acceleration, where $i=1,2, \\cdots, S$ is the index of the acceleration data. The corresponding score of $a_{k}^{i}$ is represented by $J^{i}$. Suppose the unknown reward function $R$ on time step $k$ is given by $$\nR\\left(x_{k}\\right)=w^{\\top} \\phi\\left(x_{k}\\right)\n$$ where $x_{k}=\\left[\\begin{array}{ll}a_{k} & k\\end{array}\\right]^{\\top}$ is the state of the system, $\\phi\\left(x_{k}\\right)=$ $\\left[\\phi_{1}\\left(x_{k}\\right) \\phi_{2}\\left(x_{k}\\right) \\cdots \\phi_{N}\\left(x_{k}\\right)\\right]^{\\top}$ maps the state $x_{k}$ onto a $N$ dimensional feature vector, and $w \\in \\mathbb{R}^{N}$ is the vector of weighting coefficients, i.e., $$\nw=\\left[\\begin{array}{llll}\nw_{1} & w_{2} & \\cdots & w_{N}\n\\end{array}\\right]^{\\top}\n$$ Following the standard notation of extremal learning machines (ELMs), the neuron function $\\phi_{j}\\left(x_{k}\\right)$ is defined as follows: $$\n\\phi_{j}\\left(x_{k}\\right)=\\tanh \\left(p_{j} a_{k}+h_{j} k+q_{j}\\right)\n$$ The values of the weighting factors $p_{j}, h_{j}, q_{j}(j=$ $1,2, \\cdots, N)$ are assigned randomly. The structure of the ELM is shown in Fig. 3. [IMAGE_2] Fig.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:50",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "Based on the relative percentage increase (RPI) metric, how does the performance distribution of the MEDA algorithm compare against the best makespan found in the literature for flexible job-shop scheduling problems?",
      "topics": [
        "comparison"
      ],
      "answer": "The MEDA algorithm's results are comparatively concentrated within a range of [0, 0.05] RPI over the best makespan found in the literature. This concentration suggests that MEDA produces solutions that are consistently close to the optimal makespan. However, the text also mentions the existence of outliers, indicating instances where MEDA's performance deteriorates relative to the best-known solutions.",
      "chunk_id": 21835,
      "paper_id": "A hybrid estimation of distribution algorithm for flexible job-shop scheduling problems with process plan flexibility",
      "paper_title": "A hybrid estimation of distribution algorithm for flexible job-shop scheduling problems with process plan flexibility",
      "paper_year": "2018",
      "chunk_content": "We measure the relative percentage increase (RPI) as: $\\operatorname{RPI}\\left(c_{i}\\right)=\\left(c_{i}-c^{*}\\right) / c^{*}$ where $c_{i}$ is the makespan obtained in the [FORMULA272] th replication, and $c^{*}$ is the best makespan found and reported in the literature. The distribution of the experimental results in every interval is presented in Table 4. It is clear from the table that the results of the MEDA algorithm are comparatively concentrated, which is mainly in the range of $[0,0.05]$, over the best solution found, i.e., makespan, for all the datasets. In addition, Table 4 describes the outliers, i.e., deteriorations produced by MEDA. Based on the results the MEDA is a suitable approach for solving real flexible manufacturing processes. ### 4.2 Comparison with multi-objective algorithms Furthermore, in order to enhance the novelty of the MEDA, we consider evaluating the MEDA with the same characteristics of the industrial environment. A hundred flexible job-shop instances have been developed replicating real flexible manufacturing processes, to analyze the solution capability of the MEDA on the FJSP-PPF. The instances contain a different number of jobs, machines, operations, process plans for each job and alternative machines for performing each operation.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:52",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing the HMOEDALLE algorithm in C++, what is the formula used to determine the value of a key parameter related to population size reduction over generations, and how are its components typically initialized?",
      "topics": [
        "convergence",
        "HMOEDALLE",
        "parameter setting"
      ],
      "answer": "The formula used to determine the population size reduction is r * |Pop| * e^(-t/T), where |Pop| represents the population size, t is the current generation, T is the maximum generation, and r is a reduction rate factor, typically initialized to 0.7. This formula is used within Algorithm 3 of the HMOEDALLE implementation.",
      "chunk_id": 1235,
      "paper_id": "HMOEDA LLE  A hybrid multi-objective estimation of distribution algorithm combining locally linear embedding",
      "paper_title": "HMOEDA LLE  A hybrid multi-objective estimation of distribution algorithm combining locally linear embedding",
      "paper_year": "2014",
      "chunk_content": "Among them ZDT1.1, ZDT2.1, ZDT6.1, DTLZ2.1 have linear variable linkages and the rest ones have nonlinear variable linkages. The inverted generational distance (IGD) is used to measure both the convergence and diversity of an approximate solution set to the true PF. ## A. General Experimental Setting The proposed HMOEDALLE is implemented with $\\mathrm{C}++$, the test environment is: CPU: Intel Pentium G630, 2.7 GHz ; Memory: 4GB; OS: Windows7; Programming environment: Microsoft Visual Studio 2008. 1) Population size: For all the bi-objective problems, the population size is set to 100 , and 200 for three objective problems. 2) Parameter setting in HMOEDALLE: The value of [FORMULA69] in Algorithm3 is: $$\nr *|\\operatorname{Pop}| * e^{-\\frac{t}{T}}\n$$ The value of [FORMULA71] is $|\\operatorname{Pop}|-N_{1}$ accordingly, here $r$ is set to 0.7 . The neighbor parameter K is 2 for all the bi-objective problems and 3 for all the three objective problems. [FORMULA74] is set to 0.1 for all the test problems, where $t$ is the current generation and $T$ is the maximum generation.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:53",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the fitness of an individual (a set of selected channels) within the Estimation of Distribution Algorithm (EDA) described?",
      "topics": [
        "fitness"
      ],
      "answer": "The fitness of an individual in the EDA is evaluated based on the errors in predicting zonal and bulk power. Fitness points are assigned as 1 for each zone and 3 for bulk power prediction. If all power distribution errors are within specified limits, the individual is rewarded 10 points. The maximum possible fitness is 3570 points, based on 357 power distributions.",
      "chunk_id": 10938,
      "paper_id": "Selection of fuel channels for Thermal Power Measurement in 700MWe Indian PHWR by evolutionary algorithm",
      "paper_title": "Selection of fuel channels for Thermal Power Measurement in 700 MWe Indian PHWR by evolutionary algorithm",
      "paper_year": "2012",
      "chunk_content": "Fitness points for predicting zonal power is chosen to be 1 for each zone since all zones are equally important. Higher points (=3) are chosen for bulk power prediction since it implies error in zone powers also. The objective function (also called fitness function) given by Eq. (3) has to be evaluated to determine the fitness of any given set of selected channels. It can be seen from Eq. (3) that if all the errors are within specified limit for a power distribution the pattern (individual) is rewarded 10 points. There are total 357 power distributions and thus a suitable individual will acquire maximum 3570 points. ## 5. Estimation of Distribution Algorithm (EDA) As mentioned in Section 1, EDA is a population based evolutionary algorithm. The two operators in EDA are estimation of probability distribution and sampling. In each generation, [FORMULA88] individuals (choices of 44 ICs) are generated by sampling certain probability distribution. They are generated (during initial and subsequent generations) in such a way that first three constraints are satisfied. The fitness is evaluated for each of them as described by Eq. (3) and the best $M(<N)$ individuals are selected to estimate probability distribution.",
      "question_type": "definition",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:52:55",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs when applied to grouping problems with dependencies and relationship costs?",
      "topics": [
        "algorithm"
      ],
      "answer": "The algorithmic approach aims to minimize the number of groups and the total cost of grouping items, subject to constraints defined by a weighted undirected graph \\(G_{p}\\) representing item relationships and a directed graph \\(G_{d}\\) representing item dependencies. The objective function \\(\\min |S|+\\sum_{s \\in \\mathcal{S}} C(s)\\) seeks to minimize the number of groups \\(|S|\\) and the sum of costs \\(C(s)\\) for each group \\(s\\). Constraints ensure that related items are appropriately grouped based on the weights in \\(G_{p}\\) and that dependencies in \\(G_{d}\\) are respected by assigning dependent items to the same or higher-indexed groups.",
      "chunk_id": 22181,
      "paper_id": "Analysing a Hybrid Model-Based Evolutionary Algorithm for a Hard Grouping Problem",
      "paper_title": "Analysing a Hybrid Model-Based Evolutionary Algorithm for a Hard Grouping Problem",
      "paper_year": "2018",
      "chunk_content": "The groups have a maximum size of $N$. The relationships between the items are described by an undirected weighted graph $G_{p}=(\\mathcal{I}, \\mathcal{R})$. The vertices are items and the weight of the edges are the costs of putting two items into the same group. There is a directed graph $G_{d}=(\\mathcal{I}, \\mathcal{D})$ with the items as vertices and edges describing dependencies between items. If an item $a$ depends on another item $b$, item $C(\\mathcal{S}) \\rightarrow \\mathbb{R}$ must be either in the same group as $G_{p}$, or in a group with a higher index in $S(\\mathcal{I}) \\rightarrow \\mathbb{Z}$. Such a dependency is modelled as an edge in the dependency graph $(a, b) \\in D$. $$\n\\begin{array}{r}\n\\min |S|+\\sum_{s \\in \\mathcal{S}} C(s) \\\\\n\\text { s.t. }(a, b) \\in R \\\\\nS(a) \\leq S(b) \\\\\n$$ $$\n\\begin{gathered}\n\\forall_{s \\in \\mathcal{S}} \\forall_{a \\in s, b \\in s} \\\\\n\\forall_{(a, b) \\in \\mathcal{D}} \\\\\n\\forall_{s \\in \\mathcal{S}}\n\\end{gathered}\n$$ We want to find the assignment to the smallest number of groups with the lowest total cost. The function $C(\\mathcal{S}) \\rightarrow \\mathbb{R}$ calculates the costs of a group using the weights on $G_{p}$. If an edge in [FORMULA25] has a high weight placing the two items connected by that edge in the same group is unfavourable. Two items that do not share an edge in [FORMULA26] must not be in the same group which is ensured by constraint 2. Constraint 3 guarantees that every item that depends on another one is either in the same group or in a group with a bigger index than the item it depends on.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:52:58",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of selecting a black-box algorithm based on its properties help in solving a problem like DeceivingLeadingBlocks (DLB)?",
      "topics": [
        "heuristic"
      ],
      "answer": "The algorithmic mechanism involves identifying classes of black-box algorithms that contain fast algorithms for the problem at hand. These algorithms are used to determine the direction in which to search for a good established algorithm. The process involves assessing if a certain property of black-box algorithms is helpful for the problem, such as DeceivingLeadingBlocks (DLB), and then examining established heuristics in that class to see if one has good performance.",
      "chunk_id": 3621,
      "paper_id": "Choosing the Right Algorithm With Hints From Complexity Theory",
      "paper_title": "Choosing the Right Algorithm With Hints From Complexity Theory (Hot-off-the-Press Track at GECCO 2022)",
      "paper_year": "2022",
      "chunk_content": "Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). GECCO '22 Companion, July 9-13, 2022, Boston, MA, USA (c) 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9268-6/22/07. https://doi.org/10.1145/3520304.3534069 by giving an indication which of the existing algorithms could be useful for a particular problem. In simple words, what we propose is trying to find out which classes of black-box algorithms contain fast algorithms for the problem at hand. These algorithms may well be artificial as we use them only to determine the direction in which to search for a good established algorithm for our problem. Only once we are sufficiently optimistic that a certain property of black-box algorithms is helpful for our problem, we regard the established heuristics in this class and see if one of them indeed has a good performance. To show that this abstract heuristic approach towards selecting a good algorithm can indeed be successful, we regard the DeceivingLeadingBlocks (DLB) problem recently proposed by Lehre and Nguyen [9].",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:52:59",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to solve the two-dimensional rectangular knapsack problem with guillotine constraint, how should practitioners represent the problem's parameters, such as the number of rectangle types ($m$) and the quantity of each rectangle type ($u_j$), to facilitate the algorithm's search for the optimal subset of rectangles?",
      "topics": [
        "optimization",
        "Estimation of Distribution Algorithms"
      ],
      "answer": "Practitioners should represent the problem parameters by defining $m$ as the number of rectangle types, where each type $i$ (from 1 to $m$) has a width $w_i$, a height $h_i$, and a profit $p_i$. The quantity of each rectangle type $j$ is represented by $u_j$. These parameters define the search space for the EDA, guiding it to select a subset of rectangles to maximize the total profit while adhering to the guillotine constraint and the dimensions of the rectangular strip.",
      "chunk_id": 16398,
      "paper_id": "An EDA for the 2D knapsack problem with guillotine constraint",
      "paper_title": "An EDA for the 2D knapsack problem with guillotine constraint",
      "paper_year": "2019",
      "chunk_content": "Keywords Cutting-packing $\\cdot$ Knapsack $m$ Guillotine-cut $w_{i}$ EDA ## 1 Introduction In the two-dimensional rectangular knapsack problem (2DKP) with guillotine constraint we have a set of $m$ types of rectangles with $w_{i}$ widths, $h_{i}$ heights and a profit $p_{i}$ where $i=1,2, \\ldots, m$. The $j$ th type contains $u_{j}$ rectangles. The goal is to cut a selected subset of the rectangles without overlap onto a rectangular strip of width $m, n=u_{1}+$ and height $u_{2}+\\cdots+u_{m}$, so that the total profit of the selected rectangles is maximal. All cuts are orthogonal guillotine-cuts and the rectangles have to be laid out orthogonally on the strip. [^0] [^0]: (c) Istvan Borgulya borgulya@ktk.pte.hu 1 Faculty of Business Economics, University of Pecs, Pecs, Hungary A guillotine-cut is parallel to the horizontal or vertical side of the strip and cuts the strip into two smaller separated rectangular strips.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:01",
      "generation_style": "practical_application"
    },
    {
      "question": "Based on the information provided, what evidence suggests that the Estimation of Distribution Algorithm (EDA) exhibits a better \"quality of convergence\" compared to Particle Swarm Optimization (PSO) when applied to the problem of minimizing makespan?",
      "topics": [
        "convergence"
      ],
      "answer": "The evidence for better \"quality of convergence\" in the EDA, relative to PSO, is indicated by the consistently lower Standard Deviation (SD) values of the solutions produced by the EDA. Specifically, for 300 job instances, the average SDs of the EDA and PSO algorithms are reported as 5.04 and 43.27, respectively. A lower SD suggests that the EDA's solutions are more tightly clustered around the mean, indicating more consistent and reliable convergence to solutions with similar makespan values.",
      "chunk_id": 24678,
      "paper_id": "Minimizing makespan in a no-wait flowshop with two batch processing machines using estimation of distribution algorithm",
      "paper_title": "Minimizing makespan in a no-wait flowshop with two batch processing machines using estimation of distribution algorithm",
      "paper_year": "2016",
      "chunk_content": "Especially, the improvements realised through the EDA over PSO become more Table 7. Results for 20 job instances. Table 8. Results for 50 job instances. Table 9. Results for 100 job instances. pronounced as the number of jobs in the instances increases. For example, the EDA is better than PSO by, on average, $6.96 \\%$ for 50 job instances, while the EDA is better than PSO by $14.01 \\%$ for 300 job instances. Besides, for each problem category, the EDA achieves better the best, average and worst [FORMULA153] when compared to PSO. In addition, the SD values of the solutions produced by the EDA are consistently less than those produced by PSO. For example, the average SDs of the EDA and PSO algorithms are, respectively, 5.04 and 43.27 for 300 job instances, which indicates that the EDA has the better quality of convergence. Figures 5-8 graphically show the effect of different problem factors, including number of jobs, job sizes, job ready times and setup times, on the average improvement in makespan when the EDA and PSO algorithms are compared, respectively.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:03",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the reduction control parameter \\$\\varepsilon\\$ in PSIEDADK-KVDE influence performance, compared to the penalty parameter \\$C\\$ in SVMC-IPSA, based on the information provided?",
      "topics": [
        "performance"
      ],
      "answer": "In PSIEDADK-KVDE, the reduction control parameter \\$\\varepsilon\\$ is set to 0.6. In SVMC-IPSA, the penalty parameter \\$C\\$ of the error term is set to 1. The text indicates that K-fold Cross-Validation assisted tuning was used to determine these hyper-parameter values to guarantee the performance of both algorithms.",
      "chunk_id": 20920,
      "paper_id": "Language model based interactive estimation of distribution algorithm",
      "paper_title": "Language model based interactive estimation of distribution algorithm",
      "paper_year": "2020",
      "chunk_content": "For the SVMC-IPSA, the maximal iteration is 160 , the Gaussian function is used as the kernel function, and the penalty parameter $C$ of the error term is 1 . To PSIEDADK-KVDE, reduction control parameter $\\varepsilon$ is set as 0.6 . As for these aforementioned hyper-parameters, K-fold Cross-Validation assisted tuning has been conducted for guaranteeing the performance of compared algorithms; thus, a convincing conclusion can be drawn. [IMAGE1] Fig. 2. Interface for user preference extraction. [IMAGE2] $\\bullet 100$-UGC\\&Wikipedia $\\cong 300$-UGC [FORMULA143]-UGC\\&Wikipedia Fig. 3. Performance vs. vector size and training corpus. ### 4.3. Experiments on parameters First, two experiments are conducted for determining the values of the critical parameters: vector size, training corpus, vocabulary size [FORMULA144], and the generated document size $|d o c|$. Their influences on the proposed algorithm are conveyed, and the number of evaluated items with a lower value indicates a better performance. ### 4.3.1.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:04",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "How should a developer structure the code for the VND search's perturbation step, based on the description, and what speed-up strategies can be employed?",
      "topics": [
        "scheduling"
      ],
      "answer": "The perturbation step in the VND search should be implemented using the critical path-based neighborhood searches as detailed in Algorithm 5. To accelerate this perturbation, the code should utilize speed-up strategies based on the general property of the permutation-based model for scheduling problems, as referenced in '[41]', to efficiently calculate the makespan of each neighbor.",
      "chunk_id": 18889,
      "paper_id": "A matrix-cube-based estimation of distribution algorithm for the distributed assembly permutation flow-shop scheduling problem",
      "paper_title": "A matrix-cube-based estimation of distribution algorithm for the distributed assembly permutation flow-shop scheduling problem",
      "paper_year": "2021",
      "chunk_content": "Obviously, each neighborhood search works like a drill bit, constantly digging down to reach a local optimum in solution space. ### 3.3.2. Perturbation and exploitation In the VND search, the perturbation is needed to escape from the current local optimal solution reached by the exploitation. Because each of the critical path-based neighborhood searches in Section 3.3.1 can quickly find a promising but different solution near the local optimal one, all of them are utilized to execute perturbation in our VND search (see Section 3.3.3). The details of the designed perturbation are provided in Algorithm 5. The non-critical path-based neighborhood searches in Section 3.3.1 can reach a larger number of neighbors in the quite different regions of solution space, which is beneficial to guiding the search toward high-quality local optima. Thus, the exploitation in our VND search is performed via these production-based and job-based neighborhood searches. The details of the designed exploitation are given in Algorithms 6 and 7, respectively. To accelerate the search process in Algorithms 5-7, the speed-up strategies based on the general property of the permutation-based model for scheduling problem [41] are utilized to calculate the makespan of each neighbor.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:05",
      "generation_style": "implementation_focused"
    },
    {
      "question": "Within the Estimation of Distribution Algorithm (EDA) framework, how is the probabilistic model \\(p_t()\\) evaluated at each generation \\(t\\) to ensure it effectively captures the distribution of promising solutions?",
      "topics": [
        "optimization"
      ],
      "answer": "The probabilistic model \\(p_t()\\) is evaluated by maximizing the likelihood function \\(p(D \\mid \\theta)\\) to obtain a point estimate \\(\\hat{\\theta}\\), where \\(D\\) represents the selected individuals \\({\\mathbf{x}^{1}, \\ldots, \\mathbf{x}^{M_{\\text {std }}}\\}\\) and \\(\\theta\\) represents the parameters of the probabilistic model. The point estimate \\(\\hat{\\theta}\\) is obtained via \\(\\hat{\\theta}=\\arg \\max _{\\theta} p(\\left\\{\\mathbf{x}^{1}, \\ldots, \\mathbf{x}^{M_{\\text {std }}}\\right\\} \\mid \\theta)\\), which is then used to sample \\(G_c\\) individuals from \\(p_{t}(\\mathbf{x} \\mid \\hat{\\theta})\\).",
      "chunk_id": 13246,
      "paper_id": "Bayesian inference in estimation of distribution algorithms",
      "paper_title": "Bayesian inference in estimation of distribution algorithms",
      "paper_year": "2007",
      "chunk_content": "TABLE I GENERAL PSEUDOCODE FRAMEWORK FOR AN EDA. Given: population size $M$, selection parameter $\\tau$ BEGIN (set $t=0$ ) Generate $M_{\\text {std }}<M$ individuals at random REPEAT for $t=1,2, \\ldots$ until stopping criterion is met Select $M_{\\text {std }}<M$ individuals via truncation selection Maximise the likelihood $p(D \\mid \\theta)$ to obtain a point estimate $\\hat{\\theta}$ Sample ${ }_{c}^{G}$ individuals from $p_{t}(\\mathbf{x} \\mid \\hat{\\theta})$ $t=t+1$ ENDREPEAT UMDA ${ }_{c}^{G}$. Some related work concerning Bayesian inference and optimization is discussed in Section V. Finally, Section VI presents some conclusions and outlines directions of future work. ## II. Maximum Likelihood and Bayesian Model Fitting in EDAs ## A. Framework Consider the optimization problem $$\n\\min (f(\\mathbf{x})), \\mathbf{x} \\in \\mathcal{X}\n$$ where $f(\\mathbf{x})$ is the fitness or objective function, $\\mathbf{x}$ is an individual solution point and $$\n\\hat{\\theta}=\\arg \\max _{\\theta} p\\left(\\left\\{\\mathbf{x}^{1}, \\ldots, \\mathbf{x}^{M_{\\text {std }}}\\right\\} \\mid \\theta\\right)\n$$ is the feasible search space. EDAs build a probabilistic model $p_{t}()$ over $D=$ at each generation $t$ of the algorithm based on selected individuals.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:07",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs that utilize Gaussian Mixture Models, as exemplified by BGMMEDA and MB-GNG?",
      "topics": [
        "optimization"
      ],
      "answer": "The primary objective is to estimate the underlying probability distribution of promising solutions by modeling the search space with a mixture of Gaussian distributions. Algorithms like BGMMEDA use boosting to estimate the Gaussian mixture, while MB-GNG employs growing neural gas to determine the location of the mixture components, adapting its topology to minimize error and account for outliers. These methods aim to capture the structure of the problem to guide the search process more effectively than simple sampling.",
      "chunk_id": 10973,
      "paper_id": "A review on probabilistic graphical models in evolutionary computation",
      "paper_title": "A review on probabilistic graphical models in evolutionary computation",
      "paper_year": "2012",
      "chunk_content": "rBOA (Ahn et al. 2004) first learns a GBN to obtain a decomposition of the problem variables into smaller subproblems. Then, a separate mixture of GBNs is learnt for each of the subproblems by clustering the solutions in that subproblem. In BGMMEDA (Li et al. 2006), instead of clustering the samples, a boosting technique is applied to estimate a Gaussian mixture model. MB-GNG (Marti et al. 2011) adopts growing neural gas, a specific single-layer neural network, to determine the location of the components of the mixture of Gaussian distributions. This model learning algorithm is sensitive to, and therefore does not neglect, outliers and is able to automatically adapt its topology while decreasing the accumulated error of the network nodes. The multi-model EDA framework (Weise et al. 2011) extends these mixture methods by applying traditional EA recombination operators to the individual models learnt for each of the clusters in order to improve search space exploration. RM-MEDA (Zhang et al. 2008) learns a piece-wise continuous manifold for multiobjective optimization using the local principle component analysis algorithm. Each model component consists of a hyper-rectangle with a Gaussian noise. # 3.2.2 Other modeling approaches Posik $(2008,2009$ a) proposed the use of Cauchy distribution for the purpose of preventing premature convergence.",
      "question_type": "synthesis summary",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:09",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of incorporating copulas, such as Gaussian copulas or Archimedean copulas, differ from the approach of using multivariate elliptical copulas to address premature convergence?",
      "topics": [
        "planning"
      ],
      "answer": "EDAs can incorporate different types of copulas. Some EDAs use Gaussian copulas or multivariate extensions of Archimedean copulas. To address the drawback of premature convergence, a new EDA based on multivariate elliptical copulas was presented. This approach involves estimating the parameters of the multivariate elliptical copulas and using a population diversity technique. This differs from simply using Gaussian or Archimedean copulas by explicitly focusing on parameter estimation and maintaining diversity to avoid getting stuck in local optima.",
      "chunk_id": 18369,
      "paper_id": "An Estimation of Distribution Algorithm With Filtering and Learning",
      "paper_title": "An Estimation of Distribution Algorithm With Filtering and Learning",
      "paper_year": "2021",
      "chunk_content": "An EDA based on Gaussian copulas was proposed in [36] to solve multiobjective problems and applied to RFID network planning. An EDA using multivariate extension of the Archimedean copula was presented in [37]. A hybrid algorithm was proposed in [38] combining Archimedean copula-based EDA and artificial bee colony algorithm to achieve faster convergence. A new EDA based on parallel copula was also proposed to improve the efficiency [39]. These algorithms were tested on a small number of benchmark functions. To deal with the major drawbacks, including too much emphasis on parameters and premature convergence, a new EDA based on multivariate elliptical copulas was presented, where the parameters were estimated and a population diversity technique was used [40]. EDAs have also been applied to solve problems in practical settings. For example, an EDA was proposed in [41] to solve problems involving various types of constraints, while EDAs were applied in [42] and [43] to solve flow-shop scheduling problems. ## C. Kalman Filtering Process Kalman filtering [44] is a recursive estimator that uses a series of predictions and observations to make estimation of unknown variables. For a discrete-time linear system, Kalman filtering model is used to predict the true state $\\mathbf{x} \\in \\mathbb{R}^{n}$ at time $\\mathbf{y} \\in \\mathbb{R}^{m}$ from the state at $(k-1)$ according to a linear stochastic difference equation $$\n\\mathbf{x}(k)=\\mathbf{A} \\mathbf{x}(k-1)+\\mathbf{B} \\boldsymbol{\\vartheta}(k)+\\mathbf{w}(k)\n$$ ## The Kalman filtering process 1: Set initial values $\\mathbf{x}(0)$ and $\\mathbf{P}(0)$ for state variable and error covariance; Set generation number $k=1$.",
      "question_type": "relationship",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:11",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners approach the generation of the next population from the current one, given the advantages EDAs offer over NSGA-II?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "NSGA-II",
        "optimization"
      ],
      "answer": "Practitioners implementing an EDA should replace the application of variation operators, as used in algorithms like NSGA-II, with a method that learns and simulates a joint probability distribution. This distribution is based on individuals selected from the current population. This approach avoids the need to design variation operators tailored to the optimization problem and reduces the number of parameters that the user needs to assess.",
      "chunk_id": 11445,
      "paper_id": "Optimal Genes Selection with a New Multi-objective Evolutional Algorithm Hybriding NSGA-II with EDA",
      "paper_title": "Optimal genes selection with a new multi-objective evolutional algorithm hybriding NSGA-II with EDA",
      "paper_year": "2008",
      "chunk_content": "According to the complexity of probability models for learning the interdependencies between the variables from the selected individuals, EDAs can be categorized to dependency-free, bivariate dependencies and multivariate dependencies. ### 3.1 Comparison of NSGA-II and EDA The same with other general EAs, NSGA-II uses traditional variation operators to generate the offspring population. Assignment of parameters, like replacement rate, crossover rate, and mutation rate, determines the final performance of the algorithm. Setting proper parameters is not easy for inexperienced users. On the other hand, neglect of the relations between variables in an individual may lead to blindness of searching for optimal individuals. These two main shortcomings of NSGA-II motivate us to combine NSGA-II and EDA, whose advantages just make up shortcomings of NSGA-II. Compared to the rest of EAs including NSGA-II, the characteristic of EDAs is that EDAs replace the application of variation operators in order to generate the next population from the current one at each iteration by learning and subsequent simulation of a joint probability distribution for those individuals selected from the current population by means of the selection method. It results in two important advantages of EDAs over NSGA-II: The sometimes necessary design of variation operators tailored to the particular optimization problem at hand is avoided, and the number of parameters to be assessed by the user is reduced.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:13",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Estimation of Distribution Algorithms, how is the Markov fitness model (MFM) used as a surrogate model, and what is the bound on its error?",
      "topics": [
        "fitness"
      ],
      "answer": "The Markov fitness model (MFM) is utilized as a $k$-error surrogate, where the model is based on a Markov network formed by a set of maximal cliques $K=\\left\\{K_{1}, \\ldots, K_{m}\\right\\}$. The fitness function model is given by $-\\ln (f(\\mathbf{x}))=\\sum_{i} \\alpha_{i} V_{K_{i}}(\\mathbf{x})$, where $V_{k}$ are the characteristic functions of a Walsh decomposition of the fitness function, and $\\alpha_{i}$ are the model coefficients. The MFM provides a [FORMULA50]-error surrogate model where [FORMULA51] can be estimated as the greatest difference, among all the solutions, between the original fitness value and its approximation.",
      "chunk_id": 475,
      "paper_id": "Critical Issues in Model-Based Surrogate Functions in Estimation of Distribution Algorithms",
      "paper_title": "Critical Issues in Model-Based Surrogate Functions in Estimation of Distribution Algorithms",
      "paper_year": "2013",
      "chunk_content": "Examples of models that can be respectively analyzed as a $k$-error surrogate and an $[14,16]$-ranking surrogate are the Markov fitness model (MFM) model [5] and the Boltzmann distribution used in the context of of EDAs [28]. The MFM is based on a Markov network formed by a set of maximal cliques $K=\\left\\{K_{1}, \\ldots, K_{m}\\right\\}$. Then, for any solution $\\sum_{\\mathbf{x}} p(\\mathbf{x})=0.9$, the model of the fitness function is given by: $$\n-\\ln (f(\\mathbf{x}))=\\sum_{i} \\alpha_{i} V_{K_{i}}(\\mathbf{x})\n$$ where $V_{k}$ are the characteristic functions of a Walsh decomposition of the fitness function, and $\\alpha_{i}$ are the model coefficients [5]. Given a sufficiently-sized set of solutions and their fitness, the MFM can be found solving a system of equations in the parameters. The Boltzmann probability distribution $\\hat{p}(\\mathbf{x})$ is defined as $$\n\\hat{p}(\\mathbf{x})=\\frac{e^{\\frac{f(x)}{T}}}{\\sum_{\\mathbf{x}^{\\prime}} e^{\\frac{f\\left(\\mathbf{x}^{\\prime}\\right)}{T}}}\n$$ where $\\sum_{\\mathbf{x}^{\\prime}} e^{\\frac{f\\left(\\mathbf{x}^{\\prime}\\right)}{T}}$ is the so-called partition function, and $T$ is the temperature of the system that can be used as a parameter to smooth the probabilities. By definition, the MFM provides a [FORMULA50]-error surrogate model where [FORMULA51] can be estimated as the greatest difference, among all the solutions, between the original fitness value and its approximation.",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:15",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What computational trade-offs are described between the BOA algorithm using the K2 scoring metric and a greedy search procedure, specifically concerning the maximum number of incoming edges to a node?",
      "topics": [
        "constraints"
      ],
      "answer": "The BOA algorithm uses a scoring metric (K2) and a search procedure (greedy algorithm) to learn the Bayesian Network structure. The text states that the maximum number of incoming edges to a node directly influences the complexity of the algorithm for constructing the network and generating offspring. Therefore, restricting the number of incoming edges reduces the computational complexity of both network construction and offspring generation within the BOA framework.",
      "chunk_id": 11014,
      "paper_id": "Modified Bayesian optimization algorithm for EMC complex system design",
      "paper_title": "Modified Bayesian Optimization Algorithm for EMC Complex System Design",
      "paper_year": "2012",
      "chunk_content": "The parameters are represented by the conditional probabilities for each variable given any instance of variables that this variable depends on. Mathematically, a BN encodes the joint probabilistic distribution: $$\np(\\mathbf{C})=\\prod_{i=1}^{n} p\\left(X_{i} \\mid \\pi_{i}\\right)\n$$ Where $\\mathrm{X}=\\left(X_{0}, X_{0}, \\ldots, X_{k}\\right)$ is the vector of variable of problem, $\\pi_{i}$ is the set of parents of variable $X_{i}$, and $p\\left(X_{i} \\mid \\pi_{i}\\right)$ is the conditional probability of $\\varepsilon_{c p}$ conditioned on the variables $\\varepsilon_{r}$. In BOA, both structure and parameters of BN are learned in order to best fit the promising solution. There are two basic components of the algorithm for learning BN: scoring metric and search procedure. The scoring metric quantifies the quality of the given network. Prior knowledge about the problem can be incorporated into the metric as well. The search engine is used to explore the space of all possible networks in order to maximize the value of scoring metric as high as possible. The exploration is usually restricted by problem constraints i.e. maximum number of incoming edge to one node. This number directly influences the complexity of algorithm for constructing network, and generating offspring. In this work, we use K2 as scoring metric [4] and greedy algorithm as search procedure. The next section will explain how to generate new offspring using distribution extracted from constructed network.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:16",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model described, focusing on the Bayesian inference components and the formation of the dominant group?",
      "topics": [
        "control"
      ],
      "answer": "The EDA model execution involves these steps: First, establish a probability distribution estimation model based on Bayesian inference. This model generates a posteriori probability model, incorporating prior and conditional distributions. New individual generators are then formed into a dominant group. The information from each individual within this dominant group is subsequently used to update the probability model. Finally, the updated posteriori probability is used for sampling until an optimal solution for the distributed generation optimal allocation is obtained, minimizing the loss function denoted as min f = min(T_Loss).",
      "chunk_id": 21930,
      "paper_id": "Applied Research on Distributed Generation Optimal Allocation Based on Improved Estimation of Distribution Algorithm",
      "paper_title": "Applied Research on Distributed Generation Optimal Allocation Based on Improved Estimation of Distribution Algorithm",
      "paper_year": "2018",
      "chunk_content": "Articles ## Applied Research on Distributed Generation Optimal Allocation Based on Improved Estimation of Distribution Algorithm Lei Yang ${ }^{1}$, Xiaohui Yang ${ }^{1, *}$, Yue $\\mathrm{Wu}^{2}$ and Xiaoping Liu $$\n\\min f=\\min \\left(T_{\\text {Loss }}\\right)\n$$1 College of Information Engineering, Nanchang University, Nanchang 330031, China; 416114316029@email.ncu.edu.cn (L.Y.); xpliu@sce.carleton.ca (X.L.)2 State Grid Jilin Electric Power Company, Changchun 130000, China; 416114316039@email.ncu.edu.cn* Correspondence: yangxiaohui@ncu.edu.cn Received: 28 July 2018; Accepted: 24 August 2018; Published: 7 September 2018 #### Abstract Most of the current algorithms used to solve the optimal configuration problem in the distributed generation (DG) of electricity depend heavily on control parameters, which may lead to local optimal solutions. To achieve a rapid and effective algorithm of optimized configuration for distributed generation, a hybrid approach combined with Bayesian statistical-inference and distribution estimation is proposed. Specifically, a probability distribution estimation model based on the theory of Bayesian inference is established, then a posteriori probability model with the prior distribution and the conditional distribution is generated, and new individual generators are formed into a dominant group. The information of each individual of this dominant group is used to update the probability model and the updated posteriori probability is used for sampling until the optimal solution is obtained.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:18",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the effectiveness of the EDA model in the FBG sensor network, as described?",
      "topics": [
        "optimization"
      ],
      "answer": "The effectiveness of the EDA model is evaluated based on the Bragg wavelength detection error. The described method achieves a Bragg wavelength detection error of below 1 picometer when the reflection spectra are distorted. Localization error is also used, with the method achieving an average localization error of less than 2 mm when the reflection spectra of five FBGs are distorted in a nine-FBGs sensor network.",
      "chunk_id": 5527,
      "paper_id": "Distortion Tolerant Method for Fiber Bragg Grating Sensor Network Using Estimation of Distribution Algorithm and Convolutional Neural Network",
      "paper_title": "Distortion Tolerant Method for Fiber Bragg Grating Sensor Network Using Estimation of Distribution Algorithm and Convolutional Neural Network",
      "paper_year": "2024",
      "chunk_content": "As the number of distortions in the FBG network increases, our method's effectiveness diminishes. It is primarily suited for networks with a small number of distorted FBG nodes. In such scenarios, it can play a significant role in extending the measurement system's life and maintaining accuracy. But, when distortions become too concentrated, or if there is a significant increase in the number of distorted nodes, the system's measurement error increases correspondingly. ## VI. CONCLUSION In this article, we present a localization technique with self-healing ability based on EDA and CNN for FBG sensor networks with distorted spectra. To ensure the reliability of the sensing information, EDA is utilized to determine the Bragg wavelength of the distorted spectrum caused by FBG failed from the perspective of function optimization. The CNN model is trained by learning from the wavelength data of FBGs in the sensor network and applied to quickly identify the position of external load based on the detection results of EDA. The experimental results show that the average localization error of the proposed method is less than 2 mm when the reflection spectra of five FBGs are distorted in nine-FBGs sensor network. The sensing information can be accurately obtained with the Bragg wavelength detection error of below 1 pm in spectral distortion.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:19",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs regarding the Sampling block, specifically contrasting HMOBEDA with alternative approaches?",
      "topics": [
        "objectives"
      ],
      "answer": "The primary objective is to generate new individuals by sampling from a joint probability distribution, $p(\\mathbf{y})$, factorized using a Bayesian Network (BN) structure that encodes dependencies between objectives ($Z_r$), decision variables ($X_q$), and LS parameters ($P_l$). The key difference between HMOBEDA and alternative approaches lies in how evidences are fixed: HMOBEDA uses maximum objective values (ideal point), while alternatives use parameters and probabilities from the CPT or combinations of maximum and minimum objective values to approximate the Pareto Front.",
      "chunk_id": 14334,
      "paper_id": "Probabilistic Analysis of Pareto Front Approximation for a Hybrid Multi-objective Bayesian Estimation of Distribution Algorithm",
      "paper_title": "Probabilistic Analysis of Pareto Front Approximation for a Hybrid Multi-objective Bayesian Estimation of Distribution Algorithm",
      "paper_year": "2017",
      "chunk_content": "$\\mathbf{Y}=\\left(Y_{1}, \\ldots Y_{M}\\right)=\\left(Z_{1}, \\ldots Z_{R}, X_{1}, \\ldots X_{Q}, P_{1}, \\ldots P_{L}\\right)$ ). This way the BN structure encodes a factorization of the joint probability distributions or the probability mass function (pmf) given by: $$\np(\\mathbf{y})=\\prod_{r=1}^{R} p\\left(z_{r} \\mid \\mathbf{p a}_{r}^{B}\\right) \\cdot \\prod_{q=1}^{Q} p\\left(x_{q} \\mid \\mathbf{p a}_{q}^{B}\\right) \\cdot \\prod_{l=1}^{L} p\\left(p_{l} \\mid \\mathbf{p a}_{l}^{B}\\right)\n$$ where $\\mathbf{p a}_{r}^{B}, \\mathbf{p a}_{q}^{B}$ and $\\mathbf{p a}_{l}^{B}$ represent combinations of values for the parents of objective, decision variable and LS parameter nodes respectively, with $\\mathbf{P a}_{r}^{B}=\\emptyset, \\mathbf{P a}_{q}^{B} \\subseteq\\left\\{Z_{1}, \\ldots Z_{R}\\right\\}$ and $\\mathbf{P a}_{l}^{B} \\subseteq\\left\\{Z_{1}, \\ldots Z_{R}\\right\\}$. New $N_{\\text {smp }}$ individuals are generated from this joint probability distribution using probabilistic logic sampling, in the Sampling block. As previously discussed, the main difference between the two proposed approaches and the original version of HMOBEDA described in [7] concerns the Sampling block. HMOBEDA considers evidences fixed as maximum values for all objectives (i.e. the ideal point [FORMULA96] ). [FORMULA97] considers evidences fixed according to the parameters and probabilities estimated to compose the CPT. And finally, [FORMULA98] considers evidences fixed as combinations (all of them with the same probability of occurrence) of maximum and minimum values for the objectives, i.e., the ideal point [FORMULA99] plus the estimated extreme points of the current approximation of the Pareto Front. These values are uniformly distributed according the number of objectives in each generation (excluding the combination with minimum values for all objectives).",
      "question_type": "application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:21",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the representation and manipulation of job sequences \\(J_{1}, J_{2}, \\ldots, J_{n}\\) in the context of the Distributed Assembly Permutation Flow-Shop Scheduling Problem (DAPFSP), where the goal is to minimize the makespan across \\(F\\) factories each containing \\(m\\) machines?",
      "topics": [
        "mutation"
      ],
      "answer": "The mathematical principles governing job sequence representation in the DAPFSP, relevant to EDAs, center on permutation encoding. Given \\(n\\) jobs \\(J_{1}, J_{2}, \\ldots, J_{n}\\), each requiring a sequence of \\(m\\) operations \\(O_{i,1}, O_{i,2}, \\ldots, O_{i,m}\\), the EDA must effectively model the probability distribution over all possible job permutations. The processing time \\(t_{i,j}\\) for operation \\(O_{i,j}\\) on machine \\(M_{j}\\) influences the fitness of a given permutation. The assembly time \\(t_{i}^{A}\\) at the assembly machine \\(M_{A}\\) further contributes to the overall makespan. The EDA's distribution model must capture dependencies between job order and processing times across the \\(F\\) factories to efficiently sample promising job sequences.",
      "chunk_id": 25849,
      "paper_id": "An Estimation of Distribution Algorithm-Based Memetic Algorithm for the Distributed Assembly Permutation Flow-Shop Scheduling Problem",
      "paper_title": "An Estimation of Distribution Algorithm-Based Memetic Algorithm for the Distributed Assembly Permutation Flow-Shop Scheduling Problem",
      "paper_year": "2016",
      "chunk_content": "Finally, the conclusion is given in Section V. ## II. Distributed Assembly Permutation Flow-Shop Scheduling Problem The DAPFSP [24] is illustrated in Fig. 1. There are $n$ jobs $\\left\\{J_{1}, J_{2}, \\ldots, J_{n}\\right\\}$ to be processed and assembled to produce $S$ final products $\\left\\{P_{1}, P_{2}, \\ldots, P_{S}\\right\\}$. Each product consists of some defined jobs and each job belongs to a defined product, i.e., $\\sum_{r=1}^{S}\\left|P_{r}\\right|=n$. The production procedure consists of two stages: processing and assembly ones. There are $F$ factories at the processing stage. Each factory contains $m$ machines $\\left\\{M_{1}, M_{2}, \\ldots, M_{m}\\right\\}$. Each job $J_{i}$ requires a sequence of $\\left\\{O_{i, 1}, O_{i, 2}, \\ldots, O_{i, m}\\right\\}$ operations $\\left\\{O_{i, 1}, O_{i, 2}, \\ldots, O_{i, m}\\right\\}$ to be processed one after another in any one of the factories. Operation $O_{i, j}$ is executed on machine $M_{j}$ with processing time $t_{i, j}$. Besides, the jobs cannot be transferred to another factory during the processing procedure. As for the assembly stage, an assembly machine $M_{A}$ in an assembly factory assembles all jobs into products. After all the corresponding jobs of product $P_{r}$ are finished at the processing stage, they are assembled with an assembly time $t_{i}^{A}$.",
      "question_type": "theory",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:24",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm like PMBGNP, how should practitioners address the bootstrap problem, especially when using a small population size?",
      "topics": [
        "evolutionary"
      ],
      "answer": "To address the bootstrap problem in PMBGNP with a small population size, practitioners can apply genetic operations to probability vectors, denoted as $P_{r}$, to maintain population diversity. The new populations are generated by sampling the probability vectors $\\sigma_{n}^{t}(i, k, j)$. This involves applying selection, crossover, and mutation to the probability vectors to produce new probability vectors $P_{r}^{\\prime}$, guiding the evolution based on the structure of promising individuals.",
      "chunk_id": 14682,
      "paper_id": "Probabilistic model building Genetic Network Programming using multiple probability vectors",
      "paper_title": "Probabilistic Model Building Genetic Network Programming Using Multiple Probability Vectors",
      "paper_year": "2010",
      "chunk_content": "On the other hand, with a small population size, the bootstrap problem ${ }^{1}$ will be significantly emerged in Evolutionary Robotics [13]. Therefore, if the initial population has poor performances, the outstanding individuals would hardly be generated by PMBGNP. One possible solution to overcome this problem is to simply increase the population size, however, which will be with the increasing cost of computational time. In this paper, another solution is proposed by applying genetic [^0] [^0]: $A(i, k)$ The bootstrap problem is often described as that: if all individuals from the first initialized population perform equally poorly, the evolutionary process would hardly generate interesting solutions. [IMAGE_1] Fig. 2. Flowchart of PMBGNP $\\delta_{n}^{t}(i, k, j):$. operations to the probability vectors, which will not only maintain and improve the diversity of population, but also keep the small population size. The selection, crossover and mutation are applied to the constructed probability vectors $P_{r}$ to produce new probability vectors $P_{r}^{\\prime}$. The new $\\sigma_{n}^{t}(i, k, j)$ populations will be generated by sampling the probability vectors $\\sigma_{n}^{t}(i, k, j)=\\ell \\quad$. Crossover and mutation are used to generate the new population in GNP, as a result, the strongly related sub-structures sometimes will be broken down to produce uninteresting individuals, while the probability model is carried out by learning the structure of promising individuals to guide the evolution in PMBGNP $n$.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:25",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the discrete compact Genetic Algorithm (cGA) used on the discrete OneMax problem, what modifications were made to the general version of the discrete cGA presented in Section 3.2 for this analysis, and how might these changes impact its theoretical convergence properties compared to the original algorithm?",
      "topics": [
        "analysis"
      ],
      "answer": "For the discrete OneMax problem, the general version of the discrete cGA was modified by removing the problem-dependent operations: `smartInitialization()`, `crossover()`, `mutationOne()` and `mutationTwo()` functions. The resulting algorithm is similar to Algorithm 1 but is able to handle integer variables in the interval [FORMULA94]. The impact on theoretical convergence is not explicitly stated, but removing these operations suggests a move towards a more general, potentially less specialized, optimization process. Without the problem-specific heuristics, the convergence might be slower or require a larger population size to achieve similar performance compared to a version with these operations included.",
      "chunk_id": 21052,
      "paper_id": "A GPU-Enabled Compact Genetic Algorithm for Very Large-Scale Optimization Problems",
      "paper_title": "A GPU-Enabled Compact Genetic Algorithm for Very Large-Scale Optimization Problems",
      "paper_year": "2020",
      "chunk_content": "and G.I.; Writing--review \\& editing, G.I. Both authors contributed equally to this work. Both authors have read and agreed to the published version of the manuscript. Funding: This research received no external funding. Acknowledgments: We gratefully acknowledge the support of NVIDIA Corporation with the donation of the TITAN Xp GPU used for this research. Conflicts of Interest: The authors declare no conflict of interest. # Appendix A. Additional Results on OneMax Problem ## Appendix A.1. Discrete OneMax We present here additional results obtained with the discrete cGA on a discrete version of the OneMax problem. For this analysis, we used the general version of the discrete cGA presented in Section 3.2, where we removed all the problem-dependent operations implemented for the casting scheduling problem, namely the smartInitialization(), crossover(), mutationOne() and mutationTwo() functions. The algorithm obtained has a structure similar to Algorithm 1, but it is able to handle integers variables. Similar to the casting scheduling problem, we considered integer variables in the interval [FORMULA94]. - Results: We performed 10 runs on four dimensionalities, setting the virtualPopulation to 100. The maximum number of iterations was set to 5000 for problem instances up to 32 M variables and 1000 for the 1B case.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:27",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of UMDA compare to GCEDA, CVEDA, and DVEDA in optimizing the Sphere function, specifically regarding the number of function evaluations required?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "UMDA",
        "GCEDA",
        "CVEDA",
        "DVEDA",
        "function evaluations",
        "optimization"
      ],
      "answer": "UMDA exhibits the best behavior in terms of the number of function evaluations when optimizing the Sphere function. While GCEDA, CVEDA, and DVEDA also find the global optimum in all 30 independent runs, they require a greater number of function evaluations compared to UMDA. This difference arises because UMDA only considers marginal information, which is sufficient for the Sphere function due to the lack of strong dependencies between variables. In contrast, GCEDA, CVEDA, and DVEDA require the calculation of more parameters to represent relationships between variables, necessitating larger populations to compute them reliably, and thereby increasing the number of function evaluations.",
      "chunk_id": 2291,
      "paper_id": "copulaedas An R Package for Estimation of Distribution Algorithms Based on Copulas",
      "paper_title": "copulaedas: An R Package for Estimation of Distribution Algorithms Based on Copulas",
      "paper_year": "2014",
      "chunk_content": "If various processing units are available, it can be speeded up significantly by running the experiments in parallel. The snow package (Tierney, Rossini, Li, and Sevcikova 2013) offers a great platform to achieve this purpose, since it provides a high-level interface for using a cluster of workstations for parallel computations in R. The functions clusterApply or clusterApplyLB can be used to call runExperiment for each element of the experiments list in parallel, with minimal modifications to the code presented here. A summary of the results of the algorithms with the critical population size is shown in Table 3. Overall, the five algorithms are able to find the global optimum of Sphere in all the 30 independent runs with similar function values but only GCEDA, CVEDA and DVEDA optimize Summation Cancellation. In the rest of this section we provide some comments about results of the algorithms on each function. UMDA exhibits the best behavior in terms of the number of function evaluations in Sphere. There are no strong dependences between the variables of this function and the results suggest that considering the marginal information is enough to find the global optimum efficiently. The rest of the algorithms being tested require the calculation of a greater number of parameters to represent the relationships between the variables and hence larger populations are needed to compute them reliably (Soto et al.",
      "question_type": "COMPARATIVE EVALUATION",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:29",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model described, specifically regarding the three phases and their failure conditions?",
      "topics": [
        "Estimation of Distribution Algorithms"
      ],
      "answer": "The EDA model proceeds through three phases. Phase 1 continues until the substring position exceeds $2n/3$, failing if the initial search point's substring position is higher than $\\delta(x_i | 1)$. Phase 2 lasts while the substring position is at least $\\delta(x_i | 1)$ and less than $\\mathbf{P}_{t}(\\mathbf{x})$, failing if it ends with fewer than $n/(12(1+e^{\\chi}))$ 0-bits in the prefix. Phase 3 starts with substring position $\\omega(n^{2+\\beta} \\log n)$ and at least $n/((12(1+e^{\\chi}))$ 0-bits in the prefix, continuing until the optimum is found, and failing if it lasts less than $\\tau$ iterations.",
      "chunk_id": 14068,
      "paper_id": "When is an estimation of distribution algorithm better than an evolutionary algorithm",
      "paper_title": "When Is an Estimation of Distribution Algorithm Better than an Evolutionary Algorithm?",
      "paper_year": "2009",
      "chunk_content": "Phase 1 starts with the initial iteration and lasts until the substring position is higher than $2 n / 3$. A failure occurs in Phase 1 if the initial search point has substring position higher than $\\delta\\left(x_{i} \\mid 1\\right)$. Phase 2 lasts as long as the substring position is at least $$\n\\delta\\left(x_{i} \\mid 1\\right):= \\begin{cases}1, & x_{i}=1 \\\\ 0, & x_{i}=0\\end{cases}\n$$ and less than $\\mathbf{P}_{t}(\\mathbf{x}):=\\left(p_{t, 1}\\left(x_{1}\\right), p_{t, 2}\\left(x_{2}\\right), \\ldots, p_{t, n}\\left(x_{n}\\right)\\right)$. A failure occurs in Phase 2 if the phase ends with a search point having less than $n /\\left(12\\left(1+e^{\\chi}\\right)\\right)$ 0 -bits in the prefix. Phase $\\beta$ begins with a search point with substring position $\\omega\\left(n^{2+\\beta} \\log n\\right), M=\\omega\\left(n^{2+\\beta} \\log n\\right)$ and at least $n /\\left(\\left(12\\left(1+e^{\\chi}\\right)\\right)\\right.$ 0 -bits in the prefix and lasts until the optimum has been found. A failure occurs in Phase 3 if the phase lasts less than $\\tau$ iterations for some $c$. The theorem follows if we can prove that the probability of failure in any of the three phases is exponentially small. By union bound, the failure probability in Phase 1 is less than $n 2^{-\\alpha n}=e^{-\\Omega(n)}$. To bound the failure probability in Phase 2, first note that by using the same arguments as in the proof of Theorem 17 in Droste et al [5], it can be shown that there exists a constant $c_{1}$ such that the probability that the duration of Phase 2 is shorter than $c_{1} n^{2}$ iterations is $(i \\in\\{1, \\ldots, n\\})$. Given that the duration of Phase 2 is at least $$\nT_{i}:=\\min \\left\\{t ; p_{t, i}\\left(x_{i}^{*}\\right)=1\\right\\}\n$$ with overwhelming probability, we are now in position to bound the failure probability during Phase 2.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:31",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the EDA's output be measured using the criteria of generating a new population of solutions?",
      "topics": [
        "crossover"
      ],
      "answer": "The quality of the EDA's output, specifically the new population of solutions, is measured by how well the probability distribution, estimated from selected promising solutions of the previous generation, guides the search toward a promising area. The new population is generated by sampling this probability distribution, and its effectiveness is determined by its ability to escape local minima and lead the Hopfield Neural Network (HNN) to better solutions.",
      "chunk_id": 13963,
      "paper_id": "Competitive Hopfield Network Combined With Estimation of Distribution for Maximum Diversity Problems",
      "paper_title": "Competitive Hopfield Network Combined With Estimation of Distribution for Maximum Diversity Problems",
      "paper_year": "2009",
      "chunk_content": "The local minimum problem is caused by the gradient descent dynamics of the update rule of the DHNN. When the network is trapped in a local minimum, a perturbation operator is applied to the local minimum to generate a new starting point for the HNN. It is desirable that the generated starting point should be in a promising area in the search space. Therefore, in this section, we propose an EDA mutation operator as the perturbation operator in the HNN. The EDA mutation operator can generate a new starting point for the further HNN search. It is expected that the further search is guided to a promising area by the probability model. In the proposed algorithm, the solution mutation or perturbation is always applied to the current local minima. In the following section, we first briefly review the EDA, and then, the DCHNN combined with the EDA is proposed. ## A. EDA EDA is a new area of evolutionary computation. In EDAs, there is neither crossover nor mutation operator. Instead, new population is generated by sampling the probability distribution which is estimated from the selected promising solutions or individuals of previous generation. Thus, these algorithms have a theoretical foundation in probability theory. An algorithmic framework of most EDAs can be described as follows.",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:32",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs, specifically concerning the evolution of IF-THEN-ELSE rules for predicting cyanobacteria abundance?",
      "topics": [
        "search"
      ],
      "answer": "The primary objective is to evolve a rule model for predicting cyanobacteria abundance using a Hybrid Evolutionary Algorithm (HEA). This HEA employs a two-layered approach: the outer layer uses Genetic Programming (GP) to search for optimal model structures, while the inner layer utilizes population-based optimization algorithms to determine the optimal continuous parameters for the model. The process takes water-quality data as input to predict Chl-a concentrations based on evolved IF-THEN-ELSE rules.",
      "chunk_id": 1192,
      "paper_id": "Parameter Optimization Algorithms for Evolving Rule Models Applied to Freshwater Ecosystems",
      "paper_title": "Parameter Optimization Algorithms for Evolving Rule Models Applied to Freshwater Ecosystems",
      "paper_year": "2014",
      "chunk_content": "In Section III, we present the experimental results using the six algorithms on two data sets and statistically compare their performances. In Section IV, further experiments are described based on the best models achieved in Section III. Finally, we draw some conclusions and identify future work in Section V. ## II. HyBRID Evolutionary Algorithm FOR Evolving IF-THEN-ElSE RULES A HEA was proposed to evolve the rule model for predicting the abundance of cyanobacteria. The HEA contains two layers. The first or outer layer searches for model structures using GP, while the second or inner layer searches for the optimal continuous parameters of the model using some populationbased optimization algorithms. Fig. 1 shows a conceptual diagram of HEA based on water-quality data as input and cyanobacterial abundance data as output. The details of the GP and the population-based algorithms are briefly described in what follows. IF $\\mathbf{i}(\\mathbf{W T}=22.5) \\mathrm{AND}(\\mathbf{p H}=10.8))$ OR $\\left(\\mathrm{TP} * \\mathrm{DO}==523.5\\right)$ ) THEN Chl-a $=\\mathrm{DO} * \\exp (\\mathrm{pH})+\\mathrm{NTU} * \\mathrm{SiO},{ }^{*} 34.5$ ELSE Chl-a $=\\mathrm{WT} / 2.1+\\ln \\left(\\mathrm{TP} * \\mathrm{pH}-25.6\\right)$ Fig. 2. Example of a rule model for predicting Chl-a concentrations. ## A. Structure Optimization of Rule Models 1) Model Representation: We used GP [18], [19] as the main technique for evolving the rule model structure.",
      "question_type": "process",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:34",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "For the specific EDAs mentioned, what fundamental properties distinguish the Decentralized Continuous EDA (DC-EDA) from the centralized continuous EDA, based on their performance in the described system?",
      "topics": [
        "comparison"
      ],
      "answer": "The Decentralized Continuous EDA (DC-EDA) and the centralized continuous EDA are compared based on Monte Carlo simulations. The results indicate that the DC-EDA converges faster. Furthermore, the DC-EDA finds better solutions, as evidenced by a smaller median solution, average solution, and standard deviation compared to the centralized EDA. These properties suggest the DC-EDA is more effective in this particular problem, which involves a multimodal and nonderivable function with a domain of [-5,5] for each dimension, population size N=1000, E=2, M=0.1N, and [FORMULA86].",
      "chunk_id": 17475,
      "paper_id": "A Decentralized Continuous Estimation of Distribution Algorithm for Networked Systems",
      "paper_title": "A Decentralized Continuous Estimation of Distribution Algorithm for Networked Systems",
      "paper_year": "2019",
      "chunk_content": "This function is a multimodal function and nonderivable. We use a domain of $[-5,5]$ for each dimension. For the system, the optimal solution is $\\mathbf{x}^{*}=0, F\\left(\\mathbf{x}^{*}\\right)=0$ without random offset and random rotation matrix. We set $N=1000, E=2, M=0.1 N$ and [FORMULA86] like the former problem. The results are as follows. [IMAGE5] Fig. 6: Example 2: Results for the system Figure 6 shows the best individual at each generation, averaged over 40 Monte Carlo simulations. From Figure 6, we can find that the DC-EDA converges faster than the centralized continuous EDA. More details of the solution and decision variables are shown in the following figure and tables. [IMAGE6] Fig. 7: Boxplot of 40 Monte Carlo Simulation Results for Centralized Continuous EDA and Decentralized Continuous EDA Table 2: Comparison between centralized continuous EDA and DC-EDA From Figure 7 and Table 2 we can find that the decentralized EDA can find better solutions than the centralized EDA in this problem. The median solution, average solution and standard deviation of the decentralized EDA are smaller than the centralized EDA.",
      "question_type": "advantage",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:36",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm using the BDe metric with a Dirichlet prior, how should practitioners calculate the BDe metric given the parameters $\\alpha_{m j k}$ and observed data?",
      "topics": [
        "algorithm"
      ],
      "answer": "To calculate the BDe metric, practitioners should use the formula \\begin{equation*} p(B \\mid P o p)=\\prod_{m=1}^{M} \\prod_{j=1}^{t_{m}} \\frac{\\Gamma(\\alpha_{m j})}{\\Gamma(\\alpha_{m j}+N_{m j})} \\prod_{k=1}^{s_{m}} \\frac{\\Gamma(\\alpha_{m j k}+N_{m j k})}{\\Gamma(\\alpha_{m j k})} \\end{equation*}, where $N_{m j k}$ represents the number of observations in the dataset $Pop$ for which $z_{R}(\\mathbf{x})$ assumes a certain value given its parents' values. The term $\\Gamma(x)$ is equivalent to $(x-1)!$, and $\\alpha_{m j} = \\sum_{k=1}^{s_{m}} \\alpha_{m j k}$. The product over $j$ iterates through all combinations of parents, and the product over $k$ iterates through all possible values of $x_q$. In the K2 metric, the Dirichlet parameters are often set to 1, indicating no prior information.",
      "chunk_id": 19147,
      "paper_id": "Analysis of Bayesian Network Learning Techniques for a Hybrid Multi-objective Bayesian Estimation of Distribution Algorithm a case study on MNK Landscape",
      "paper_title": "Analysis of Bayesian Network Learning Techniques for a Hybrid Multi-objective Bayesian Estimation of Distribution Algorithm: a case study on MNK Landscape",
      "paper_year": "2021",
      "chunk_content": "It assumes the Dirichlet distribution (with parameters $\\alpha_{m j k}$ ) as prior probability and a uniform prior distribution of all possible structures (Larranaga et al. 2012). The BDe metric is defined by Eq. 3: $$\np(B \\mid P o p)=\\prod_{m=1}^{M} \\prod_{j=1}^{t_{m}} \\frac{\\Gamma\\left(\\alpha_{m j}\\right)}{\\Gamma\\left(\\alpha_{m j}+N_{m j}\\right)} \\prod_{k=1}^{s_{m}} \\frac{\\Gamma\\left(\\alpha_{m j k}+N_{m j k}\\right)}{\\Gamma\\left(\\alpha_{m j k}\\right)}\n$$ where $N_{m j k}$ is the number of observations in the data set $P o p$ for which $\\left.z_{R}(\\mathbf{x})\\right): \\mathcal{B}^{N} \\rightarrow \\mathcal{R}^{R}$ assumes the $\\mathbf{x}$-th value given the $z_{r}(\\mathbf{x})$-th combination of values from its parents, with $\\mathbf{N}_{m j}=$ $\\left\\{N_{m j 1}, \\ldots, N_{m j s_{m}}\\right\\}, \\Gamma(x)=(x-1)$ ! and $\\alpha_{m j k}=\\sum_{k=1}^{s_{m}} \\alpha_{m j k}$. The product over $j \\in\\left\\{1, \\ldots, t_{m}\\right\\}$ runs for all combinations of parents of $$\n\\begin{aligned}\n& \\max _{\\mathbf{x}} \\mathbf{z}(\\mathbf{x})=\\left(z_{1}(\\mathbf{x}), \\ldots, z_{R}(\\mathbf{x})\\right) \\\\\n& \\text { subject to } \\mathbf{x} \\in\\{0,1\\}^{N} \\\\\n& \\text { with } \\\\\n& z_{r}(\\mathbf{x})=\\frac{1}{N} \\sum_{q=1}^{N} f_{r, q}\\left(x_{q}, \\Pi_{r}\\left(x_{q}\\right)\\right) \\\\\n& r \\in\\{1, \\ldots R\\} \\\\\n& q \\in\\{1, \\ldots N\\}\n\\end{aligned}\n$$ and the product over $k \\in\\left\\{1, \\ldots, s_{m}\\right\\}$ runs for all possible values of $x_{q}$. The Dirichlet parameter $[0,1]$ stands for prior information about the number of instances that have $r=1$ set to its $N=6$-th value and the set of parents of $K_{1}=2$ is instantiated to its $(r=1)$-th combination. In the so-called K2 metric (Cooper and Herskovits 1992) for instance, parameters $r=$ are set to 1 as there is no prior information about the problem, and Eq.",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:38",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, what characteristics define the Gaussian model used by [FORMULA169], and how does it differ from conventional ML estimator approaches for mean and variance estimation?",
      "topics": [
        "performance"
      ],
      "answer": "[FORMULA169] is an EDA variant for continuous optimization problems that uses a Gaussian model. However, unlike conventional Estimation of Distribution Algorithms, [FORMULA169] does not directly estimate the mean and variance using the conventional Maximum Likelihood (ML) estimator. The precise method by which [FORMULA169] estimates the mean and variance differently from the conventional ML estimator is not specified further in the provided content.",
      "chunk_id": 24784,
      "paper_id": "Enhance Continuous Estimation of Distribution Algorithm by Variance Enlargement and Reflecting Sampling",
      "paper_title": "Enhance Continuous Estimation of Distribution Algorithm by Variance Enlargement and Reflecting Sampling",
      "paper_year": "2016",
      "chunk_content": "All the model-based EAs, including CMA-ES, AMaLGaM, and UMDAc, satisfy this property. Even so, the above comparisons clearly demonstrate that [FORMULA161] performs significantly better than the three competitors. TABLE IV. The MEAN AND THE STANDARD DEVIATION (MEAN $\\pm$ StD DEV ( $\\gamma$-TEST)) OF FEV5 OF THE REST SOLUTIONS DERIVED FROM SIX ALGORITHMS ${ }^{*} N_{0}, N_{s}, N_{w}$ denote the numbers of functions on which the performance of an algorithm is significantly better, similar to, and significantly worse than that of $\\mathrm{EDA}_{s \\text { se-re }}$, respectively. The performance of an algorithm on a function is significantly better (worse) than that of [FORMULA166] at a level of 0.05 if the corresponding $s$-test value is less than -2.064 (greater than 2.064), otherwise we say that they have similar performance. ## V. CONCLUSION In this paper, a new EDA variant called [FORMULA168] is presented for continuous optimization problems. [FORMULA169] keeps using Gaussian model, but does not primly estimate the mean and the variance with the conventional ML estimator.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:40",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of the R-EDA model compare to the QLR model, based on the convergence rates provided when the exponent p is greater than 1?",
      "topics": [
        "bounds"
      ],
      "answer": "For an exponent p > 1, QLR is not optimal. R-EDA provably reaches a convergence rate defined by [FORMULA340], whereas QLR has a convergence rate of [FORMULA341]. This indicates that R-EDA performs better than QLR in this specific scenario.",
      "chunk_id": 23832,
      "paper_id": "Handling expensive optimization with large noise",
      "paper_title": "Handling Expensive Optimization with Large Noise",
      "paper_year": "2011",
      "chunk_content": "For the rightmost column, it is important to point out that we run experiments with QLR without knowledge of the parameter [FORMULA336], so that the comparison with other algorithms is fair. In particular, there is a single algorithm, R-EDA, which provably realizes the upper bounds above; a better algorithm should be better for all cases simultaneously without problem-specific parametrization. The original results of our paper are presented by three last rows and the rightmost column; in particular we have shown: - The upper and lower bounds for an exponent $p>1$; - For [FORMULA338] and [FORMULA339], QLR is not optimal; R-EDA reaches (provably) [FORMULA340] whereas QLR has convergence [FORMULA341]. By construction, it is probably difficult for QLR to do better than [FORMULA342]; - For [FORMULA343] and [FORMULA344], QLR and R-EDA perform equivalently $(1 / \\sqrt{n})$; the lower bound does not match the upper bound. For R-EDA we have a mathematical proof and for QLR empirical evidence. - For [FORMULA346] and [FORMULA347], QLR and R-EDA perform equivalently [FORMULA348]; the lower bound does not match the upper bound.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:42",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the DEUM model mentioned, specifically focusing on how the initial population restriction impacts the algorithm's ability to sample intervention points?",
      "topics": [
        "search"
      ],
      "answer": "The DEUM model, as referenced, relies on model building. A key algorithmic step involves creating an initial population of solutions. Restricting this initial population limits the algorithm's ability to sample a range of possible intervention points. In the early stages of evolution, DEUM operates on solutions that may use infeasible numbers of interventions. However, by convergence (around 200 generations), this process results in more accurate targeting of interventions, balancing objectives and leading to improved solutions.",
      "chunk_id": 11600,
      "paper_id": "Bio-control in mushroom farming using a Markov network EDA",
      "paper_title": "Bio-control in Mushroom Farming using a Markov Network EDA",
      "paper_year": "2008",
      "chunk_content": "This might reflect a pragmatic decision to encourage algorithms to generate only practicable solutions. The results show that, while such a control is useful for UC GA, and fairly neutral for TInSSel and CalEB, it actually impedes DEUM [FORMULA79]. This reflects the different dynamics of the search. For the UC GA, as the initialisation control is relaxed, more and more interventions creep into solutions and the weighted penalty is ineffective at removing them. Thus the quality of solutions that can be found deteriorates. For the GAs using the directed intervention crossovers, TInSSel and CalEB, the relaxation of initialisation control has little effect because the crossovers are designed to direct interventions to points in the schedule where they have best effect rather than to blindly accumulate them. For DEUM [FORMULA80], which relies on model building the restriction on the initial population restricts the ability of the algorithm to sample a range of possible intervention points. It is clear from the analyses in Section V that, in the early stages of the evolution, DEUM [FORMULA81] operates on solutions that use infeasible numbers of interventions. However by convergence at 200 generations, this has resulted in more accurate targeting of interventions that more successfully balances the two objectives, giving significantly better solutions.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:43",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of applying an Estimation of Distribution Algorithm to the redundancy allocation problem for a series-parallel system, what specific performance indicators related to reliability, availability, and maintainability should be considered to evaluate the effectiveness of the algorithm in optimizing system design?",
      "topics": [
        "design"
      ],
      "answer": "When evaluating an EDA applied to the redundancy allocation problem, key performance indicators include measures of system reliability, availability, and maintainability. Reliability assesses the probability of failure-free operation over a specified duration. Availability quantifies the proportion of time the system is operational and ready for use. Maintainability reflects the ease and speed with which the system can be restored to operational status following a failure. The objective is to maximize availability while minimizing cost, volume, or weight, which can be used as constraints or objectives in the EDA's optimization process.",
      "chunk_id": 9808,
      "paper_id": "Redundancy Allocation problem for a Series-Parallel system using Estimation of Distribution Algorithm",
      "paper_title": "Redundancy Allocation problem for a Series-Parallel system using Estimation of Distribution Algorithm",
      "paper_year": "2012",
      "chunk_content": "Redundancy Allocation problem for a Series-Parallel system using Estimation of Distribution Algorithm Haydee MeloGraduate School of Production,Information and SystemsWaseda University2-7 Hibikino, Wakamatsu, KitakyushuFukuoka, Japanmelo.haydee@asagi.waseda.jp #### Abstract Reliability is an engineering field that recently has captivated the attention of researches. Its goal is to develop new techniques to improve the security and performance of the systems. The increasing complexity in the systems as a result of growing technology makes them more susceptible for failures. In the redundancy allocation problem (RAP) its principal objective is to maximize the availability while reducing the cost, volume or weight of the system. In this research an Estimation-ofDistribution Algorithm (EDA) approach is proposed for solving the redundancy allocation problem for a series-parallel system. Index Terms-Reliability, Availability; Maintainability, Redundancy, Estimation of Distribution Algorithm. ## I. InTRODUCTION Redundancy is used to offer protection and safety to the system by keeping the critical production services working even though some parts of the system fails. To improve system performance is necessary to increase its reliability by increasing the design lifetime, eliminating or reducing the failures or risks, and increasing its operation time.",
      "question_type": "evaluation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:45",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs when designing a system, considering factors beyond just achieving higher availability?",
      "topics": [
        "constraints"
      ],
      "answer": "The algorithmic approach aims to design systems that not only maximize availability, represented by the formula involving the product of component reliabilities, but also consider constraints such as the total cost and weight of the system. The cost formula accounts for the cost of each component, and the approach recognizes that actions like component repair are taken to avoid system stoppages due to failures.",
      "chunk_id": 9816,
      "paper_id": "Redundancy Allocation problem for a Series-Parallel system using Estimation of Distribution Algorithm",
      "paper_title": "Redundancy Allocation problem for a Series-Parallel system using Estimation of Distribution Algorithm",
      "paper_year": "2012",
      "chunk_content": "Even if one of the elements failures, it does not affect the performance of the system. This configuration is used to allocate the redundant components. TABLE II BASIC CONCEPTS OF AVAILABILITY, RELIABILITY AND MAINTAINABILITY The formulas to calculate the Availability, Maintainability and Reliability of the system for each configuration in the RBD are given in table II . Therefore, by using the eq. (6) for a single component and the formulas in table II, the number of $x_{i j}$ components necessary in parallel in a series-parallel system is represented as follows: $$\n\\max _{x} A(x)=\\left[1-\\left[\\prod_{i=1}^{S} 1-\\prod_{j=1}^{n} 1-\\left[m_{i j}\\left(1-r_{i j}\\right)\\right]^{x_{i j}}\\right]\\right]\n$$ ## C. Constraints Desingning a system, our goal is not only to achieve a higher availability, but also the designer has to take into account the total cost and the weight of the system or other constraints in the design. [IMAGE_1] Fig. 2. MTRR and cost relationship In this case the cost formula only consider the cost of each component in the system, but in a system when a failure or certain component in the system may had a malfunction that lead to the stop or partial stop of the system, there are some action that are taken to avoid such occurrence as the repair of the component.",
      "question_type": "synthesis summary",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:46",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms that use Gaussian Mixture Models (GMM), what fundamental properties distinguish approaches that construct GMMs via clustering versus those that use the Expectation Maximization (EM) algorithm, specifically regarding prior knowledge requirements and convergence speed?",
      "topics": [
        "convergence"
      ],
      "answer": "EDAs constructing Gaussian Mixture Models (GMM) via clustering often require prior knowledge, such as a predefined cluster number or a minimal distance between clusters, as seen in some implementations. For example, Rival Penalized Competitive Learning (RPCL) is a clustering method that attempts to detect the real number of clusters without prior knowledge, though it may initially assign more clusters than actually present. In contrast, EDAs using the Expectation Maximization (EM) algorithm assume that the number of components in the GMM is known, which is often not the case. Furthermore, the EM algorithm is sensitive to initial parameters, making it prone to getting trapped in local maxima of the likelihood function. EDAs incorporating clustering techniques typically divide the probability density function estimation into two steps: clustering the selected individuals and estimating the distributions, which often necessitates larger populations and results in slower convergence speeds compared to EM-based approaches, assuming EM converges.",
      "chunk_id": 26593,
      "paper_id": "Continuous optimization based-on greedy estimation of GMM",
      "paper_title": "Continuous optimization based-on greedy estimation of GMM",
      "paper_year": "2005",
      "chunk_content": "While Gaussian Mixture Model(GMM) can model both distribution of variables and the interdependences among them, thus is adopted by later continuous EDAs ${ }^{[15][16][17][18]}$. Two different approaches are usually adopted to construct a Gaussian Mixture Model: One is by means of clustering; the other is by means of the expectation maximization (EM) algorithm. The clustering algorithms adopted by previous EDAs usually require prior knowledge, either a predefined cluster number or an minimal distance between different clusters, such as that in ${ }^{[15][17]}$. In [16], authors used another clustering method, Rival Penalized Competitive Learning (RPCL) ${ }^{[19]}$, which can detect the real number of clusters without any prior knowledge. This clustering method usually assigns a larger number of clusters than the actual number in the data set, at the end of clustering, some clusters are discarded according some criterion. In fact, all the EDAs incorporating clustering techniques divide the probability density function estimation step into two steps: cluster the selected individuals and estimate the distributions. So these EDAs usually need larger population and the convergence speed is slow. For EDAs, the EM algorithm has some limitations: It assumes that the number of components is known, which is usually not the case in the estimation task of EDAs; and it is sensitive to the initial parameters, which makes it easily get trapped in local maxima of the likelihood function.",
      "question_type": "definition",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:48",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm (EDA) within a Many-Objective Optimization Problem (MaOP), how should practitioners incorporate reference vectors to enhance selection pressure, specifically addressing the challenge of large search spaces?",
      "topics": [
        "search"
      ],
      "answer": "To enhance selection pressure when using an EDA for MaOPs, practitioners should incorporate a reference vector-based diversity mechanism. This involves selecting a group of neighbors using uniformly distributed reference vectors. To further improve selection pressure and prevent adverse DR phenomenon, a diversity repairing mechanism should be developed to push new solutions toward a closer proximity to the Pareto Front (PF). Dimension reduction techniques should be employed a priori to evolution to reduce the cost for exploration search. Convergence is guaranteed by repairing diversity and sampling solutions based on the reference vectors, while diversity is facilitated by selecting solutions with the nearest perpendicular distances to the reference vectors.",
      "chunk_id": 21301,
      "paper_id": "Improved Regularity Model-Based EDA for Many-Objective Optimization",
      "paper_title": "Improved Regularity Model-Based EDA for Many-Objective Optimization",
      "paper_year": "2018",
      "chunk_content": "$i>0,0<j<\\left|F_{i}\\right|$ Typically, these MaOEAs can be classified into three basic categories: Dominance-based, Decomposition-based and Hypervolume-based. MOEA/D and HypE are from the second and third categories, respectively. NSGA-III and GrEA are the hybridization of the first and the second categories. generate new solutions in the proposed algorithm are built based on a group of neighbors selected by a uniformly distributed reference vectors. In order to improve the selection pressure, diversity repairing mechanism is developed to prevent the adverse DR phenomenon in each generation and push the new solutions toward having a closer proximity to the PF. Furthermore, dimension reduction technique is employed priori to the evolution to reduce the cost for exploration search. Specifically, convergence in the proposed algorithm is guaranteed by repairing diversity and sampling solutions based on the reference vectors, while diversity is facilitated by selecting solutions with the nearest perpendicular distances to the reference vectors. Compared to traditional MaOEAs and EDAs, the contributions of the proposed algorithm are summarized as follows: 1) Extend the uses of regularity model-based EDAs to MaOPs. In addition, reference vectors-based diversity mechanism is incorporated into the proposed algorithm to enhance the selection pressure. 2) Large search space poses a challenge for regularity model-based EDAs as do to all MaOEAs.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:50",
      "generation_style": "practical_application"
    },
    {
      "question": "In the described Estimation of Distribution Algorithm (EDA), how is the probability vector $\\mathcal{P}_{\\text {Mutation}}$ updated using the PBIL model, and what role does the generation variable 'g' play in this update?",
      "topics": [
        "mutation"
      ],
      "answer": "The probability vector $\\mathcal{P}_{\\text {Mutation}}$ is updated according to Algorithm 5 (not provided in the chunk). The generation variable 'g' is incremented by 1 at each generation $\\beta(\\mathrm{g})$ via the update rule $\\mathrm{g} \\leftarrow \\mathrm{g}+1$. The learning rate changes dynamically with the generation, influencing the probability update. The parameter $\\Gamma$ controls the contributions of location and global information, balancing exploitation and exploration during the update of $\\mathcal{P}_{\\text {Mutation}}$.",
      "chunk_id": 16537,
      "paper_id": "An evolutionary algorithm based hyper-heuristic framework for the set packing problem",
      "paper_title": "An evolutionary algorithm based hyper-heuristic framework for the set packing problem",
      "paper_year": "2019",
      "chunk_content": "At each generation $\\beta(\\mathrm{g})$, a parent set, say Parent $(g)$, is formed by selecting the best $\\frac{N_{p}}{}$ candidate solutions from the current population $\\operatorname{Pop}(g)$. Variable $14: \\mathrm{g} \\leftarrow \\mathrm{g}+1$ is the learning rate, which changes dynamically with the generation. As $\\Gamma=$ controls the contributions of location and global information and helps to maintain a balance between exploitation and exploration. The pseudo-code of the probability update vector using the PBIL model is given in Algorithm 5.  After the probability vector $\\mathcal{P}_{\\text {Mutation }}$ is updated, the GM operator is applied to generate a new solution. Set $\\sum_{i=1}^{|\\Gamma|}\\left|\\mathcal{H}_{\\Gamma_{i}} \\cap \\Gamma\\right|>0$, initially empty, is a partial solution in which objects are added either by sampling the probability vector $\\mathcal{H}_{\\Gamma_{i}}$ or directly copying from the global best solution, say $\\mathcal{B}_{s}$.",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:52",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "In the described parallel Genetic Algorithm (GA) implementation, what is the role of Pthreads in managing the computation, and how does this compare to the clustering algorithm's use of Pthreads?",
      "topics": [
        "search"
      ],
      "answer": "The parallel GA uses 3 Pthreads to represent slave processes and 1 Pthread for the master process. In contrast, the clustering algorithm uses 5 Pthreads created by the Master to concurrently execute values of k (where k ranges from 2 to 6). The GA uses these threads to parallelize the evolutionary process itself, while the clustering algorithm uses threads to explore different parameter settings (k values) concurrently.",
      "chunk_id": 10407,
      "paper_id": "Asynchronous Strategy of Parallel Hybrid Approach of GA and EDA for Function Optimization",
      "paper_title": "Asynchronous Strategy of Parallel Hybrid Approach of GA and EDA for Function Optimization",
      "paper_year": "2012",
      "chunk_content": "GA was also executed using parallel Pthreads to match the conditions with other two algorithms. Furthermore the results are compared with best results of BBOB 2009. For every algorithm, each dimension was executed in 15 instances to try to reach target value $f_{\\mathrm{t}}$. ## A. Test Functions We used 24 benchmark functions whose parameter settings are well explained in [21], [23]. For every function the execution was done in dimensions $2,3,5,10,20$ and 40 in $[-55]$ search space. In 24 functions we have 5 subgroups categorized as separable functions, moderate functions, ill-conditioned functions, multi-modal functions and weakly structured multimodal functions. ## B. Algorithms Parameter Settings Crossover and mutation probabilities used in GAs are 1 and 0.08 respectively with population size of 50 . GA uses Stochastic Remainder method for selection. 3 Pthreads were used to represent slaves and 1 Pthread to represent master process. We used three iterations in each tactic (phase), hence making total of twelve sequentially executed iterations in every instance. The phase change was set to occurs after every three iterations. The clustering algorithm used values of $k>1$ from 2 to 6 executed concurrently by 5 Pthreads created by Master.",
      "question_type": "advantage",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:53",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model update, given a maximum depth constraint, using the matrix representation described?",
      "topics": [
        "representation"
      ],
      "answer": "The EDA model update, incorporating a maximum depth constraint, proceeds as follows: First, extract the update matrix from the grammar using equations 22 and 23. Next, formally define the set of rules that terminate at the specified depth using equation 24. Derive a general form for the rule probability, combining normal probabilities with depth-induced distortions, as shown in equation 25. Adjust the update matrix for distortion at each depth using equation 26. Iteratively apply this adjusted update matrix to derive rule frequencies in observed trees satisfying the depth constraint, as detailed in equations 27 and 28. Sum these frequencies across all available depths to obtain the overall rule frequencies (equation 29). Finally, calculate the relative rule probability by summing over all rules from the same nonterminal and normalizing (equation 30), and determine the bias as the difference between this probability and the SCFG-specified probability (equations 31 and 32).",
      "chunk_id": 25204,
      "paper_id": "Recursion-Based Biases in Stochastic Grammar Model Genetic Programming",
      "paper_title": "Recursion-Based Biases in Stochastic Grammar Model Genetic Programming",
      "paper_year": "2016",
      "chunk_content": "Fortunately, matrix representations can help. We already have the core update formula in matrix form, equation 4, from computational linguistics [23], [26]. Before presenting the detailed analysis, we outline the process informally. The matrix for update is extracted from the grammar (equations 22, 23). If a maximum depth is imposed, the frequency update represented by the matrix changes, so we give a formal definition of the set of rules that can still terminate at a given depth (equation 24), and use this to derive a general form for the rule probability combining the normal probabilities and the distortions induced by the depth limit. (equation 25). Using this form, we can obtain the update matrix adjusted by the distortion at each depth (equation 26). Repeatedly applying the update matrix, we can derive the frequency of rules in observed trees satisfying the depth constraint (equations 27, 28). Summing these frequencies over all available depths, we obtain the rule frequencies (equation 29). Summing over all rules from the same nonterminal and normalising give us the relative rule probability (equation 30). Finally, the bias is the difference between this probability and the probability specified in the SCFG (equations 31, 32).",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:53:56",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of the EDAPSO algorithm, how is the exploration versus exploitation trade-off managed, and what parameter controls this balance?",
      "topics": [
        "optimization"
      ],
      "answer": "The exploration versus exploitation trade-off in EDAPSO is managed using the inertia weight, denoted as $\\omega$. Increasing this factor enhances exploration, while decreasing it enhances exploitation.",
      "chunk_id": 705,
      "paper_id": "Novel Particle Swarm Optimization for unconstrained problems",
      "paper_title": "Novel Particle Swarm Optimization for Unconstrained Problems",
      "paper_year": "2013",
      "chunk_content": "In this paper, we propose a novel version of PSO (EDAPSO). To avoid premature convergence of PSO and to accelerate the convergence speed, we combine the exploration of EDA with the exploitation of PSO. In EDAPSO, new solutions are sampled from a probability model and then the whole population updates according to the improved PSO with a novel velocity updating equation. This paper provides a comprehensive set of experimental verifications of our proposed algorithm. ## 2 PARTICLE SWARM OPTIMIZATION ALGORITHM PSO is a population based optimization algorithm. The algorithm is initialized with a population of random particles in search space and searches for optima by updating direction vectors and velocity vectors. In n-dimensional search space, the direction vector of the i-th particle can be represented by vector $X_{i}=\\left(x_{i 1}, x_{i 2}, \\cdots x_{i n}\\right)$ and the velocity vector of the i-th particle can be represented by another vector $V_{i}=\\left(v_{i 1}, v_{i 2}, \\cdots v_{i n}\\right)$. At each generation, the particles update their velocities and directions according to the following two equations: $$\n\\begin{gathered}\nv_{i d}^{(t+1)}=\\omega v_{i d}^{(t)}+c_{1} r_{1}\\left(p_{i d}^{(t)}-s_{i d}^{(t)}\\right)+c_{2} r_{2}\\left(p_{g d}^{(t)}-s_{i d}^{(t)}\\right) \\\\\nx_{i d}^{(t+1)}=x_{i d}^{(t)}+v_{i d}^{(t+1)}\n\\end{gathered}\n$$ where $\\omega$ is called inertia weight. This factor has the function of balancing exploration and exploitation. If the factor is increased, the exploration of the algorithm will be enhanced. The exploitation will be enhanced with the decreasing of the factor.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:53:57",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of the specific EDA discussed in the context of theoretical features?",
      "topics": [
        "search"
      ],
      "answer": "The key takeaway is the extensive research into the theoretical properties of Estimation of Distribution Algorithms (EDAs), encompassing analyses of their behavior on diverse problems, investigations into global convergence for certain EDA classes, assessments of drift and scaling effects, and theoretical explanations of their dynamics, leading to numerous surveys and identification of future research directions.",
      "chunk_id": 4056,
      "paper_id": "A roadmap for solving optimization problems with estimation of distribution algorithms",
      "paper_title": "A roadmap for solving optimization problems with estimation of distribution algorithms",
      "paper_year": "2022",
      "chunk_content": "In addition to the previous references on the solution of real-world problems, a large number of papers have been published regarding theoretical features of EDAs. To name a few, Echegoyen et al. $(2012,2013)$ analyzed the behavior of EDAs on different types of problems, Zhang and Muhlenbein (2004) investigated the global convergence of a class of EDAs, Shapiro (2005) addressed the effects of drift and scaling in EDAs, and Krejca and Witt (2018) proposed a theoretical explanation of their dynamics. Motivated by the large amount of works published on the topic, a number of journal papers and books have surveyed the proposed designs and identified research lines for future investigations. Some relevant works are, amongst others, the general review paper by Pelikan et al. (2002), a paper on challenges and open problems by Santana et al. (2007), on bioinformatics by Armananzas et al. (2008) or on permutation problems by Ceberio et al. (2012), and the books edited by Larranaga and Lozano (2002); Lozano et al. (2006) and Pelikan et al. (2006).",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:53:58",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of shape and topology optimization, and given the categorization of methods into homogenization or interpolation methods, level set methods, phase field methods, heuristics, and meta-heuristics, what fundamental properties distinguish meta-heuristics from the other categories in addressing topology optimization problems, specifically considering the algorithm's ability to discover the number and shape of holes and structural elements?",
      "topics": [
        "search"
      ],
      "answer": "The provided text categorizes methods for shape and topology optimization into homogenization or interpolation methods, level set methods, phase field methods, heuristics, and meta-heuristics. Topology optimization involves discovering the number and shape of holes and structural elements. While the text does not provide specific algorithmic details for each category, it implies that meta-heuristics, unlike the other methods, offer a different approach to solving this problem. The distinction lies in how meta-heuristics, compared to homogenization, level set, phase field, and heuristics, explore the design space to identify optimal shapes and topologies, especially with respect to determining the number and shape of holes and structural elements. The text does not elaborate on the specific differences in their mechanisms or mathematical foundations.",
      "chunk_id": 22365,
      "paper_id": "A meta-heuristic for topology optimization using probabilistic learning",
      "paper_title": "A meta-heuristic for topology optimization using probabilistic learning",
      "paper_year": "2018",
      "chunk_content": "36885, Mexico 2 Center for Research in Mathematics, CIMAT, A.C. Jalisco S/N, Valenciana, Guanajuato, C.P. 36000 Mexico ``` function. For the example in Fig. 1 the optimal shape is shown in blue. The objective functions are usually defined in two fashions: the first one is to maximize rigidity or stiffness at a given mass [27]. The second formulation consists in minimizing the mass, subject to stiffness (or stress) constraints [45], this is the one used in our proposal. Topology optimization, is a kind of shape optimization problem, where the optimization algorithm must discover the number and shape of the holes and structural elements which compose the optimum design. In Fig. 1, if no initial holes are inserted in the structure, then the problem is a topology optimization problem. If the boundaries and initial holes are parameterized it is considered a shape optimization problem. If the structure does not change its shape and we are only looking for dimensions, the problem is considered a dimensional optimization problem. We briefly review the state of the art methods for shape and topology optimization, with the purpose of contrasting the novelty of our proposal with existing ones. The methods can be grouped into five categories: homogenization or interpolation methods, level set methods, phase field methods, heuristics, and meta-heuristics.",
      "question_type": "conceptual deep",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:00",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm using the EA-HH approach, how should practitioners balance exploration and exploitation when using heuristics such as LH1, LH2, LH3, LH4, LH5, and LH6?",
      "topics": [
        "convergence"
      ],
      "answer": "When implementing the EA-HH approach with heuristics like LH1 through LH6, practitioners should aim for a balanced tradeoff between exploration and exploitation. Individual heuristics may be skewed towards either exploration (potentially moving the search away from the global optima) or exploitation (potentially leading to local optima). The EA-HH approach utilizes both exploitative and exploratory features of the individual heuristics to find satisfactory solutions.",
      "chunk_id": 16557,
      "paper_id": "An evolutionary algorithm based hyper-heuristic framework for the set packing problem",
      "paper_title": "An evolutionary algorithm based hyper-heuristic framework for the set packing problem",
      "paper_year": "2019",
      "chunk_content": "6. Convergence speed of EA-HH, LH1, LH2, LH3, LH4, LH5, and LH6 approaches on instances pb1000rnd07, pb500rnd15, and pb500rnd09. # 6.6. Performance comparison of the individual heuristic with the EA-HH approach This section presents the performance comparison of heuristics LH1, LH2, LH3, LH4, LH5, and LH6 when they are applied individually. Each individual heuristic has its own advantage and limitation: some of them have exploration ability and others have exploitation ability. Exploration may take the search process away from the global optima, whereas exploitation may takes the search process toward the local optima. Therefore, it is necessary to achieve a balanced tradeoff between exploration and exploitation in the search process to get a satisfactory solution in a reasonable amount of computational time. Tables 4-6 present the results of individual heuristics; the results show that each individual heuristic underperformed because of skewed nature of either exploration or exploitation, whereas the EA-HH approach utilized the exploitative and exploratory features of the individual heuristic to reach the best-known solutions.",
      "question_type": "practical application",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:02",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the mean inverted generational distance (MIGD) performance indicator, how is it mathematically defined to measure the convergence and diversity of solutions obtained by an algorithm in the context of dynamic multiobjective optimization?",
      "topics": [
        "performance"
      ],
      "answer": "The mean inverted generational distance (MIGD) is defined as $\\operatorname{IGD}\\left(\\mathrm{POF}_{t}^{*}, \\mathrm{POF}_{t}^{o b}\\right)=\\frac{\\sum_{g \\in \\mathrm{POF}_{t}^{*}} d\\left(g, \\mathrm{POF}_{t}^{o b}\\right)}{\\left|\\mathrm{POF}_{t}^{*}\\right|}$, where $\\mathrm{POF}_{t}^{*}$ represents a set of uniformly distributed solutions in the true Pareto optimal front (POF) at time $t$, and $\\mathrm{POF}_{t}^{o b}$ is the POF approximation obtained by an algorithm at time $t$. The term $d\\left(g, \\mathrm{POF}_{t}^{o b}\\right)$ calculates the minimum Euclidean distance between a solution $g$ in $\\mathrm{POF}_{t}^{*}$ and the points in $\\mathrm{POF}_{t}^{o b}$, while $\\left|\\mathrm{POF}_{t}^{*}\\right|$ is the number of solutions in $\\mathrm{POF}_{t}^{*}$.",
      "chunk_id": 20686,
      "paper_id": "Novel Prediction Strategies for Dynamic Multiobjective Optimization",
      "paper_title": "Novel Prediction Strategies for Dynamic Multiobjective Optimization",
      "paper_year": "2020",
      "chunk_content": "The DF test suite consists of functions taken from other test suites, e.g., FDA [8], dMOP [10], ZJZ [33], and JY [13]. The time instance [FORMULA117] used in the test suite is given as $t=\\left(1 / n_{t}\\right)\\left\\lfloor\\left(\\tau / \\tau_{t}\\right)\\right\\rfloor$, where $n_{t}$, $\\tau$, and $\\tau_{t}$ are the severity of change, the number of iterations, and the frequency of change, respectively. The dimension $(D)$ of benchmark functions is set to 10 . The definition of these test instances can be found in [15]. ## B. Performance Indicators Performance indicators play an important role in assessing the performance of algorithms. In this paper, we adopts the following three performance indicators. 1) Mean Inverted Generational Distance: The mean inverted generational distance (MIGD), a widely adopted measure [30], [32], [34], is mainly employed to measure the convergence and diversity of solutions obtained by an algorithm. Suppose that $\\mathrm{POF}_{t}^{*}$ is a set of uniformly distributed solutions in the true POF and $\\mathrm{POF}_{t}^{o b}$ is a POF approximation, at time [FORMULA125], IGD can be calculated as $$\n\\operatorname{IGD}\\left(\\mathrm{POF}_{t}^{*}, \\mathrm{POF}_{t}^{o b}\\right)=\\frac{\\sum_{g \\in \\mathrm{POF}_{t}^{*}} d\\left(g, \\mathrm{POF}_{t}^{o b}\\right)}{\\left|\\mathrm{POF}_{t}^{*}\\right|}\n$$ where $d\\left(g, \\mathrm{POF}_{t}^{o b}\\right)$ refers to the minimum Euclidian distance between $g$ and the points in [FORMULA129], and $\\left|\\mathrm{POF}_{t}^{*}\\right|$ is the number of solutions in [FORMULA131].",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:04",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How is the output power of a wind generating unit ($P_W$) computed based on wind speed ($v$) relative to the cut-in speed ($v_v$), rated speed ($v_o$), and cut-out speed ($v_o$)?",
      "topics": [
        "optimization"
      ],
      "answer": "The output power $P_W$ is calculated as 0 when the wind speed $v$ is less than the cut-in speed $v_v$ or greater than the cut-out speed $v_o$. When the wind speed $v$ is between the cut-in speed $v_v$ and the rated speed $v_o$, the output power is equal to $P_v$, the nominal power.",
      "chunk_id": 4026,
      "paper_id": "Reliability evaluation of a composite power system in the presence of renewable generations",
      "paper_title": "Reliability evaluation of a composite power system in the presence of renewable generations",
      "paper_year": "2022",
      "chunk_content": "The probability of each interval is determined by the numerical integral method. In this paper, wind speed distribution is divided into five equal intervals, as exhibited in Fig. 1. According to the wind speed, the output power of a wind generating unit is computed using the speed-to-power conversion function as follows [35]: $$\n\\begin{aligned}\n& 0, \\text { for } v\\left(v_{i} \\text { and } v\\right) v_{o} \\\\\n& \\omega_{v}, \\text { for } v_{v} \\leq v \\leq v_{o}\n\\end{aligned}\n$$ Where, $P_{W}, P_{v}, v_{o}, v_{v}$ and $v_{o}$ are WT output power, nominal power, cutin speed, rated speed, and cut-out speed, respectively. ### 2.2. PV system With ever-increasing PV technology, more large-scale PV units are being integrated into power systems where low-cost land and abundant solar energy are readily available. The output power of PV units is a function of uncertain parameters including ambient temperature and solar irradiance. Due to extreme variations of ambient temperature and solar irradiance throughout the day, the active power of PV panels has intense fluctuations in spite of many advances in the maximum power point tracing (MPPT) techniques. As reported in the literature [36,37], at a particular location, the distribution of hourly irradiance is generally in accordance with a bimodal distribution, which can be expressed as a linear combination of two unimodal distribution functions.",
      "question_type": "definition",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:06",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "Based on the description, what is the primary difference between Tree-EDA and UMDA concerning the energies of solutions they find?",
      "topics": [
        "optimization"
      ],
      "answer": "Tree-EDA and Tree-EDA [FORMULA100] find solutions with improved energies compared to UMDA, as indicated by the difference between the energies of the best solutions found by each algorithm.",
      "chunk_id": 12993,
      "paper_id": "The role of a priori information in the minimization of contact potentials by means of estimation of distribution algorithms",
      "paper_title": "The role of a priori information in the minimization of contact potentials by means of estimation of distribution algorithms",
      "paper_year": "2007",
      "chunk_content": "1436.10 | 1 | 1386.09 | 2hgf | 2923.62 | 1 | 2848.07 | 3046.69 | 1 | 2960.59 | | 1sp2 | 673.46 | 1 | 608.36 | 673.46 | 18 | 652.37 | 2r63 | 1829.48 | 1 | 1678.30 | 1912.22 | 1 | 1802.58 | | 1ter | 528.31 | 4 | 503.82 | 528.31 | 26 | 513.96 | 4mt2 | 1864.31 | 1 | 1809.06 | 1884.13 | 2 | 1849.47 | | 5cro | 1527.11 | 1 | 1449.72 | 1605.88 | 1 | 1516.00 | | | | | | | | [IMAGE_2] Fig. 3. Improvement in the energies of the solutions found by Tree-EDA and TreeEDA [FORMULA100] with respect to those found by UMDA for the 61 protein instances. a) Difference between the energies of the best solutions overall run found by Tree-EDA and UMDA. b) Difference between the energies of the best solutions found by Tree-EDA [FORMULA101] and UMDA.",
      "question_type": "comparison",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:07",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the Thermal Power Measurement System (TPMS) be measured using the inter-comparison of Instrumented Channels (ICs) and Zone Control Devices (ZCDs)?",
      "topics": [
        "comparison"
      ],
      "answer": "The quality of the TPMS can be assessed by ensuring that each ZCD is surrounded by a minimum of two ICs, and the ICs at ZCD nearby locations are useful for inter-comparison purpose. This inter-comparison helps in validating the measurements of zone powers by the 14 ZCDs.",
      "chunk_id": 10932,
      "paper_id": "Selection of fuel channels for Thermal Power Measurement in 700MWe Indian PHWR by evolutionary algorithm",
      "paper_title": "Selection of fuel channels for Thermal Power Measurement in 700 MWe Indian PHWR by evolutionary algorithm",
      "paper_year": "2012",
      "chunk_content": "Thus 11 ICs in each fuel channel set (as per first constraint) should get instrumented. The third constraint arises from symmetry. The core has reflective symmetry along the $\\pm 2 \\%$-axis whereas the reactivity devices disturb reflective symmetry along $\\times 4$-axis. The symmetry of ICs about $=\\mathrm{N}$-axis is avoided for better coverage and less redundancy. There are 14 ZCDs to measure the zone powers. The fourth constraint in the design of TPMS is that each ZCD should be surrounded by minimum of two ICs. The ICs at ZCD nearby location are useful for inter-comparison purpose. ## 3. Core configurations The choice of nominal and off-nominal core configurations (discussed in Section 1.1) has to be made. First of all, it should be noted that, in present studies, all these configurations have a burnup distribution which is the so-called \"equilibrium time-averaged burn-up distribution\". This term can be explained as follows. At the beginning of life, the PHWR contains all fresh fuel. This state is unique in its lifetime. After about $4-5$ months, the reactor needs to be fueled continuously. Every day, the burnt fuel from more than one fuel channel is replaced by fresh fuel.",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:09",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs involving the Guided Edge Histogram-Based Graph-sampling Algorithm, and how does it relate to service dependency discovery?",
      "topics": [
        "algorithm"
      ],
      "answer": "The primary purpose is to iteratively refine candidate solutions within an EDA framework by learning an Edge Histogram Matrix (EHM) from high-quality solutions. The Guided Edge Histogram-Based Graph-sampling Algorithm is used to sample new valid candidate solutions, forming the next population \\(\\mathcal{P}^{g+1}\\). Furthermore, the approach incorporates an ontology-based querying technique to identify service dependencies by finding predecessor services, leveraging concepts like robust causal links between services \\(S_{i} \\rightarrow S_{j}\\) and marking tree nodes with service sets \\(O_{C}\\) and \\(I_{C}\\) to ensure these links.",
      "chunk_id": 21612,
      "paper_id": "Towards Fully Automated Semantic Web Service Composition Based on Estimation of Distribution Algorithm",
      "paper_title": "Towards Fully Automated Semantic Web Service Composition Based on Estimation of Distribution Algorithm",
      "paper_year": "2018",
      "chunk_content": "The following steps (Step. 5 to Step. 9) will be repeated until the maximum number of generations is reached: we sample $c$ new valid candidate solutions from $d$ using our proposed Guided Edge Histogram-Based Graph-sampling Algorithm. These newly sampled candidate solutions form the next population $\\mathcal{P}^{g+1}$ and will be evaluated and selected to learn $\\mathcal{E} \\mathcal{H} \\mathcal{M}^{g+1}$. In summary, we propose a way of learning EHM from high-quality solutions discovered by EDA so far and a novel sampling technique for building valid solutions from EHM. # 3.2 Discovery of Service Dependency Service dependency represents a relationship between two services (i.e., one service $S_{j}$ and its predecessor $S_{i}$ ) that are determined by the existence of robust causal links [14] between these two services. In other words, one service can be either partially or fully satisfied by its predecessor, denoted as $S_{i} \\rightarrow S_{j}$. To identify service dependencies regarding each service, we proposed an ontology-based querying technique to efficiently find their predecessor services in $\\mathcal{P}^{g}=$. We first create labels for concept nodes of a taxonomy tree in $\\left[\\mathcal{G}_{0}^{g}, \\ldots, \\mathcal{G}_{k}^{g}, \\ldots, \\mathcal{G}_{m-1}^{g}\\right]$ with task-related services using Algorithm 2. In this Algorithm, we mark each tree node with two sets of services, i.e., $O_{C}$ and $I_{C}$, where robust causal links can be ensured from services in $S_{3}=\\left(\\{f, h\\},\\{i\\}, Q o S_{S_{3}}\\right)$ and services in $S_{4}=\\left(\\{a\\},\\{f, g, h\\}, Q o S_{S_{4}}\\right)$.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:11",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of robotic imitation learning, how does the algorithmic mechanism of reinforcement learning enable robots to learn behaviors, and what is the core principle that drives the policy updates?",
      "topics": [
        "search"
      ],
      "answer": "In robotic imitation learning, reinforcement learning enables robots to learn behaviors through iterative trials by evaluating a current policy using cost functions and updating the current policy according to the values from these cost functions. The core principle is to learn new knowledge through trials, making the algorithm effective for robotic imitation learning by utilizing rewards from iterative trials. The algorithm uses cost functions to evaluate and update the policy.",
      "chunk_id": 1435,
      "paper_id": "Integration of evolutionary computing and reinforcement learning for robotic imitation learning",
      "paper_title": "Integration of Evolutionary Computing and Reinforcement Learning for Robotic Imitation Learning",
      "paper_year": "2014",
      "chunk_content": "Robotic imitation learning (also called learning from demonstration, learning by programming, etc.) enables robot to learn behaviors and skills from humans quickly. Current research on imitation learning can be categorized into two types [3]: one is trying to train robots to extract and learn the motion dynamics[4], and the other is trying to train robots to learn higher-level behaviors and action primitives through imitation [5] [6]. Such learning methods, especially the first type, provide a possible solution for researchers and engineers to program motions for high Degrees-Of-Freedom robots, especially humanoid robots. An important skill for intelligent robots is to learn behaviors or skills through trials like human children. Some researchers applied reinforcement learning robots to learn desired knowledge by utilizing rewards from iterative trials. A typical flowchart for applying reinforcement learning in robotic imitation learning is shown below: [IMAGE_0] Fig.1. Reinforcement Learning Flowchart The basic idea of reinforcement learning algorithm is to evaluate a current policy using some cost functions and to update the current policy according to these values from cost functions. Reinforcement learning is very attractive to researchers in robotic imitation learning community because it illustrates a way of learning newly knowledge through trials and such algorithms are approved to be effective for robotic imitation learning.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:12",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "In which practical scenarios does the Gaussian polytree EDA outperform the [FORMULA128] according to the performance comparisons, and what specific performance metric demonstrates this?",
      "topics": [
        "comparison"
      ],
      "answer": "The Gaussian polytree EDA reaches better values than the [FORMULA128] requiring a lesser number of function evaluations in all functions except for the Rosenbrock function, where both show a similar performance. The specific performance metric demonstrating this is the number of function evaluations required to reach better fitness values.",
      "chunk_id": 23524,
      "paper_id": "Global Optimization with the Gaussian Polytree EDA",
      "paper_title": "Global Optimization with the Gaussian Polytree EDA",
      "paper_year": "2011",
      "chunk_content": "The comparison for the Sphere function is shown in Figure 5, for the Rosenbrock function in Table 6, for the Griewangk in Table 7, and for the Ackley function in Table 8. Table 4. Set of test functions of Experiment 2 Table 5. Comparative for the Sphere function with a dimension of 10 and 50 (optimum fitness value $=0$ ) Table 6. Comparative for the Rosenbrock function with a dimension of 10 and 50 (optimum fitness value [FORMULA126] ) Table 7. Comparative for the Griewangk function with a dimension of 10 and 50 (optimum fitness value [FORMULA127] ) Comments to Experiment 2. The proposed Gaussian polytree EDA reaches better values than the [FORMULA128] requiring lesser number of function evaluations in all function (except for the Rosenbrock were both show a similar performance). Table 8. Comparative for the Ackley function with a dimension of 10 and 50 (optimum fitness value [FORMULA129] ) # 6 Conclusions In this paper we described a new EDA based on Gaussian polytrees.",
      "question_type": "application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:13",
      "generation_style": "practical_application"
    },
    {
      "question": "Given the described scheduling context, if the number of tasks assigned to worker \\mathrm{FCB}_{m}=\\sum_{p=1}^{m-1} \\sum_{j=1}^{J} \\sum_{k=1}^{m} FC_{p} Z_{m, j, k} Z_{p, j,(k-1)} exceeds the threshold \\mathrm{TL}_{m}=\\max _{1 \\leq i \\leq W}\\left(V_{m, n} T_{n} \\beta_{n, i} Y_{i}\\right), \\forall i, what impact does this have on the worker's average task time within a seru, and how is this mathematically represented?",
      "topics": [
        "scheduling"
      ],
      "answer": "If the number of tasks assigned to worker \\mathrm{FCB}_{m} is greater than \\mathrm{TL}_{m}, then the worker's average task time within a seru will be longer than her or his task time within the original flow line. This is mathematically expressed by the condition that \\mathrm{FL}_{m}=\\sum_{n=1}^{N} \\sum_{i=1}^{W} V_{m, n} T_{n} \\beta_{n, i} Y_{i}+\\left(B_{m}-1\\right) T L_{m}, where \\mathrm{FL}_{m} represents the total task time for worker \\mathrm{FL}_{m}, V_{m, n} is a variable related to product type, T_{n} is a task time, \\beta_{n, i} is the skill level, and Y_{i} is a binary variable indicating if a worker is left in the flow line. B_{m} represents an upper bound on the number of tasks, and TL_{m} represents the maximum task time across workers.",
      "chunk_id": 19370,
      "paper_id": "A cooperative coevolution algorithm for complex hybrid seru-system scheduling optimization",
      "paper_title": "A cooperative coevolution algorithm for complex hybrid seru-system scheduling optimization",
      "paper_year": "2021",
      "chunk_content": "$\\eta_{i}$ : Upper bound on the number of tasks for worker $\\mathrm{FC}_{m}=\\frac{B_{m} \\mathrm{TC}_{m} W}{\\sum_{i=1}^{W} \\sum_{j=1}^{J} \\sum_{k=1}^{M} X_{i, j} Z_{m, j, k}}$ in a seru. If the number of tasks assigned to worker $\\mathrm{FCB}_{m}=\\sum_{p=1}^{m-1} \\sum_{j=1}^{J} \\sum_{k=1}^{m} F C_{p} Z_{m, j, k} Z_{p, j,(k-1)}$ is more than $\\mathrm{TL}_{m}=\\max _{1 \\leq i \\leq W}\\left(V_{m, n} T_{n} \\beta_{n, i} Y_{i}\\right), \\quad \\forall i$, then worker $\\mathrm{FL}_{m}=\\sum_{n=1}^{N} \\sum_{i=1}^{W} V_{m, n} T_{n} \\beta_{n, i} Y_{i}+\\left(B_{m}-1\\right) T L_{m}$ 's average task time within a seru will be longer than her or his task time within the original flow line. $\\varepsilon_{i}$ : Worker $C_{\\max }=\\max _{1 \\leq m \\leq M}\\left(\\mathrm{FLB}_{m}+\\mathrm{FL}_{m}\\right)$ 's coefficient of influencing level of doing multiple tasks. $\\beta_{m}$ : Skill level of worker $s$ for each task of product type $F C B_{m}+F C_{m}$. Binary variables: $Y_{i}: 1$, if worker $\\mathrm{C}_{\\max }=\\max _{1 \\leq m \\leq M}\\left(\\mathrm{FLB}_{m}+\\mathrm{FL}_{m}\\right)$ is left in the flow line; 0 , otherwise. $X_{i, j}: 1$, if worker $j(j>0)$ is assigned to seru $j ; 0$, otherwise. $Z_{m, j, k}: 1$, if product batch $\\{011023\\}$ is assigned to seru $W-1$ in sequence $k ; 0$, otherwise. $O_{m, r}: 1$, if product batch $P_{w}$ is processed in flow line in sequence $r ; 0$, otherwise. Four binary variables correspond to the four decision processes: worker allocation, seru formation, seru scheduling and flow line scheduling.",
      "question_type": "theory",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:16",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What computational trade-offs are described between the shrinking step and the interpolation point generation in the UOBDQA trust region method when optimizing functions like $f_{2}$ or $f_{4}$?",
      "topics": [
        "optimization"
      ],
      "answer": "The shrinking step in the UOBDQA trust region method involves computing new points $u^{i}$ based on the formula $u^{i}=v^{1}+\\frac{1}{2}(v^{i}-v^{1})$ for $i=2,3, \\ldots, n+1$, replacing $v^{2}, v^{3}, \\ldots, v^{n+1}$ with these new points. This contrasts with generating interpolation points, including $3.654 \\times 10^{-21}$, to form a quadratic model. The trade-off lies in whether to refine the search space through shrinking versus improving the accuracy of the quadratic model by generating and interpolating new points within a trust region defined by radius $\\rho$.",
      "chunk_id": 27139,
      "paper_id": "Hybrid estimation of distribution algorithm for global optimization",
      "paper_title": "Hybrid Estimation of Distribution Algorithm for Global Optimization",
      "paper_year": "2004",
      "chunk_content": "$f\\left(v^{n}\\right) \\leq f\\left(v^{\\text {ref }}\\right)<$ $c=\\left(\\left(a_{1}+b_{1}\\right) / 2, \\ldots,\\left(a_{n}+b_{n}\\right) / 2\\right)$, compute $$\nv^{\\mathrm{oc}}=\\bar{v}+\\frac{1}{2}\\left(v^{\\mathrm{ref}}-\\bar{v}\\right)\n$$ if $f\\left(v^{\\text {oc }}\\right) \\leq f\\left(v^{\\text {ref }}\\right)$, replace $f_{1} \\cdot f_{10}$ by $v^{\\text {oc }}$ and terminate the iteration; otherwise go to Step 6. Step 6. Shrinking. Compute $$\nu^{i}=v^{1}+\\frac{1}{2}\\left(v^{i}-v^{1}\\right), \\quad i=2,3, \\ldots, n+1\n$$ Replace $v^{2}, v^{3}, \\ldots, v^{n+1}$ by $u^{2}, u^{3}, \\ldots, u^{n+1}$ and terminate the iteration. # 2.5 UOBDQA Trust region methods using quadratic interpolation model, developed by Powell (2002), is a new class of derivative free local optimization algorithms for finding a local minimum of the objective function $f_{2}, f_{4}$. Such algorithms start with an initial point $v, \\rho_{\\text {beg }}$ and $\\rho_{\\text {end }}$, the initial and final values of a trust region radius $\\rho$. The algorithms generate a set of interpolation points (including $3.654 \\times 10^{-21}$ ) in a neighbourhood of [FORMULA157]. Then an initial quadratic model is formed by interpolating these points. The algorithm generates a new point, either by minimizing the current quadratic model within a trust region, or by a procedure that improves the accuracy of the model. One of the interpolation points is replaced by the resultant point. The typical distance between successive points at which [FORMULA158] is calculated are of magnitude of the trust region radius [FORMULA159].",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:18",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the specific EDA mentioned that utilizes a multivariate Gaussian distribution (MGD), what data structures are required to represent and manipulate the covariance matrix and what programming considerations apply?",
      "topics": [
        "complexity"
      ],
      "answer": "For an EDA using an MGD, you need to represent the mean vector \\$\\boldsymbol{\\mu}\\$ as an array of dimension $\\left(n^{2}+3 n\\right) / 2$, where $n$ is the number of random variables. The covariance matrix \\$\\boldsymbol{\\Sigma}\\$ requires an $n \\times n$ symmetric and positive semidefinite matrix. Given the need for matrix inversion (to obtain the precision matrix) and sampling (via Cholesky decomposition), efficient matrix libraries are crucial. Furthermore, ensure the covariance matrix remains positive definite to guarantee its inverse exists and Cholesky decomposition is possible; regularization techniques might be needed to enforce this property during the update steps of the EDA.",
      "chunk_id": 329,
      "paper_id": "Regularized continuous estimation of distribution algorithms",
      "paper_title": "Regularized continuous estimation of distribution algorithms",
      "paper_year": "2013",
      "chunk_content": "2. Background ### 2.1. Multivariate Gaussian distribution A joint multivariate Gaussian distribution (MGD) for $n$ random variables $X_{1}, \\ldots, X_{n}$ is determined with two overall parameters: $\\lambda(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, where $\\boldsymbol{\\mu}$ is an $\\left(n^{2}+3 n\\right) / 2$-dimensional vector of mean values for each variable, and $\\boldsymbol{\\Sigma}$ is a $n \\times n$ symmetric and positive semidefinite covariance matrix. The total number of individual parameters (free parameters) that have to be estimated in order to determine an MGD is $\\left(n^{2}+3 n\\right) / 2$, i.e. of $O\\left(n^{2}\\right)$ complexity. Positive definite matrices are interesting since they are fullranked and non-singular, implying that their inverse exists. The inverse of a positive definite covariance matrix, which is called the precision or concentration matrix, represents partial covariances between variables and any zero entry in this matrix implies that the corresponding two variables are conditionally independent given all other variables. Therefore, the zero pattern of the precision matrix directly induces the graphical structure of a Markov network. The positive definiteness of the covariance matrix also allows for a unique triangular decomposition, known as Bartlett or Cholesky decomposition [48], that can be used to generate samples from the corresponding MGD. These types of sampling algorithms have also been extended to work for MGDs with positive semidefinite matrices.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:20",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the effectiveness of the EDA model in the context of power system reliability, and what algorithms are they compared against?",
      "topics": [
        "search"
      ],
      "answer": "Loss of Load Probability (LOLP) and Expected Power Not Supplied (EPNS) are the specific metrics used to evaluate the effectiveness of the EDA model. The EDA model's performance, in terms of these metrics, is compared against Genetic Algorithm with Improved State Space Pruning (GA-ISSP), Crude Monte Carlo (CE), and EDA with Improved State Space Pruning (EDA-ISSP).",
      "chunk_id": 14443,
      "paper_id": "Fast reliability evaluation method for composite power system based on the improved EDA and double cross linked list",
      "paper_title": "Fast reliability evaluation method for composite power system based on the improved EDA and double cross linked list",
      "paper_year": "2017",
      "chunk_content": "5 Conclusion This paper proposes a new approach based on EDA and intelligent storage to evaluate the reliability of the power system, which is tested in a typical IEEE RTS-79 system. The conclusions are: (i) Keeping the LOLP and EPNS calculated by MCS as the reference, the proposed algorithm is performed with higher [IMAGE7] Fig. 8 Convergence process of LOLP and [FORMULA107] in different algorithms [IMAGE8] Fig. 9 Convergence process of EPNS and [FORMULA108] in different algorithms Table 3 Comparison of LOLP with different algorithms Table 4 Comparison of EPNS with different algorithms precision and computational efficiency compared with GA-ISSP, CE and EDA-ISSP. (ii) The computational time for MCS decreases with the increase of the number of normal states pruned, while the computational time for state pruning increases. Thus, in the field of ISSP, the future research will focus on how to reasonably choose the optimal level of state pruning and harmonise the contradiction between the pruning time and simulation time. (iii) New energies, such as PV, have a strong dependence on the time sequence.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:21",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs using the attention mechanism and Restricted Boltzmann Machine (RBM)?",
      "topics": [
        "representation"
      ],
      "answer": "The algorithmic approach aims to integrate attention weights with a Restricted Boltzmann Machine (RBM) to model user preferences in Estimation of Distribution Algorithms (EDAs). Specifically, an attention layer calculates weights, denoted as A(x), reflecting user preference for decision variables. These weights are then used to create AM-based attention weights at(x_i), which serve as input for an AM-based RBM module. This module learns AM-based representations to model user preferences within the EDA framework.",
      "chunk_id": 20781,
      "paper_id": "Enhanced Interactive Estimation of Distribution Algorithms with Attention Mechanism and Restricted Boltzmann Machine",
      "paper_title": "Enhanced Interactive Estimation of Distribution Algorithms with Attention Mechanism and Restricted Boltzmann Machine",
      "paper_year": "2020",
      "chunk_content": "According to the self-attention mechanism mentioned in [18], the attention weight $A(\\mathbf{x})$ of individual $\\min \\left(E_{\\theta_{2}}\\right)$ is calculated: $$\nA(\\mathbf{x})=\\operatorname{softmax}\\left(a\\left(V_{r b m_{1}}(\\mathbf{x}), \\mathbf{w}_{1}\\right)\\right)\n$$ where $\\operatorname{softmax}()$ guarantees that the sum of all parts is 1 . The function $a\\left(V_{r b m_{1}}(\\mathbf{x}), \\mathbf{w}_{1}\\right)$ measures the attention weights of the user preferences on individual $10.32 \\%$, and is calculated as follows: $$\na\\left(V_{r b m_{1}}(\\mathbf{x}), \\mathbf{w}_{1}\\right)=V_{r b m_{1}}(\\mathbf{x}) \\cdot \\mathbf{w}_{1}^{T}\n$$ The attention weight $50 \\%$ expresses the preference degree of the current user for decision variables in individual $20 \\%$. The attention layer considers the attention weight $5.52 \\%$ to obtain the integrated attention weights $A \\mathbf{t}(\\mathbf{x})$ of the individual $7.32 \\%$ in $\\mathrm{CDs} \\_$. $$\nA \\mathbf{t}(\\mathbf{x})=\\left[a t\\left(\\mathbf{x}_{1}\\right), a t\\left(\\mathbf{x}_{2}\\right), \\cdots, a t\\left(\\mathbf{x}_{|D|}\\right)\\right]^{T}\n$$ where at $\\left(\\mathbf{x}_{i}\\right)$ is the AM-based attention weights of individual [FORMULA80], $$\na t\\left(\\mathbf{x}_{i}\\right)=\\mathbf{x}_{i}+A\\left(\\mathbf{x}_{i}\\right) \\times \\mathbf{x}_{i}\n$$ The current user has different preferences for the attribution features of items so that she/he expresses a preference dependency on the decision variables of individuals. The attention layer extracts and integrates the attention weight [FORMULA82] of users on the decision variables of items. The AM-based attention weights [FORMULA83] will be used as the input of the AM-based RBM module. (3) AM-based RBM Module The structure of the AM-based RBM model is similar to that of the RBM-based attention weight model, but the visible and hidden units in the AM-based RBM model are both real values. Integrating the AM by the attention layer, the AMbased representations [FORMULA84] of the individuals in [FORMULA85] are used as the training data to train the AM-based RBM use preference model.",
      "question_type": "synthesis summary",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:23",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the Population-Based Incremental Learning (PBIL) algorithm, proposed by Balaja, differ fundamentally from Genetic Algorithms (GAs) regarding evolutionary operators?",
      "topics": [
        "optimization"
      ],
      "answer": "The Population-Based Incremental Learning (PBIL) algorithm, which is an Estimation of Distribution Algorithm (EDA), differs from Genetic Algorithms (GAs) in that EDAs, including PBIL, do not utilize evolution operators such as crossover and mutation. While GAs rely on these operators to evolve a population, EDAs like PBIL estimate a probability distribution from which new candidate solutions are sampled.",
      "chunk_id": 22865,
      "paper_id": "A new Algorithm based on the Gbest of Particle Swarm Optimization algorithm to improve Estimation of Distribution Algorithm",
      "paper_title": "A new Algorithm based on the Gbest of Particle Swarm Optimization algorithm to improve Estimation of Distribution Algorithm",
      "paper_year": "2018",
      "chunk_content": "Based on the existing research, this paper explores a new algorithm. The particle swarm optimization algorithm is used to improve the distribution estimation of distribution algorithm. We try to model the population optimal individual as the estimation of distribution algorithm for each iteration of particle swarm algorithm. The algorithm combining particle swarm optimization algorithm in the characteristics of slow convergence speed quickly before they improving the convergence of the algorithm, at the same time, which can ensure the distribution estimation algorithm achieved through modeling operation such as sampling precision. This paper is arranged as follows: the first section introduces the back ground and significance of this research. The second section introduces the algorithm principles of estimation of distribution algorithm and its development status. The third section introduces the idea of particle swarm algorithm steps of the simulation experiment. Section 5 compares the convergence and numerical accuracy of the simulation experiment. And then the finally is the conclusion. ## II. Estimation of Distribution Algorithm Balaja proposed the PBIL algorithm, which is the earliest recognized \"Estimation of Distribution Algorithm (EDA)\" model[7] in 1994.However, the concept of this algorithm was not yet available in academia at that time until it was first proposed by Larranaga P and Lozano J A in 1996[8].EDA shares many similarities with evolutionary computation techniques such as Genetic Algorithms (GA).However, unlike GA, EDA has no evolution operators such as crossover and mutation.",
      "question_type": "comparison",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:25",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "In which practical scenarios does the MOEDA model outperform other metaheuristics when addressing optimization problems, and what performance degradation patterns are observed as problem scale increases?",
      "topics": [
        "performance"
      ],
      "answer": "MOEDA demonstrates statistically significant performance advantages over other compared metaheuristics, particularly on larger-scale instances. While all compared metaheuristics perform well on small-scale instances (e.g., n=20), their performance degrades as problem scales become more complicated, with MOEDA exhibiting less performance decline. The performance reduction is especially pronounced for the compared metaheuristics over larger instances, as observed in the interaction plots.",
      "chunk_id": 4006,
      "paper_id": "Scheduling of energy-efficient distributed blocking flowshop using pareto-based estimation of distribution algorithm",
      "paper_title": "Scheduling of energy-efficient distributed blocking flowshop using pareto-based estimation of distribution algorithm",
      "paper_year": "2022",
      "chunk_content": "There is a need to take additional tests to identify whether the results between different approaches are statistically significant. For this purpose, two statistical tests including a parametric test and a non-parametric test were conducted, which are described as follows. For the parametric test, two ANOVA tests were implemented, in which the performance indicators [FORMULA388] and [FORMULA389] were considered as the response variables. The results grouped by different factories are presented in the ANOVA plots with [FORMULA390] confidence intervals in Fig. 12. As can be seen from Fig. 12, there are no overlapping intervals between MOEDA and other compared metaheuristics on each single test problem, which implies clearly that MOEDA has statistical differences with compared algorithms. In other words, MOEDA is statistically significantly better than other metaheuristics. Moreover, it can be observed that all the compared metaheuristics performed well on small-scale instances (e.g. $n=20$ ), but degrade more or less when the problem scales become more complicated. To investigate such influence, the interaction plots between algorithms and the combination of [FORMULA392] are demonstrated in Fig. 13. As revealed in Fig. 13(a), the compared metaheuristics have shown strong competitiveness over small-scale instances, the obtained results of Hypervolume are very close to those of MOEDA. With the increment of the instance scales, the performances of all metaheuristics have declined. Especially for compared metaheuristics, the obtained results have shown that their performances sharply reduced over larger instances.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:26",
      "generation_style": "practical_application"
    },
    {
      "question": "How does increasing problem dimensionality affect the performance of RegEDAs compared to other Gaussian-based EDAs, specifically considering the population size?",
      "topics": [
        "optimization"
      ],
      "answer": "The increase in problem dimensionality, with a logarithmic population size in the number of variables, affects the performance of the proposed RegEDAs less than other Gaussian-based EDAs. The statistical analysis results show that RegEDAs are able to obtain BAVs (best-so-far values) that are significantly better than the other algorithms for larger dimensions of most functions. RegEDA-LARS-AND and RegEDAShr have a better average optimization behavior, with RegEDA-LARS-AND having statistically better overall performance for larger dimensions.",
      "chunk_id": 372,
      "paper_id": "Regularized continuous estimation of distribution algorithms",
      "paper_title": "Regularized continuous estimation of distribution algorithms",
      "paper_year": "2013",
      "chunk_content": "This improves performance in high-dimensional settings, while keeping the population size and therefore the number of function evaluations relatively low. Two approaches to regularized estimation of Gaussian distributions were introduced, and several alternative methods proposed in the statistics literature were discussed within each approach. It was shown that an important factor affecting regularized model estimation is the level of variable dependencies in the problem. Considering a high-dimensional setting, the different methods in these approaches were analyzed from several points of view: true structure recovery, time complexity, and likelihood. The results of these analyses helped to select some of the regularized model estimation techniques for use in RegEDA. These different versions of RegEDA were applied to a set of continuous optimization functions, featuring different properties, and the results were compared with those of other Gaussian-based EDAs. The results show that the increase in problem dimensionality, with a logarithmic population size in the number of variables, affects the performance of the proposed RegEDAs less than other Gaussian-based EDAs. Specific problem properties can play a vital role in algorithm performance. The statistical analysis results show that RegEDAs are able to obtain BAVs that are significantly better than the other algorithms for larger dimensions of most functions. Of all the versions of RegEDA, RegEDA-LARS-AND and RegEDAShr have proved to have a better average optimization behavior, with RegEDA-LARS-AND having statistically better overall performance for larger dimensions.",
      "question_type": "advantage",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:27",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "Considering the context of dynamic multiobjective optimization problems (DMOPs), how do evolutionary algorithms adapt to frequent changes in the Pareto optimal front (POF) or Pareto optimal set (POS) during the evolution process?",
      "topics": [
        "Estimation of Distribution Algorithms (EDAs)",
        "Dynamic multiobjective optimization problems (DMOPs)",
        "Pareto optimal front (POF)",
        "Pareto optimal set (POS)",
        "Evolutionary algorithms",
        "Change response mechanism"
      ],
      "answer": "In dynamic multiobjective optimization problems (DMOPs), evolutionary algorithms must adapt to frequent changes in the Pareto optimal front (POF) or Pareto optimal set (POS) as the evolution process unfolds. The chunk mentions change response mechanisms as a key aspect of these algorithms when facing such dynamic environments.",
      "chunk_id": 3430,
      "paper_id": "Combining a hybrid prediction strategy and a mutation strategy for dynamic multiobjective optimization",
      "paper_title": "Combining a hybrid prediction strategy and a mutation strategy for dynamic multiobjective optimization",
      "paper_year": "2022",
      "chunk_content": "Combining a hybrid prediction strategy and a mutation strategy for dynamic multiobjective optimization Ying Chen ${ }^{\\mathrm{a}, \\mathrm{b}}$, Juan Zou ${ }^{\\mathrm{a}, \\mathrm{b}, \\mathrm{c}}$, Yuan Liu ${ }^{\\mathrm{a}, \\mathrm{d}}$, Shengxiang Yang ${ }^{\\mathrm{a}, \\mathrm{d}}$, Jinhua Zheng ${ }^{\\mathrm{a}, \\mathrm{b}, \\mathrm{e}}$, Weixiong Huang ${ }^{b}$${ }^{a}$ Key Laboratory of Intelligent Computing and Information Processing, Ministry of Education, School of Computer Science and School of Cyberspace Science of XiangtanUniversity, Xiangtan, Hunan Province, China${ }^{b}$ Faculty of School of Computer Science and School of Cyberspace Science of Xiangtan University, Xiangtan, 411105, China${ }^{c}$ Hunan Provincial Key Laboratory of Intelligent Information Processing and Application, Hengyang, 421002, China${ }^{d}$ School of Computer Science and Informatics, De Montfort University, Leicester LEI 9RH, U.K ## A R T I C L E I N F O Keywords: Dynamic multiobjective optimization problems Evolutionary algorithms Change response mechanism ## A B STR A C T The environments of the dynamic multiobjective optimization problems (DMOPs), such as Pareto optimal front (POF) or Pareto optimal set (POS), usually frequently change with the evolution process.",
      "question_type": "COMPARATIVE EVALUATION",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:29",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing an Estimation of Distribution Algorithm that utilizes a crossover rate (CR) as a key parameter, how should a developer structure the code to dynamically adjust the CR based on the number of iterations completed, specifically taking into account the difference in optimal CR values for a maximum of 1000 iterations versus 5000 iterations?",
      "topics": [
        "crossover"
      ],
      "answer": "To dynamically adjust the crossover rate (CR) in an Estimation of Distribution Algorithm, the code should include a conditional statement that checks the current iteration number against the maximum number of iterations ($t_{max}$). If $t_{max}$ is 1000, CR should be set to 0.3. If $t_{max}$ is 5000, CR should be set to 0.158. This ensures that the algorithm uses the optimized CR value for the given number of iterations, as identified for case study 1 and case study 2 respectively, to balance exploration and exploitation effectively. The scaling factor ($F_{DE}$), population size ($P_{size}$), maximum weight ($w_{max}$), minimum weight ($w_{min}$), and position parameter ($p$) would also be set according to the number of iterations.",
      "chunk_id": 19007,
      "paper_id": "An improved gravitational search algorithm to the hybrid flowshop with unrelated parallel machines scheduling problem",
      "paper_title": "An improved gravitational search algorithm to the hybrid flowshop with unrelated parallel machines scheduling problem",
      "paper_year": "2021",
      "chunk_content": "The average fitness statistics of the AVG in Table 2 for each level, the ranges, and the ranks are shown in Table 3. The smallest values of the AVG are $23.805,23.860$, $23.880,23.845$ and 23.860 which corresponding to level $4\\left(P_{\\text {size }}=50\\right)$, level $4(C R=0.3)$, level $1\\left(F_{D E}=0.4\\right)$, level $3\\left(w_{\\max }=0.9\\left(w_{\\min }=0.3\\right)\\right)$, and level $2(p=20)$, respectively. We also find that the population size [FORMULA286] has the greatest influence on the IGS algorithm, followed by the crossover rate $C R$ and the maximum weight [FORMULA288]. The position parameter [FORMULA289] is ranked fourth (when the value of [FORMULA290] is 20 , the global search strategy is added at the iteration number of $20 \\% \\cdot t_{\\max }=20 \\% \\cdot 1000=200$ ), and the scaling factor [FORMULA292] is the fifth. Taking both the optimisation quality and efficiency into account, when the maximum number of iterations $\\left(t_{\\max }\\right)$ is 1000 , we set the five key parameters for the case study 1 as follows: $P_{\\text {size }}=50, C R=0.3, F_{D E}=0.4$, $w_{\\max }=0.9\\left(w_{\\min }=0.3\\right), p=20$. Similarly, when the maximum number of iterations [FORMULA296] is 5000 , we set the five key parameters for the case study 2 as follows: $P_{\\text {size }}=50, C R=0.158$, $F_{D E}=0.5, w_{\\max }=0.7\\left(w_{\\min }=0.1\\right), p=30$.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:31",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metric is used to evaluate the difference between levels generated by content generators, as mentioned in the context of Estimation of Distribution Algorithms (EDAs)?",
      "topics": [
        "performance"
      ],
      "answer": "Compression distance is used to measure the difference between two pairs of levels in the content generator's output. This metric falls under the category of level distance metrics and helps assess the EDA's performance in generating varied content.",
      "chunk_id": 4525,
      "paper_id": "Using estimation of distribution algorithm for procedural content generation in video games",
      "paper_title": "Using estimation of distribution algorithm for procedural content generation in video games",
      "paper_year": "2022",
      "chunk_content": "While pattern-based count and pattern-based occurrence are two specialized methods in producing high number and unique meso-patterns, respectively, they tend to perform poorly in other expressivity metrics. EDAPCG's position alongside these methods is noteworthy considering its admissible rankings in other metric categories. Surprisingly, ORE shows great potential in replicating original game patterns, considering the fact that it is a constructive method. Moreover, the poor performances of Launchpad, hopper and notch are in part due to the fact that patternfocused metrics are introduced after the introduction of these methods. Table 9 shows the performance of the content generators in the compression distance metric. In the category of level distance metrics, compression distance measures the difference between two pairs of levels in the content generator's output [3]. EDAPCG's ability to perform well in this metric is twofold; first, it is an evolutionary method and can be configured to have great variation in solution generation. Second, the probabilistic modeling provides extensive knowledge about the estimation of distribution of DE in promising individuals of previous generations, which in turn informs the PCG systems and biases the generation function to produce different solutions for the next generation. This can be observed by visual inspection of sample levels in the next section.",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:33",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs when compared to Genetic Algorithms (GAs)?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The primary objective is to learn and sample the probability distribution of highly fit individuals in each generation, differing from GAs' reliance on crossover and mutation. This approach aims to enhance population diversity by sampling from a probability distribution, thereby reducing the likelihood of becoming trapped in local minima and improving robustness in comparison to GAs.",
      "chunk_id": 8911,
      "paper_id": "Pilot Pattern Optimization for Sparse Channel Estimation in OFDM Systems",
      "paper_title": "Pilot Pattern Optimization for Sparse Channel Estimation in OFDM Systems",
      "paper_year": "2015",
      "chunk_content": "The EDA is an evolutionary algorithm, which learns and samples the probability distribution of the best individuals of a population at each generation rather than operates the crossover and mutation in the GA. Hence, it can be regarded as a probability version of the GA. The EDA introduces the population diversity by sampling individuals from the probability distribution rather than producing one specific individual, and hence is unlikely to trap into local minima and more robust than the GA. Simulation results show that the pilot pattern optimized by EDA is more effective in exploiting channel sparsity than the ones obtained by random search, CEO and GA. The remainder of this letter is organized as follows. In Section II, the sparse channel estimation model and the pilot pattern optimization problem are formulated mathematically. Then, the EDA-based pilot pattern optimization method is proposed in Section III. Finally, simulation results are presented in Section IV for evaluating the performance of the proposed approach. [^0]: Manuscript received December 12, 2014; accepted May 2, 2015. Date of publication May 5, 2015; date of current version July 8, 2015. This work was supported in part by the National Natural Science Foundation of China under Grants 91338201 and 91438109, by a Smart Futures Fellowship funded by the Queensland Government of Australia, and by the Key Laboratory of Universal Wireless Communications (Beijing University of Posts and Telecommunications), Ministry of Education, P.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:34",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the significance function $\\operatorname{sig}_{c}:\\{(1 / n),(1 / 2), 1-(1 / n)\\} \\times\\{0,1\\}^{*} \\rightarrow$ [up, stay, down], and how do the parameters $\\tau_{i} \\in\\{1 / n, 1 / 2\\}$ and $j < i$ influence the algorithm's ability to detect true significances versus false significances?",
      "topics": [
        "bounds"
      ],
      "answer": "The significance function $\\operatorname{sig}_{c}:\\{(1 / n),(1 / 2), 1-(1 / n)\\} \\times\\{0,1\\}^{*} \\rightarrow$ [up, stay, down] scans subsequences of a history, starting from the latest entry, to determine significance. The parameter $\\tau_{i} \\in\\{1 / n, 1 / 2\\}$ serves as the exponent in 'with high probability' (w.h.p.) bounds, influencing the probability of detecting a false significance. A smaller value of $j < i$ reduces the chance of incorrectly identifying a significance by a polynomial factor. However, it concurrently increases the number of samples needed to alter a frequency, resulting in a linear factor of $\\tau_{j}=1-1 / n$ in the runtime. The algorithm also ensures that at least logarithmically many samples are considered before concluding that a significance is found, which eliminates incorrect updates due to small sample sizes with high probability. The algorithm checks if a binomially distributed random variable $X$ deviates significantly from a hypothesis $Y \\sim \\operatorname{Bin}(k, \\tau)$, where $k$ is a positive integer and $\\tau$ is between 0 and 1, meaning there exists a $\\operatorname{Pr}[\\bar{A}]$ such that $\\operatorname{Pr}\\{|X-E[Y]| \\leq s(\\varepsilon, E[Y])| \\leq n^{-\\varepsilon}$, for some $\\varepsilon \\in \\mathbb{R}^{+}$.",
      "chunk_id": 20559,
      "paper_id": "Significance-Based Estimation-of-Distribution Algorithms",
      "paper_title": "Significance-Based Estimation-of-Distribution Algorithms",
      "paper_year": "2020",
      "chunk_content": "The maximum ensures that we consider at least logarithmically many samples before we conclude that we found a significance, eliminating wrong updates due to small samples sizes w.h.p. The parameter $\\tau_{i} \\in\\{1 / n, 1 / 2\\}$ effectively turns into the exponent of the w.h.p. bounds. Thus, a larger value of $j<i$ decreases the probability of detecting a false significance by a polynomial amount. However, it also increases the number of samples necessary in order to change a frequency. This results in a linear factor of $\\tau_{j}=1-1 / n$ in the run time. We provide more details on how $O$ should be chosen at the end of this section (after Corollary 1). We say, for an $\\varepsilon \\in \\mathbb{R}^{+}$, that a binomially distributed random variable $X$ deviates significantly from a hypothesis $Y \\sim \\operatorname{Bin}(k, \\tau)$, where $k \\in \\mathbb{N}^{+}$and $\\tau \\in[0,1]$, if there exists a $\\operatorname{Pr}[\\bar{A}]$ such that $\\operatorname{Pr}\\{|X-E[Y] \\mid \\leq s(\\varepsilon, E[Y])| \\leq n^{-\\varepsilon}$. We now state our significance function $\\operatorname{sig}_{c}:\\{(1 / n),(1 / 2), 1-(1 / n)\\} \\times\\{0,1\\}^{*} \\rightarrow$ [up, stay, down], which scans a history for a significance. However, it does not scan the entire history but multiple subsequences of a history (always starting from the latest entry).",
      "question_type": "conceptual deep",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:37",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners use the UMDA${}^G$ algorithm to address parameter estimation in a Direct Ethanol Fuel Cell (DEFC) model?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "UMDA${}^G$",
        "parameter estimation",
        "Direct Ethanol Fuel Cell (DEFC)"
      ],
      "answer": "Practitioners can use the UMDA${}^G$ algorithm to estimate parameters for mathematical models of DEFCs. This involves obtaining a set of estimated parameters that reproduce the expected profiles of polarization and power curves by changing the operating temperatures. The number of parameters to estimate defines the modeling of the DEFC, providing a feasible simulation to describe the performance of the physical problem.",
      "chunk_id": 7819,
      "paper_id": "Parameter estimation for empirical and semi-empirical models in a direct ethanol fuel cell",
      "paper_title": "Parameter estimation for empirical and semi-empirical models in a direct ethanol fuel cell",
      "paper_year": "2023",
      "chunk_content": "Despite the two presented models having different complexities, the semi-empirical model recovers the values for the parameters of the empirical model; this is, with the estimated parameters in the semi-empirical estimation, we recover similar values of $E_{o v}=0.14698, b=0.14503$ and $C_{2}=16.434$ for the empirical model, see Table $4-60^{\\circ} \\mathrm{C}$. With this result, we unify the two mathematical models. The advantage of our methodology is that our mathematical proposals reproduce the expected profiles of the polarization and power curves by changing the operating temperatures in a DEFC by only using the results of a set of estimated parameters with the UMDA ${ }^{G}$, with this, the limitations, as a surrogate mathematical model, are that the modeling of the DEFC is defined by the number of parameters to estimate, but this is the idea of a surrogate mathematical model, to give us a feasible simulation to describe the performance of a physical problem as simple as possible conserving the physical expected behavior (Asher and Crokei, 2015). ## 5. Conclusions The numerical simulation of DEFC requires accurate models and precise parameters to reproduce its performance. Some of these parameters are not directly measured in the laboratory; hence a method for estimating them is needed.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:38",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, how is the probabilistic model defined using the Gaussian network model, and what are the key parameters that characterize the local density function for a continuous variable?",
      "topics": [
        "optimization"
      ],
      "answer": "The probabilistic model in the EDA is the Gaussian network model. For each continuous variable $X_{i}$ belonging to the set of variables $X$, the local density function is defined as $f(x_{i} \\\\mid p a_{i}^{\\'}, \\\\theta_{i}) \\\\equiv N(x_{i} ; m_{i}+\\\\sum_{x_{i} \\\\in m_{i}} b_{i j}(x_{i}-m_{j}), \\\\frac{1}{v_{i}})$, where $N(x ; \\\\mu, \\\\sigma^{2})$ is the univariate normal distribution with mean value $\\\\mu$ and variance $\\\\sigma^{2}$. The local parameter $b_{i}$ includes $m_{i}$, $b_{i}$, and $v_{i}$, where $m_{i}$ denotes the unconditional mean of $X_{i}$, $b_{i}$ is a column vector, $v_{i}$ is the conditional variance of $X_{i}$ under assigned $p a_{i}^{\\'}$, and $b_{i j}$ is the linear coefficient reflecting the dependence between $T_{f}=\\\\frac{v_{opt}}{v_{inf}}$ and $v_{opt}$.",
      "chunk_id": 20947,
      "paper_id": "A novel strategy for power sources management in connected plug-in hybrid electric vehicles based on mobile edge computation framework",
      "paper_title": "A novel strategy for power sources management in connected plug-in hybrid electric vehicles based on mobile edge computation framework",
      "paper_year": "2020",
      "chunk_content": "EDA tries to obtain the solution by evaluating the general performance in each iteration, rather than digging out the personally optimum in each iteration from the perspective of individual performance solved by GA or PSO. In addition, the unknown information of optimization space can be detected by the probabilistic model, so as to increase the probability of obtaining the optimal solutions [46]. The execution process of EDA is shown in Fig. 3 (a). In the implementation process of EDA, the fitness value function of evaluating the macroscopic performance can be expressed as: $F_{c}=\\int \\omega_{i}\\left(x_{k}, u_{k}\\right)+\\omega_{i} \\frac{P_{\\text {turi }}}{Q_{\\text {inc }}} d_{i}$, where $\\omega_{k}$ is the weight on electric energy consumption. The inequality constraints can be expressed as: $$\n\\left\\{\\begin{array}{l}\nS O C_{\\min } \\leq S O C \\leq S O C_{\\max } \\\\\nP_{\\text {turi, } \\min } \\leq P_{\\text {turi }} \\leq P_{\\text {turi, max }} \\\\\nT_{\\text {con, } \\min } \\leq T_{\\text {con }} \\leq T_{\\text {con, max }} \\\\\nT_{\\text {req, } \\min } \\leq T_{\\text {req }} \\leq T_{\\text {req, max }} \\\\\n\\omega_{\\text {con, } \\min } \\leq \\omega_{\\text {con }} \\leq \\omega_{\\text {con, max }} \\\\\n\\omega_{\\text {req, } \\min } \\leq \\omega_{\\text {req }} \\leq \\omega_{\\text {req, max }}\n\\end{array}\\right.\n$$ where the subscripts min and max denotes the minimum and maximum value of each variable, respectively. The optimization control by EDA is performed in distance domain for better satisfying the local constraints. In the distance domain calculation, velocity at next location can be described as: $v(k+1)=v(k)+\\bar{v}(k) \\frac{2 \\Delta s}{v(k)+v(k+1)}$ where $\\Delta s$ is the calculation step in distance, and $k$ and $k+1$ denote the location at current and next step. The probabilistic model in EDA is the Gaussian network model [47], and each continuous variable $X_{i} \\in X$ in the local density function can be written as: $f\\left(x_{i} \\mid p a_{i}^{\\prime}, \\theta_{i}\\right) \\equiv N\\left(x_{i} ; m_{i}+\\sum_{x_{i} \\in m_{i}} b_{i j}\\left(x_{i}-m_{j}\\right), \\frac{1}{v_{i}}\\right)$ where $N\\left(x ; \\mu, \\sigma^{2}\\right)$ is the univariate normal distribution with mean value $\\mu$ and variance $\\sigma^{2} ; b_{i}$ is the local parameter which includes $m_{i}, b_{i}$ and $v_{i} . m_{i}$ denotes the unconditional mean of $X_{i} ; b_{i}$ is a column vector; $v_{i}$ is the conditional variance of $X_{i}$ under assigned $p a_{i}^{\\prime}$; and $b_{i j}$ is the linear coefficient reflecting the dependence between $T_{f}=\\frac{v_{\\text {opt }}}{v_{\\text {inf }}}$ and $v_{\\text {opt }}$.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:41",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "In the context of intertask alignment for Estimation of Distribution Algorithms (EDAs), how does minimizing the sum of squared residuals (SSR) using an affine transformation strategy, specifically involving the transformation matrix $\\mathbf{A}_{ab}$ and translation vector $\\mathbf{t}_{ab}$, compare to directly optimizing the individual correspondence $\\boldsymbol{C}_{ab}$ between extracted individual sets $\\mathbf{E}_a$ and $\\mathbf{E}_b$?",
      "topics": [
        "fitness"
      ],
      "answer": "The approach minimizes $\\sum_{i=1}^{NS} \\|\\mathbf{p}_{(\\boldsymbol{C}_{ab}(i))}^{\\mathrm{b}}-(\\mathbf{A}_{ab} \\cdot \\mathbf{p}_{(i)}^{a}+\\mathbf{t}_{ab})\\|^{2}$ subject to $\\boldsymbol{C}_{ab}(i) \\neq \\boldsymbol{C}_{ab}(j)$ for $i, j \\in \\{1, \\ldots, NS\\}$, which means that the correspondence $\\boldsymbol{C}_{ab}$ is optimized indirectly through the affine transformation defined by $\\mathbf{A}_{ab}$ and $\\mathbf{t}_{ab}$, aiming to minimize the SSR between transformed individuals from $\\mathbf{E}_a$ and corresponding individuals in $\\mathbf{E}_b$. In contrast, directly optimizing $\\boldsymbol{C}_{ab}$ without the affine transformation would involve maximizing a similarity criterion $J(\\boldsymbol{C}_{ab}(\\mathbf{E}_a), \\boldsymbol{M}_{ab}(\\mathbf{E}_a))$ without explicitly minimizing the squared differences after transformation, potentially leading to a different correspondence and overall alignment performance.",
      "chunk_id": 6974,
      "paper_id": "Aligning heterogeneous optimization problems with optimal correspondence assisted affine transformation for evolutionary multi-tasking",
      "paper_title": "Aligning heterogeneous optimization problems with optimal correspondence assisted affine transformation for evolutionary multi-tasking",
      "paper_year": "2023",
      "chunk_content": "The motivation for this operation lies in that the promising solutions contain the main characteristics of the corresponding fitness landscape and thus facilitate a more reliable transformation. Let $\\mathbf{E}_{a}=\\left\\{\\mathbf{p}_{(1)}^{a}, \\ldots, \\mathbf{p}_{(N)}^{a}\\right\\}$ and $\\mathbf{E}_{b}=\\left\\{\\mathbf{p}_{(1)}^{b}, \\ldots, \\mathbf{p}_{(N)}^{b}\\right\\}$ denote the two extracted individual sets, respectively, then the problem of intertask alignment can be formulated as: $\\max _{\\boldsymbol{C}_{a b} \\cdot \\boldsymbol{M}_{a b}} J\\left(\\boldsymbol{C}_{a b}\\left(\\mathbf{E}_{a}\\right), \\boldsymbol{M}_{a b}\\left(\\mathbf{E}_{a}\\right)\\right)$ s.t. $\\boldsymbol{C}_{a b}\\left(\\mathbf{p}_{(i)}^{a}\\right) \\neq \\boldsymbol{C}_{a b}\\left(\\mathbf{p}_{(j)}^{a}\\right), i, j \\in\\{1, \\ldots, N S\\}$ and $i \\neq j$, where $\\boldsymbol{C}_{a b}: \\mathbf{E}_{a} \\rightarrow \\mathbf{E}_{b}$ denotes the individual correspondence between $\\mathbf{E}_{a}$ and $\\mathbf{E}_{b}, \\boldsymbol{M}_{a b}$ is the transformation to be derived, and $J$ is the criterion for measuring the similarity between $\\boldsymbol{C}_{a b}\\left(\\mathbf{E}_{a}\\right)$ and $\\boldsymbol{M}_{a b}\\left(\\mathbf{E}_{a}\\right)$. In this study, we set $D_{\\mathrm{m}}=\\max \\left(d_{\\mathrm{H}}, d_{\\mathrm{B}}\\right)$ to the sum of squared residuals (SSR) and employ affine transformation as the basic transformation strategy. As a result, Eq. (2) becomes $\\min _{\\mathbf{A}_{a b} \\cdot \\mathbf{t}_{a b} \\cdot \\boldsymbol{C}_{a b}(i) \\in\\{1, \\ldots, N S\\}} \\sum_{i=1}^{N S}\\left\\|\\mathbf{p}_{(\\boldsymbol{C}_{a b}(i))}^{\\mathrm{b}}-\\left(\\mathbf{A}_{a b} \\cdot \\mathbf{p}_{(i)}^{a}+\\mathbf{t}_{a b}\\right)\\right\\|^{2}$, s.t. $\\boldsymbol{c}_{a b}(i) \\neq \\boldsymbol{c}_{a b}(j), i, j \\in\\{1, \\ldots, N S\\}$ and $\\mathbf{A}_{a b}^{\\mathrm{a}}, \\mathbf{t}_{a b}^{\\mathrm{a}}$ [IMAGE_2] Fig. 3. The overall flowchar of OCAT. where \".\" denotes the operator for matrix-matrix multiplication, $\\mathbf{p}_{(k, \\mathrm{~d})}^{\\mathrm{b}}{ }_{(i)}$ is the corresponding individual of $\\mathbf{p}_{(i)}^{\\mathrm{b}}$ in $\\mathbf{E}_{b}, \\mathbf{A}_{a b}$ is a $D_{\\mathrm{m}} \\times D_{\\mathrm{m}}$ ( $D_{\\mathrm{m}}=\\max \\left(d_{\\mathrm{H}}, d_{\\mathrm{B}}\\right)$ ) matrix, and $\\mathbf{t}_{\\mathrm{ab}}$ is a $D_{\\mathrm{m}} \\times 1$ translation vector.",
      "question_type": "comparison",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:44",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model described for personalized search?",
      "topics": [
        "search",
        "Estimation of Distribution Algorithm",
        "Collaborative filtering"
      ],
      "answer": "The interactive estimation of distribution algorithm (IEDA) combines content-based and collaborative filtering techniques to create a dual-probabilistic model. This model is then directly used in the initialization of the IEDA to address sparsity issues. The algorithm aims to efficiently search and locate satisfactory items for users by leveraging both historical search data and social preferences.",
      "chunk_id": 15730,
      "paper_id": "PM-IEDA- Dual probabilistic Model Assisted Interactive Estimation of Distribution Algorithm for Personalized Search",
      "paper_title": "DPM-IEDA: Dual probabilistic Model Assisted Interactive Estimation of Distribution Algorithm for Personalized Search",
      "paper_year": "2019",
      "chunk_content": "Hybrid recommendation by articulating the content-based and collaborative filtering techniques is popular and effective for the personalized recommendation, but has not been developed to improve the performance of IEA for fulfilling the personalized search. Accordingly, we here propose an enhanced interactive estimation of distribution algorithm by designing dual-probabilistic models based on the hybrid recommendation for personalized search. The concept of hybrid personalized search is first defined from the viewpoint of using not only the historical search information but also the social or group preference. A dual-probabilistic model by sufficiently combining the content-based and collaborative filtering is presented and used to design the effective interactive estimation of distribution algorithm (IEDA). The probabilistic model is directly combined with the initialization of IEDA for illuminating the sparsity of the traditional IEA in encoding. The effectiveness of the proposed algorithm in fast and efficient searching with a lower computational cost is experimentally illustrated by two typical personalized searches on movies and TV series described with documents. #### Abstract INDEX TERMS Collaborative filtering, estimation of distribution algorithm, interactive evolutionary computation, personalized search. ## I. INTRODUCTION Personalized search is a powerful solution to the explosive growth in the volume of available products for customers who do the shopping online. The central task in personalized search is to help users/customers locate as more satisfactory ones as possible from innumerable items; thus, it is essentially an optimization problem.",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:45",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What characteristic of the Restricted Boltzmann Machine (RBM) differentiates it from other Estimation of Distribution Algorithms (EDAs), and how does this impact performance?",
      "topics": [
        "optimization"
      ],
      "answer": "The learning characteristic of an RBM differentiates it from other EDAs. Specifically, the RBM learns the distribution of the input data and stores this information in the network weights and biases. This characteristic gives flexibility to the network in determining the probability distribution and the linkage information of the input data, influencing the overall performance of the EDA.",
      "chunk_id": 714,
      "paper_id": "An Energy-Based Sampling Technique for Multi-Objective Restricted Boltzmann Machine",
      "paper_title": "An Energy-Based Sampling Technique for Multi-Objective Restricted Boltzmann Machine",
      "paper_year": "2013",
      "chunk_content": "Several MO estimations of distribution algorithms (MOEDAs) have been developed. Recently, Tang et al. [16] modeled a novel EDA based on a restricted Boltzmann machine for MO optimization (REDA). A restricted Boltzmann machine (RBM) [17]-[21] is an energy-based stochastic neural network with unsupervised learning. The network has a two-layer architecture comprising an input layer and a hidden layer. The network learns the distribution of the input data and stores this information in the network weights and biases. The stability of the network is measured through the energy function of the network and training stops when the network reaches a certain degree of energy equilibrium. The learning characteristic of an RBM differentiates it from the other EDAs. This characteristic gives flexibility to the network in determining the probability distribution and the linkage information of the input data. While the REDA has been shown to be able to solve high-dimensional problems with a large number of decision variables and objective functions [16], its performance, and those of MOEDAs in general, is particularly dependent on the probabilistic modeling and sampling techniques used. Over the past few years, many modeling approaches, including Bayesian tree, decision tree, and principle component analysis (PCA), have been studied.",
      "question_type": "advantage",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:46",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs when applied to the TEAM Workshop benchmark problem 22, and how do the Hybrid Model (HM) and local approximation contribute to the optimization process?",
      "topics": [
        "search"
      ],
      "answer": "The primary objective of the hybrid EDA is to optimize a superconducting magnetic energy storage (SMES) system by minimizing the strayed field while maintaining a prescribed level of stored energy and ensuring the superconducting state of the coils. The Hybrid Model (HM) learns statistical relationships among variables to identify solutions with good performance, while the local approximation refines these solutions by accurately representing the vicinity of selected points, enabling the SQP algorithm to converge to a locally optimal point. The improved points then enhance the HM's global modeling capabilities by training the statistical model to generate points with the desired probabilistic distribution in the search space.",
      "chunk_id": 13799,
      "paper_id": "Hybrid Estimation of Distribution Algorithm Using Local Function Approximations",
      "paper_title": "Hybrid Estimation of Distribution Algorithm Using Local Function Approximations",
      "paper_year": "2009",
      "chunk_content": "[IMAGE_1] Fig. 2. TEAM Workshop Problem 22 (3 variables version): (a) parametric optimization model; (b) solution found by the hybrid EDA. It is interesting to notice the two levels of learning performed by this hybrid EDA: the HM tries to uncover statistical relationships among the variables that result in solutions of good performance. The local approximation, on the other hand, tries to accurately represent the vicinity of a single selected point, in order to allow for the SQP algorithm to converge to a locally optimal point. This operator also contributes to the more global modeling performed by the HM, since a point with higher performance will be employed to train the statistical model, which will in turn be able to generate more points with the desired probabilistic distribution in the search space. ## V. RESULTS The proposed methodology was applied for the solution of a benchmark problem in electromagnetic design. The TEAM Workshop benchmark problem 22 [11] consists on the optimization of a superconducting magnetic energy storage (SMES) system. The objective here is to minimize the strayed field while maintaining a prescribed level for the energy stored in the magnetic field generated. Another constraint deals with the maintenance of the superconducting state for the coils.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:48",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, how does the algorithmic mechanism of post-hoc tests, such as Nemenyi and Bonferroni-Dunn, determine significant differences between algorithms based on their average ranking?",
      "topics": [
        "optimization"
      ],
      "answer": "Post-hoc tests, including Nemenyi and Bonferroni-Dunn, are used to reveal statistically detailed differences between algorithms after a Friedman test indicates significant differences among them. The test principle involves comparing the difference in average ranking between algorithms with $C D$ to determine if a significant difference exists. The Bonferroni-Dunn method, along with Holm, Hochberg, Hommel, Holland, and Rom methods, calculate a set of adjusted $p$-values for the experimental hypotheses.",
      "chunk_id": 5667,
      "paper_id": "An enhanced Kalman filtering and historical learning mechanism driven estimation of distribution algorithm",
      "paper_title": "An enhanced Kalman filtering and historical learning mechanism driven estimation of distribution algorithm",
      "paper_year": "2024",
      "chunk_content": "The results of the Friedman, Friedman Aligned, and Quade tests illustrate that the proposed KFHLEDA has a rank of 5.00, 193.24, and 5.62 on 10D, 3.22, 176.88, and 3.41 on 30D, 2.64, 176.22, 2.85 on 50D, 13.83, 305.21, 13.71 on 100D, respectively. From the experimental results, a significant difference exists between the KFHLEDA and most algorithms. The KFHLEDA is a superior algorithm. ### 4.5. Post-hoc test The statistical results of the Friedman test show whether there are significant differences among [FORMULA313] algorithms. If differences exist, a posthoc test is performed further, which reveals statistically detailed differences between algorithms. Common post-hoc test approaches include Nemenyi, Bonferroni-Dunn, Holm, Hochberg, Hommel, Holland, and Rom tests. The test principle is to compare the difference in average ranking between algorithms with $C D$ to determine the significant difference. The Bonferroni-Dunn, Holm, Hochberg, Hommel, Holland, and Rom methods are introduced to calculate a set of adjusted $p$-values for the experimental hypotheses.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:50",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to solve multi-objective optimization problems, what are some of the pioneering multi-objective approaches that use selection mechanisms based on Pareto ranking and fitness sharing?",
      "topics": [
        "genetic"
      ],
      "answer": "Some of the pioneering multi-objective approaches include the Niched Pareto Genetic Algorithm (NPGA), the Non-dominated Sorting Genetic Algorithm (NSGA), and the Strength Pareto Evolutionary Algorithm (SPEA). These algorithms primarily use selection mechanisms based on Pareto ranking and fitness sharing to maintain diversity within the population while approximating the Pareto optimal set.",
      "chunk_id": 2524,
      "paper_id": "Improved RM-MEDA with local learning",
      "paper_title": "Improved RM-MEDA with local learning",
      "paper_year": "2014",
      "chunk_content": "Keywords Multi-objective optimization problem Estimation of distribution algorithm $\\min F(x)=\\left(f_{1}(x), f_{2}(x), \\ldots, f_{m}(x)\\right)^{T}$ Local learning ## 1 Introduction In real world application, most of the optimization problems involve several objectives. Different from single objective [^0]optimization problems, objectives in a multi-objective optimization problem (MOP) often conflict with each other and these objectives cannot be optimized by a single solution at the same time. So there is a need to find a set of solutions to approximate the Pareto optimal set (PS) for decision maker to make choice. As multi-objective evolutionary algorithms (MOEAs) use a population-based approach which can produce a group of solutions to approximate the PS in a single run, they become the most effective methods in solving multi-objective optimization problems. During the past two decades, MOEAs have been obtained an increasing attention among optimization community. A number of evolutionary algorithms have been developed for multi-objective problems. The Niched Pareto Genetic Algorithm (NPGA) (Horn et al. 1994), the Non-dominated Sorting Genetic Algorithm (NSGA) (Srinivas and Deb 1994) and the Strength Pareto Evolutionary Algorithm (SPEA) (Zitzler and Thiele 1999) constitute the pioneering multi-objective approaches which mainly using selection mechanisms based on Pareto ranking and fitness sharing to maintain diversity. In the past few years, MOEAs characterized by the use of elitism strategy were proposed such as the Pareto Archived Evolution Strategy (PAES) (Knowles and Corne 2000), the Pareto Envelope based Selection Algorithm (PESA) (Corne et al.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:51",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the PBIL algorithm's performance in cloud computing resource scheduling as described, what advantage is suggested in multi-task resource scheduling based on the observed trends in total time taken to complete tasks when varying the number of virtual resources?",
      "topics": [
        "scheduling"
      ],
      "answer": "The analysis suggests that the PBIL algorithm exhibits a significant advantage in multi-task resource scheduling. Specifically, when the number of tasks is fixed, the total time taken to complete the tasks decreases as the number of virtual resources increases. The rate of decrease in total time is observed to be fastest for the PBIL algorithm compared to other algorithms, indicating a more efficient utilization of resources as the resource pool expands.",
      "chunk_id": 1742,
      "paper_id": "A cloud computing resource scheduling scheme based on estimation of distribution algorithm",
      "paper_title": "A Cloud Computing Resource Scheduling Scheme Based on Estimation of Distribution Algorithm",
      "paper_year": "2014",
      "chunk_content": "[IMAGE3] Figure 4. The comparison of the total time in the simulation scenario 2 $(V m s=40)$. TABLE 6:THE COMPARISON OF THE TOTALTIME WITH 4000 TASKS SCHEDULING [IMAGE4] Figure 5. The comparison of the total time with 4000 tasks scheduling and different Vms. As noted in Figure 3 and 4, it can be seen from Table 5 that in certain resources case, when it comes to cloud computing resource scheduling of multitask, the PBIL algorithm proposed in this paper uses the shortest time. Table 6 shows the comparison among the total time Total/ime taken to complete task by three algorithms respectively when 4000 tasks call 20-40 virtual resources. It can be seen from Table 6 that in cloud computing resource scheduling, when the task number is given, the total time taken to complete task reduces gradually with the increase of the number of the resources. From Figure 5, we find the trend of the curve. But the margins of decrease of total time taken by the PBIL algorithm to complete task is the fastest, which suggests that the PBIL algorithm has a significant advantage in the multi-task resource scheduling.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:52",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What is the initial step in the general procedure of an Estimation of Distribution Algorithm (EDA), according to the provided information?",
      "topics": [
        "search"
      ],
      "answer": "The initial step involves generating \\(\\xi_{1} \\leftarrow N\\) individuals using the initial probability distribution, which marks the beginning of the 0th generation. Then, \\(t \\leftarrow 1\\) to indicate the end of the 0th generation.",
      "chunk_id": 15155,
      "paper_id": "Analysis of Computational Time of Simple Estimation of Distribution Algorithms",
      "paper_title": "Analysis of Computational Time of Simple Estimation of Distribution Algorithms",
      "paper_year": "2010",
      "chunk_content": "T. Chen, K. Tang, and G. Chen are with the Nature Inspired Computation and Applications Laboratory, School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China (e-mail: cetacy@mail.ustc.edu.cn; ketang@ustc.edu.cn; glchen@ustc.edu.cn). X. Yao is with the Nature Inspired Computation and Applications Laboratory, School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230027, China, and also with the Center of Excellence for Research in Computational Intelligence and Applications, School of Computer Science, University of Birmingham, Edgbaston, Birmingham B15 2TT, U.K. (e-mail: x.yao@cs.bham.ac.uk). Digital Object Identifier 10.1109/TEVC.2009.2040019 TABLE I GENERAL PROCEDURE OF EDA ``` \\(\\xi_{1} \\leftarrow N\\) individuals are generated by the initial probability distribution; \\% Beginning of the 0th generation. \\(t \\leftarrow 1 ; \\quad \\%\\) End of the 0th generation.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:54:54",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "How should a developer structure the code for the EDA to handle noise and outliers when estimating vanishing points, based on the techniques described?",
      "topics": [
        "heuristic"
      ],
      "answer": "To handle noise and outliers in the estimation of vanishing points within the EDA, the implementation should incorporate mean shift clustering and Laplace linear regression. Mean shift clustering is used to group similar data points, effectively filtering out isolated noise points. Laplace linear regression, expressed as  $$p(\\mathbf{y} \\mid \\mathbf{x}, \\mathbf{w})=\\operatorname{Laplace}(\\mathbf{y} \\mid \\mathbf{w}^{T} \\mathbf{x}) \\propto \\exp (-|\\mathbf{y}-\\mathbf{w}^{T} \\mathbf{x}|)$$, is then applied to the clustered data to provide a robust estimation of the vanishing points, where the goal is to approximate each human body as a vertical pole with head and foot locations. The parameters $\\mathbf{y}$, $\\mathbf{x}$, and $\\mathbf{w}$ are used within the Laplace distribution to model the relationship between observed data and the estimated vanishing points, thus minimizing the impact of outliers during the regression process.",
      "chunk_id": 26177,
      "paper_id": "Camera self-calibration from tracking of moving persons",
      "paper_title": "Camera Self-Calibration from Tracking of Moving Persons",
      "paper_year": "2016",
      "chunk_content": "The advantages of EDAs against most of other metaheuristics are discussed in detail in the review paper [14], including ability to adapt their operators to the structure of the problem, reduced memory requirements, etc. Because parallel computation can be adopted in sampling the population at each generation, the efficiency of EDA can be much higher compared to many other nonlinear optimization approaches. The rest of this paper proceeds by describing the computation of vanishing points in Section 2. The selfcalibration process and optimization of parameters are covered in Section 3. Section 4 presents experimental results and discussions. Finally, Section 5 concludes this paper. ## II. COMPUTATION OF VANISHING POINTS In this section, we first introduce the adaptive segmentation and tracking system adopted, and how the head/foot locations are determined from the results. We then illustrate the process of estimating vanishing points, in which noise and outliers are dealt with using mean shift clustering and Laplace linear regression. ## A. Object Tracking and Head/Foot Localization To find $$\np(\\mathbf{y} \\mid \\mathbf{x}, \\mathbf{w})=\\operatorname{Laplace}\\left(\\mathbf{y} \\mid \\mathbf{w}^{T} \\mathbf{x}\\right) \\propto \\exp \\left(-|\\mathbf{y}-\\mathbf{w}^{T} \\mathbf{x}|\\right)\n$$ and $\\mathbf{x}$, we approximate each human body to be a vertical pole with head and foot locations at its ends. They are extracted based on the tracking result and segmented foreground blob of each object.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:54:56",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific fitness function is used to evaluate the quality of solutions generated by the Estimation of Distribution Algorithm (EDA) for the nurse scheduling problem, considering that feasibility cannot be guaranteed?",
      "topics": [
        "encoding"
      ],
      "answer": "The fitness function used is a penalty function approach, calculated as the sum of $p_{ij}x_{ij}$ over all i and j, plus a penalty term $w_{\\text {demand }} \\sum_{k=1}^{14} \\sum_{s=1}^{p} \\max \\left[R_{k s}-\\sum_{i=1}^{n} \\sum_{j=1}^{m} q_{i s} a_{j k} x_{i j} ; 0\\right]$, where $w_{\\text {demand }}$ is the penalty weight, and the penalty is proportional to the number of uncovered shifts. This function aims to minimize the cost, penalizing solutions that do not meet the demand requirements ($R_{ks}$), where $x_{ij}$ represents the decision variable, $q_{is}$ represents the demand of nurse i for skill s, and $a_{jk}$ represents whether nurse j is assigned to shift k.",
      "chunk_id": 13233,
      "paper_id": "An estimation of distribution algorithm for nurse scheduling",
      "paper_title": "An estimation of distribution algorithm for nurse scheduling",
      "paper_year": "2007",
      "chunk_content": "3.5 Fitness function Independent of the rules used, the fitness of completed solutions has to be calculated. Unfortunately, feasibility cannot be guaranteed. This is a problem-specific issue and cannot be changed. Therefore, we still need a penalty function approach. Since the chosen encoding automatically satisfies constraint set (3) of the integer programming formulation, we can use the following formula, where $w_{\\text {demand }}$ is the penalty weight, to calculate the fitness of solutions. Note that the penalty is proportional to the number of uncovered shifts. $$\n\\sum_{i=1}^{n} \\sum_{j=1}^{m} p_{i j} x_{i j}+w_{\\text {demand }} \\sum_{k=1}^{14} \\sum_{s=1}^{p} \\max \\left[R_{k s}-\\sum_{i=1}^{n} \\sum_{j=1}^{m} q_{i s} a_{j k} x_{i j} ; 0\\right] \\rightarrow \\min !\n$$ Table 1 Comparison of results over 52 instances Table 1 (Continued) # 4 Computational results ### 4.1 Details of algorithms To test the robustness of the proposed approach, each data set was run 20 times by fixing the parameters and varying the pseudo random number seeds. The results are listed in Table 1, in which N/A indicates no feasible solution was found and the last row contains the mean value of all columns. Note that when computing the mean, a censored cost value of 255 has been used in case an algorithm failed to find a feasible solution (N/A). - IP: Optimal or best-known solutions found with IP software (Dowsland and Thompson 2000); - GA: Best result out of 20 runs from a parallel genetic algorithm with multiple subpopulations and intelligent parameter adaptation (Aickelin and Dowsland 2000); - Rd: the most simplified EDA, where only the random rule is used, i.e.",
      "question_type": "definition",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:54:59",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs regarding probability distribution and population modification?",
      "topics": [
        "fitness"
      ],
      "answer": "The algorithmic approach aims to generate and refine a population of individuals by iteratively estimating a probability distribution, \\(\\Gamma_{l}=P(x_{1}, x_{2}, \\cdots, x_{n} | \\eta_{l-1})\\), based on selected individuals \\(\\eta_{l . l}\\). The generated population is then modified to satisfy defined constraints, ensuring feasibility before re-evaluating fitness in subsequent iterations.",
      "chunk_id": 23845,
      "paper_id": "EDA-based Joint Power",
      "paper_title": "EDA-based Joint Power, Subcarrier Allocation and Relay Assignment Scheme for Multiuser Relaying in OFDMA-based Cognitive Radio Systems",
      "paper_year": "2011",
      "chunk_content": "This is done by random sampling according to the uniform distribution, i.e. probability of getting 1 and 0 on any bit position in an individual being equal to 0.5 which means, $$\np_{i}\\left(x_{i}=1\\right)=p_{i}\\left(x_{i}=0\\right)=0.5, i=1,2, \\ldots, n\n$$ The resultant generated population is passed through various defined constraints and if they are not satisfied the population is modified to do so. For iterations $1=1,2, \\ldots, I_{T e r}$ steps 2-7 are followed: Step 2: Rate the individuals of the current population $\\Delta_{l . l}$ by evaluating the fitness function at each particular individual. Step 3: If $D 4: \\sum_{k=1}^{N} p_{l}^{*} \\leq p_{l}^{n+1} \\quad \\forall l$ number of iterations have been processed then terminate, else proceed. Step 4: Select the best $p_{s} \\Delta_{l . l}$ individuals. Step 5: Estimate the probability density distribution on the basis of $\\eta_{l . l}$ best individuals which is, (7) $$\n\\Gamma_{l}=P\\left(x_{1}, x_{2}, \\cdots, x_{n} \\mid \\eta_{l-1}\\right)\n$$ Step 6: Generate new $D 8: \\varepsilon_{l, k}^{*} \\in(0,1) \\quad \\forall(l, k, n)$ individuals on the basis of the new probability distribution (7) and again pass it through all the mentioned constraints to get modified individuals which satisfy all constraints. Step 7: Go to step 2 and repeat. The easiest way to calculate an estimate of the required probability distribution is to treat all the variables of the binary vector as if they were statistically independent from each other.",
      "question_type": "synthesis summary",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:00",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the representation of a UAV in the distributed estimation quantum genetic algorithm (DE-QGA) for the task allocation problem, specifically considering the vector \\$A_{U_{i}}=\\left\\{V_{U_{i}}, F T \\max _{U_{i}}, v_{U_{i}}, P_{U_{i}}\\right\\}\\$?",
      "topics": [
        "genetic"
      ],
      "answer": "In the distributed estimation quantum genetic algorithm (DE-QGA), a UAV, denoted as \\$U_{i}\\$, is represented by a vector \\$A_{U_{i}}=\\left\\{V_{U_{i}}, F T \\max _{U_{i}}, v_{U_{i}}, P_{U_{i}}\\right\\}\\$. This vector consists of \\$V_{U_{i}}\\$ which represents the value of the UAV, \\$F T \\max _{U_{i}}\\$ which represents the maximum flight time of the UAV, \\$v_{U_{i}}\\$ which represents the flight speed of the UAV, and \\$P_{U_{i}}\\$ which represents the coordinates of the UAV. Furthermore, \\$R_{U_{i}}=\\left[r_{U_{i}}^{1}, r_{U_{i}}^{2}, \\ldots, r_{U_{i}}^{N_{r}}\\right]\\$ denotes the vector of resources carried by the UAV, where \\$r_{U_{i}}^{k}\\$ is the amount of the \\$k^{t h}\\$ resource carried by the UAV and \\$N_{r}\\$ is the number of resource types.",
      "chunk_id": 18830,
      "paper_id": "Multi-task Allocation of Multi-UAV Coalition Based on Improved Quantum Genetic Algorithm",
      "paper_title": "Multi-task Allocation of Multi-UAV Coalition Based on Improved Quantum Genetic Algorithm",
      "paper_year": "2021",
      "chunk_content": "The rest of the paper is organized as follows: Section 2 formulates mathematical model for task allocation problem of multi-UAV coalition. Section 3 describes the proposed distributed estimation quantum genetic algorithm (DE-QGA) in detail. Section 4 presents and discusses the simulation results. Finally, Section 5 concludes the paper and notes the directions of future work. ## 2 Problem description and formulation ### 2.1 Problem description As shown in Fig. 1, the problem in this paper is to allocate an optimal coalition of several UAVs for each task. The UAV, task and UAV coalition are described firstly. [IMAGE_0] Fig. 1: Task allocation diagram UAV: $U=\\left\\{U_{1}, U_{2}, \\ldots, U_{N_{u}}\\right\\}$ is the set of $N_{u}$ UAVs. The information of $U_{i}$ can be represented by a vector $A_{U_{i}}=$ $\\left\\{V_{U_{i}}, F T \\max _{U_{i}}, v_{U_{i}}, P_{U_{i}}\\right\\}$, which is made up of the value, maximum flight time, flight speed and coordinates of $k^{t h}$. $R_{U_{i}}=\\left[r_{U_{i}}^{1}, r_{U_{i}}^{2}, \\ldots, r_{U_{i}}^{N_{r}}\\right]$ denotes the vector of resources carried by $N_{t}$, where $r_{U_{i}}^{k}$ is the amount of $k^{t h}$ resource carried by $A_{T_{j}}=\\left\\{Q_{T_{j}}, E x c T_{T_{j}}, P_{T_{j}}\\right\\}$ and $N_{r}$ is the number of resource types.",
      "question_type": "process",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:03",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners calculate the boundaries of the global area, specifically when updating the probability model?",
      "topics": [
        "fitness"
      ],
      "answer": "To calculate the boundaries of the global area in an Estimation of Distribution Algorithm, practitioners should use the formula denoted as Eq. (10) to determine the boundaries \\([L_{i}^{\\'}, U_{i}^{\\'}]\\). This calculation is performed in line 4, before updating the probability model \\(Pr_{i, j}^{C}\\) by Eq. (9) from line 10 to line 12. The bin width \\(W_{\\text {width }}\\) of the promising area is calculated using Eq. (8) in line 3, and the boundaries \\([L_{i}^{\\'}, U_{i}^{\\'}]\\) are then assigned to \\([L_{i}, U_{i}]\\) in line 13.",
      "chunk_id": 5049,
      "paper_id": "A coevolutionary estimation of distribution algorithm based on dynamic differential grouping for mixed-variable optimization problems",
      "paper_title": "A coevolutionary estimation of distribution algorithm based on dynamic differential grouping for mixed-variable optimization problems",
      "paper_year": "2024",
      "chunk_content": "(7); $3 W_{\\text {width }} \\leftarrow$ calculate the bin width of the promising area by Eq. (8); $4\\left[L_{i}^{\\prime}, U_{i}^{\\prime}\\right] \\leftarrow$ calculate the boundaries of the global area by Eq. (10); 5 for [FORMULA211] to [FORMULA212] do $6 k \\leftarrow$ the bin of the variable [FORMULA214] in the j -th individual; $7 v \\leftarrow\\left\\lfloor\\left(k-I_{i}\\right) / W_{\\text {width }}\\right\\rfloor+2$; $8 A_{i} \\leftarrow A_{v}+w e i g t h_{\\text {max }}-j$; end 10 for $v \\leftarrow 1$ to $W_{\\text {column }}+2$ do $11 \\quad P r_{i, j}^{C} \\leftarrow$ update the probability model by Eq. (9); end $13\\left[L_{i}, U_{i}\\right] \\leftarrow\\left[L_{i}^{\\prime}, U_{i}^{\\prime}\\right] ;$ Parameter [FORMULA221] in line 7 means the value [FORMULA222] falling into the [FORMULA223] th bin. Note that the better the individual's fitness, the greater the weight for the corresponding bin. From line 10 to line 12, the probability model [FORMULA224] of the variable [FORMULA225] is updated by Eq. (9). ### 3.3. The framework of $\\mathrm{CoEDA}_{m c}$ Based on the methods introduced previously, the framework of [FORMULA227] algorithm can be presented in Algorithm. 4. From line 3 to line 5, [FORMULA228] updates the grouping results regularly.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:05",
      "generation_style": "practical_application"
    },
    {
      "question": "How does the text describe the behavior of derivative-based algorithms with respect to the objective function in the context of the [FORMULA180] algorithm's comparison?",
      "topics": [
        "comparison"
      ],
      "answer": "The text notes that a derivative-based algorithm could never approach the objective function that is being optimized by the [FORMULA180] algorithm.",
      "chunk_id": 19637,
      "paper_id": "Robust parameter estimation of a PEMFC via optimization based on probabilistic model building",
      "paper_title": "Robust parameter estimation of a PEMFC via optimization based on probabilistic model building",
      "paper_year": "2021",
      "chunk_content": "Also, notice that a derivative-based algorithm could never approach this objective function. # 4.3. Comparison with the most recently reported estimations using a standard dataset We compare the [FORMULA180] respect to the competitive Adaptive Differential Evolution algorithm (ADE) presented in [10] in 2014, and the JAYA and JAYA-NM [52] that are two competitive algorithms published in 2019. Table 6 shows the best solution reported for all the comparing algorithms. In the previous comparison, the data is extracted from the plot of the polarization curve using specialized software, on the contrary, in this case, all the algorithms use exactly the same dataset as it is provided by [52]. The [FORMULA181] version used here delivers the best results, recall that the difference between the results in Table 4 and those in Table 6 is due to the small differences in the Table 5 Comparison of the best results reported in the literature with those of the [FORMULA182] and [FORMULA183] for Case 2 with $P_{o}=1$ bar, $P_{c}=1$ bar, $T=343.15 \\mathrm{~K}$. Table 6 Comparison of [FORMULA187]-SSE results using the experimental polarization curve reported in [52].",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:06",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What limitation is described regarding the Estimation of Distribution Algorithm (EDA) when applied to the Single Row Facility Layout Problem (SRFLP)?",
      "topics": [
        "optimization"
      ],
      "answer": "The Estimation of Distribution Algorithm (EDA) efficiently improves the solution quality in the first few runs, but the diversity loss grows rapidly as more iterations are run when solving the Single Row Facility Layout Problem (SRFLP).",
      "chunk_id": 567,
      "paper_id": "Hybrid Estimation of Distribution Algorithm for solving Single Row Facility Layout Problem",
      "paper_title": "Hybrid Estimation of Distribution Algorithm for solving Single Row Facility Layout Problem",
      "paper_year": "2013",
      "chunk_content": "Hybrid Estimation of Distribution Algorithm for solving Single Row Facility Layout Problem Chao Ou-Yang ${ }^{\\mathrm{a}, \\mathrm{a}}$, Amalia Utamima ${ }^{\\mathrm{a}, \\mathrm{b}}$${ }^{a}$ Department of Industrial Management, National Taiwan University of Science and Technology, Taipei City, 106, Taiwan, ROC${ }^{\\mathrm{b}}$ Information System Department, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia ## A R T I C L E I N F O Article history: Received 18 July 2012 Received in revised form 23 May 2013 Accepted 31 May 2013 Available online 12 June 2013 ## Keywords: Single row facility layout Estimation of Distribution Algorithm Particle Swarm Optimization Tabu Search ## A B S T R A C T The layout positioning problem of facilities on a straight line is known as Single Row Facility Layout Problem (SRFLP). The objective of SRFLP, categorized as NP Complete problem, is to arrange the layout so that the sum of distances between all facilities' pairs can be minimized. Estimation of Distribution Algorithm (EDA) efficiently improves the solution quality in first few runs, but the diversity loss grows rapidly as more iterations are run.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:08",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "How should a developer structure the code for the EDA, based on the description, to specifically address the preservation of diversity and avoidance of premature convergence?",
      "topics": [
        "convergence"
      ],
      "answer": "To address diversity in EDA implementation, the code should incorporate mechanisms for maintaining multiple subpopulations or niches. This involves creating data structures to represent these distinct regions of the search space, allowing the algorithm to explore different areas concurrently. The implementation should ensure that solutions within each niche remain relatively diverse, preventing any single subpopulation from dominating the evolutionary process and leading to premature convergence on a local optimum. This multi-population approach is essential for handling complex problems like multimodal, hierarchical, dynamic, or multiobjective optimization.",
      "chunk_id": 13397,
      "paper_id": "Avoiding premature convergence in estimation of distribution algorithms",
      "paper_title": "Avoiding premature convergence in Estimation of Distribution Algorithms",
      "paper_year": "2009",
      "chunk_content": "The fact that the probability model contains information about the individuals in a population makes possible to identify which zones in the search space will be explored more intensely, but also those which will not. Such information can be taken advantage of so that the algorithm generates diverse solutions and does not stagnate in a local optima. This article is divided into 5 sections besides this introduction. Section 2 describes the state of the art in preservation of diversity on EDAs. Next, Section 3 explains the motivation of this work, and Section 4 describes with detail the proposed method. Afterwards, in Section 5, an experimental study and an analysis of the results are carried out. Last, Section 6 presents our conclusions and proposals for future work. ## II. DIVERSITY AND EDAS As commented above, preservation of diversity is a key point to avoid premature convergence and stagnation in local optima of evolutionary algorithms. This property is, moreover, essential for the success of this kind of algorithms when solving more complex problems, such as the multimodal, hierarchical, dynamic, or multiobjective ones. The main techniques for preservation of diversity try to maintain several subpopulations or niches [4], each one containing solutions belonging to a different region of the search space, in the evolutionary process.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:09",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific optimization problem is addressed using the Estimation of Distribution Algorithm (EDA)?",
      "topics": [
        "optimization"
      ],
      "answer": "The Estimation of Distribution Algorithm (EDA) is applied to optimize the initial core loading pattern (LP) of AHWR-LEU.",
      "chunk_id": 25941,
      "paper_id": "Performance of Estimation of distribution algorithm for initial core loading optimization of AHWR-LEU",
      "paper_title": "Performance of Estimation of distribution algorithm for initial core loading optimization of AHWR-LEU",
      "paper_year": "2016",
      "chunk_content": "Performance of Estimation of distribution algorithm for initial core loading optimization of AHWR-LEU Amit Thakur ${ }^{\\mathrm{a}, *}$, Baltej Singh ${ }^{\\mathrm{a}}$, Anurag Gupta ${ }^{\\mathrm{b}}$, Vibhuti Duggal ${ }^{\\mathrm{b}}$, Kislay Bhatt $x$, P.D. Krishnani $U$${ }^{a}$ Reactor Physics Design Division, Bhabha Atomic Research Centre, Mumbai 400085, India$19.75 \\%$ Computer Division, Bhabha Atomic Research Centre, Mumbai 400085, India ## A R T I C L E I N F O Article history: Received 9 October 2015 Received in revised form 4 February 2016 Accepted 27 May 2016 Available online 20 June 2016 Keywords: Core reloading Loading pattern optimization (LPO) Initial core Evolutionary algorithm Genetic algorithm ## A B S T R A C T Population based evolutionary algorithms now form an integral part of fuel management in nuclear reactors and are frequently being used for fuel loading pattern optimization (LPO) problems. In this paper we have applied Estimation of distribution algorithm (EDA) to optimize initial core loading pattern (LP) of AHWR-LEU.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:10",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of FSS-EBNA, specifically concerning the reliability of cross-validation estimates during the feature subset selection process?",
      "topics": [
        "search"
      ],
      "answer": "The key takeaway is that cross-validation estimates used to guide the search in FSS-EBNA can be overtly optimistic due to the search process itself potentially leading to 'overfitting'. Feature subsets may become well-adapted to the training set but perform poorly on new instances. The amount of training instances is critical; smaller training sets can lead to poor feature subset selection, impacting the generalization performance of the Naive-Bayes classification. Therefore, the accuracy used to guide the search should not be reported as the final accuracy.",
      "chunk_id": 13293,
      "paper_id": "Feature Subset Selection by Bayesian network-based optimization",
      "paper_title": "Feature Subset Selection by Bayesian network-based optimization.Artificial Intelligence, 123(1-2), 157–184",
      "paper_year": "2000",
      "chunk_content": "However, Aha [3] and Kohavi [49], in personal communications, alerted us of the overtly optimistic nature of the crossvalidation estimates which guide the search. Due to the search nature of FSS-EBNA, it is possible that one feature subset (from the big amount of subsets visited) could be very well adapted to the training set, but when presented to new instances not presented in the training process, the accuracy could dramatically decay: an 'overfitting' [78] can occur internally in the FSS process. Although it was not done by some authors, we recommend not to report the accuracy used to guide the search as the final accuracy of an FSS process. Jain and Zongker [40] reported for a non-deceptive function in a pattern recognition problem that the quality of selected feature subsets for small training sets was poor; however, improved as the training set increased. Kohavi [46] also noted in a wrapper machine learning approach that the principal reason of 'overfitting' was the low amount of training instances. To study this issue for FSS-EBNA, we have carried out a set of experiments with different training sizes of the Waveform-40 dataset [15] with the Naive-Bayes classification [IMAGE_3] Fig.",
      "question_type": "evaluation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:12",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of the Multi-Objective Restricted Boltzmann Machine (MORBM) framework, what mathematical principles govern the selection step, specifically how does the probability \\(P(v_{i}=1)\\) influence the inclusion of individuals \\(x_{i}\\) into the new subpopulation, and what is the theoretical basis for using a random number generator in this process?",
      "topics": [
        "initialization"
      ],
      "answer": "Within the MORBM framework, the selection step involves creating a new subpopulation based on the fitness assigned to individuals in \\(Pop(t)\\). The inclusion of an individual \\(x_{i}\\) into this subpopulation is determined by comparing a random number generated from a uniform distribution between 0 and 1, denoted as \\(random(0,1)\\), with the probability \\(P(v_{i}=1)\\). If \\(random(0,1)\\) is less than or equal to \\(P(v_{i}=1)\\), then \\(x_{i}\\) is assigned a value of 1, indicating its selection; otherwise, \\(x_{i}\\) is assigned a value of 0, meaning it is not selected. This process implements a binary tournament selection, where the probability \\(P(v_{i}=1)\\) presumably reflects the fitness of individual \\(x_{i}\\) derived from Pareto ranking and Crowding distance. The use of \\(random(0,1)\\) introduces stochasticity, allowing for exploration of the search space while favoring individuals with higher fitness values as indicated by their higher \\(P(v_{i}=1)\\) values.",
      "chunk_id": 15618,
      "paper_id": "An investigation on sampling technique for multi-objective restricted Boltzmann machine",
      "paper_title": "An Investigation on Sampling Technique for Multi-objective Restricted Boltzmann Machine",
      "paper_year": "2010",
      "chunk_content": "In this section, the main feature of MORBM together with their mathematical formulation and process flow will be presented. Assuming that the test problems consist of $n$ decision variables and $m$ objective functions to be optimized, which can be formularized as Minimize: $F(x)=\\left(f_{1}(x), f_{2}(x), \\ldots, f_{m}(x)\\right)$ Where $x=\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$ The overall framework of MORBM is presented as below: Step (1) Initialization: At generation $t=0$, randomly generate $N$ initial individuals $\\operatorname{Pop}(0)$, and evaluate the fitness of all solutions in $\\operatorname{Pop}(t)$. Step (2) Fitness assignment: For all solutions in $\\operatorname{Pop}(t)$, assign fitness to each individual by Pareto ranking and Crowding distance [2]. Step (3) Selection: Based on the fitness assigned in step (2), select $$\nx_{i}= \\begin{cases}1 & \\text { if } \\operatorname{random}(0,1) \\leq P\\left(v_{i}=1\\right) \\\\ 0 & \\text { otherwise }\\end{cases}\n$$ individuals based on binary tournament selection to form a new subpopulation. Step (4) Probabilistic modeling based on RBM: From the selected subpopulation, a RBM network is built and then trained until the stopping criterion is reached. Let the objective functions, consisting of $x_{i}$ variables, be optimized. In the implementation, $P \\cup \\operatorname{Pop}(t)$ variables imply $\\operatorname{Pop}(t+1)$ input units in RBM.",
      "question_type": "relationship",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:15",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, specifically SHEDA, how should practitioners determine the number of individuals ($N_b$) to select for learning the probabilistic model, considering the number of evaluated individuals ($N_{\\text{arch}}$)?",
      "topics": [
        "search"
      ],
      "answer": "When using SHEDA, the number of individuals for learning the probabilistic model ($N_b$) should be determined relative to the number of evaluated individuals ($N_{\\text{arch}}$). Smaller values of $N_b$ are generally preferred. While the original SHEDA uses $N_b = 0.45 \\times N_{\\text{arch}}$, variants with $N_b = 0.3 \\times N_{\\text{arch}}$ and $N_b = 0.4 \\times N_{\\text{arch}}$ perform similarly and can provide better accuracy and require fewer GPU days than using $N_b = 0.5 \\times N_{\\text{arch}}$, $N_b = 0.6 \\times N_{\\text{arch}}$, or $N_b = 0.7 \\times N_{\\text{arch}}$.",
      "chunk_id": 7702,
      "paper_id": "Surrogate-Assisted Hybrid-Model Estimation of Distribution Algorithm for Mixed-Variable Hyperparameters Optimization in Convolutional Neural Networks",
      "paper_title": "Surrogate-Assisted Hybrid-Model Estimation of Distribution Algorithm for Mixed-Variable Hyperparameters Optimization in Convolutional Neural Networks",
      "paper_year": "2023",
      "chunk_content": "Concluding from the above, the [FORMULA185] can influence the search effectiveness of SHEDA, but the search effectiveness is not that sensitive to [FORMULA186]. ## G. Influence of Individual Number for Learning Probabilistic Models To investigate the influence of the [FORMULA187] (i.e., the number of individuals selected for learning probabilistic model), the TABLE V Comparisons With SHEDA Variants Using the Different Number of Individuals for Updating Probabilistic Models SHEDA is compared with its variants using $N_{b}=0.3 \\times N_{\\text {arch }}$, $N_{b}=0.4 \\times N_{\\text {arch }}, N_{b}=0.5 \\times N_{\\text {arch }}, N_{b}=0.6 \\times N_{\\text {arch }}$, and $N_{b}=0.7 \\times N_{\\text {arch }}$, where $N_{\\text {arch }}$ contains the number of evaluated individuals. The comparison results are given in Table V. The results show that smaller [FORMULA192] is better than larger [FORMULA193] in terms of accuracy and GPU days. According to the Wilcoxon's rank sum test, the original SHEDA ( $N_{b}=$ $\\left.0.45 \\times N_{\\text {arch }}\\right)$ performs similar to its variants with $N_{b}=0.3 \\times$ [FORMULA197] and $N_{b}=0.4 \\times N_{\\text {arch }}$, but significantly better than its variants with $N_{b}=0.5 \\times N_{\\text {arch }}, N_{b}=0.6 \\times N_{\\text {arch }}$, and [FORMULA200] $0.7 \\times N_{\\text {arch }}$. This may be because stricter criterion for choosing the individuals (i.e., smaller [FORMULA202] ) can build a more accurate probabilistic model and accelerate the optimization convergence.",
      "question_type": "practical application",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:16",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Estimation of Distribution Algorithms (EDAs) discussed, what search ability guarantees are provided for the EDAs that do not hybridize with other local search algorithms when applied to the flexible job shop scheduling problem?",
      "topics": [
        "scheduling"
      ],
      "answer": "The text states that EDAs which do not hybridize with other local search algorithms have \"well search ability\" when applied to the flexible job shop scheduling problem. It further elaborates that the probability model, built by considering the information of neighboring operations appearing in superior population, reflects the character of the flexible job shop scheduling problem and improves the ability of the EDAs to solve it. However, no specific theorems, complexity notations, or mathematical proofs are provided to quantify or guarantee this \"well search ability\". The evaluation of the algorithm's efficiency is based on simulation results.",
      "chunk_id": 24265,
      "paper_id": "An New Estimation of Distribution Algorithm Based Edge Histogram Model for Flexible Job-Shop Problem",
      "paper_title": "An New Estimation of Distribution Algorithm Based Edge Histogram Model for Flexible Job-Shop Problem",
      "paper_year": "2011",
      "chunk_content": "It shows that the EDAS which don't hybrid other local search algorithm in this paper has well search ability. # 5 Conclusions In this paper, we adopt EDAS to solve the flexible job shop scheduling problem. The probability model is built through considering the information of neighboring operations appearing in superior population. This makes the probability model well reflect the character of flexible job shop scheduling problem, and improves the ability of edas to solve flexible job shop scheduling problem. The simulation results show that the proposed algorithm is efficient. Acknowledgments. This paper is supported by The Shanxi Science Foundation for Young Scientists under Grant 2010021017-2, China.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:18",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of the RHC (Randomized Hill Climbing) algorithm compare when used to generate 'easy' versus 'hard' instances of the NK fitness landscape, given that the branch-and-bound algorithm is used to compute the optimum for each new instance?",
      "topics": [
        "optimization"
      ],
      "answer": "The RHC algorithm, with a maximum of 50 evaluations, is used to generate both 'easy' and 'hard' instances for the NK fitness landscape. The distinction lies in the properties of the instances generated, as assessed by the branch-and-bound algorithm. The branch-and-bound algorithm, which guarantees finding optimal solutions, is executed every time a new instance is generated by RHC to determine the new optimum of the NK fitness landscape function. The experiment aims to investigate if network measures can capture the differences between these 'easy' and 'hard' instance sets, implying that RHC's effectiveness in modifying the landscape differs depending on whether it's creating an 'easy' or 'hard' instance, as classified by the branch-and-bound algorithm's performance.",
      "chunk_id": 10205,
      "paper_id": "Evolving NK-complexity for evolutionary solvers",
      "paper_title": "Evolving NK-complexity for Evolutionary Solvers",
      "paper_year": "2012",
      "chunk_content": "If the optimization function is improved, then the new instance is accepted, otherwise another possible modification of the neighborhood structure is proposed. The maximum number of evaluations allowed to RHC was 50. To generate our benchmark, we used an initial dataset of 9000 instances, 1000 for every possible combination of $n \\in\\{20,28,34\\}$ and $K \\in\\{4,5,6\\}$. All instances were solved using a branch-and-bound algorithm as described in [3]. The branch-and-bound algorithm ${ }^{1}$ guarantees that the optimal solutions are found. Every time a new instance is generated using RHC we need to run the branch-and-bound algorithm to compute the new optimum of the NK fitness landscape function. Starting from each of the 9,000 instances we generated two additional instances, one easy and one hard instance. The final benchmark comprises these additional 18,000 instances. ## 3. EXPERIMENTS The objective of the experiments is to investigate whether the network measures extracted from the evolved instances capture the differences between the sets of easy and hard instances. First, we compute for each instance a large set [^0] [^0]: [FORMULA10] We use the implementation by the author, available from http://medal.cs.umsl.edu/software.php of network measures that serve as topological descriptors.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:20",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing the weighted sum method for solving a multi-objective optimization problem like economic/emission dispatch, what programming considerations should be taken into account regarding the weights assigned to each objective?",
      "topics": [
        "evolutionary"
      ],
      "answer": "When implementing the weighted sum method, a developer must consider that different weights assigned to the objectives (e.g., cost and emission) will yield different non-dominated solutions. The method requires multiple runs with different weight combinations to explore the Pareto-optimal front. Furthermore, the weighted sum method might fail to find Pareto-optimal solutions if the Pareto-optimal front is non-convex.",
      "chunk_id": 21765,
      "paper_id": "NSGA-II EDA Hybrid Evolutionary Algorithm for Solving Multi-objective Economic Emission Dispatch Problem",
      "paper_title": "NSGA-II/EDA Hybrid Evolutionary Algorithm for Solving Multi-objective Economic/Emission Dispatch Problem",
      "paper_year": "2018",
      "chunk_content": "Different strategies to reduce atmospheric emissions include installation of pollutant-cleaning equipment, switching to low-emission fuels, and replacement of aged fuel burners and generator units [2, 3]. These approaches, however, lead to increased cost of operation and can be taken as long-term solutions. An attractive short-term alternative is emission dispatching option in which both emission and generation cost are to be minimized [4]. This makes the problem a multi-objective optimization problem. Several methods have been used in the literature to solve the multi-objective economic/emission power dispatch (EED) problem and they can be classified as classical, evolutionary algorithm (EA), and hybrid approaches. Classical aggregation methods include the weighted sum method [5] and the $\\epsilon$-constraint method [6, 7]. In the weighted sum method, the two objectives are linearly combined into one by forming their weighted sum. Although this method is simple to implement, it requires multiple runs with different weights to obtain different non-dominated solutions. It also cannot find Pareto-optimal solutions located in a non-convex Pareto-optimal front [8, 9]. The $g_{j}$-constraint method optimizes a most preferred objective and restricts the others within user-specified values.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:22",
      "generation_style": "implementation_focused"
    },
    {
      "question": "According to Muhlenbein and Mahnig's restatement of Holland's conjecture, how should the probability of a schema \\(\\xi\\) at time \\(t\\), denoted as \\(P(\\xi, t)\\), change in relation to its observed usefulness?",
      "topics": [
        "search"
      ],
      "answer": "The probability \\(P(\\xi, t)\\) should increase or decrease at a rate proportional to the difference between the average fitness of the schema \\(\\xi\\) at time \\(t\\), denoted as \\(\\hat{\\mu}_{\\xi}(t)\\), and the average fitness of the entire population at time \\(t\\), denoted as \\(\\hat{\\mu}(t)\\). This relationship is expressed as \\(\\frac{d P(\\xi, t)}{d t}=\\left(\\hat{\\mu}_{\\xi}(t)-\\hat{\\mu}(t)\\right) P(\\xi, t)\\).",
      "chunk_id": 26627,
      "paper_id": "On the convergence of an estimation of distribution algorithm based on linkage discovery and factorization",
      "paper_title": "On the convergence of an estimation of distribution algorithm based on linkage discovery and factorization",
      "paper_year": "2005",
      "chunk_content": "This suggests that by using higherorder information about the fitness function, the FDA is better able to escape being trapped in local optima. Zhang and Muhlenbein [28] have shown that an infinite population model of the FDA with real variables will converge to the optimum for an additively decomposable fitness function with the running intersection property. An algorithm that learns a factorization, the LFDA, is described but only minimally developed. A hybrid algorithm between the FDA and the LFDA is applied to graph bipartitioning problems in [17]. Muhlenbein and Mahnig [18, 17] point out how sampling according to the Boltzmann distribution fulfills a conjecture by Holland [8] on a good population-based search algorithm. In the following, $P(\\xi, t)$ is the probability of schema $\\xi$ at time $t$. Conjecture 1. Each (schema) $\\mathcal{L}=\\{0,1, \\ldots, \\ell-1\\}$ represented in the current population $B(t)$ should increase (or decrease) in a rate proportional to its \"observed\" \"usefulness\" $\\hat{\\mu}_{\\xi}(t)-\\hat{\\mu}(t)$ (average fitness of schema $m$ minus average fitness of the population) $$\n\\frac{d P(\\xi, t)}{d t}=\\left(\\hat{\\mu}_{\\xi}(t)-\\hat{\\mu}(t)\\right) P(\\xi, t)\n$$ This is Muhlenbein and Mahnig's [18] restatement of Holland's conjecture ([8], page 88).",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:23",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs using the PBIL model, particularly regarding balancing exploration and exploitation?",
      "topics": [
        "control"
      ],
      "answer": "The primary purpose of the described algorithmic approach using the PBIL model within the EDA framework is to dynamically control the balance between exploration and exploitation. This is achieved by updating a probability vector, denoted as $\\mathcal{P}_{\\text {Mutation}}$, and using a learning rate 'g' that changes with each generation $\\beta(\\mathrm{g})$. The parameter $\\Gamma$ controls the contributions of location and global information, specifically leveraging the global best solution $\\mathcal{B}_{s}$ and probability vector $\\mathcal{H}_{\\Gamma_{i}}$, to maintain this balance during the generation of new solutions using the GM operator.",
      "chunk_id": 16537,
      "paper_id": "An evolutionary algorithm based hyper-heuristic framework for the set packing problem",
      "paper_title": "An evolutionary algorithm based hyper-heuristic framework for the set packing problem",
      "paper_year": "2019",
      "chunk_content": "At each generation $\\beta(\\mathrm{g})$, a parent set, say Parent $(g)$, is formed by selecting the best $\\frac{N_{p}}{}$ candidate solutions from the current population $\\operatorname{Pop}(g)$. Variable $14: \\mathrm{g} \\leftarrow \\mathrm{g}+1$ is the learning rate, which changes dynamically with the generation. As $\\Gamma=$ controls the contributions of location and global information and helps to maintain a balance between exploitation and exploration. The pseudo-code of the probability update vector using the PBIL model is given in Algorithm 5.  After the probability vector $\\mathcal{P}_{\\text {Mutation }}$ is updated, the GM operator is applied to generate a new solution. Set $\\sum_{i=1}^{|\\Gamma|}\\left|\\mathcal{H}_{\\Gamma_{i}} \\cap \\Gamma\\right|>0$, initially empty, is a partial solution in which objects are added either by sampling the probability vector $\\mathcal{H}_{\\Gamma_{i}}$ or directly copying from the global best solution, say $\\mathcal{B}_{s}$.",
      "question_type": "application",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:25",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of RM-MEDA utilize local principal component analysis to model population movements, and what role does the number of clusters, denoted as $\\varepsilon$, play in the effectiveness of this modeling?",
      "topics": [
        "optimization"
      ],
      "answer": "RM-MEDA, which is a kind of Estimation of Distribution Algorithm, models population movements by employing the ($m-1$)-dimensional local principal component analysis, shortened to ((m-1)-D local PCA), over candidate solutions. In RM-MEDA, the population is divided into a number of clusters to build the model. The number of clusters is represented by $\\varepsilon$, and it has a significant effect on the performance of RM-MEDA. Ideally, if the number of clusters is equal to the required one, the cluster $Ci$ obtained by the model could approximate one of the pieces of the PS effectively, where PS represents the Pareto Set.",
      "chunk_id": 21887,
      "paper_id": "A full variate Gaussian model-based RM-MEDA without clustering process",
      "paper_title": "A full variate Gaussian model-based RM-MEDA without clustering process",
      "paper_year": "2018",
      "chunk_content": "Thus, the PS of a continuous bi-objective or tri-objectives optimization problem is a piecewise continuous curve or surface in $\\xi=\\zeta+\\varepsilon$. Based on the above analysis of regularity, a regularity model-based multi-objective estimation of distribution algorithm named RM-MEDA has been proposed by Zhang et al. [20]. As a kind of EDAs [7], RM-MEDA describes the movements of populations by employing the ( $m-1$ )-dimensional local principal component analysis ((m-1)-D local PCA) [21] to build the model over candidate solutions in the decision space. In RM-MEDA, the population is divided into a number of clusters to build the model. Furthermore, the number of clusters is fixed in the algorithm even when solving different kinds of problems. Nevertheless, Wang et al. found that the number of clusters $\\varepsilon$ had a significant effect on the performance of RM-MEDA. There will be 3 situations when determining the value of $n$, that is to say, the given value is smaller, larger than, or equal to the required one. Ideally, if $P t$ is equal to the required one, the cluster $C i$ obtained by the model could approximate one of the pieces of the PS effectively.",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:27",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners generate new solutions given that EDAs use an explicit probability model?",
      "topics": [
        "Estimation of Distribution Algorithms"
      ],
      "answer": "When implementing an EDA, new solutions are generated using an explicit probability model. The probabilities for this model are computed directly from the frequency statistics of the selected top individuals from a population. These probabilities indicate the likelihood of a particular solution being included in a set of top quality solutions based on prior observations.",
      "chunk_id": 20237,
      "paper_id": "Multi-speed Gearbox Synthesis Using Global Search and Non-convex Optimization",
      "paper_title": "Multi-speed Gearbox Synthesis Using Global Search and Non-convex Optimization",
      "paper_year": "2020",
      "chunk_content": "For this reason, the evaluation function of a goal state is not the cost of the configuration but is the lower bound defined above. Our algorithm does not terminate when a goal state is found, but when the gap between lower and upper bound is 0 or all the states have been explored. To solve the transmission ratio path assignment problem, we run another BFS using the assignment model in Sect. 5.2, where we start from a state where none of the transmission ratios is assigned and actions correspond to the assignment of a simple path to a transmission ratio. # 6.3 Estimation of Distribution Algorithm The last type of global search method is EDA, a class of meta-heuristic approaches based on the evolution of populations [14]. While typical evolutionary algorithms, such as genetic algorithms, use variation operators such as cross-over and mutation, EDAs use an explicit probability model to generate new solutions, showing advantages in terms of performance, theoretical convergence, and capturing the structure of the problem space [14]. The probabilities are computed by directly using the frequency statistics of the selected top individuals from a population and indicate the likelihood of a particular solution being included in a set of top quality solutions based on prior observations. EDA has also been successfully applied to the configuration design of vehicle suspension systems [7].",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:28",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the MODEDA model discussed, what empirical convergence observations were made when compared to NSGA-II and deep network models in the context of the 10 knapsack problems?",
      "topics": [
        "convergence"
      ],
      "answer": "The MODEDA model exhibited convergence to a higher hypervolume value with fewer iterations compared to the NSGA-II model. Specifically, NSGA-II required 20 times more generations than MODEDA to achieve similar results. While the convergence value of MODEDA was slightly lower than that of RBM-EDA and DBN-EDA models, MODEDA ultimately converged to a higher hypervolume value than those deep network models.",
      "chunk_id": 3942,
      "paper_id": "Multi-Objective Deep Network-Based Estimation of Distribution Algorithm for Music Composition",
      "paper_title": "Multi-Objective Deep Network-Based Estimation of Distribution Algorithm for Music Composition",
      "paper_year": "2022",
      "chunk_content": "For the 10 knapsack problems, the convergence profile is shown in Figure 7. We observed that the MODEDA model converges to a higher hypervolume value with lower iteration than the NSGA-II model. Under the same conditions, NSGA-II had to use 20 times more generations to achieve the same results compared to the proposed algorithm. Although the convergence value was slightly lower than that of RBM-EDA and DBN-EDA models, our model finally converged to a higher hyper-volume value, demonstrating superiority in terms of convergence of our model compared to deep network models. ## C. MELODY COMPOSITION EXPERIMENT ## 1) EXPERIMENT SETTINGS In this paper, two experiments were conducted to evaluate the performance of the proposed algorithm (MODEDA), the Knapsack problem and the music generation. We demonstrated the scalability of the model in the LSMOP problem by sampling a set of solutions similar to the probability distribution of the population through VAE-based dimension reduction by comparing it with RBM-EDA and DBN-EDA through the knapsack experiment. On the other hand, the music experiment focused on how wide and dense the pareto set is optimized for the two objective functions of music stability and tension.",
      "question_type": "evaluation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:29",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "According to the details in the chunk, what advantages does BMDA have over UMDA and EBNA in solving NK landscape problems with large size or high connectivity, based on solution quality and convergence speed?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "BMDA",
        "UMDA",
        "EBNA",
        "NK landscapes",
        "model complexity",
        "solution quality",
        "convergence speed"
      ],
      "answer": "BMDA outperforms UMDA and EBNA on NK landscape problems with large size or high connectivity, such as NK(10,6) and NK(30,6), in terms of solution quality and convergence speed. While UMDA is effective for simple, small problems, it may suffer from premature convergence on more complex problems. Although EBNAs have even higher model complexity than BMDA, they may not perform satisfactorily on complex problems and converge slower than UMDA and BMDA.",
      "chunk_id": 682,
      "paper_id": "Effect of model complexity for estimation of distribution algorithm in NK landscapes",
      "paper_title": "Effect of Model Complexity for Estimation of Distribution Algorithm in NK Landscapes",
      "paper_year": "2013",
      "chunk_content": "EDAs for small [FORMULA173] and [FORMULA174]. However, as [FORMULA175] or [FORMULA176] increases, the significance emerges: BMDA outperforms other methods on the problems with large size or high connectivity (e.g. $\\mathrm{NK}(10,6)$ and $\\mathrm{NK}(30,6)$ ). Similar to the results of anytime behavior, the three EBNAs have insignificant difference in the solution quality. This outcome reveals that EBNA is insensitive to the order of degrees for the graphical structure in terms of solution quality. According to the above results on the NK landscape problems, EDAs with low model complexity like UMDA render good optimizers for simple small problem, namely, smaller problem size and low interaction between variables. Nonetheless, for more complex problems, such EDAs may suffer from premature convergence. BMDA, on the other hand, can nicely resolve the complex problems (large size and strong interaction) in terms of solution quality and convergence speed. Surprisingly, EDAs with even higher model complexity like EBNA may not perform satisfactorily on the complex problems, as shown in our experiments. Additionally, EBNAs converge slower than UMDA and BMDA.",
      "question_type": "COMPARATIVE EVALUATION",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:31",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the Bayesian Information Criterion (BIC) as modified for a joint mixture distribution in Estimation of Distribution Algorithms, what programming considerations are necessary to handle the mixture components and their parameters?",
      "topics": [
        "complexity"
      ],
      "answer": "When implementing the modified BIC, a developer must account for `K` mixture components. Each component requires storing the expected number of selected individuals, denoted as `|S_i|`, which are drawn from a probability distribution `f_((ζ, θ_i))(Y)`. Furthermore, the parameters `θ_i` for each of the `i`-th mixture components must be managed. The summation \\(\\sum_{i=1}^{K}\\) suggests an iterative process in the code, looping through each mixture component to calculate its contribution to the overall BIC score. The weighting factor \\(\\beta_{i j}\\) where \\(\\sum_{j=0}^{c_{i}-1} \\beta_{i j}=1\\) is the number of mixture components, should be enforced as a constraint in the implementation.",
      "chunk_id": 26982,
      "paper_id": "Real-Coded Bayesian Optimization Algorithm Bringing the Strength of BOA into the Continuous World",
      "paper_title": "Real-Coded Bayesian Optimization Algorithm  Bringing the Strength of BOA into the Continuous World",
      "paper_year": "2004",
      "chunk_content": "In (2), the first and second terms represent the model fitting error and the model complexity, respectively. Since minimal negative log-likelihood is equivalent to minimal entropy, (2) is rewritten as $$\nB I C\\left(f_{(\\zeta, \\boldsymbol{\\theta})}(\\mathbf{Y}), \\mathcal{S}\\right)=|\\mathcal{S}| h\\left(f_{(\\zeta, \\boldsymbol{\\theta})}(\\mathbf{Y})\\right)+\\lambda \\ln (|\\mathcal{S}|)|\\boldsymbol{\\theta}|\n$$ [5], [6] where $h\\left(f_{(\\zeta, \\boldsymbol{\\theta})}(\\mathbf{Y})\\right)$ represents the differential entropy of $\\mathbf{Z}^{i}, \\beta_{i j}$. Although the BIC fails to exactly capture the types of interaction between variables, the important point is to have a knowledge of the variables which are dependent regardless of linearity or nonlinearity. The reason for this assertion is that the dependent type itself is learned in the model fitting phase (in Section 2.2). However, the BIC might lead to incorrect factorization if there is some kind of symmetry in the selected individuals. In other words, there is a high possibility that the dependent variables are learned as independent ones. In order to avoid this problem as well as to enhance the reliability of learning dependency, a (joint) mixture distribution is employed for modeling the selected individuals. With this in view, the BIC in (3) can be modified as (4) $$\nB I C\\left(K, f_{(\\zeta, \\boldsymbol{\\theta})}(\\mathbf{Y}), \\mathcal{S}\\right)=\\sum_{i=1}^{K}\\left\\{\\left|\\mathcal{S}_{i}\\right| h\\left(f_{(\\zeta, \\boldsymbol{\\theta}_{i})}(\\mathbf{Y})\\right)\\right\\}+K \\lambda \\ln \\left(|\\mathcal{S}|\\right)\\left|\\boldsymbol{\\theta}_{i}\\right|\n$$ where $\\sum_{j=0}^{c_{i}-1} \\beta_{i j}=1$ is the number of mixture components, $\\left|\\mathcal{S}_{i}\\right|$ is the expected number of selected individuals drawn from a probability distribution $f_{\\left(\\zeta, \\boldsymbol{\\theta}_{i}\\right)}(\\mathbf{Y})$, and $\\boldsymbol{\\theta}_{i}$ is parameters of $i$ th mixture component.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:33",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the assembly system's performance be assessed, considering the processing routes of type-1 to type-4 parts, specifically focusing on the sequences denoted by 'w' and 'p'?",
      "topics": [
        "algorithm"
      ],
      "answer": "The quality of the assembly system's performance can be assessed by analyzing the processing routes of type-1 to type-4 parts, represented by \\$w_{1}=o_{1s} o_{11} o_{12} o_{13} o_{14} o_{1e}\\, \\$w_{2}=o_{2s} o_{21} o_{22} o_{23} o_{2e}\\, \\$w_{3}=o_{3s} o_{31} o_{32} o_{33} o_{3e}\\, and \\$w_{4}=o_{4s} o_{41} o_{42} o_{43} o_{44} o_{4e}\\, respectively, and \\$p_{1s} t_{11} p_{11} t_{12} p_{12} t_{13} p_{13} t_{14} p_{14} t_{15} p_{1e}\\, \\$p_{2s} t_{21} p_{21} t_{22} p_{22} t_{14} p_{14} t_{15} p_{1e}\\, \\$p_{3s} t_{31} p_{31} t_{32} p_{32} t_{33} p_{33} t_{34} p_{3e}\\, and \\$p_{4s} t_{41} p_{41} t_{42} p_{42} t_{43} p_{43} t_{33} p_{33} t_{34} p_{3e}\\, respectively. The sequences 'w' and 'p' detail the order of operations and transitions, allowing for the identification of potential bottlenecks or inefficiencies. Evaluating the time spent in each operation (o) and transition (t), as well as the utilization of resources \\$r_{1}, r_{2}, r_{3}, r_{4}, r_{5}, r_{6}, r_{7}\\$, provides insights into the system's overall performance.",
      "chunk_id": 21448,
      "paper_id": "Deadlock-Free Scheduling of Flexible Assembly Systems Based on Petri Nets and Local Search",
      "paper_title": "Deadlock-Free Scheduling of Flexible Assembly Systems Based on Petri Nets and Local Search",
      "paper_year": "2018",
      "chunk_content": "The system can manufacture four types of parts and assemble two types of products. The part type set is $J=\\left\\{J_{i} \\mid i \\in \\mathbb{N}_{4}\\right\\}$. Type-1 parts are manufactured on $r_{1}, r_{2}$, and $r_{3}$ and type-2 parts on $r_{4}$ and $r_{6}$ sequentially. Then type-1 and type-2 parts are assembled on $r_{2}$. Type-3 parts are manufactured on $r_{5}$ and $t_{q}$ and type-4 parts on $r_{1}, r_{5}$, and $r_{7}$ sequentially. Then type-3 and type-4 parts are assembled on $S\\left(N\\right.$. The processing routes of type-1 4 parts are $w_{1}=o_{1 s} o_{11} o_{12} o_{13} o_{14} o_{1 e}$, $w_{2}=o_{2 s} o_{21} o_{22} o_{23} o_{2 e}, w_{3}=o_{3 s} o_{31} o_{32} o_{33} o_{3 e}$, and $w_{4}=o_{4 s} o_{41} o_{42} o_{43} o_{44} o_{4 e}$, respectively. The processing routes of type-1 4 are modeled by $p_{1 s} t_{11} p_{11} t_{12} p_{12} t_{13} p_{13} t_{14} p_{14} t_{15} p_{1 e}$, $p_{2 s} t_{21} p_{21} t_{22} p_{22} t_{14} p_{14} t_{15} p_{1 e}, p_{3 s} t_{31} p_{31} t_{32} p_{32} t_{33} p_{33} t_{34} p_{3 e}$, and $p_{4 s} t_{41} p_{41} t_{42} p_{42} t_{43} p_{43} t_{33} p_{33} t_{34} p_{3 e}$, respectively. Since $o_{14}$ and $o_{23}$ represent the same activity, their activity places and start and completion transitions are the same. So are $o_{1 e}$ and $o_{2 e}, o_{33}$ and $o_{44}, o_{3 e}$ and $o_{4 e}$. Then, $P=\\left\\{p_{11}, p_{12}, p_{13}\\right.$, $\\left.p_{14}, p_{21}, p_{22}, p_{31}, p_{32}, p_{33}, p_{41}, p_{42}, p_{43}\\right\\}, P_{s}=\\left\\{p_{1 s}, p_{2 s}\\right.$, $\\left.p_{3 s}, p_{4 s}\\right\\}, P_{e}=\\left\\{p_{1 e}, p_{3 e}\\right\\}$, and $P_{R}=\\left\\{r_{1}, r_{2}, r_{3}, r_{4}, r_{5}\\right.$, $\\left.r_{6}, r_{7}\\right\\}$.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:37",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs in solving the Flexible Job Shop Problem (FJSP)?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The primary purpose is to leverage probability analysis to construct and maintain a probabilistic model that characterizes satisfactory solutions for the FJSP. This model guides the exploration of the search space, facilitating the evolutionary process and effectively overcoming the disruption of blocks, a problem that traditional Genetic Algorithms (GAs) often struggle with, especially in high-dimensional complex problems.",
      "chunk_id": 24257,
      "paper_id": "An New Estimation of Distribution Algorithm Based Edge Histogram Model for Flexible Job-Shop Problem",
      "paper_title": "An New Estimation of Distribution Algorithm Based Edge Histogram Model for Flexible Job-Shop Problem",
      "paper_year": "2011",
      "chunk_content": "1 Introduction The job-shop scheduling problem is one of the most popular manufacturing optimization models used in practice. Flexible job shop problem (FJSP) is a generalization of the job shop that allows one operation to be processed on any machine from a set of alternative machines. It is closer to the real manufacturing situation. The extension includes two tasks: Assignment operations to appropriate machines and sequencing the operations on each machine. These features of the FJSP obviously increase the complexity of finding optimal solutions even of approximately optimal solutions. So the task is more challenging than the classical job shop[1,2]. In recent years, metaheuristic techniques such as simulated annealing, tabu search and evolutionary algorithms have been adopted to solve the FJSP to find a promise solution and many promising results have been obtained $[3,4,5]$. A new evolutionary algorithm based on probability analysis, i.e., estimation of distribution algorithms (EDAS) has become very popular. The algorithm relies on the construction and maintenance of a probability model that characterizes satisfactory solutions for a problem. And this probabilistic model is used to guide further exploration of search space and realize evolution process. EDAS effectively overcomes the problem of blocks being disrupted, has shown to perform very well on a wide variety of problem that traditional GA difficult to solved, especially on high dimensional complex problems $[6,7]$.",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:39",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the two-stage assignment strategy (TSAS1) contribute to minimizing the total disturbance amount in the scheduling scheme, and what are the potential drawbacks of this approach?",
      "topics": [
        "scheduling"
      ],
      "answer": "The two-stage assignment strategy (TSAS1) aims to minimize the total disturbance amount in the scheduling scheme by selecting the most proficient worker for a given operation. Specifically, for an operation, TSAS1 selects the worker from the set of alternative workers \\$\\mathbf{M}_{j}^{W}\\$ by maximizing their proficiency, according to the policy of \\$\\max p f_{i j}^{\\prime \\prime}, \\forall i, j, r=1, \\ldots, K\\$. In this equation, \\$pf_{ij}^{\\prime \\prime}\\$ represents the proficiency of worker i for job j. The advantage of using TSAS1 is that it can minimize the processing time variance for each operation. However, a potential drawback is that when workers are consecutively assigned and become constrained resources, the degree of deterioration of makespan also increases.",
      "chunk_id": 17301,
      "paper_id": "A two-stage assignment strategy for the robust scheduling of dual-resource constrained stochastic job shop scheduling problems",
      "paper_title": "A two-stage assignment strategy for the robust scheduling of dual-resource constrained stochastic job shop scheduling problems",
      "paper_year": "2019",
      "chunk_content": "3.2 A two-stage assignment strategy based on worker proficiency and workload balancing We provide an approach involving two stages to assigning the machine and worker resources in the job shop environment. The first stage determines the sequence of the jobs on the machines. Besides, the second stage minimizes the influence of worker insufficiency on makespan, by assigning workers according to the its availability constraints. The procedures of the assignment method are briefly described as follows. Step 1. Sorting the jobs on the machines. Step 2. Assigning the workers to the machines based on the proposed assignment strategy; Step 3. Save the sequences of the jobs on the machines and the sequence of the workers. Assigning the most proficient workers to the corresponding operation can minimize the total disturbance amount in the scheduling scheme, which is beneficial to improve the scheduling robustness of the DR-SJSSP-ESP. Thereafter, we first give a proficiency priority based worker assignment strategy, which is represented by TSAS1. TSAS1 For a given operation, selecting the most proficient worker in the set of alternative workers $\\mathbf{M}_{j}^{W}$ by the policy of $\\max p f_{i j}^{\\prime \\prime}, \\forall i, j, r=1, \\ldots, K$. Using TSAS1 can minimize the processing time variance for each operation. However, when workers are consecutively assigned to become constrained resources, the degree of deterioration of makespan also increases.",
      "question_type": "advantage",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:41",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to minimize makespan ($C_{max}$) in a two-machine flowshop with batch processing machines, how should practitioners determine the batch assignments ($X_{jb}$) considering the dynamic arrival of jobs with ready times ($r_j$) and sizes ($s_j$) when the first machine has a capacity $Q_1$?",
      "topics": [
        "search"
      ],
      "answer": "Practitioners implementing an EDA for this problem should represent batch assignments using the binary variable $X_{jb}$, where $X_{jb} = 1$ if job $j$ belongs to batch $b$, and $0$ otherwise. When creating batches, ensure that the sum of job sizes ($s_j$) within each batch does not exceed the first machine's capacity ($Q_1$). The ready time ($r_j$) of each job must be considered to ensure that jobs are not assigned to a batch that starts processing before the job's arrival. The decision variable $Y_b$ (1 if the $b$th batch is not empty, 0 otherwise) also needs to be considered in conjunction with $X_{jb}$ to ensure feasibility. The processing time of the $b$th batch on machine $m$, denoted as $P_{bm}$, is a crucial factor influenced by batch assignments.",
      "chunk_id": 24660,
      "paper_id": "Minimizing makespan in a no-wait flowshop with two batch processing machines using estimation of distribution algorithm",
      "paper_title": "Minimizing makespan in a no-wait flowshop with two batch processing machines using estimation of distribution algorithm",
      "paper_year": "2016",
      "chunk_content": "$s_{j} \\quad$ The size of job $j$. $p_{j m} \\quad$ The processing time of job $S$ on machine $m$. $r_{j} \\quad$ The ready time of job $b$. $S$ The capacity of the first machine. $T$ The set-up time on the second machine. Decision variables: - $X_{j b} \\quad 1$, if job $U_{b 1} \\quad$ belongs to the $b$ th batch; 0 , otherwise. - $Y_{b} \\quad 1$, if the $s_{j}$ th batch is not empty; 0 , otherwise. - $P_{b m} \\quad$ The processing time of the $C_{\\max }$ th batch on machine $F_{2} \\mid p-\\operatorname{batch}(1), s-\\operatorname{batch}(2)$. - $R_{b} \\quad$ The ready time of the $$\n\\text { Minimize } \\quad C_{\\max }\n$$ th batch. - $U_{b 1} \\quad$ The starting time of the $$\nU_{b 1}, D_{b 1}, C_{b 2} \\geq 0, \\quad b=1, \\ldots, n\n$$ th batch processed on the first machine. - $C_{\\max } \\quad$ The makespan. The problem considered in this research can be formally stated as follows. (1) There are $$\nP(t)=\\left(\\begin{array}{ccc}\np_{11}(t) & \\ldots & p_{1 n}(t) \\\\\n\\vdots & \\ddots & \\vdots \\\\\np_{n 1}(t) & \\ldots & p_{n n}(t)\n\\end{array}\\right)\n$$ jobs to be processed in a two-machine flowshop with BPMs. The jobs arrive dynamically at the first machine and the ready time of job $p_{j k}(t)$ is denoted by $r_{j}$. Each job has a size $s_{j}$ and the processing time of job $p_{j k}(0)$ on machine $1 / n$ is denoted by $p_{j m}$. (2) The first machine has a capacity $Q_{1}$ and can process a batch of jobs as long as the total size of all the jobs in the batch does not exceed the capacity.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:43",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, how is a Bayesian Network (BN) utilized, and what are the key components that define the relationships between features in the model?",
      "topics": [
        "performance"
      ],
      "answer": "The EDA uses a Bayesian Network (BN) to model the causal relationships between the features of a model. A BN consists of a Directed Acyclic Graph (DAG) denoted as $\\mathcal{G}$ and a set of parameters $\\Theta$. The DAG $\\mathcal{G}$ is composed of a set of vertices $\\mathcal{V}$ representing the features $\\overrightarrow{x_{i}} \\in \\mathbb{R}^{d}$ and a set of directed edges $\\mathcal{E}$ representing the relationships between these features. The parameters $\\Theta$ define the strength and shape of these relationships, allowing the BN to model complex, non-deterministic systems. The parameters $y_{i} \\in \\mathbb{R}$ are calculated based on the defined graph.",
      "chunk_id": 6391,
      "paper_id": "Bayesian Network-Based Multi-objective Estimation of Distribution Algorithm for Feature Selection Tailored to Regression Problems",
      "paper_title": "Bayesian Network-Based Multi-objective Estimation of Distribution Algorithm for Feature Selection Tailored to Regression Problems",
      "paper_year": "2024",
      "chunk_content": "Multivariate EDAs do not necessarily limit the degree of interactions among variables and can be modelled with unrestricted Bayesian Networks. The choice of probabilistic model can have a major influence on the performance and efficiency of EDAs. # 2.3 Bayesian Networks A Bayesian Network (BN) [13] is a probabilistic graphical model which provides a robust general approach especially suited to modeling complex non-deterministic systems. A BN models the causal relationships between the features of a model. It consists of a Directed Acyclic Graph (DAG) $\\mathcal{G}$ [13], and a set of parameters $\\Theta$, defining the strength and the shape of the relationships between features. To use a BN, one must define the graph $\\overrightarrow{x_{i}} \\in \\mathbb{R}^{d}$ and then calculate its parameters $y_{i} \\in \\mathbb{R}$. Defining the graph $i \\in\\{1 \\ldots n\\}$ is a task that can be done by learning through data or by consulting human experts in a specific field [13]. $\\mathcal{M}$ consists of a set of vertices $\\mathcal{V}$ and a set of directed edges $\\mathcal{E}$. The vertices in $\\mathcal{M}(\\vec{x})$ represent the features whose relationship is modeled by the BN, and the edges in $y$ represent the relationships between the features.",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:45",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of the improved RM-MEDA compare to the traditional RM-MEDA on the F9 test instance, and what is the proposed reason for any difference in performance?",
      "topics": [
        "algorithm"
      ],
      "answer": "On the F9 test instance, the improved RM-MEDA performs worse than the traditional RM-MEDA. The proposed reason is that F9 has many local Pareto fronts, and the improved RM-MEDA's elitist strategy, which selects only the elite individuals, may lead to a loss of population diversity, causing the algorithm to become trapped in local optima. This is considered a potential shortcoming of the RM-MEDA based on an elitist strategy.",
      "chunk_id": 15670,
      "paper_id": "The RM-MEDA Based on Elitist Strategy",
      "paper_title": "The RM-MEDA Based on Elitist Strategy",
      "paper_year": "2010",
      "chunk_content": "However, the difference is very small. And what encourages us is the result on F7. F7 is the hardest instance for the traditional RM-MEDA due to the fact that the Pareto optimal solutions of F7 are not uniformly distributed in its linkage counterpart. The RM-MEDA based on elitist strategy takes the better half solutions of the whole populations to create the manifold model may be the key factor for this. # 5.6 Test Instances with Many Local Pareto Fronts Here we compare improved RM-MEDA with traditional RM-MEDA on a continuous MOPs with nonlinear variable linkages. Furthermore, this instance has many local Pareto fronts. Table 9 shows the results with [FORMULA107] metric and [FORMULA108] metric. Table 9. The statistic results on F9 on 10 runs However, improved RM-MEDA falls back with the traditional RM-MEDA over F9. This is due to the fact that this instance has many local Pareto fronts. Since improved RM-MEDA choice the elite individuals and abandoned the other individuals, this may loses diversity metric of populations to some extent, so the algorithm is more easily trapped into local optimum. This may be the shortcoming of the RM-MEDA based on elitist strategy.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:47",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing Gene Expression Programming (GEP), what programming techniques are suggested to optimize the EDA's performance when applying mutation operators, especially considering the fixed-length gene expression representation?",
      "topics": [
        "mutation",
        "Gene Expression Programming",
        "Estimation of Distribution Algorithms",
        "RNC array"
      ],
      "answer": "To optimize mutation in GEP with its fixed-length gene expression, a developer should consider implementing a random numerical constant (RNC) array. This array improves the constant fitting efficiency during the mutation process. Furthermore, given the fixed-length gene expression, standard genetic operators like crossover and various mutation operators can be directly applied to increase the diversity of the offspring. The mutation operators should be designed to vary gene components within a single parent individual.",
      "chunk_id": 20316,
      "paper_id": "Information Fusion in Offspring Generation- A Case Study in Gene Expression Programming",
      "paper_title": "Information Fusion in Offspring Generation: A Case Study in Gene Expression Programming",
      "paper_year": "2020",
      "chunk_content": "In Fig. 1, the maximum arity functions are binary operators \" + \" and \" - \". Hence, under this circumstance, the maximum arity is 2 . By this mechanism, each symbolic expression tree can be converted to a linear gene expression, and each linear gene expression can be converted to a valid symbolic expression tree. For example, the gene expression in Fig. 1 can be converted to an expression tree as illustrated in Fig.2. One of the main contributions of GEP is its representation, which uses a fixed-length gene expression to represent a variety of symbolic expressions. This representation directly leads to an advantage in the offspring generation. As representation with a fixed-length gene expression, many offspring generation operators can be applied. Basically, there are two kinds of operators that have widely used in GEP. The first one is genetic operators, such as crossover and mutation operators. These kinds of operators choose one or two parent individuals, exchange the gene components between parent individuals or vary the gene components in a single parent individual. In traditional GEPs [10], various mutation operators and crossover operators are proposed for generating offspring. These operators increase the diversity of the population. A random numerical constant (RNC) array has also been proposed for improving the constant fitting efficiency.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:49",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of power flow models, how is the matrix $\\mathbf{M}$ defined based on the network's buses and lines?",
      "topics": [
        "optimization"
      ],
      "answer": "The matrix $\\mathbf{M}$ is an incident matrix of size $N \\times L$, where $N$ is the number of buses and $L$ is the number of lines. Each entry $M_{nl}$ indicates the relationship between bus $n$ and line $l$: it is 1 if line $l$ originates from bus $n$, -1 if line $l$ terminates at bus $n$, and 0 otherwise.",
      "chunk_id": 22885,
      "paper_id": "Multiple Power Line Outage Detection in Smart Grids Probabilistic Bayesian Approach",
      "paper_title": "Multiple Power Line Outage Detection in Smart Grids: Probabilistic Bayesian Approach",
      "paper_year": "2018",
      "chunk_content": "Transmission line network (a) interconnect grid system and (b) power flow model. set of nodes (buses) and $\\mathcal{E}=\\{(m, n)\\} \\subseteq \\mathcal{N} \\times \\mathcal{N}\\}$ denotes the set of possible edges. The incident matrix of $\\mathcal{G}$ is a matrix $\\mathbf{M}$ of size $N \\times L$. An entry at row $n \\in\\{1,2, \\cdots, N\\}$ and column $l \\in\\{1,2, \\cdots, L\\}$ of $n, P_{n m}$ is given by: $$\nM_{n l}=\\left\\{\\begin{array}{ll}\n1, & \\text { if line } l \\text { originates from bus } n \\\\\n-1, & \\text { if line } l \\text { terminates at bus } n \\\\\n0, & \\text { otherwise }\n\\end{array}\\right.\n$$ Fig. 2(a) shows an example of a network with $L=5$ lines and $N=4$ buses. For this network, the matrix $$\nP_{n m}=\\frac{1}{x_{m n}}\\left(\\theta_{n}-\\theta_{m}\\right)\n$$ is obtained as: $$\n\\mathbf{M}=\\left[\\begin{array}{cccc}\n1 & 0 & 0 & -1 & 1 \\\\\n-1 & 1 & 0 & 0 & 0 \\\\\n0 & -1 & 1 & 0 & -1 \\\\\n0 & 0 & -1 & 1 & 0\n\\end{array}\\right]\n$$ This work adopts a linear DC power flow model [17]. The conservation of power must be satisfied i.e. for each bus, the total power inflow must be equal to the sum of all power outflows. Mathematically, $$\nP_{n}=\\sum_{m \\in \\mathcal{N}(n)} P_{n m}\n$$ where $P_{n}$ denotes the power flowing in to bus $n, P_{n m}$ is the power flowing out from bus $n$ to bus $m$ and $\\mathcal{N}(n)$ denotes the set of all buses which have direct connection with bus $N \\times N$. Moreover, $$\nP_{n m}=\\frac{1}{x_{m n}}\\left(\\theta_{n}-\\theta_{m}\\right)\n$$ where, $x_{m n}=x_{m n}$ is the reactance of line between buses $x_{m n}$ and $\\boldsymbol{\\theta}$ and $\\theta_{n}$ and $\\theta_{m}$ are their respective voltage phasor angles. With real power injections $\\mathbf{p}=\\left[p_{1}, \\cdots, p_{N}\\right]^{T} \\in \\mathbb{R}^{N}$ and voltage phasor angles $\\boldsymbol{\\theta} \\in\\left[\\theta_{1}, \\cdots, \\theta_{N}\\right]^{T} \\in \\mathbb{R}^{N}$, the above equations can be stacked in vector notation as $$\n\\mathbf{p}=\\mathbf{B} \\boldsymbol{\\theta}\n$$ where $\\mathbf{B}=\\left\\{B_{n m}\\right\\} \\in \\mathbb{R}^{N}$ is an $N \\times N$ matrix with entry at $\\mathcal{E}^{\\prime}=\\mathcal{E} \\backslash \\mathcal{E}$-th row and $$\n\\mathbf{p}^{\\prime}=\\mathbf{B}^{\\prime} \\boldsymbol{\\theta}^{\\prime}=\\mathbf{p}+\\boldsymbol{\\eta}\n$$-th column given by, $$\nB_{m n}=\\left\\{\\begin{array}{ll}\n-x_{m n}^{-1}, & \\text { if }(m, n) \\in \\mathcal{E} \\\\\n\\sum_{n \\in \\mathcal{N}(m)} x_{m n}^{-1}, & \\text { if } m=n \\\\\n0, & \\text { otherwise }\n\\end{array}\\right.\n$$ The above equation shows that $\\mathbf{B}$ is dependent upon the network topology as well as line reactance values $x_{m n}$. The topological changes in network as a result of line outages affect the voltage phasor angle vector $\\boldsymbol{\\theta}$ in (5).",
      "question_type": "evaluation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:50",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of using uniform design, specifically arrays denoted as U_N(q^n), in the context of generating points for Estimation of Distribution Algorithms?",
      "topics": [
        "design"
      ],
      "answer": "The primary objective is to generate points z^1 to z^N in the space C=[0,1]^n such that the discrepancy D_p(P), which measures the deviation between the empirical distribution \\(\\mathscr{N}(\\mathscr{P},[0, x))\\) and the volume Vol([0, x)), is minimized. This minimization aims to achieve a more uniform distribution of points compared to randomly generated points, which can be beneficial for the performance of EDAs.",
      "chunk_id": 27134,
      "paper_id": "Hybrid estimation of distribution algorithm for global optimization",
      "paper_title": "Hybrid Estimation of Distribution Algorithm for Global Optimization",
      "paper_year": "2004",
      "chunk_content": "Let $\\mathscr{P}=\\left\\{z^{1}, z^{2}, \\ldots, z^{N}\\right\\}$ be a set of $R^{n}$ points in $C=[0,1]^{n}$, The $u^{k}=\\left(u_{1}^{k}, \\ldots, u_{n}^{k}\\right)(1 \\leq k \\leq N)$-discrepancy of $P$ over $C$ is defined as follows (Fang and Wang, 1994): $$\nD_{\\mathrm{p}}(\\mathscr{P})=\\left\\{\\int_{C}\\left|\\frac{\\mathscr{N}(\\mathscr{P},[0, x))}{n}-\\operatorname{Vol}([0, x))\\right|^{p} \\mathrm{~d} x\\right\\}^{1 / p}\n$$ where $[0, x)=\\left[0, x_{1}\\right) \\times\\left[0, x_{2}\\right) \\times \\cdots \\times\\left[0, x_{n}\\right), \\mathscr{N}(\\mathscr{P},[0, x))$ is the number of points of $\\mathscr{P}$ falling in $[0, x)$ and $\\operatorname{Vol}([0, x))=\\prod_{i=1}^{n} x_{i}$ is the volume of $\\left[a_{j}, b_{i}\\right]$. The goal of uniform design is to generate $z^{1}, z^{2}, \\ldots, z^{N}$ to minimize $D_{\\mathrm{p}}(\\mathscr{P})$. Uniform design provides a series of uniform arrays for different $n$ and $q$. A uniform array is often denoted by $U_{N}\\left(q^{n}\\right)$. The following is an example of uniform arrays: $$\nU_{9}\\left(9^{6}\\right)=\\left[\\begin{array}{llllll}\n1 & 2 & 4 & 5 & 7 & 8 \\\\\n2 & 4 & 8 & 1 & 5 & 7 \\\\\n3 & 6 & 3 & 6 & 3 & 6 \\\\\n4 & 8 & 7 & 2 & 1 & 5 \\\\\n5 & 1 & 2 & 7 & 8 & 4 \\\\\n6 & 3 & 6 & 3 & 6 & 3 \\\\\n7 & 5 & 1 & 8 & 4 & 2 \\\\\n8 & 7 & 5 & 4 & 2 & 1 \\\\\n9 & 9 & 9 & 9 & 9 & 9\n\\end{array}\\right]\n$$ Here are $$\np_{i}\\left(x_{i}\\right)=\\left\\{\\begin{array}{cc}\n\\frac{W(i, 1)}{M} \\frac{H}{b_{i}-a_{i}} & a_{i} \\leq x_{i}<a_{i}+\\frac{1}{H}\\left(b_{i}-a_{i}\\right) \\\\\n\\frac{W(i, 2)}{M} \\frac{H}{b_{i}-a_{i}} & a_{i}+\\frac{1}{H}\\left(b_{i}-a_{i}\\right) \\leq x_{i}<a_{i}+\\frac{2}{H}\\left(b_{i}-a_{i}\\right) \\\\\n\\vdots & \\vdots \\\\\n\\frac{W(i, H)}{M} \\frac{H}{b_{i}-a_{i}} & a_{i}+\\frac{H-1}{H}\\left(b_{i}-a_{i}\\right) \\leq x_{i} \\leq b_{i}\n\\end{array}\\right.\n$$ rows in $\\tilde{x}=\\left(\\tilde{x}_{1}, \\tilde{x}_{2}, \\ldots, \\tilde{x}_{n}\\right)$ and each row represents a point in $R^{n}$. All these points are integer points in $[1, q]^{n}$. Let $u^{k}=\\left(u_{1}^{k}, \\ldots, u_{n}^{k}\\right)(1 \\leq k \\leq N)$ be points (rows) in $n+1$, we can generate $R^{2}$ in $[0,1]^{n}$ in the following way: $$\nz^{i}=\\left(\\frac{2 u_{1}^{i}-1}{2 q}, \\frac{2 u_{2}^{i}-1}{2 q}, \\ldots, \\frac{2 u_{n}^{i}-1}{2 q}\\right), \\quad 1 \\leq i \\leq N\n$$ The discrepancy of these points generated in this way is much smaller than that of $s$ randomly generated points over $$\nf\\left(v^{1}\\right) \\leq f\\left(v^{2}\\right) \\leq \\ldots \\leq f\\left(v^{n+1}\\right)\n$$.",
      "question_type": "synthesis summary",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:52",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of switching between Multi-Permutation Representation (MPR) and Full Permutation Representation (FPR) in the Hyper Heuristic-based Memetic Algorithm (HHMA) contribute to its performance in solving the Distributed Assembly Permutation Flow-shop Scheduling Problem (DAPFSP)?",
      "topics": [
        "Estimation of Distribution Algorithms"
      ],
      "answer": "The Hyper Heuristic-based Memetic Algorithm (HHMA) utilizes both a Multi-Permutation Representation (MPR) and a Full Permutation Representation (FPR) to solve the DAPFSP. The algorithmic mechanism involves using MPR in the global search stage to improve search efficiency, while FPR is used for local search to maintain potential solutions. This switching between representations aims to leverage the benefits of both MPR and FPR, which, according to the text, cannot be fully utilized when either MPR or FPR is used exclusively, as in existing algorithms for the DAPFSP. The HHMA also incorporates a critical-products-based knowledge to restrict searching areas and allocate computational resources, further enhancing its performance.",
      "chunk_id": 6669,
      "paper_id": "An effective hyper heuristic-based memetic algorithm for the distributed assembly permutation flow-shop scheduling problem",
      "paper_title": "An effective hyper heuristic-based memetic algorithm for the distributed assembly permutation flow-shop scheduling problem",
      "paper_year": "2023",
      "chunk_content": "Moreover, the characteristics of global and local search of the HHMA make the coding scheme conflicting. A novel searching-stage-based solution representation method is then presented to handle this issue. Specifically, a Multi-Permutation Representation (MPR) of a solution is used in the global search stage to improve search efficiency while a Full Permutation Representation (FPR) is used for the local search to maintain potential solutions. It is worth pointing out that either MPR or FPR was used in the existing algorithms for the DAPFSP and hence the benefits of them cannot be fully utilized. Parameter testing is carried out for the HHMA based on the Taguchi method of Design-ofExperiment (DOE). Finally, a benchmark instance set is employed to evaluate the performance of the proposed HHMA. Comparison results show the superiority of HHMA over the state-of-the-arts for the DAPFSP. The main contribution of this paper is summarized as follows: (1) A new critical-products-based knowledge is found for the DAPFSP to help reasonably restrict the searching areas and allocate the computational resources. (2) An effective HHMA with EDAHH and CP-RLS being the global and local search strategy, respectively, is proposed to solve the DAPFSP based on the critical-products-based knowledge.",
      "question_type": "algorithmic mechanism",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:55:54",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "When configuring an EDA, what parameter settings optimize the performance of the Sliding Window compact Genetic Algorithm (SWeGA) when used to optimize the parameters of a game-playing agent?",
      "topics": [
        "search"
      ],
      "answer": "The provided text mentions using a multi-valued version of the Sliding Window compact Genetic Algorithm (SWeGA), an EDA, to optimize the parameters of a game agent. However, it doesn't specify particular parameter settings for SWeGA to optimize performance. It only indicates that SWeGA is used as a baseline algorithm alongside NTBEA and Grid Search.",
      "chunk_id": 22827,
      "paper_id": "The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation",
      "paper_title": "The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation",
      "paper_year": "2018",
      "chunk_content": "For our experiments we used an implementation with the following rules: - No neutral planets: the ships on each planet are either owned by player 1 or player 2. - At each game tick, a player moves by shifting ships to or from a player's ship buffer, or by moving it's current planet of focus. - When a player transfers ships it is always between it's buffer and the current planet of focus. - At each game tick the score for each player is the sum of all the ships on each planet it owns, plus the ships stored in it's buffer. We have two versions of the game: the easiest for the planning agents, and the one used in this paper also adds in the ships in a player's buffer to it's score, a more deceptive version of the game does not include this. For research purposes, the advantage of the modified action space is that it makes it compatible with the General Video Game AI framework, giving direct access to a large number of game-playing agents for comparison purposes. ## IV. APPLICATIONS AND RESULTS We aim to optimise the parameters of the [FORMULA91] agent (described in Section III-A) with NTBEA and two baseline algorithms, Grid Search and a multi-valued version of the Sliding Window compact Genetic Algorithm (SWeGA), an EDA proposed by Lucas et al.",
      "question_type": "parameter selection",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:55:55",
      "generation_style": "practical_application"
    },
    {
      "question": "Based on the comparison of algorithms, what convergence properties are observed for EDAPSO (Estimation of Distribution Algorithm with Particle Swarm Optimization) relative to PSO, LDWPSO, and CFPSO, specifically concerning accelerate rates (AR)?",
      "topics": [
        "search"
      ],
      "answer": "The comparison of accelerate rates (AR) indicates that EDAPSO has the fastest convergence speed among the four algorithms: PSO, LDWPSO, CFPSO, and EDAPSO. The comparisons for SR (success rates) and AR (accelerate rates) show that EDAPSO has the fastest convergence speed and best robustness among the four algorithms.",
      "chunk_id": 711,
      "paper_id": "Novel Particle Swarm Optimization for unconstrained problems",
      "paper_title": "Novel Particle Swarm Optimization for Unconstrained Problems",
      "paper_year": "2013",
      "chunk_content": "The comparisons of average acceleration rate indicate that EDAPSO has the fastest Table1. Comparison of Four Algorithms for Low-dimensional Problems convergence speed among the four algorithms. The comparison shows that EDAPSO has the best robustness and the fastest convergence among the four algorithms. The optimization ability is the best among the four algorithms on the whole. ## 5 CONCLUSION In this paper, the search idea of EDA has been employed to collect global information. An improved velocity updating equation was used to accelerate the speed of the algorithm. Embedding these two improvements, we proposed EDAPSO. PSO, LDWPSO, CFPSO and EDAPSO were compared in terms of robustness and convergence speed. The comparisons for SR (success rates) and AR (accelerate rates) show that EDAPSO has the fastest convergence speed and best robustness among the four algorithms. Utilizing the idea of EDA to obtain more global information and improving the solution accuracy of the problems is a new method. Further studies are still required to investigate its advantages, disadvantages and limitations. This paper can be viewed as a first step in this direction. Possible directions for future work include adaptive setting of e (size of elite set) or applying the idea of EDA to other popular algorithms.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:55:57",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the utilization rate, calculated as $\\mu(I_{j})=\\left(\\sum_{r i \\in R} h_{i} w_{i}\\right) /\\left(\\sum_{B i \\in I_{j}} H_{i} W_{i}\\right)$, contribute to the fitness function's evaluation of packing quality compared to the fullness proportion, $F_{i, j}=\\left(\\sum_{r i, j, k \\in B_{i, j}} h_{i, j, k} w_{i, j, k}\\right) /\\left(H_{i, j} W_{i, j}\\right)$, within the described Estimation of Distribution Algorithm?",
      "topics": [
        "fitness"
      ],
      "answer": "The utilization rate, $\\mu(I_{j})$, measures the ratio of the total area of rectangles ($h_i w_i$) to the total area of bins ($H_i W_i$) within an individual $I_j$, providing a global measure of how densely the bins are packed. In contrast, the fullness proportion, $F_{i, j}$, calculates the ratio of the sum of weighted heights and widths ($h_{i,j,k} w_{i,j,k}$) of rectangles within a specific bin $B_{i,j}$ to the bin's total area ($H_{i, j} W_{i, j}$), indicating the bin's fill level. The fitness function uses these values in a positional notation hierarchy ($f=f2+f1$ or $f=f3+f2+f1$) where the least significant part ($f1$) measures the improvement of the fullness proportion after reallocation attempts. By assessing fullness proportions, the algorithm can modify bin allocation to improve the overall packing, which can lead to deleting bins or searching for alternatives, thus influencing the utilization rate indirectly. The utilization rate, however, provides a direct measure of packing efficiency across all bins in the individual, which is not explicitly captured by the fullness proportion alone.",
      "chunk_id": 5730,
      "paper_id": "A hybrid estimation of distribution algorithm for the offline 2D variable-sized bin packing problem",
      "paper_title": "A hybrid estimation of distribution algorithm for the offline 2D variable-sized bin packing problem",
      "paper_year": "2024",
      "chunk_content": "The value of $(F B)$ is a given parameter of the procedure, usually its value has been chosen from the set $\\{0.85,0.90,0.95,0.99\\}$. Let $$\nF_{i, j}=\\left(\\sum_{r i, j, k \\in B_{i, j}} h_{i, j, k} w_{i, j, k}\\right) /\\left(H_{i, j} W_{i, j}\\right)\n$$ be the fullness proportion of the bin $B_{i, j}$. It is clear that $F_{i, j} \\leq 1$. If $$\n\\lambda \\leq F_{i, j}\n$$ then we declare the bin full, and it belongs to the set of full bins $(F B)$. Otherwise the bin belongs to the set of non-full bins $(N F B)$. If $I_{j}$ is an individual then the utilization rate for $$\nf_{B P P}=\\left(\\sum_{k=1}^{n b}\\left(F_{k}\\right)^{q}\\right) / n b\n$$ is $$\n\\mu\\left(I_{j}\\right)=\\left(\\sum_{r i \\in R} h_{i} w_{i}\\right) /\\left(\\sum_{B i \\in I_{j}} H_{i} W_{i}\\right)\n$$ ### 2.3 Fitness function The fitness function $(f)$ computes the value that measures the quality of the packing in the individual, lower values correspond to better packing. Our fitness function uses a positional notation of a hierarchy of two or three separate fitness parts, corresponding to three functions measuring different aspects of the quality of packing: $f=f 2+f 1$ or $f=f 3+f 2+f 1$. The least significant part $(f 1)$ measures the improvement of the fullness proportions after attempting to reallocate rectangles among the bins. It is an important part of the fitness function because the operators and local searches of the algorithm can modify the fullness proportions and based on the modifications we can delete bins or can search for other bins to use.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:00",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the specific EDA mentioned, what data structures are required to represent the Bayesian network and the dependencies between variables, considering the use of Archimedean copulas?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "Bayesian Networks",
        "Archimedean Copulas",
        "Data Structures"
      ],
      "answer": "The EDA implementation requires data structures to represent a Bayesian network. Specifically, it needs to store the dependencies between variables modeled using multivariate Archimedean copula functions. The univariate marginals are described using the beta distribution. The implementation also needs to store Kendall's tau coefficient for measuring relations between variables.",
      "chunk_id": 10190,
      "paper_id": "An EDA based on Bayesian networks constructed with Archimedean copulas",
      "paper_title": "An EDA Based on Bayesian Networks Constructed with Archimedean Copulas",
      "paper_year": "2012",
      "chunk_content": "An EDA Based on Bayesian Networks Constructed with Archimedean Copulas Mario Rogelio Flores Mendez and Ricardo LandaInformation Technology LaboratoryCINVESTAV TamaulipasCd. Victoria, Tamaulipas, MEXICOEmail: mflores@tamps.cinvestav.mx, rlanda@tamps.cinvestav.mx #### Abstract In this paper, an estimation of distribution algorithm that adopts a copula Bayesian network as probabilistic graphic model is presented. Multivariate Archimedean copula functions with one parameter are used to model the dependences between variables and the beta distribution is used to describe the univariate marginals. The learning process of the Bayesian network is assisted through a simple technique that relies on the associative property of Archimedean copulas, the use of Kendall's tau coefficient for measuring relations between variables and the relation between tau coefficients and bivariate Archimedean copulas. This paper presents the proposal, together with some initial experiments, which show encouraging results. Keywords-numerical optimization; estimation of distribution algorithm; Archimedean copulas ## I. INTRODUCTION Optimization is a task present in many fields of human activity.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:01",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics, beyond pure Renyi entropy, are proposed to evaluate the performance of a non-Gaussian cascade control system?",
      "topics": [
        "control"
      ],
      "answer": "To evaluate the performance of a non-Gaussian system, a Renyi entropy benchmark is combined with a mean benchmark. The pure Renyi entropy benchmark is defined as  $\\eta_{m e}=\\frac{H_{m v}\\left(C_{1}\\right)}{H_{2}\\left(C_{1}\\right)}$ where $H_{2}(C_1)$ represents the Renyi entropy of the primary loop and $H_{me}(C_1)$ is calculated by ignoring the expanded coefficients of $a_{1}(k), a_{2}(k)$ when calculating the minimum entropy. The rationale is that entropy alone cannot reflect shifts in the output mean value, necessitating the inclusion of a mean benchmark alongside the Renyi entropy benchmark.",
      "chunk_id": 16447,
      "paper_id": "Improved Renyi Entropy Benchmark for Performance Assessment of Common Cascade Control System",
      "paper_title": "Improved Renyi Entropy Benchmark for Performance Assessment of Common Cascade Control System",
      "paper_year": "2019",
      "chunk_content": "In this paper, the second order Renyi entropy is used to construct the system performance benchmark. For refined discrete random variable $X=\\left\\{x_{1}, x_{2}, \\ldots, x_{n}\\right\\}$, the entropy can be expressed as, $$\nH_{2}(X)=-\\ln \\sum_{i=0}^{n} p_{i}^{2}\n$$ where the $p_{i}$ is the probability of each parameter $x_{i}(i=$ $1,2, \\ldots, n)$ and $\\sum_{i=0}^{n} p_{i}=1$. When calculate the entropy of discrete variables, the output value will be divided into several non-overlapping regions. Then we can calculate the respective probabilities of each interval regions by counting the number of each interval to obtain the Renyi entropy. Considering the cascade control system which is shown in Fig.1, the Renyi entropy of the primary loop is expressed as, $$\n\\begin{aligned}\nH_{2}\\left(C_{1}\\right)=H\\left(\\left(Q_{11}\\right.\\right. & \\left.\\left.+S_{1} q^{-d_{1}}\\right) a_{1}(k)+\\left(Q_{12}+S_{2} q^{-d_{1}}\\right) a_{2}(k)\\right. \\\\\n& \\left.+q^{-d 1-d 2}\\left(M_{1} a_{1}(k)+M_{2} a_{2}(k)\\right)\\right)\n\\end{aligned}\n$$ Similar to MVC method, the optimal control law can be obtained when the feedback dependent variable is zero. Thus, $$\n\\begin{aligned}\nH_{m e}\\left(C_{1}\\right)=H\\left(\\left(Q_{11}+S_{1} q^{-d_{1}}\\right) a_{1}(k)\\right. & \\\\\n& \\left.+\\left(Q_{12}+S_{2} q^{-d_{1}}\\right) a_{2}(k)\\right)\n\\end{aligned}\n$$ In literature [10], [11], he used the criterion of information theory to expand and decompose the polynomials of feedback invariants. He ignored the expanded coefficients of $a_{1}(k), a_{2}(k)$ when calculated the minimum entropy. While it has been proved that the entropy and coefficients are closely related in literature [14], [15], so the pure benchmark of Renyi entropy is, $$\n\\eta_{m e}=\\frac{H_{m v}\\left(C_{1}\\right)}{H_{2}\\left(C_{1}\\right)}\n$$ But according to the research, entropy is the index to describe the shape of the distribution, the pure entropy cannot reflect the shift of the output mean value. Thus in this paper the Renyi entropy benchmark is combined with the mean benchmark to evaluate the performance of non-Gaussian system.",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:03",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs within the Surrogate-assisted Cooperative Signal Optimization (SCSO) framework, and how does the RBF-EDA contribute to this purpose?",
      "topics": [
        "heuristic"
      ],
      "answer": "The primary objective of the RBF-EDA within the SCSO framework is to efficiently optimize traffic signal plans in large-scale networks by reducing the average delay time of vehicles. The RBF-EDA achieves this by using a Radial Basis Function (RBF) model to reduce the number of expensive traffic simulations required and by employing an Estimation of Distribution Algorithm (EDA) to find high-quality signal plans. The RBF-EDA optimizer improves the signal optimization efficiency of sub-networks without adding excessive computational burden, facilitating efficient signal optimization within tight computational budgets.",
      "chunk_id": 17776,
      "paper_id": "Surrogate-assisted cooperative signal optimization for large-scale traffic networks",
      "paper_title": "Surrogate-assisted cooperative signal optimization for large-scale traffic networks",
      "paper_year": "2021",
      "chunk_content": "By comparison, the computation time spent on all the other operations, including surrogate model training and optimization, becomes less significant. Overall, the RBF-EDA [FORMULA161] optimizer can efficiently improve the signal optimization efficiency of sub-networks without adding too much computational burden. ## 5. Conclusions and future works In this paper, a surrogate-assisted cooperative signal optimization (SCSO) algorithm is proposed for fixed-time traffic signal optimization in large-scale traffic networks with the goal of reducing the average delay time of vehicles. Different from existing methods that directly tackle the whole traffic network with metaheuristic optimization algorithms, SCSO first decomposes a largescale traffic network into a set of smaller sub-networks using Newman fast algorithm, and then employs a surrogate-assisted optimizer named RBF-EDA [FORMULA162] to efficiently optimize the signal plan of sub-networks. The use of RBF model significantly reduces the requirement of expensive traffic simulations and the adopted EDA [FORMULA163] algorithm indicates strong ability in finding high-quality signal plans. Different sub-networks cooperate with each other by sharing their current best sub-solutions for traffic simulation in the solution evaluation process. By virtue of the above characteristics, SCSO demonstrates efficient signal optimization ability in large-scale traffic networks with tight computational budget. The performance of SCSO is comprehensively studied in a large-scale traffic network containing 43 signalized junctions.",
      "question_type": "theory",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:05",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of Estimation of Distribution Algorithms for personalized search, what fundamental properties distinguish the DPM-IEDA algorithm from the content-based methods SVR and IGA, given that DPM-IEDA outperforms them on ARPR and average rating but shows lower diversity (ILD)?",
      "topics": [
        "performance"
      ],
      "answer": "The DPM-IEDA algorithm, as mentioned, achieves better performance on ARPR (presumably Average Rating Prediction Rate) and average rating compared to the content-based methods SVR and IGA. However, SVR and IGA yield better performance regarding ILD, which refers to diversity. This suggests that DPM-IEDA prioritizes accuracy in rating prediction over the diversity of the recommended items, while SVR and IGA prioritize the diversity of the results even if it means lower accuracy in rating prediction. The observation highlights a trade-off between prediction accuracy and diversity among different algorithms.",
      "chunk_id": 15756,
      "paper_id": "PM-IEDA- Dual probabilistic Model Assisted Interactive Estimation of Distribution Algorithm for Personalized Search",
      "paper_title": "DPM-IEDA: Dual probabilistic Model Assisted Interactive Estimation of Distribution Algorithm for Personalized Search",
      "paper_year": "2019",
      "chunk_content": "13337 standard compliant users from MovieLens 20M and 255 standard compliant users from Amazon Dataset (Movie and TV 5-core) are adopted to clearly analyze the performance of compared algorithms. All involved experiments are repeated thirty times. Results w.r.t. the three indicators are shown in Table 7, and the average rating is bracketed after the number of found satisfactory items. The following observations are obtained: 1. The presented algorithm, DPM-IEDA outperforms all the other compared ones on ARPR and average rating. When considering the diversity (ILD), two contentbased methods (SVR and IGA) produce better performance than others, while the rest perform similarly. 2. When comparing the CF-based approach and the CF-based+ one, i.e. data in the third and fourth columns, it is easy to find that the proposed improvement on CF-based model is effective. 3. When the fourth and fifth columns come into focus, the improvement brought by the content-based probabilistic model is therefore noticed. 4. The worst performance is obtained by content-based (SVR) and content-based (IGA) personalized search methods, although their search results are of better diversity. Meanwhile, the algorithm runtime is also recorded.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:07",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm using the HLB potential function, how should practitioners set the parameter $\\delta$ to ensure a small expected gain of the potential in one iteration, and what is the implication of this setting on the algorithm's progress through the search space?",
      "topics": [
        "search"
      ],
      "answer": "Practitioners should set $\\delta = \\Theta(\\frac{1}{n})$ to ensure that the expected gain of the HLB potential function ($\\operatorname{HLB}_{\\delta}$) in one iteration is $O(\\frac{1}{n^{2}})$. This setting ensures a small progress in the whole search space. Furthermore, the potential $h_{t}$ can be defined as $\\operatorname{HLB}_{\\delta}(x^{(t+S)})$, where $S$ denotes the first time that $2+2m-\\ell \\leq r \\leq n-\\ell$ has an HLB value not less than $\\frac{2n}{3}$. This potential also has a drift of only $r+\\ell < 2m+2$, impacting the lower bound of $r+\\ell$ according to the additive drift theorem.",
      "chunk_id": 5832,
      "paper_id": "Choosing the right algorithm with hints from complexity theory",
      "paper_title": "Choosing the right algorithm with hints from complexity theory",
      "paper_year": "2024",
      "chunk_content": "Lemma 7 then implies that $E\\left[\\operatorname{HLB}_{\\delta}(y) \\mid\\right.$ $Y>0] \\leq 2 m+4$, thus we have $$\nE\\left[\\operatorname{HLB}_{\\delta}(y)-\\operatorname{HLB}_{\\delta}(x) \\mid Y>0\\right] \\leq 2 m+4-2 m=4\n$$ On the other hand, to bound $\\operatorname{Pr}[Y>0]=\\operatorname{Pr}[\\operatorname{DLB}(y)>\\operatorname{DLB}(x)]$ we invoke Lemma 2 and compute $$\n\\begin{aligned}\n& \\operatorname{Pr}[\\operatorname{DLB}(y)>\\operatorname{DLB}(x)] \\\\\n& =\\sum_{r=2}^{n-2 m} \\operatorname{Pr}\\left[r_{V}=r\\right] \\operatorname{Pr}[\\operatorname{DLB}(y)>\\operatorname{DLB}(x) \\mid r_{V}=r] \\\\\n& =\\sum_{r=2}^{n-2 m} \\operatorname{Pr}\\left[r_{V}=r\\right]\\binom{n-2 m-2}{r-2}\\binom{n}{r}^{-1} \\\\\n& \\leq \\max _{2 \\leq r \\leq n-2 m}\\binom{n-2 m-2}{r-2}\\binom{n}{r}^{-1}\n\\end{aligned}\n$$ where $(n / \\Omega(1))^{d}$ can be restricted to $2 \\leq r \\leq n-2 m$ since otherwise $O\\left(n^{2.5}\\right)$ would certainly be zero. With $E_{r}:=\\binom{n-2 m-2}{r-2}\\binom{n}{r}^{-1}$, we have $$\n\\frac{E_{r+1}}{E_{r}}=\\frac{2(n-m-(m+1) r)}{(r-1)(n-r)}+1<1\n$$ by our hypothesis $n-3 m \\leq 0$ and $r \\geq 2$. As in Lemma 8 we conclude that $E_{r}$ is decreasing in $O\\left(\\frac{1}{n}\\right)$ and $$\n\\begin{aligned}\n& \\operatorname{Pr}[Y>0] \\\\\n& =\\operatorname{Pr}[\\operatorname{DLB}(y)>\\operatorname{DLB}(x)] \\leq \\max _{2 \\leq r \\leq n-2 m} E_{r}=E_{2}=\\binom{n}{2}^{-1} \\leq \\frac{4}{n^{2}}\n\\end{aligned}\n$$ Combining (5), (6) and (7), we obtain $E[Y] \\leq \\frac{16}{n^{2}}$. With the estimates on the change of the potential HLB above, we can now easily prove the main result of this section. By taking $\\delta=\\Theta\\left(\\frac{1}{n}\\right)$ suitably, we ensure that the expected gain of the potential in one iteration is only $O\\left(\\frac{1}{n^{2}}\\right)$, at least in a sufficiently large range of the potential space. To have a small progress in the whole search space, as necessary to apply the additive drift theorem, we regard the potential $h_{t}=\\operatorname{HLB}_{\\delta}\\left(x^{(t+S)}\\right)$, where $S$ denotes the first time that $2+2 m-\\ell \\leq r \\leq n-\\ell$ has an HLB value not less than $\\frac{2 n}{3}$. This potential also has a drift of only $r+\\ell<2 m+2$ and thus the additive drift theorem easily gives the lower bound of $r+\\ell$.",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:09",
      "generation_style": "practical_application"
    },
    {
      "question": "The presented functions ES, GP, DJ, MG, and Gr are combined into a Value function for evaluating performance. Given that $DJ(x_{1}, x_{2}, x_{3})=x_{1}^{2}+x_{2}^{2}+x_{3}^{2}$ with $-5.12 \rleq x_{j} \rleq 5.12$ for $j=1,2,3$, and $MG(x)=\\left(x_{1}-x_{2}\\right)^{2}+\\left(\\left(x_{1}+x_{2}-10\\right) / 3\\right)^{2}$ with $-20 \rleq x_{j} \rleq 20$ for $j=1,2$, how does the domain restriction on the input variables $x_j$ for the DJ and MG functions individually impact the possible range of values that the Value function can take, assuming the other functions (ES, GP, and Gr) remain constant?",
      "topics": [
        "performance"
      ],
      "answer": "The domain restriction on $x_j$ directly limits the minimum and maximum contributions of the $DJ(x_{1}, x_{2}, x_{3})$ and $MG(x)$ functions to the overall Value. For $DJ$, since each $x_j$ is squared, the minimum contribution is 0 (when $x_j = 0$ for all $j$) and the maximum contribution is $3*(5.12)^2$ (when $x_j = \\pm 5.12$ for all $j$). For $MG(x)$, the minimum contribution is 0 (when $x_1 = x_2 = 5$) and the maximum contribution is achieved at the boundaries of the domain, influencing the Value function accordingly. Specifically, these restrictions define a bounded search space within which the improved hybrid ant colony algorithm operates, affecting the range of possible Value outcomes when combined with $ES(r)$, $GP(r)$, and $Gr(r)$.",
      "chunk_id": 1432,
      "paper_id": "An Improved Ant Colony Algorithm Based on Distribution Estimation",
      "paper_title": "An Improved Ant Colony Algorithm based on Distribution Estimation",
      "paper_year": "2014",
      "chunk_content": "$D J\\left(x_{1}, x_{2}, x_{3}\\right)=x_{1}^{2}+x_{2}^{2}+x_{3}^{2}$. The domain of variable is $-5.12 \\leq x_{j} \\leq 5.12$, $j=1,2,3$. $M G(x)=\\left(x_{1}-x_{2}\\right)^{2}+\\left(\\left(x_{1}+x_{2}-10\\right) / 3\\right)^{2}$. The domain of variable is $-20 \\leq x_{j} \\leq 20, j=1,2$. $G r(x)=\\sum_{i=1}^{n}\\left(x_{j}^{2} / 4000\\right)-\\prod_{j=1}^{n} \\cos \\left(x_{j} / \\sqrt{j}\\right)+1 \\quad$. The domain of variable is $-600 \\leq x_{j} \\leq 600, j=1,2, \\ldots 10$. $\\operatorname{Value}(r)=E S(r)+G P(r)+D J(r)+M G(r)+G r(r)$. $\\operatorname{Value}(m)=E S(m)+G P(m)+D J(m)+M G(m)+G r(m)$ $\\operatorname{Value}(\\lambda)=E S(\\lambda)+G P(\\lambda)+D J(\\lambda)+M G(\\lambda)+G r(\\lambda)$. Relation between $r, u, \\rho, m, \\lambda$ and Value is shown in Figure.1, Figure.2, Figure.3, Figure. 4 and Figure. 5 respectively. [IMAGE0] Figure 1. Relation between [FORMULA55] and Value ## IV. SIMULATIONS In order to illustrate the effectiveness of the improved hybrid ant colony algorithm, the other three standard test functions are selected to test the performance of them. The result of MG function is shown in Figure.6, the result of matyas function is shown in Figure. 7 and the result of sum function is shown in Figure. 8. [IMAGE1] Figure 2. Relation between $\\boldsymbol{H}$ and Value [IMAGE2] Figure 3. Relation between $\\boldsymbol{\\rho}$ and Value [IMAGE3] Figure 4. Relation between $\\boldsymbol{m}$ and Value [IMAGE4] Figure 5. Relation between [FORMULA59] and Value [IMAGE5] Figure 6.",
      "question_type": "relationship",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:12",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What performance trade-offs exist between the Cauchy EDA and G3PCX algorithms, specifically concerning speed and target level achievement?",
      "topics": [
        "search"
      ],
      "answer": "The G3PCX algorithm is described as being orders of magnitude faster than the Cauchy EDA for a broad range of target levels. However, for target levels at about $10^{-5}$ and tighter, the Cauchy EDA outperforms G3PCX, achieving better results and finding target levels that G3PCX is potentially unable to reach due to possible stagnation or stopping criterion limitations.",
      "chunk_id": 14954,
      "paper_id": "Comparison of cauchy EDA and G3PCX algorithms on the BBOB noiseless testbed",
      "paper_title": "Comparison of Cauchy EDA and G3PCX Algorithms on the BBOB Noiseless Testbed",
      "paper_year": "2010",
      "chunk_content": "After finding the right model, the Cauchy EDA is sometimes able to close the gap and take the lead (e.g. for both ellipsoid functions 2 and 10 and for discus function 11 for dimensions lower than 20 , or for the sharp ridge function 13). G3PCX failed on functions 7 (Step-ellipsoid) and on both Schaffer's functions 17 and 18. It seems that for these functions the global point of view represented by a unimodal Cauchy distribution is a better approach. Also for function 13 (sharp ridge problem), the global probabilistic model seems better, at least for tight target levels. Interesting results may be found for function 14. G3PCX algorithm is orders of magnitude faster than Cauchy EDA for a broad range of target levels. But for target levels at about $10^{-5}$ and tighter, Cauchy EDA takes over and its results are much better. It seems that G3PCX is not even able to find some of the tighter target levels. One explanation for that might be that the stopping criterion for G3PCX is not set properly and actually prevents the algorithm from finding these target levels. Another explanation may be that for these tight target levels the population of G3PCX often converges to certain subspace of the search space and the algorithm stagnates.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:14",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the R-EDA model, specifically concerning the fitness function and convergence to the optimum?",
      "topics": [
        "bounds"
      ],
      "answer": "The R-EDA algorithm involves applying the model to a fitness function of the form x ↦ [FORMULA356], where [FORMULA357] is the optimum and [FORMULA358] is an increasing mapping. The algorithm converges to the optimum under these conditions. Furthermore, if [FORMULA359] is differentiable in 0 with a non-zero derivative, the convergence rate meets specific rates [FORMULA360]. A generalized rate is achieved if [FORMULA361] is [FORMULA362] times differentiable in 0, with the p-1 first derivatives null, relating to a given [FORMULA364].",
      "chunk_id": 23834,
      "paper_id": "Handling expensive optimization with large noise",
      "paper_title": "Handling Expensive Optimization with Large Noise",
      "paper_year": "2011",
      "chunk_content": "R-EDA can be applied to any fitness of the form $x \\mapsto$ [FORMULA356] with [FORMULA357] the optimum and [FORMULA358] an increasing mapping, and will converge to the optimum. If, in addition, if [FORMULA359] is differentiable in 0 with non-zero derivative, then the convergence rate is guaranteed to meet the rates [FORMULA360] presented above. More generally, if [FORMULA361] is [FORMULA362] times differentiable in 0 , with the $p-1$ first derivatives null, then the convergence rate is the general rate presented above for a given [FORMULA364]. A relevant further work would be to extend the algorithm to non-spherical models (i.e. no spherical symmetry around the optimum), in order to have more general convergence bounds. Given the convergence rate table above, one can see that lower bounds for [FORMULA365] or [FORMULA366] are not tight. A relevant further work would be either to find out how to reach these bounds, or to prove lower bounds achieving tightnesswhich seems more likely, given that the current lower bounds are quite optimistic. ## 7. ACKNOWLEDGEMENTS This work was supported by the IST Programme of the European Community, under the PASCAL2 Network of Excellence, IST-2007-216886; by Ministry of Higher Education and Research, Nord-Pas de Calais Regional Council and FEDER through the \"CPER 2007-2013\".",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:16",
      "generation_style": "implementation_focused"
    },
    {
      "question": "Given that finding exact solutions for large-scale Preemptive Resource Leveling Problems (PRLPs) is computationally infeasible due to its NP-hard nature, what benchmarking approach is outlined to assess the meta-heuristic algorithm designed to efficiently obtain satisfactory solutions for the PRLP, considering the generalization of the NP-hard Resource Leveling Problem (RLP)?",
      "topics": [
        "heuristic"
      ],
      "answer": "The provided content does not specify a benchmarking approach for assessing the meta-heuristic algorithm designed for the PRLP. It only states that exact algorithms can hardly find a satisfactory solution in a reasonable time for the large-scale PRLPs and that a meta-heuristic algorithm will be designed to efficiently obtain satisfactory solutions. The relationship between PRLP and RLP is established by noting that PRLP reduces to RLP when allowing zero interruptions, indicating PRLP is a generalization of RLP.",
      "chunk_id": 19659,
      "paper_id": "PREEMPTIVE RESOURCE LEVELING IN PROJECTS",
      "paper_title": "PREEMPTIVE RESOURCE LEVELING IN PROJECTS",
      "paper_year": "2021",
      "chunk_content": "Obviously, the object function (1) is non-linear due to the existence of $u_{k t}^{2}$. In addition, the set $i_{q}$ in constraints (6) also make the constraints non-linear. Therefore, the above model is non-linear. However, based on the linearization method proposed in Rieck et al. (2012) and Liu et al. (2019), this non-linear model can be transformed into a linear integer programming model. The PRLP can be reduced to the RLP by allowing each activity to be interrupted zero times. This means that the PRLP is a generalization of the NP-hard RLP (Neumann et al., 2003). Therefore, the PRLP is also NP-hard. In this case, exact algorithms can hardly find a satisfactory solution in a reasonable time for the large-scale PRLPs. We will design a meta-heuristic algorithm in Section 3 to efficiently obtain satisfactory solutions for the PRLP. # 2.3 Example We use an example to illustrate the PRLP and show that it is possible to obtain a more leveled schedule after considering preemption. Figure 1 displays a project network with 4 non-dummy activities.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:17",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of using Teaching Learning Based Optimization (TLBO) in the context of power system problems?",
      "topics": [
        "optimization"
      ],
      "answer": "The overall purpose of using Teaching Learning Based Optimization (TLBO) is to provide an effective optimization algorithm for practical problems like preventive maintenance scheduling, due to its simple implementation and reduced computational effort.",
      "chunk_id": 21528,
      "paper_id": "An insight to the performance of estimation of distribution algorithm for multiple line outage identification",
      "paper_title": "An insight to the performance of estimation of distribution algorithm for multiple line outage identification",
      "paper_year": "2018",
      "chunk_content": "The double line outage [^0] [^0]: a Corresponding author. E-mail address: muhammadnaeem@gmail.com (M. Naeem). Table 1 Overview of state of the art for line outage identification problem. identification was addressed by the authors using system topology information before and after the line outage in addition to the knowledge of the PMUs measurements. In [10], preventive integrated equipment maintenance scheduling problems in power systems are discussed. Teaching Learning Based Optimization (TLBO) has been used as a prime optimization tool, as it has been proven to be very effective optimization algorithm when applied to various practical optimization problems and its implementation is simple involving less computational effort. The model for unit commitment considering generator outages is formulated in [11], where the reliability requirement is incorporated into the spinning reserve constraint in the optimization design. An intelligent technique based on cascade neural network (CNN) is presented in [12] for identification of the overloaded transmission lines in a power system and for prediction of overloading amount in the identified overloaded lines. The task of security enhancement is formulated as a multi-objective optimization problem with minimization of fuel cost and minimization of FACTS device investment cost as objectives in [13]. A binary particle swarm optimization (BPSO) based methodology for the optimal placement of PMUs is proposed in [14], using a mixed measurement set.",
      "question_type": "advantage",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:19",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of compressing the dimension of particles using components of the first nontrivial eigenvector contribute to the effectiveness of the proposed particle swarm optimization method for community detection?",
      "topics": [
        "optimization"
      ],
      "answer": "The proposed algorithm compresses the dimension of particles by using components of the first nontrivial eigenvector to describe the community centers. This approach leverages the spectral method weighted distance, which combines eigenvalues and eigenvectors to measure the dissimilarity of two vertices. By focusing on the first nontrivial eigenvector, the algorithm reduces the computational burden while still capturing essential structural information for community detection. Furthermore, the algorithm encodes the number of communities implicitly within the particle representation, removing the need for prior knowledge about the number of communities and allowing it to be determined automatically.",
      "chunk_id": 1983,
      "paper_id": "PSO-Based Community Detection in Complex Networks",
      "paper_title": "An EDA-based Community Detection in Complex Networks",
      "paper_year": "2014",
      "chunk_content": "Compared with the partition result represented in GN, our algorithm only grouped the cluster of node $11,24,50,69$ into Mountain West conference, and grouped the cluster of node 59,63,97,58 into SEC conference. The accuracy of the partition was not influenced. [IMAGE_9] Figure 11. Partition result of American College Football Network by our algorithm ( The ID number is as same as the number provided by Newman ${ }^{[18]}$ ). ## 5. CONCLUSIONS In this paper, we proposed a new method based on particle swarm optimization for community detection. There are three innovations in this paper. Firstly, based on Capocci algorithm, spectral method weighted distance which combined eigenvalues and eigenvectors is advanced to measure the dissimilarity of two vertices. Since smaller eigenvalues make less contribution to the distance, this measure enhanced the robustness when the exact number of communities can hardly be found. Secondly, the dimension of particles is compress by using components of the first nontrivial eigenvector to describe the community centers. Finally, the number of communities is encoded impliedly in the particle. It does not need any prior knowledge and the number of communities can be determined automatically.",
      "question_type": "conceptual deep",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:20",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm that considers sequence-dependent setup times, how should practitioners construct the probability model, specifically detailing the initialization and update mechanisms for the probability matrices?",
      "topics": [
        "design"
      ],
      "answer": "To construct the probability model for sequence-dependent setup times in an Estimation of Distribution Algorithm, practitioners should use \\(n\\) probability matrices. For the first position, initialize \\(\\rho_{i}(0)=1 / n\\), representing the equal probability of each job \\(i\\) being selected. For subsequent positions, initialize \\(\\rho_{i j l}(0)=\\frac{1}{n(n-1)}\\), where \\(\\rho_{i j l}(g)\\) represents the probability of job \\(i\\) being selected at position \\(l\\) after job \\(j\\). Update \\(\\rho_{i j l}(g)\\) in subsequent generations using the formula \\(\\rho_{i j l}(g+1)=(1-\\alpha) \\rho_{i j l}(g)+n_{i j l}(g) \\times \\frac{\\alpha}{E}\\), where \\(\\alpha\\) is a learning rate, \\(n_{i j l}(g)\\) represents the number of times job \\(i\\) follows job \\(j\\) at position \\(l\\) in generation \\(g\\), and \\(E\\) is a normalization factor. The probability of a job \\(i\\) being selected at position \\(l\\) given job \\(j\\) was at position \\(l-1\\) is calculated as \\(p(\\pi(l)=i \\mid \\pi(l-1)=j)=\\frac{\\rho_{i j l}(g)}{\\sum_{l} \\rho_{i j l}(g)}\\).",
      "chunk_id": 17414,
      "paper_id": "Modified Estimation of Distribution Algorithm for Solving Flow-shop Scheduling Problem with Setup Times",
      "paper_title": "Modified Estimation of Distribution Algorithm for Solving Flow-shop Scheduling Problem with Setup Times",
      "paper_year": "2019",
      "chunk_content": "Probability Model It is critical to construct an appropriate probability model for the EDA. The probability model represents the characteristics of distribution and affects the sampling result. Considering sequence dependent setup times, the probability model employs n probability matrixes. Each probability matrix reflects a specific position and contains relative information about the jobs in this position and the jobs in the former position. Therefore, a probability model is designed as n probability matrixes as follows: (1) Job $\\rho_{i j l}(g)$ without the previous job: $$\nP_{1}(g)=\\left[\\rho_{1}(g), \\rho_{2}(g), \\ldots, \\rho_{n}(g)\\right]\n$$ where $P_{1}(g)$ symbolizes a probability matrix which is at the first position in the $g$-th generation. Besides, in the equation $\\rho_{i}(g)=p(\\pi(1)=i), \\rho_{i}(g)$ represents the probability that the job $$\n\\rho_{i j l}(g+1)=(1-\\alpha) \\rho_{i j l}(g)+n_{i j l}(g) \\times \\frac{\\alpha}{E}\n$$ is selected at the first position. The initial probability of $\\rho_{i}(g)$ is $\\rho_{i}(0)=1 / n$. It represents each job has the equal probability to be selected at the first position and the sum of probabilities is 1 in the initial. (2) Job $n \\times n$ with the previous job $l t h$ : $$\nP_{l}(g)=\\left[\\begin{array}{ccc}\n0 & \\cdots & \\cdots, g \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\rho_{n l l}(g) & \\cdots & \\\\\n& &\n\\end{array}\\right] \\leq n\n$$ where $P_{l}(g)$ symbolizes a probability matrix which is at the $$\np(\\pi(l-1)=j)=\\sum_{l} \\rho_{i j l}(g)\n$$-th position in the $$\np(\\pi(l)=i \\mid \\pi(l-1)=j)=\\frac{\\rho_{i j l}(g)}{\\sum_{l} \\rho_{i j l}(g)}\n$$ th generation. Besides, in the equation $\\rho_{i j l}(g)=p(\\pi(l-1)=j, \\pi(l)=i), \\rho_{i j l}(g)$ represents the probability that the job $n-l$ is selected at the $$\n\\rho_{s r t}^{\\prime}(g)=\\frac{\\rho_{s r t}(g) \\times p(\\pi(t-1)=r)}{p(\\pi(t-1)=r)-\\rho_{l r t}(g)}\n$$-th position and right after job $j$. The initial probability of $\\rho_{i j l}(g)$ is $\\rho_{i j l}(0)=\\frac{1}{n(n-1)}$.",
      "question_type": "application",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:23",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, how is the evolution of the probability vector $\\mathbf{p}_{i}$ over iterations mathematically represented, and what functions govern this evolution?",
      "topics": [
        "selection"
      ],
      "answer": "The probability vector $\\mathbf{p}_{i}$ at each iteration $i$ of the EDA is expressed as $\\mathbf{p}_{i+1}=\\mathcal{G}(\\mathbf{p}_{i}, \\pi)$, where $\\mathcal{G}$ is a function that maps from the space of probability vectors $\\Omega_{|X|}$ and a permutation $\\Pi$ to the space of probability vectors $\\Omega_{|X|}$. This function $\\mathcal{G}$ is a composition of the selection and factorization functions, specifically $\\mathcal{G} = \\mathcal{M} \\circ \\phi$, where $\\mathcal{M}$ is the approximation function and $\\phi$ represents the factorization step. The probability vector at iteration $K_{i}$ is expressed as $\\mathcal{G}^{t}(\\mathbf{p}_{0}, \\pi)$, where $\\mathcal{G}^{t}$ represents $t$ iterations of the function $\\mathcal{G}$ applied to the initial probability vector $\\mathbf{p}_{0}$.",
      "chunk_id": 8750,
      "paper_id": "Comprehensive characterization of the behaviors of estimation of distribution algorithms",
      "paper_title": "Comprehensive characterization of the behaviors of estimation of distribution algorithms",
      "paper_year": "2015",
      "chunk_content": "Therefore, the probability vector $\\mathbf{p}^{s}$ has to be related with the corresponding solutions $O_{j}^{\\pi_{2}} \\in G_{\\pi_{2}}$ by means of the ranking $O_{j}^{\\pi_{1}}=O_{j}^{\\pi_{2}}$ as explained before. The approximation step is defined as a function $\\mathcal{M}: \\Omega_{|X|} \\times \\Pi \\rightarrow \\Omega_{|X|}$ and it is computed as $\\mathbf{p}^{s}=\\mathcal{M}\\left(\\mathbf{p}^{s}, \\pi\\right)$ in the algorithm. The approximation $\\mathcal{M}$ is the only operator in $\\boldsymbol{X}_{K_{i}}$ that takes into account the specific $\\boldsymbol{x}_{K_{i}}$ values of the variables (genotype) belonging to the problem solutions. Therefore, only $\\mathbf{p}_{0}=\\left(p_{1}, p_{2}, \\ldots, p_{|\\mathcal{K}|}\\right)$ can translate the difference between functions to different behaviors of the algorithm. ## 4. Equivalent functions and equivalence classes In this section we discuss the concept of equivalence between functions and provide the formal definitions. An EDA $p_{i}>0$ generates deterministic sequences of probability vectors when it is applied to solve a given problem. An iteration of this algorithm can be expressed by a function $\\mathcal{G}: \\Omega_{|X|} \\times \\Pi \\rightarrow \\Omega_{|X|}$ as $\\mathbf{p}_{i+1}=\\mathcal{G}\\left(\\mathbf{p}_{i}, \\pi\\right)$. The function $\\mathcal{G}$ is a composition of the selection $p^{x}\\left(\\boldsymbol{X}_{K_{i}}=\\boldsymbol{x}_{K_{i}}, t=0\\right)=1$ and the factorization function $\\mathcal{G}^{1}\\left(\\mathbf{p}_{0}, \\pi_{1}\\right)=\\mathcal{G}^{1}\\left(\\mathbf{p}_{0}, \\pi_{2}\\right)$ (steps 4 and 5 in Algorithm 2) such that $\\mathcal{G}=\\mathcal{M} \\circ \\phi$. Thus, the sequence of probability vectors induced by $p^{x}\\left(\\boldsymbol{X}_{K_{i}}=\\boldsymbol{y}_{K_{i}}, t=0\\right)=1$ can be expressed as the iterations of the function $Q^{\\boldsymbol{x}_{K_{i}}}$ : $$\n\\mathbf{p}_{0}, \\mathcal{G}\\left(\\mathbf{p}_{0}, \\pi\\right), \\mathcal{G}^{2}\\left(\\mathbf{p}_{0}, \\pi\\right), \\mathcal{G}^{3}\\left(\\mathbf{p}_{0}, \\pi\\right), \\ldots\n$$ where $\\mathcal{G}^{2}\\left(\\mathbf{p}_{0}, \\pi\\right)=\\mathcal{G}\\left(\\mathcal{G}\\left(\\mathbf{p}_{0}, \\pi\\right), \\pi\\right)$ and then the following iterations are computed accordingly. The probability vector at iteration $K_{i}$ is expressed as $\\mathcal{G}^{t}\\left(\\mathbf{p}_{0}, \\pi\\right)$.",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:25",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the probability calculation differ for selecting a job at the first position, represented by $P_1(g)$, compared to selecting a job at position $l$ given the previous job $j$, represented by $P_l(g)$?",
      "topics": [
        "design"
      ],
      "answer": "For the first position, $P_1(g)$, the probability $\\rho_{i}(g)$ that job $i$ is selected is initialized as $1/n$, where $n$ is the total number of jobs, indicating equal initial probability for each job. The probability is updated in subsequent generations using the formula $\\rho_{i j l}(g+1)=(1-\\alpha) \\rho_{i j l}(g)+n_{i j l}(g) \\times \\frac{\\alpha}{E}$. For position $l$ with previous job $j$, $P_l(g)$ uses $\\rho_{i j l}(g)$, representing the probability of selecting job $i$ at position $l$ immediately after job $j$. The initial probability for this is $\\rho_{i j l}(0)=\\frac{1}{n(n-1)}$. The probability $p(\\pi(l)=i \\mid \\pi(l-1)=j)$ is calculated as $\\frac{\\rho_{i j l}(g)}{\\sum_{l} \\rho_{i j l}(g)}$, which conditions on the previous job.",
      "chunk_id": 17414,
      "paper_id": "Modified Estimation of Distribution Algorithm for Solving Flow-shop Scheduling Problem with Setup Times",
      "paper_title": "Modified Estimation of Distribution Algorithm for Solving Flow-shop Scheduling Problem with Setup Times",
      "paper_year": "2019",
      "chunk_content": "Probability Model It is critical to construct an appropriate probability model for the EDA. The probability model represents the characteristics of distribution and affects the sampling result. Considering sequence dependent setup times, the probability model employs n probability matrixes. Each probability matrix reflects a specific position and contains relative information about the jobs in this position and the jobs in the former position. Therefore, a probability model is designed as n probability matrixes as follows: (1) Job $\\rho_{i j l}(g)$ without the previous job: $$\nP_{1}(g)=\\left[\\rho_{1}(g), \\rho_{2}(g), \\ldots, \\rho_{n}(g)\\right]\n$$ where $P_{1}(g)$ symbolizes a probability matrix which is at the first position in the $g$-th generation. Besides, in the equation $\\rho_{i}(g)=p(\\pi(1)=i), \\rho_{i}(g)$ represents the probability that the job $$\n\\rho_{i j l}(g+1)=(1-\\alpha) \\rho_{i j l}(g)+n_{i j l}(g) \\times \\frac{\\alpha}{E}\n$$ is selected at the first position. The initial probability of $\\rho_{i}(g)$ is $\\rho_{i}(0)=1 / n$. It represents each job has the equal probability to be selected at the first position and the sum of probabilities is 1 in the initial. (2) Job $n \\times n$ with the previous job $l t h$ : $$\nP_{l}(g)=\\left[\\begin{array}{ccc}\n0 & \\cdots & \\cdots, g \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\rho_{n l l}(g) & \\cdots & \\\\\n& &\n\\end{array}\\right] \\leq n\n$$ where $P_{l}(g)$ symbolizes a probability matrix which is at the $$\np(\\pi(l-1)=j)=\\sum_{l} \\rho_{i j l}(g)\n$$-th position in the $$\np(\\pi(l)=i \\mid \\pi(l-1)=j)=\\frac{\\rho_{i j l}(g)}{\\sum_{l} \\rho_{i j l}(g)}\n$$ th generation. Besides, in the equation $\\rho_{i j l}(g)=p(\\pi(l-1)=j, \\pi(l)=i), \\rho_{i j l}(g)$ represents the probability that the job $n-l$ is selected at the $$\n\\rho_{s r t}^{\\prime}(g)=\\frac{\\rho_{s r t}(g) \\times p(\\pi(t-1)=r)}{p(\\pi(t-1)=r)-\\rho_{l r t}(g)}\n$$-th position and right after job $j$. The initial probability of $\\rho_{i j l}(g)$ is $\\rho_{i j l}(0)=\\frac{1}{n(n-1)}$.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:28",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing the MIMIC, MIMICwBM, or MIMICwM EDAs, how can a developer measure the difference between the stored probabilistic model and the probabilistic model estimated from sampled genetic information, and what mathematical formulation is used to quantify this difference?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "MIMIC",
        "Probabilistic Models",
        "KL-divergence"
      ],
      "answer": "To measure the difference between the stored probabilistic model ($p_{l}^{s}(\\cdot)$) and the estimated probabilistic model ($p^{e}(\\cdot)$) in MIMIC, MIMICwBM, or MIMICwM, Kullback-Leibler (KL) divergence ($K(\\mathbf{x})$) can be used. The formula for KL-divergence is: $K(\\mathbf{x})= -p_{l}^{s}(X_{i_{n}}) \\log \\frac{p_{l}^{s}(X_{i_{n}})}{p^{e}(X_{i_{n}})} -\\sum_{j=1}^{n-1} p_{l}^{s}(X_{i_{j}} \\mid X_{i_{j+1}}) \\log \\frac{p_{l}^{s}(X_{i_{j}} \\mid X_{i_{j+1}})}{p^{e}(X_{i_{j}} \\mid X_{i_{j+1}})}$, where $p_{l}^{s}(X_{i_{n}})$ represents the stored probabilistic model at generation $l$ for variable $X_{i_{n}}$, and $p^{e}(X_{i_{n}})$ is the probabilistic model estimated from sampled genetic information for the same variable. The summation component calculates the divergence for conditional probabilities between variables $X_{i_{j}}$ and $X_{i_{j+1}}$. This provides a quantitative measure of how well the estimated model matches the stored model.",
      "chunk_id": 13087,
      "paper_id": "The effectiveness of mutation operation in the case of Estimation of Distribution Algorithms",
      "paper_title": "The effectiveness of mutation operation in the case of Estimation of Distribution Algorithms",
      "paper_year": "2007",
      "chunk_content": "6. Experimental results on four-peaks problems. Genetic information sampled by MIMIC, MIMICwBM, and MIMICwM. Individuals are sorted in a certain order in Section 4.3. A dot is painted for value 1's at corresponding loci. sampling with mutation taking account into the structure of the probabilistic model. 3. For each sampling method, a probabilistic model is estimated by using the same structure of the stored probabilistic model. Furthermore, how the estimated probabilistic model differs from the stored probabilistic model is measured by using KL-divergence $K(\\mathbf{x})$ : $$\n\\begin{aligned}\nK(\\mathbf{x})= & -p_{l}^{s}\\left(X_{i_{n}}\\right) \\log \\frac{p_{l}^{s}\\left(X_{i_{n}}\\right)}{p^{e}\\left(X_{i_{n}}\\right)} \\\\\n& -\\sum_{j=1}^{n-1} p_{l}^{s}\\left(X_{i_{j}} \\mid X_{i_{j+1}}\\right) \\log \\frac{p_{l}^{s}\\left(X_{i_{j}} \\mid X_{i_{j+1}}\\right)}{p^{e}\\left(X_{i_{j}} \\mid X_{i_{j+1}}\\right)}\n\\end{aligned}\n$$ where $p_{l}^{s}(\\cdot)$ denote the stored probabilistic model at [FORMULA105] th generation. On the other hand, $p^{e}(\\cdot)$ is the probabilistic model estimated from genetic information sampled by each sampling method. [IMAGE5] pop. size [IMAGE6] pop. size [IMAGE_7] Fig. 5 shows the temporal changes of several quantities in the above experience. The KL-divergence between stored probabilistic model and the probabilistic model estimated from genetic information sampled by each sampling method, entropy of stored probabilistic model, the best and averaged fitness over 30 runs.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:31",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific algorithm is used to obtain the estimation of distribution model for generating new offspring in machine assignment and off-duty hours work shift within the Estimation of Distribution Algorithm?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The COMIT algorithm, introduced by Baluja and Davies, is used to obtain the estimation of distribution model for generating new offspring in both machine assignment and off-duty hours work shift.",
      "chunk_id": 832,
      "paper_id": "Simulation optimization for a flexible jobshop scheduling problem using an estimation of distribution algorithm",
      "paper_title": "Simulation optimization for a flexible jobshop scheduling problem using an estimation of distribution algorithm",
      "paper_year": "2014",
      "chunk_content": "Assigning each continuous value to the corresponding fixed integer number that belongs to each operation and setting each fixed integer number to a job to finish. Figure 12 details an example of a real vector and its decoding. The second probabilistic graph model aims to determine an estimation of distribution model to generate new offspring (machine assignment) using a subset of ' [FORMULA100] ' selected assignments (individuals). To obtain the estimation, we used the COMIT algorithm introduced by Baluja and Davies [25]. The third probabilistic graph model aims to determine an estimation of distribution model to generate new offspring (off-duty hours work shift) using a subset of ' [FORMULA101] ' selected offduty hours work shift (individuals). Although the number of hours is defined for any shift based on current legal guidelines, the start time of the shift is not, so we aim to generate individuals representing the off-duty hours that would be the operators before starting their activities. Again, we used the COMIT algorithm to obtain the estimation. ### 5.4 Diversity SEDA uses a Tabu search method in order to avoid losing diversity on the evolutionary progress. The Tabu search method is based on Pinedo's [26] research.",
      "question_type": "relationship",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:32",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs, specifically concerning the PMFEA-EDA algorithm's process for Web Service Composition?",
      "topics": [
        "mutation"
      ],
      "answer": "The PMFEA-EDA algorithm aims to generate a set of composition solutions by iteratively refining a population of permutations. This involves decoding permutations into DAGs, calculating fitness-related metrics such as \\( f_{j}^{\\Pi_{k}^{g}} \\), \\( r_{j}^{\\Pi_{k}^{g}} \\), \\( \\varphi^{\\Pi_{k}^{g}} \\), and \\( \\tau^{\\Pi_{k}^{g}} \\), generating offspring populations via multiple NHMs learning and sampling, and updating the population based on the fitness of the offspring. The core objective is to find the best permutation \\( \\Pi_{j}^{\\star} \\) over all generations for workflow construction.",
      "chunk_id": 7490,
      "paper_id": "Using an Estimation of Distribution Algorithm to Achieve Multitasking Semantic Web Service Composition",
      "paper_title": "Using an Estimation of Distribution Algorithm to Achieve Multitasking Semantic Web Service Composition",
      "paper_year": "2023",
      "chunk_content": "Algorithm 1: PMFEA-EDA for WSC-MQP Input : $T_{j}, K$, and $g_{\\max }$ Output: A set of composition solutions Randomly initialize population $\\mathcal{P}^{g}$ of $\\mathcal{P}_{0}^{g+1}$ permutations $T_{1}, T_{3}$ as solutions (where ${ }_{4}$ and $k=1, \\ldots, m$ ); 2: Decode each $\\operatorname{QoSM}(\\Pi)$ into DAG $\\mathcal{G}_{k}^{g}$ using the decoding method; 3: Calculate $f_{j}^{\\Pi_{k}^{g}}, r_{j}^{\\Pi_{k}^{g}}, \\varphi^{\\Pi_{k}^{g}}$ and $\\tau^{\\Pi_{k}^{g}}$ of $\\sum_{k=7}^{8} w_{k}=1$ over ${ }^{1}$, where ${ }^{2}$ 4: $\\quad$ Encode each solution $b_{\\text {total }}$ in $k$ with another permutation $\\Pi_{k}^{g} ;$ 5: while $g<g_{\\max }$ do 6: $w_{3}, w_{4}, w_{5}$ Generate offspring population $\\mathcal{P}_{o}^{g+1}$ via multiple NHMs learning and sampling using Algorithm 2 ; 7: Decode solutions in $w_{7}$ into DAG $\\mathcal{G}_{k}^{g+1}$ using the decoding method; 8: Calculate $f^{\\Pi_{k}^{g+1}}$ of solutions in $b_{\\text {ratio }}$ on the selected tasks related to the skill factors determined in its corresponding NHM; 9: $0.2,0.02,0.002$ Encode each solution [FORMULA215] in [FORMULA216] with an another permutation [FORMULA217]; 10: $\\mathcal{P}^{g+1}=\\mathcal{P}^{g} \\cup \\mathcal{P}_{o}^{g+1}$; 11: Update $r_{j}^{\\Pi_{k}^{g+1}}, \\varphi^{\\Pi_{k}^{g+1}}$ and $\\tau^{\\Pi_{k}^{g+1}}$ of offspring in $\\mathcal{P}^{g+1}$; 12: Keep top half the fittest individuals in [FORMULA222] based on $\\varphi^{\\Pi_{k}^{g+1}}$ 13: Return the best $\\Pi_{j}^{\\star}$ over all the generations for [FORMULA225]; workflow using a decoding method proposed in [17].",
      "question_type": "synthesis summary",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:34",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of evaluation relaxation differ between endogenous and exogenous models, and how might these approaches be combined to enhance performance?",
      "topics": [
        "fitness"
      ],
      "answer": "Evaluation relaxation in EDAs aims to improve performance by reducing the number of expensive fitness evaluations. Endogenous models estimate fitness values for new candidate solutions based on the fitness values of previously generated and evaluated solutions. In contrast, exogenous models employ a faster, less accurate surrogate model for some evaluations, particularly early in the run. These two approaches can be combined to maximize benefits; for example, Sastry et al. incorporated endogenous models into the UMDA algorithm (discussed in Section 3.1.1) by extending the probability vector to store statistics on the average fitness of all solutions with a 0 or a 1 in any string position, thereby estimating fitness based on past performance data, while an exogenous model could be used initially to guide the search before relying on the endogenous model for refinement.",
      "chunk_id": 23991,
      "paper_id": "An introduction and survey of estimation of distribution algorithms",
      "paper_title": "An introduction and survey of estimation of distribution algorithms",
      "paper_year": "2011",
      "chunk_content": "Several approaches to parallelizing model building in advanced EDAs with multivariate models have also been proposed [119-121]. ### 6.2. Evaluation relaxation As previously discussed, one method to help alleviate the fitness bottleneck is parallelization. Nonetheless, to further improve performance of algorithms with expensive fitness evaluation, it is sometimes possible to eliminate some of the fitness evaluations by using approximate models of the fitness function, which can be evaluated much faster than the actual fitness function. Efficiency enhancement techniques based on this principle are called evaluation relaxation techniques [19,122-125]. There are two basic approaches to evaluation relaxation: (1) endogenous models [122-125] and (2) exogenous models [126,127]. With endogenous models, the fitness values for some of the new candidate solutions are estimated based on the fitness values of the previously generated and evaluated solutions. With exogenous models, a faster but less accurate surrogate model is used for some of the evaluations, especially for those early in the run. Of course, the two approaches can be combined to maximize the benefits. Sastry et al. [128] incorporated endogenous models in the UMDA algorithm discussed in Section 3.1.1. To estimate fitness, the probability vector was extended to also store statistics on the average fitness of all solutions with a 0 or a 1 in any string position.",
      "question_type": "comparison",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:35",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm using neural networks, how should practitioners initialize the population of antibodies, considering the activation functions?",
      "topics": [
        "fitness"
      ],
      "answer": "Practitioners should initialize the population of antibodies such that all activation functions have the same probability of being chosen, except the Null function ($y_{ji}=a$). The Null function should have a higher probability to favor more parsimonious neural networks.",
      "chunk_id": 23745,
      "paper_id": "Learning Ensembles of Neural Networks by Means of a Bayesian Artificial Immune System",
      "paper_title": "Learning Ensembles of Neural Networks by Means of a Bayesian Artificial Immune System",
      "paper_year": "2011",
      "chunk_content": "Fig. 1 depicts a possible antibody for this case. The network coded in the antibody $A b_{i}$ is illustrated in Fig. 2. Since the second activation function in the antibody $x_{j}$ of Fig. 1 is the null function, the network topology is composed of only four neurons in the hidden layer. The activation function of the output layer, $f_{s}$, is always the logistic function and it is not considered in the codification. Note that in this example the neural network has just one output node, but could has more output nodes. Notice that the weights of the network are not considered in the codification, since they are adjusted by means of a conjugate gradient algorithm [44]. 2) Initial Population: To initialize the population, all activation functions have the same probability of being chosen, except the Null function, $y_{j i}=a$, which has a higher probability, thus favoring more parsimonious neural networks. 3) Fitness Function: The fitness function is defined based on the performance of the neural networks, which is calculated as a function of the number of training patterns correctly classified. The fitness function is expressed by $$\n\\operatorname{Fit}\\left(A b_{i}\\right)=\\frac{N P C C\\left(A b_{i}\\right)}{N P}\n$$ where $N P C C\\left(A b_{i}\\right)$ is the number of patterns correctly classified by the network coded in the antibody $$\nD i s=\\frac{2}{L(L-1)} \\sum_{i=1}^{L-1} \\sum_{m=i+1}^{L} D i s_{i, m}\n$$ and $N P$ is the total number of instances in the training set.",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:37",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the HEDA-DEV algorithm discussed, how does the use of five types of neighborhood structures satisfying cooperative search strategies contribute to its performance, and what specific aspect of optimization does this address?",
      "topics": [
        "scheduling"
      ],
      "answer": "The HEDA-DEV algorithm employs five types of neighborhood structures satisfying cooperative search strategies to adequately improve the local exploitation ability. This specifically addresses the need for enhanced local search within the algorithm's optimization process, allowing it to more effectively explore the solution space in the vicinity of promising solutions.",
      "chunk_id": 5760,
      "paper_id": "A hybrid estimation of distribution algorithm for solving assembly flexible job shop scheduling in a distributed environment",
      "paper_title": "A hybrid estimation of distribution algorithm for solving assembly flexible job shop scheduling in a distributed environment",
      "paper_year": "2024",
      "chunk_content": "Firstly, a new multidimensional coding method is designed based on the features of the DAFJSP. Secondly, two mutation operators and the similarity coefficient based on the probability matrix are put forward to implement the dynamic mutation. Thirdly, five types of neighborhood structures satisfying cooperative search strategies are employed to adequately improve the local exploitation ability. Finally, the comparison experiment results suggest that the proposed HEDA-DEV has competitive performance compared to the selected efficient algorithms. Moreover, a real case study is used to demonstrate that HEDA-DEV is an effective method for solving DAFJSP. ## 1. Introduction As economic globalization becomes popular, the conventional singlefactory production mode is unable to meet the increasing market demand. So, more and more enterprises have begun to set up multiple factories in different places to improve their production efficiency (Naderi and Azab, 2014; Hsu et al., 2016; Hamzadayi, 2020; Behnamian and Fatemi Ghomi, 2016). Different from the single-factory production mode, a distributed manufacturing system must address two interrelated decisions, i.e., the allocation of jobs among factories and the scheduling of production within each factory (Zhang et al., 2018). It is much more complex than the conventional single-factory production mode because it has to deal with the problem of optimizing the scheduling across multiple factories simultaneously (Zhang and Xing, 2018).",
      "question_type": "advantage",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:38",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "When applying an Estimation of Distribution Algorithm (EDA) to the sphere function, how does the percentage of generations triggering variance scaling change as the correlation threshold, denoted as $\\theta^{\\text {corr }}$, varies from -1.0 to 1.0, given a fixed population size [FORMULA164] equal to the minimally required population size for solving the problem without variance adaptation?",
      "topics": [
        "fitness"
      ],
      "answer": "The provided text describes an experiment where $\\theta^{\\text {corr }}$ is varied from -1.0 to 1.0 in steps of 0.01 across 100 independent runs on the sphere function with dimensionalities $l \\in \\{2,4,8,10,20,40,80\\}$. The population size [FORMULA164] is set to the minimally required population size for an EDA without variance adaptation to solve the sphere function to optimality. The experiment then measures the percentage of generations in which variance scaling is triggered, averaged over these 100 runs, for each value of $\\theta^{\\text {corr }}$. The magnitude of [FORMULA157] was bounded from above by $c^{\\text{AVS-MAX}} = 10.0$.",
      "chunk_id": 11348,
      "paper_id": "Matching inductive search bias and problem structure in continuous Estimation-of-distribution Algorithms",
      "paper_title": "Matching inductive search bias and problem structure in continuous Estimation-of-distribution Algorithms",
      "paper_year": "2008",
      "chunk_content": "The magnitude of [FORMULA157] was bounded from above by $c^{\\mathrm{AVS}-\\mathrm{MAX}}=10.0$. ### 5.3. Results ### 5.3.1. Parameter selection for the correlation trigger In order to select a reasonable value for $\\theta^{\\text {corr }}$, we tested when the ranked correlation coefficient between fitness and density actually triggers scaling of the variance on the sphere function. We varied [FORMULA160] from -1.0 to 1.0 in steps of 0.01 . For each value of $\\theta^{\\text {corr }}, 100$ independent runs were performed on the sphere function in dimensionalities $l \\in\\{2,4,8,10,20,40,80\\}$. Initial populations were drawn symmetrically around the optimal solution of 0 for all dimensions in a range of $[-7.5,7.5]$. The population size that was used for a certain dimensionality [FORMULA164] was equal to the minimally required population size for the EDA without variance adaptation to solve the problem with dimensionality [FORMULA165] to optimality. In other words, in that case variance scaling is not required because the sphere function is a single peak and thus, the induced bias of the normal pdf itself suffices to locate the optimum if the population size is large enough. Fig. 6 illustrates the percentage of generations in which variance scaling was nonetheless triggered (averaged over 100 runs).",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:40",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model that employs both NHM and EHM for solving ARP-MPDT, as mentioned in the text?",
      "topics": [
        "EDA",
        "NHM",
        "EHM",
        "ARP-MPDT",
        "routing"
      ],
      "answer": "The text mentions a novel EDA employing both NHM (Neighborhood-based Heuristic Model) and EHM (Evolutionary Heuristic Model) in probability modeling to solve ARP-MPDT. A coefficient is used to dynamically adjust the selection ratio of NHM to EHM during the sampling process. The specific steps for executing this EDA model are not detailed beyond this high-level description within the provided text.",
      "chunk_id": 23032,
      "paper_id": "A Multi-Model Estimation of Distribution Algorithm for Agent Routing Problem in Multi-Point Dynamic Task",
      "paper_title": "A Multi-model Estimation of Distribution Algorithm for Agent Routing Problem in Multi-point Dynamic Task",
      "paper_year": "2018",
      "chunk_content": "The objective values of the solutions of seven instances are showed in Table 3. This table includes six parameters, such as the best value, the worst value and so on. In this table, it can be discovered that the solutions are stable for different instances. The cost of the calculating time is also shown in Table 3. With the increase of the number of tasks, the calculating time becomes much longer and even unacceptable. Table 3: Statistical results *n.a.: not available due to the limit of runtime. ## 5 Conclusion In this paper, a new routing problem, ARP-MPDT which depends on task execution time and traveling time was introduced. In ARP-MPDT, the state of each task point changes non-linearly. Then, a novel EDA employing both NHM and EHM in probability modeling was proposed to solve ARP-MPDT. A coefficient was designed to adjust dynamically the selection ratio of NHM to EHM in sampling. Through the comparative experiments and general tests, the effectiveness and stability of the proposed algorithm for ARP-MPDT were proved.",
      "question_type": "algorithm",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:42",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are mentioned that indicate improvements in the computational efficiency of an EDA when using Fuzzy Logic Control (FLC)?",
      "topics": [
        "design"
      ],
      "answer": "The application of FLC in EDA is indicated to improve computational efficiency by reducing time and eliminating the requirement for multiple initial simulations to choose the best value of parameter [FORMULA234].",
      "chunk_id": 16171,
      "paper_id": "Fuzzy Logics as an Integral Part of Evolutionary Algorithms",
      "paper_title": "Fuzzy Logics as an Integral Part of Evolutionary Algorithms",
      "paper_year": "2019",
      "chunk_content": "[FORMULA232] It can be concluded that by eliminating the stiffness problem for parameter [FORMULA233], the algorithm can be made more efficient. By using FLC in EDA, the requirement of multiple initial simulations for choosing best value of [FORMULA234] is also eliminated, which results in reduction of time and improvement in computational efficiency. The algorithm is now self-adaptive in nature for choosing the best value of [FORMULA235] at each generation. Although this work deals with eliminating the stiffness problem of [FORMULA236] in EDA, treatment of the stiffness problem of other parameters, like [FORMULA237] and [FORMULA238], in EDA using fuzzy logics can also be considered. It is worth mentioning that by applying fuzzy logics to choose various internal parameters in other evolutionary algorithms, such as GA, ACO, etc., and in other LPO problems with more types of fuel clusters may lead to better results and efficient performance. ## Acknowledgments The authors are thankful to Baltej Singh, Reactor Physics Design Division (RPDD), Bhabha Atomic Research Centre (BARC) for his guidance. The authors are also thankful to Vibhuti Duggal and Kislay Bhatt of Computer Division (CD), BARC for their help in running the code in the ANUPAM system.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:43",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs when optimizing the motion planning of a macro-micro manipulator system, specifically concerning the division of the task space?",
      "topics": [
        "optimization"
      ],
      "answer": "The core purpose is to decompose the complex optimization problem by transferring the search space from the joint variable space to the division point space. This involves dividing the task space into two components, \\(\\boldsymbol{p}_{M}\\) and \\(\\boldsymbol{p}_{m}\\), where \\(\\boldsymbol{p}_{M}\\) represents the macro-manipulator's contribution to reaching the neighborhood of a task point, and \\(\\boldsymbol{p}_{m}\\) represents the micro-manipulator's adjustments to precisely reach the task point. This division aims to reduce computational complexity and improve trajectory tracking accuracy by decoupling the macro and micro manipulator movements, allowing for more efficient optimization within the context of EDAs.",
      "chunk_id": 12838,
      "paper_id": "Global optimization of motion planning for tip trajectory tracking of a flexible macro-micro manipulator system using Estimation of Distribution Algorithm",
      "paper_title": "Global optimization of motion planning for tip trajectory tracking of a flexible macro-micro manipulator system using Estimation of Distribution Algorithm",
      "paper_year": "2006",
      "chunk_content": "B. Optimization in Division Point Space Consider a robot system with an $M$-DOF macro manipulator and an $m$-DOF micro manipulator. Let $\\boldsymbol{\\theta}_{M} \\in \\mathrm{R}^{M}$ and $\\boldsymbol{\\theta}_{m} \\in \\mathrm{R}^{m}$ be the vectors of the joint variables of the macro and micro manipulator, respectively, $\\boldsymbol{\\delta} \\in \\mathrm{R}^{r}$ be the flexural displacement vector and $\\boldsymbol{p} \\in \\mathrm{R}^{n}$ be the tip vector in the n-dimensional task space. The endpoint vector $\\boldsymbol{p}$ is a nonlinear function of $\\boldsymbol{\\theta}_{\\boldsymbol{M}}, \\boldsymbol{\\theta}_{\\boldsymbol{m}}$ and $\\boldsymbol{\\delta}$ : $$\np=f\\left(\\theta_{M}, \\theta_{m}, \\delta\\right)\n$$ Define $\\quad \\boldsymbol{f}_{\\delta=0}\\left(\\boldsymbol{\\theta}_{M}, \\boldsymbol{\\theta}_{m}\\right)=\\boldsymbol{f}\\left(\\boldsymbol{\\theta}_{M}, \\boldsymbol{\\theta}_{m}, \\boldsymbol{\\theta}\\right)$ Optimization in the whole joint angle space will be timeconsuming and cannot guarantee that the tip positions locate exactly on the desired trajectory. Here we transfer the search space from the joint variables to division point, the point where the micro manipulator connects to the macro one. Task space is divided to two parts: The macro manipulator carried the micro one to the neighbourhood of the task point, while the micro manipulator sets its joint variables to make the tip on the task point. $$\np=p_{M}+p_{m}\n$$ where $\\boldsymbol{p}_{M}$ denotes the vector of the task division point, which is also the end-point of the macro manipulator and the base point of the micro one, $\\boldsymbol{p}_{m}$ denotes the vector from the division point to the tip of the whole system.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:45",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the calculation of the Average Response Variable (ARV), and how is it used to evaluate the HEDA?",
      "topics": [
        "optimization"
      ],
      "answer": "The Average Response Variable (ARV) is calculated using the formula $A R V=\\sum_{i=1}^{N} \\sum_{j=1}^{20} \\frac{\\left(\\text { Makespan }_{i j}-L B\\right)}{L B} /(20 \\cdot N)$. In this formula, Makespan $_{i j}$ is the makespan of the $i$th instance obtained by the HEDA in the $j$th run, and $L B$ is the lower bound used for comparison. $N$ represents the number of instances. The ARV is calculated as the average deviation of the makespan from the lower bound, divided by the lower bound, across all instances and runs. The ARV is used to analyze the response value changes of each parameter and determine the significance rank of each parameter for J30, J60, and J120.",
      "chunk_id": 10433,
      "paper_id": "A hybrid estimation of distribution algorithm for solving the resource-constrained project scheduling problem",
      "paper_title": "A hybrid estimation of distribution algorithm for solving the resource-constrained project scheduling problem",
      "paper_year": "2012",
      "chunk_content": "The average response variable (ARV) values for J30, J60, and J120 are the following average deviation values for $N=48$, 48,60 instances, respectively. $A R V=\\sum_{i=1}^{N} \\sum_{j=1}^{20} \\frac{\\left(\\text { Makespan }_{i j}-L B\\right)}{L B} /(20 \\cdot N)$ where Makespan $_{i j}$ is the makespan of the [FORMULA102] th instance obtained by the HEDA in the [FORMULA103] th run; $L B$ is the lower bound to compare with. Table 1 Combinations of parameter values. According to the number of parameters and the number of factor levels, we choose the orthogonal array $L_{25}\\left(5^{5}\\right)$. That is, the total number of treatment is 25 , the number of parameters is 5 , and the number of factor levels is 5 . The orthogonal arrays for J30, J60, J120 are listed in Table 2. According to the orthogonal table, we illustrate the trend of each factor level for J30, J60, and J120 in Figs. 9-11, respectively. Then, we figure out the response value changes of each parameter to analyze the significance rank of each parameter for J30, J60, and J120. The results are listed in Tables 3-5, respectively.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:47",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm within a VISSIM-MATLAB integrated traffic simulation platform for traffic signal optimization, how should practitioners configure the interaction between MATLAB and VISSIM to ensure concurrent simulation and optimization?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "VISSIM",
        "MATLAB",
        "traffic simulation",
        "signal optimization"
      ],
      "answer": "Practitioners should utilize the component object model module within the VISSIM-MATLAB integrated traffic simulation platform. This module facilitates automatic information exchange between MATLAB, where the optimization algorithm is executed and candidate signal plans are generated, and VISSIM, which simulates traffic dynamics and evaluates the effectiveness of those signal plans. Specifically, VISSIM is configured to allow MATLAB to send signal plans and receive performance metrics concurrently during the simulation.",
      "chunk_id": 17764,
      "paper_id": "Surrogate-assisted cooperative signal optimization for large-scale traffic networks",
      "paper_title": "Surrogate-assisted cooperative signal optimization for large-scale traffic networks",
      "paper_year": "2021",
      "chunk_content": "[IMAGE_3] Fig. 4. Simplified traffic network in an area of Guiyang City. ## 4. Case study ### 4.1. Experimental setup Experiments are implemented in a popular VISSIM-MATLAB integrated traffic simulation platform [33], [50] specially designed for traffic signal optimization. In this simulation platform, MATLAB is employed to execute optimization algorithm and produce candidate solutions (signal plans), and a modern microscopic traffic simulator VISSIM is used to simulate the traffic dynamics and evaluate the effectiveness of signal plans. Using VISSIM, the roads and junctions of traffic network and the traffic flows could be conveniently designed and configured. VISSIM automatically exchanges information with MATLAB through the component object model module to realize simulation and optimization concurrently. A simplified traffic network generated based on real traffic network in an area of Guiyang City in Guizhou Province, China is employed in this study. As shown in Fig. 4, there are totally 43 signalized junctions (J1-J43) in this traffic network, including 26 crossroads and 17 T-junctions. Road 1 to Road 15 (R1-R15) are both the entrances and exits of this network.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:49",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the computational complexity of the exhaustive algorithm, which utilizes a branch and bound approach with depth-first search for resource allocation, managed through pruning in the described procedure?",
      "topics": [
        "comparison"
      ],
      "answer": "The exhaustive algorithm employs pruning to manage computational complexity. Specifically, if, during the depth-first search branching procedure, a resource allocation that satisfies the deadline constraint (referred to as [FORMULA248]) is found, all subsequent nodes in that branch are pruned. Furthermore, a lower bound is calculated for the weighted resource consumption of newly generated nodes. If this lower bound is greater than or equal to the weighted resource consumption of the initial solution, the node and all nodes beyond it in that branch are eliminated. The algorithm explores nodes from an empty resource allocation set (0,0, ..., 0) to (k, k, ..., k), where k represents the maximum resource allocation in each position. These pruning techniques are used to avoid exploring the entire search space, reducing the computational burden.",
      "chunk_id": 8882,
      "paper_id": "A Boltzmann-Based Estimation of Distribution Algorithm for a General Resource Scheduling Model",
      "paper_title": "A Boltzmann-Based Estimation of Distribution Algorithm for a General Resource Scheduling Model",
      "paper_year": "2015",
      "chunk_content": "We think that with this wide range of proposal a fair and complete comparison can be conducted. 1) Exhaustive Algorithm: In this part, a branch and bound algorithm is developed. A depth-first search is adopted in the branching procedure, which allocates resources to each job in a forward manner. It begins with an empty resource allocation set $(0,0, \\ldots, 0)$, allocating one unit resource allocation to the first position so that the nodes starts from $(1,0, \\ldots, 0)$, and finally arrives at $(k, k, \\ldots, k)$ with the maximum resource allocation in each position. Suppose in one step we are at the node $\\left(r_{1}, r_{2}, \\ldots, r_{i},--, \\ldots,\\right)$ and its descendants are $\\left(r_{1}, r_{2}, \\ldots, r_{i}, 1, \\ldots,-\\right)$ to $\\left(r_{1}, r_{2}, \\ldots, r_{i}, k, \\ldots,-\\right)$, when the first resource allocation that satisfies the deadline constraint [FORMULA248] is found, then all the proceeding nodes are pruned. If a new node is generated, a lower bound for the weighted resource consumption is calculated. If it is greater than or equal to the initial solution, then the node and all the nodes beyond it in that branch are eliminated. If a completed node has a weighted resource consumption less than the initial solution, then it is substituted as the new solution. Otherwise, it is eliminated. The step continues until all the nodes are explored.",
      "question_type": "definition",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:51",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What is the key difference in how EDNA (Estimation of Dependency Networks Algorithm) and the Dependency Structure Matrix (DSM) approach handle variable interactions during crossover?",
      "topics": [
        "crossover",
        "Estimation of Distribution Algorithms",
        "EDNA",
        "Dependency Structure Matrix"
      ],
      "answer": "EDNA uses mutual information values to find the parent set for each variable, avoiding the full conditional probability calculation. The DSM approach, on the other hand, clusters pairwise interactions above a given threshold using a matrix containing information about these interactions, and then uses this to accomplish building block-wise crossover.",
      "chunk_id": 15378,
      "paper_id": "Learning Factorizations in Estimation of Distribution Algorithms Using Affinity Propagation",
      "paper_title": "Learning Factorizations in Estimation of Distribution Algorithms Using Affinity Propagation",
      "paper_year": "2010",
      "chunk_content": "- Use of a mutual information-based approximation of the network scoring metric in the structural learning step of the estimation of dependency networks algorithm (EDNA; Gamez et al., 2007). Dependency networks (Heckerman et al., 2000) are closely related to Bayesian networks, but similar to Markov networks, they are able to represent cycles between the variables. EDNA uses the mutual information values to look for the parent set of each variable. The idea is to avoid the expensive computation of the full conditional probability of each variable given a set of candidate parents. In Gamez et al. (2008), the use of bivariate statistics to approximate higher-order marginals is extended to the simulation and parametric learning tasks of EDNA. - Use of a dependency structure matrix (DSM) combined with a MDL technique to cluster pairwise interactions above a given threshold. A DSM is a matrix that contains information about the pairwise interactions between the components of a system. In Yu et al. (2003), this technique is used to extract building block (BB) information and use the information to accomplish BB-wise crossover. The maximal number of clusters and a threshold on the strength of the nonlinearity required to determine whether two variables interact are given as parameters of the method.",
      "question_type": "comparison",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:52",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model, specifically when using Probabilistic Graphical Models (PGM) to improve performance in multi-objective problems?",
      "topics": [
        "Estimation of Distribution Algorithms (EDAs)",
        "Probabilistic Graphical Models (PGM)",
        "Multi-objective Estimation of Distribution Algorithm (MOEDA)",
        "Bayesian Networks"
      ],
      "answer": "The execution involves building a probabilistic model from the current best candidate solutions. New solutions are then sampled from this model, guiding the evolution toward more promising areas of the search space. When applied to multi-objective problems, particularly combinatorial Multi-objective Optimization Problems (MOPs), Bayesian Networks are often adopted as the PGM. A local optimizer can be added to refine the solutions found by sampling from the PGM, further enhancing the MOEDA.",
      "chunk_id": 14325,
      "paper_id": "Probabilistic Analysis of Pareto Front Approximation for a Hybrid Multi-objective Bayesian Estimation of Distribution Algorithm",
      "paper_title": "Probabilistic Analysis of Pareto Front Approximation for a Hybrid Multi-objective Bayesian Estimation of Distribution Algorithm",
      "paper_year": "2017",
      "chunk_content": "Different approaches have been proposed to approximate the Pareto-optimal front (its corresponding objectives) in various scenarios [1]. Multi-objective evolutionary algorithms (MOEAs) are classical examples of these approaches. They have achieved good results in MOPs as they search multiple solutions in parallel with some advantages when compared with math programming-based approaches. Estimation of distribution algorithms (EDAs) are a class of EAs that explore the search space by building a probabilistic model from a set with the current best candidate solutions [2]. Since new solutions are sampled from the probabilistic model, the evolution is guided toward more promising areas of the search space. Probabilistic Graphical Models (PGM) [3] combining graph and the probability theory have been adopted to improve EDAs performance [2]. EDAs developed to solve multi-objective problems [4] are usually called Multi-objective Estimation of Distribution Algorithm (MOEDA) and most of those developed to deal with combinatorial MOPs adopt Bayesian Networks as their PGM. However, recently the role of the probabilistic model has been extended to model the dependencies between variables and objectives [4]. In addition, MOEDAs can be notably enhanced by adding a local optimizer that can refine the solutions found by sampling from the PGM [5], [6].",
      "question_type": "process",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:56:54",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific aspect of the sampling distribution significantly influences the success of Estimation of Distribution Algorithms, and why is this aspect critical?",
      "topics": [
        "optimization"
      ],
      "answer": "The success of an Estimation of Distribution Algorithm heavily depends on the representation and estimation of the probability distributions. This is critical because these distributions capture the interaction information of variables within the optimization problem, directly impacting the algorithm's ability to effectively explore the search space and identify promising solutions.",
      "chunk_id": 26659,
      "paper_id": "Space complexity of estimation of distribution algorithms",
      "paper_title": "Space complexity of estimation of distribution algorithms",
      "paper_year": "2005",
      "chunk_content": "The sampling probability distributions may be modelled as the product of independent marginal distributions, decomposable distributions obtained from the knowledge about the problem's interaction structures, or Bayesian networks constructed from existing samples of solutions (Muhlenbein et al., 1999; Pelikan et al., 1999; Leung et al., 2001; Larranaga et al., 2000; Larranaga and Lozano, 2001). Unlike many other stochastic local search algorithms, such as the standard genetic algorithm (Goldberg, 1989) and simulated annealing (Kirkpatrick et al., 1983), where the sampling distributions are implicitly defined by the random operators, the sampling distribution in EDAs is an explicit component of the algorithm. As a consequence, an EDA's success heavily depends on the representation and estimation of the probability distributions to capture the interaction information of variables of an optimization problem. An investigation of the complexity issues related to the representation of sampling distributions in EDAs will provide much insight into such problems as how to design efficient EDAs and what the limitations of the algorithm are. In this paper, we report our research into the complexity issues in two typical implementation schemes of EDAs: the factorized distribution algorithm and the Bayesian network-based algorithm.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:56:55",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of defining '[FORMULA223]-independent' positions within the context of analyzing the behavior of an [FORMULA218]-EDA with a fitness function [FORMULA220]?",
      "topics": [
        "fitness"
      ],
      "answer": "The purpose is to characterize the behavior of the [FORMULA218]-EDA by examining how the frequency \\(\\boldsymbol{p}_{i}^{(t)}\\) changes (or doesn't change) when a specific position \\(i\\) in the solution vector \\(\\boldsymbol{x}\\) does not affect the fitness function \\(f(\\boldsymbol{x})\\). This leads to defining whether the EDA is 'balanced' (frequency remains constant in expectation) or 'stable' (frequency converges to a symmetric distribution around 0.5).",
      "chunk_id": 26153,
      "paper_id": "EDAs cannot be Balanced and Stable",
      "paper_title": "EDAs cannot be Balanced and Stable",
      "paper_year": "2016",
      "chunk_content": "However, it is interesting to see what happens if there is no bias at a certain position. Definition 3. Given an [FORMULA217]-Bernoulli- [FORMULA218]-EDA $A$ and a fitness function [FORMULA220], we say that a position [FORMULA221] of [FORMULA222] is [FORMULA223]-independent if, for all $\\boldsymbol{x}, \\boldsymbol{y} \\in\\{0,1\\}^{n}$ such that [FORMULA225] and $\\boldsymbol{y}$ only differ in position $i, f(\\boldsymbol{x})=f(\\boldsymbol{y})$. Having an [FORMULA228]-independent position [FORMULA229], we define the following two types of behavior that [FORMULA230] might express. Definition 4. An [FORMULA231]-Bernoulli- [FORMULA232]-EDA [FORMULA233] is balanced if, for all [FORMULA234]-independent positions [FORMULA235] of [FORMULA236] and for all $t \\in \\mathbb{N}$, the frequency $\\boldsymbol{p}_{i}^{(t)}$ does not change, in expectation, after one update, i.e., $\\mathrm{E}\\left(\\boldsymbol{p}_{i}^{(t+1)} \\mid \\boldsymbol{p}_{i}^{(t)}\\right)=\\boldsymbol{p}_{i}^{(t)}$ Definition 5. An [FORMULA240]-Bernoulli- [FORMULA241]-EDA [FORMULA242] is stable if, for all [FORMULA243]-independent positions [FORMULA244] of [FORMULA245], the limit distribution of frequency [FORMULA246], as $t \\rightarrow \\infty$, exists and is symmetric around [FORMULA248], taking its maximum at [FORMULA249], and is strictly monotonically decreasing from [FORMULA250] toward the borders.",
      "question_type": "synthesis summary",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:56:59",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of reducing the number of iterations, as compared to other methods like Genetic Algorithms (GAs), contribute to the effectiveness of Estimation of Distribution Algorithms in solving inverse problems, specifically in reconstructing buried objects in layered media?",
      "topics": [
        "genetic"
      ],
      "answer": "Estimation of Distribution Algorithms (EDAs) are presented as novel stochastic optimization algorithms. The text indicates that EDAs, compared with other methods such as Genetic Algorithms (GAs), reduce the number of iterations required to solve inverse problems. This reduction in the number of iterations is highlighted as a performance advantage when reconstructing buried objects, suggesting that EDAs may converge to a solution faster than GAs in some cases, such as traveling salesman, job shop scheduling, knapsack, bin packing, neural network weight optimization, and numerical function optimization.",
      "chunk_id": 10722,
      "paper_id": "An Improved Population-Based Incremental Learning Method for Objects Buried in Planar Layered Media",
      "paper_title": "An Improved Population-Based Incremental Learning Method for Objects Buried in Planar Layered Media",
      "paper_year": "2012",
      "chunk_content": "Recent studies have shown that the EDA provides better solution for nonlinear problems than the microscopic evolutionary algorithm, such as genetic algorithm (GA) in some cases. The EDA is simpler, both computationally and theoretically, than the GA. We discuss how this can be used to calculate the permittivity and conductivity of the targets. We show preliminary results indicating the potential of reconstruction for buried objects. Compared with other methods, the experiment result shows that the EDA algorithm reduces the number of iteration. Index Terms-Buried objects, estimation of distribution algorithm, inverse scattering, layered media. ## I. INTRODUCTION THE problem of reconstructing three-dimensional objects buried in layered media is an important research issue [1]. It is useful in geophysical exploration, target identification, environmental survey, microwave imaging and nondestructive testing. Previously, many methods have been applied to solve inverse problem, such as genetic algorithm (GA) [2], Born iteration method (BIM) [3], [4], and distorted Born iteration method (DBIM) [5]. Recent researches [6] have shown that the EDA outperforms a GA on large set of optimization problems in terms of both speed and accuracy in some cases, such as traveling salesman, job shop scheduling, knapsack, bin packing, neural network weight optimization, and numerical function optimization. EDAs are a class of novel stochastic optimization algorithms, which have recently become a hot topic in the field of optimization algorithms.",
      "question_type": "application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:00",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to obtain a regularized estimate of a parameter set, say [FORMULA172], how should practitioners determine the final estimate from the population of candidate solutions?",
      "topics": [
        "design"
      ],
      "answer": "After T iterations of the EDA, resulting in \\(\\boldsymbol{\\beta}_{1}^{(T)} \\ldots \\boldsymbol{\\beta}_{M}^{(T)}\\), the final regularized estimate of [FORMULA172] should be chosen by selecting \\(\\operatorname{argmax}_{(i, 1}, \\ldots, M) l\\left(\\boldsymbol{\\beta}_{i}^{(T)}\\right)\\), which represents the parameter set that maximizes the likelihood function l.",
      "chunk_id": 23629,
      "paper_id": "Regularized logistic regression without a penalty term- An application to cancer classification with microarray data",
      "paper_title": "Regularized logistic regression without a penalty term: An application to cancer classification with microarray data",
      "paper_year": "2011",
      "chunk_content": "Different univariate, bivariate or multivariate dependencies may be designed with the benefit of having an explicit model of (possible) complex probabilistic relationships among the different parameters. Finally, the last step, say at iteration $h=T$, would contain $\\boldsymbol{\\beta}_{1}^{(T)} \\ldots \\boldsymbol{\\beta}_{M}^{(T)}$ from which $\\operatorname{argmax}_{(i, 1}, \\ldots, M) l\\left(\\boldsymbol{\\beta}_{i}^{(T)}\\right)$ would be chosen as the final regularized estimate of [FORMULA172]. ## 4. Results We illustrate how our approach really acts as a regularizer on three publicly available ${ }^{1}$ benchmark microarray data sets. First, the Breast data set (West et al., 2001) with 7129 genes and 49 tumor samples, 25 of them representing estrogen receptor-positive (ER+) and the other 24 being estrogen receptor-negative (ER-). Second, the Colon data set (Alon et al., 1999) that contains 2000 [^0] [^0]: [FORMULA174] http://bioinformatics.upmc.edu/Help/UPfTIGED.html. genes for 62 tissue samples: 40 cancer tissues and 22 normal tissues. Third, the Leukemia data set (Golub et al., 1999) that consists of 7129 genes and 72 tissue samples: 25 cases of acute myeloid leukemia (AML) and 47 cases of acute lymphoblastic leukemia (ALL).",
      "question_type": "theory",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:02",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed with proportional selection, under what mathematical conditions does the probability of an individual *x* in the next generation, denoted as *Pop'*(x, t), evolve, and how is this evolution mathematically represented?",
      "topics": [
        "fitness"
      ],
      "answer": "With proportional selection, the probability *Pop'*(x, t) of an individual *x* in the next generation is determined by the formula *Pop'*(x, t) = (E(F(x)) * p(x, t)) / H(t), where E(F(x)) represents the expectation operator for a random variable, *p(x, t)* represents the probability of individual *x* at time *t*, and H(t) is a normalization factor. This is further simplified to *Pop'*(x, t) = (f(x) * p(x)) / H(t), where f(x) is the fitness of individual x. The dynamic model for proportional selection is represented as *p(x, t+1) = (f(x) * p(x, t)) / H(t)*, describing how the probability of *x* changes over time based on its fitness.",
      "chunk_id": 26523,
      "paper_id": "Convergence of estimation of distribution algorithms in optimization of additively noisy fitness functions",
      "paper_title": "Convergence of estimation of distribution algorithms in optimization of additively noisy fitness functions",
      "paper_year": "2005",
      "chunk_content": "Dynamic Model of EDA under the Proportional Selection Proportion selection operator selects the individuals according to the fitness of an individual. The probability of an individual $x$ in the next generation can be calculated, $$\n\\begin{aligned}\nP o p^{\\prime}(x, t) & =\\frac{E(F(x)) p(x, t)}{H(t)} \\\\\n& =\\frac{f(x) p(x)}{H(t)}\n\\end{aligned}\n$$ Where $E$ is expectation operator for a random variable. This operator will be applied throughout of this paper. According to (6), we can model the proportional selection as: $$\np(x, t+1)=\\frac{f(x) p(x, t)}{H(t)}\n$$ (7) coincides with the model of EDA under the noiseless conditions. ### 3.2. Dynamic Model of EDA under the Tournament Selection Tournament selection chooses $K$ individuals from the population randomly, and compares their fitness. The best one is selected into the subpopulation. $$\np(x, t)=2 p(x, t)\\left(\\int_{D} p(y, x) \\Phi\\left(\\frac{f(x)-f(y)}{\\sqrt{2 \\sigma}}\\right) d y\\right)\n$$ is called the tournament size. Commonly $K=2$. Here we also set $$\np(x, t+1)=\\frac{p(x, t)}{C} \\times \\Phi\\left(\\frac{f(x)-\\beta(t)}{\\sigma}\\right)\n$$. $$\n\\begin{aligned}\n& \\therefore \\delta \\sim N\\left(0, \\sigma^{2}\\right) \\\\\n& \\therefore \\quad F(x) \\sim N\\left(f(x), \\sigma^{2}\\right)\n\\end{aligned}\n$$ Consider any two different individuals $C$ and $y$ from the population, $$\nF(x)-F(x)-N\\left(f(x)-f(y), 2 \\sigma^{2}\\right)\n$$ Then $$\np(F(x)-F(y) \\geq 0)=\\Phi\\left(\\frac{f(x)-f(y)}{\\sqrt{2} \\sigma}\\right)\n$$ Where $\\Phi(t)$ is the standard normal cdf. $$\n\\Phi(t)=\\int_{-\\infty}^{t} \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{t^{2}}{2}} d t\n$$ Pop $^{r}(x, t)$ can be calculated as follows: $$\n\\begin{aligned}\n& \\operatorname{Pop}^{r}(x, t)=2 p(x, t) \\int_{F(x) \\in F(x)} p(y, t) d y \\\\\n& \\quad=2 p(x, t)\\left(\\int_{D} p(y, x) \\Phi\\left(\\frac{f(x)-f(y)}{\\sqrt{2 \\sigma}}\\right) d y\\right)\n\\end{aligned}\n$$ According to (8), we can model the tournament selection as: $$\np(x, t)=2 p(x, t)\\left(\\int_{D} p(y, x) \\Phi\\left(\\frac{f(x)-f(y)}{\\sqrt{2 \\sigma}}\\right) d y\\right)\n$$ ### 3.3.",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:04",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How is the probability model $P(g)$ used to generate new solutions in the described Estimation of Distribution Algorithm (EDA)?",
      "topics": [
        "scheduling"
      ],
      "answer": "The probability model $P(g)$, a matrix where element $\\rho_{\\vec{w}}(g)$ denotes the probability that batch $b_{j}$ appears in the [FORMULA160] th position of the batch priority string at the $g$ th generation, is sampled to generate a new batch priority string. If the position of batch [FORMULA165] has already been determined, the elements in the [FORMULA166] th column of the probability matrix [FORMULA167] are set to zero to avoid redundant sampling.",
      "chunk_id": 19382,
      "paper_id": "A cooperative coevolution algorithm for complex hybrid seru-system scheduling optimization",
      "paper_title": "A cooperative coevolution algorithm for complex hybrid seru-system scheduling optimization",
      "paper_year": "2021",
      "chunk_content": "Consider an example with 7 batches and 2 serus, the related data are given in Table 1. An encoded partial solution-SS is illustrated in Fig. 8, while a decoded hybrid seru scheduling is illustrated in Fig. 9. ## Probability model and updating mechanism A probability model is built to reflect the distribution of the solution space and produce new solutions. The probability model should keep updating by a reasonable mechanism to achieve exploration [39]. In this paper, the probability model and its updating mechanism are designed as follows. According to the above encoding method, batch priority string directly affects the seru scheduling. Therefore, the following probability model $P(g)$ is designed to sample the batch priority string, where element $\\rho_{\\vec{w}}(g)$ denotes the probability that batch $b_{j}$ appears in the [FORMULA160] th position of the batch priority string at the $g$ th generation. The elements in matrix $P(0)$ are initialized as $\\rho_{\\vec{w}}(0)=1 / M(\\forall i, j)$ for uniform sampling. $$\nP(g)=\\left[\\begin{array}{ccc}\n\\rho_{11}(g) & \\rho_{12}(g) & \\ldots & \\rho_{1 M}(g) \\\\\n\\rho_{21}(g) & \\rho_{22}(g) & \\ldots & \\rho_{2 M}(g) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\rho_{M 1}(g) & \\rho_{M 2}(g) & \\ldots & \\rho_{M M}(g)\n\\end{array}\\right]\n$$ At each generation, a new batch priority string is generated by sampling the probability matrix to determine the position of each batch in the batch priority string. If the position of batch [FORMULA165] has already been determined, the whole elements in [FORMULA166] th column of probability matrix [FORMULA167] are set as zero.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:06",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model for side chain placement, based on the description provided?",
      "topics": [
        "evolutionary algorithms",
        "EDAs",
        "side chain placement",
        "rotamers",
        "probability model"
      ],
      "answer": "The EDA model for side chain placement involves these key steps: First, initialize a population of solutions, where each solution represents a configuration of rotamer angles. Second, select a subset of the best solutions from the current population. Third, construct an explicit probability model based on the selected solutions. This model captures the statistical dependencies among the rotamer angles. Fourth, sample new solutions from the constructed probability model. These new solutions form the next generation of the population. Finally, repeat steps two through four until a satisfactory solution is found or a termination criterion is met.",
      "chunk_id": 13024,
      "paper_id": "Side chain placement using estimation of distribution algorithms",
      "paper_title": "Side chain placement using estimation of distribution algorithms",
      "paper_year": "2007",
      "chunk_content": "The problem is important not only for homology modeling but also for protein design [13], where the goal is to find a protein able to fulfil a given function or to satisfy a number of structural features. A way to address the problem is to constrain the search to the discrete space by means of discrete configurations of the angles, known as rotamers $[5,18]$. The inclusion of these discrete configurations implies an important problem reduction. Nevertheless, the problem remains exponential. Therefore, the conception of efficient search procedures arises as an important research problem. Deterministic and stochastic methods have been proposed to cope with the side chain placement problem. In this paper, we introduce a stochastic optimization algorithm for the solution of this problem. This algorithm, which is based on the use of probability distributions, belongs to the family of estimation of distribution algorithms (EDAs) [19,20]. EDAs are evolutionary algorithms. They resemble genetic algorithms (GAs) [21,22] in the use of populations, but instead of employing genetic operators, they construct, at each generation, an explicit probability model of a set of selected solutions, and use this model to sample new solutions. The paper is organized as follows. In the next section, the biological basis of the side chain placement problem is reviewed.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:08",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metric is defined using the formula that includes the Gamma function ($\\Gamma(x)$) for evaluating Bayesian networks learned by an Estimation of Distribution Algorithm?",
      "topics": [
        "optimization"
      ],
      "answer": "The BDe metric is defined using the formula that includes the Gamma function. Specifically, the formula involves calculating the product over all combinations of parents of a variable and all possible values of that variable, using parameters such as $N_{m j k}$, which represents the number of observations, and $\\alpha_{m j k}$, which represents Dirichlet distribution parameters.",
      "chunk_id": 19147,
      "paper_id": "Analysis of Bayesian Network Learning Techniques for a Hybrid Multi-objective Bayesian Estimation of Distribution Algorithm a case study on MNK Landscape",
      "paper_title": "Analysis of Bayesian Network Learning Techniques for a Hybrid Multi-objective Bayesian Estimation of Distribution Algorithm: a case study on MNK Landscape",
      "paper_year": "2021",
      "chunk_content": "It assumes the Dirichlet distribution (with parameters $\\alpha_{m j k}$ ) as prior probability and a uniform prior distribution of all possible structures (Larranaga et al. 2012). The BDe metric is defined by Eq. 3: $$\np(B \\mid P o p)=\\prod_{m=1}^{M} \\prod_{j=1}^{t_{m}} \\frac{\\Gamma\\left(\\alpha_{m j}\\right)}{\\Gamma\\left(\\alpha_{m j}+N_{m j}\\right)} \\prod_{k=1}^{s_{m}} \\frac{\\Gamma\\left(\\alpha_{m j k}+N_{m j k}\\right)}{\\Gamma\\left(\\alpha_{m j k}\\right)}\n$$ where $N_{m j k}$ is the number of observations in the data set $P o p$ for which $\\left.z_{R}(\\mathbf{x})\\right): \\mathcal{B}^{N} \\rightarrow \\mathcal{R}^{R}$ assumes the $\\mathbf{x}$-th value given the $z_{r}(\\mathbf{x})$-th combination of values from its parents, with $\\mathbf{N}_{m j}=$ $\\left\\{N_{m j 1}, \\ldots, N_{m j s_{m}}\\right\\}, \\Gamma(x)=(x-1)$ ! and $\\alpha_{m j k}=\\sum_{k=1}^{s_{m}} \\alpha_{m j k}$. The product over $j \\in\\left\\{1, \\ldots, t_{m}\\right\\}$ runs for all combinations of parents of $$\n\\begin{aligned}\n& \\max _{\\mathbf{x}} \\mathbf{z}(\\mathbf{x})=\\left(z_{1}(\\mathbf{x}), \\ldots, z_{R}(\\mathbf{x})\\right) \\\\\n& \\text { subject to } \\mathbf{x} \\in\\{0,1\\}^{N} \\\\\n& \\text { with } \\\\\n& z_{r}(\\mathbf{x})=\\frac{1}{N} \\sum_{q=1}^{N} f_{r, q}\\left(x_{q}, \\Pi_{r}\\left(x_{q}\\right)\\right) \\\\\n& r \\in\\{1, \\ldots R\\} \\\\\n& q \\in\\{1, \\ldots N\\}\n\\end{aligned}\n$$ and the product over $k \\in\\left\\{1, \\ldots, s_{m}\\right\\}$ runs for all possible values of $x_{q}$. The Dirichlet parameter $[0,1]$ stands for prior information about the number of instances that have $r=1$ set to its $N=6$-th value and the set of parents of $K_{1}=2$ is instantiated to its $(r=1)$-th combination. In the so-called K2 metric (Cooper and Herskovits 1992) for instance, parameters $r=$ are set to 1 as there is no prior information about the problem, and Eq.",
      "question_type": "definition",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:09",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of introducing the state parameter $\\Omega$ into the two-machine-one-buffer system, and how does it affect the calculation of $C_{\\mathrm{pm} 2}(i)$ in the context of modeling production line failures?",
      "topics": [
        "proof"
      ],
      "answer": "The state parameter $\\Omega$ is introduced to represent the 'failure connection' between subsystems in a production line, specifically within a two-machine-one-buffer system. When $\\Omega_{i} = 1$, it signifies that a failure connection exists, causing the equivalent upstream machine to be starving. This affects the calculation of $C_{\\mathrm{pm} 2}(i)$, which represents a component related to the machine's capacity, by modifying it based on the Mean Time To Repair (MTTR), Mean Time Between Failures (MTBF), and $C_{\\mathrm{ipm}}(\\delta_{i})$, the ideal processing machine capacity. The probability of transitioning to a failure connection state, $p_{i}^{\\Omega} u_{i}^{u}$, depends on the machine's processing rate $u_{i}^{u}$ and the probability of failure $p_{i}^{\\Omega}$ after processing a work-in-progress (WIP).",
      "chunk_id": 7443,
      "paper_id": "Performance evaluation and optimization model for closed loop production lines considering preventive",
      "paper_title": "Performance evaluation and optimization model for closed-loop production lines considering preventive maintenance and rework process",
      "paper_year": "2023",
      "chunk_content": "Definition 2: A production line is decomposed into a set of subsystems consisting of a two-machine-onebuffer system. Between each subsystem, \"failure connection\" exists due to the flow of WIPs. The state of the upstream machine can thus affect the processing of the adjacent downstream machine. Introduce additional state parameter $\\Omega$ into the two-machine-one-buffer system, the system state parameter is modified to $\\mathrm{SYS}_{i}=\\left(x_{i}, \\delta_{i}^{\\mu}, \\delta_{i}^{d}, \\Omega_{i}\\right)$. When $\\Omega_{i}=0$, the equivalent upstream machine $M_{i}^{\\mu}$ operates normally, with no failure connection. When $\\Omega_{i}=1$, failure connection exists between the equivalent upstream machine $C_{\\mathrm{pm} 2}(i)$ and its upstream machine, making $$\n\\begin{aligned}\n& C_{\\mathrm{pm} 2}(i)=\\frac{T}{\\mathrm{MTTR}_{i}+\\mathrm{MTBF}_{i}} C_{\\mathrm{ipm}}\\left(\\delta_{i}\\right) \\\\\n& =\\frac{T}{\\left(1 / m_{i}\\right)+\\sum_{\\delta_{i}=0}^{\\delta_{i m}-1} 1 / p_{i, \\delta_{i}}} C_{\\mathrm{ipm}}\\left(\\delta_{i}\\right)\n\\end{aligned}\n$$ down, that is the original machine represented by the equivalent upstream machine is starving. Theorem 2: The production status of the two-machine-one-buffer system depends on its \"failure [IMAGE_2] Figure 3. An example of state transitions in the two-machine-one-buffer system. connection\" state with the upstream two-machine-onebuffer system. $$\nu_{n}(t)= \\begin{cases}f\\left(u_{i}^{u}, n_{i}^{u}, d r_{i, \\delta_{i}}^{u}, r_{i}^{u}\\right) & \\Omega_{i-1}=0 \\\\ f^{\\prime}\\left(u_{i}^{u}, n_{i}^{u}, d r_{i, \\delta_{i}}^{u}, r_{i}^{u}\\right) & \\Omega_{i-1}=1\\end{cases}\n$$ Proof: For the upstream machine $M_{i}(i=j$, the failure connection of $j+1 \\ldots k$ will occur only when the machine finishes processing a WIP. Therefore, when $C_{\\mathrm{cm} 2}(i)=0$ passes a WIP to the buffer $C_{p m 2}$, a failure connection will occur with a probability $p_{i}^{\\Omega}$, which is the probability of the state transferring from $$\n\\begin{aligned}\n& C_{p m 2}=\\sum_{i=j}^{k} C_{p m 2}(i) \\\\\n& C_{c m 2}=\\sum_{i=j}^{k} C_{c m 2}(i)\n\\end{aligned}\n$$ to $C_{r}$ : $$\n\\operatorname{Prob}\\left(\\Omega_{i}(t+\\Delta t)=1 \\mid \\Omega_{i}(t)=0, \\delta_{i}^{u} \\leq n_{i}^{u}\\right)=p_{i}^{\\Omega} u_{i}^{u} \\Delta t\n$$ where $p_{i}^{\\Omega} u_{i}^{u}$ refers to the probability of a failure connection after the machine $C_{p}$ finishes processing a WIP.",
      "question_type": "synthesis summary",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:11",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of the PMBGNP-AC algorithm, how does the algorithmic mechanism of incorporating the scalar reinforcement signal, denoted as \\(\\theta_{t}\\), into the update rules for the Gaussian distribution's mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) influence the exploration of the search space, and what is the theoretical basis for using the future state-values, i.e., \\(V(s_{t+1})\\), in formulating this signal?",
      "topics": [
        "algorithm"
      ],
      "answer": "The PMBGNP-AC algorithm uses a scalar reinforcement signal \\(\\theta_{t}\\) to adjust the mean \\(\\mu\\) and standard deviation \\(\\sigma\\) of the Gaussian distribution, which models the univariate interactions. The update rules are \\(\\mu \\leftarrow \\mu + \\alpha_{\\mu} \\nabla(\\mu ; x) \\theta_{t}\\) and \\(\\sigma \\leftarrow \\sigma + \\alpha_{\\sigma} \\nabla(\\sigma ; x) \\theta_{t}\\), where \\(\\alpha_{\\mu}\\) and \\(\\alpha_{\\sigma}\\) are learning rates, and \\(\\nabla(\\mu ; x)\\) and \\(\\nabla(\\sigma ; x)\\) represent the gradients with respect to \\(\\mu\\) and \\(\\sigma\\), respectively. The reinforcement signal \\(\\theta_{t}\\) is defined as -1 if \\(\\delta_{t} < 0\\), 0 if \\(\\delta_{t} = 0\\), and 1 if \\(\\delta_{t} > 0\\), where \\(\\delta_{t}\\) represents a performance difference. By formulating \\(\\theta_{t}\\) considering the future state-values, \\(V(s_{t+1})\\), PMBGNP-AC implicitly models multivariate interactions, thereby guiding the search process based on expected future rewards and allowing the algorithm to adapt the Gaussian distribution to promising regions of the search space. The final probability of generating an individual \\(n\\) is computed as \\(P(n)=\\prod_{i \\in N_{\\text {node }}}\\left[P_{\\mathrm{cv}}^{i}\\left(x_{i} ; \\mu_{i}, \\sigma_{i}\\right) \\prod_{b(i) \\in B(i)} P_{\\mathrm{nc}}(b(i), j)\\right]\\), where \\(N_{\\text {node }}\\) is the set of suffixes of nodes, \\(P_{\\mathrm{cv}}^{i}\\) is the probability of continuous variables, and \\(P_{\\mathrm{nc}}\\) is the probability of node connections.",
      "chunk_id": 10455,
      "paper_id": "A continuous estimation of distribution algorithm by evolving graph structures using reinforcement learning",
      "paper_title": "A Continuous Estimation of Distribution Algorithm by Evolving Graph Structures Using Reinforcement Learning",
      "paper_year": "2012",
      "chunk_content": "(6), (7) and (12), respectively; end for Calculate \\(P{\\mathrm{nc}}\\) according to Eq. (2); Generate \\(P(t+1)\\) by sampling \\(P{\\mathrm{nc}}\\) and \\(P_{\\mathrm{cv}}\\); \\(t \\leftarrow t+1\\); Go back to step 2 until the terminal criteria is met. ``` be strengthened, and vice-versa. Accordingly, we formulate a scalar reinforcement signal $\\theta_{t}$ to indicate whether the tendency to select this action should be strengthened or weakened. $$\n\\theta_{t}=\\left\\{\\begin{aligned}\n-1, & \\text { for } \\delta_{t}<0 \\\\\n0, & \\text { for } \\delta_{t}=0 \\\\\n1, & \\text { for } \\delta_{t}>0\n\\end{aligned}\\right.\n$$ Inserting this scalar reinforcement signal into Eq. (8) and (9), we get the final updating rules of the Gaussian distribution of PMBGNP-AC as follows: $$\n\\begin{aligned}\n& \\mu \\leftarrow \\mu+\\alpha_{\\mu} \\nabla(\\mu ; x) \\theta_{t} \\\\\n& \\sigma \\leftarrow \\sigma+\\alpha_{\\sigma} \\nabla(\\sigma ; x) \\theta_{t}\n\\end{aligned}\n$$ We can observe that by inserting AC, we can not only model the univariate interactions by Gaussian distribution explicitly, but also model the multivariate interactions implicitly, since the reinforcement signal [FORMULA139] is formulated by considering the future state-values, i.e., $V\\left(s_{t+1}\\right)$. ## C. Algorithm of PMBGNP-AC The algorithm of PMBGNP-AC is a combination of the probabilistic model $P_{\\mathrm{nc}}$ of node connections and $P_{\\mathrm{cv}}$ of continuous variables of each node, as shown in Algorithm 1. As a result, the final probability of generating individual [FORMULA143] by PMBGNP-AC is: $$\nP(n)=\\prod_{i \\in N_{\\text {node }}}\\left[P_{\\mathrm{cv}}^{i}\\left(x_{i} ; \\mu_{i}, \\sigma_{i}\\right) \\prod_{b(i) \\in B(i)} P_{\\mathrm{nc}}(b(i), j)\\right]\n$$ where, $N_{\\text {node }}$ : set of suffixes of nodes in one individual.",
      "question_type": "process",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:14",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm like BEDA, how should practitioners approach the initial population generation and subsequent search in the solution space?",
      "topics": [
        "EDAs"
      ],
      "answer": "For BEDA, the initial population is generated using a stochastic method. The search in the solution space is then executed through sampling and updating the probabilistic model of BEDA.",
      "chunk_id": 2485,
      "paper_id": "A Bayesian Statistical Inference-Based Estimation of Distribution Algorithm for the Re-entrant Job-Shop Scheduling Problem with Sequence-Dependent Setup Times",
      "paper_title": "A Bayesian Statistical Inference-Based Estimation of Distribution Algorithm for the Re-entrant Job-Shop Scheduling Problem with Sequence-Dependent Setup Times",
      "paper_year": "2014",
      "chunk_content": "In BEDA, the initial population was generated by using stochastic method, and the search in the solution space was executed through sampling and updating the probabilistic model of BEDA. Simulation results and comparisons based on a set of instances showed the effectiveness of the proposed BEDA. To the best of our knowledge, this is the first paper on the application of estimation of distribution algorithm (EDA) for the RJSSPST. Our future work is to develop some BEDA-based algorithms to deal with re-entrant no-wait job-shop scheduling problem. Acknowledgments. This research was partially supported by National Science Foundation of China (No. 60904081, 71101139), 2012 Academic and Technical Leader Candidate Project for Young and Middle-Aged Persons of Yunnan Province (No. 2012HB011), and Discipline Construction Team Project of Kunming University of Science and Technology (No. 14078212).",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:16",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Univariate Marginal Distribution Algorithm (UMDA) described for solving an MSG problem, how is the initial population generated, and what is the mathematical representation of the joint probability distribution used at the start of the algorithm?",
      "topics": [
        "optimization",
        "Estimation of Distribution Algorithms",
        "UMDA",
        "probability distribution"
      ],
      "answer": "The UMDA starts by randomly generating an initial population of PopSize individuals. Each individual is encoded as a binary string of length |V1| bits. The joint probability distribution, p0, is factorized as a product of univariate marginal probability distributions, represented as p0 = Π(from i=1 to |V1|). Each of these univariate marginal probability distributions follows a Bernoulli distribution with a parameter value of 0.5, such that p0(Xi) = 0.5 for i = 1, 2, ..., |V1|.",
      "chunk_id": 12587,
      "paper_id": "A hybrid estimation of distribution algorithm for the Minimal Switching Graph problem",
      "paper_title": "A hybrid estimation of distribution algorithm for the Minimal Switching Graph problem",
      "paper_year": "2006",
      "chunk_content": "get the corresponding matrix of $x^{n}, M^{n}$; ii. use $M^{n}$ to calculate the total number of desirable vertices in $x^{n}, d_{x_{n}}$; iii. if $d_{x_{n}}>d_{x}$, then A. $x=x^{n}$ B. $d_{x}=d_{x_{n}}$; C. improved [FORMULA90] true; D. go to (a) ; 5. output the corresponding matrix of $x$. The hill-climbing algorithm is a local optimization algorithm. It can exploit the information about the current search point effectively. However, the search may be trapped at a local optimum. ## 5. The Univariate Marginal Distribution Algorithm Given an MSG problem [FORMULA92], the UMDA starts with randomly generating an initial population of PopSize individuals, each of which is encoded in a binary string of $\\left|V_{1}\\right|$ bits, by sampling the probability distribution: $p_{0}=\\prod_{i=1}^{V_{1}}$, where $p_{0}\\left(X_{i}\\right)=0.5$ for $i=1,2, \\cdots,\\left|V_{1}\\right|$. Thus, the joint probability distribution is factorized as a product of [FORMULA97] univariate marginal probability distributions, each of which follows Bernouilli distribution with parameter value equaling to 0.5 . Once the initial population is created, the UMDA starts evolving the population for Gen generations.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:18",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the robustness evaluation method using expected average weighted tardiness in SJSSP compare to the Pareto optimization method for evaluating robustness under processing time disturbance, as described for dual-resource constrained stochastic job shop scheduling problems?",
      "topics": [
        "search"
      ],
      "answer": "The graph theory-based decomposition method employing expected average weighted tardiness as a robustness measure provides a performance expectation under disturbance but fails to quantify the disturbance's influence degree. In contrast, the Pareto optimization method evaluates schedule performance under an initial scenario and robustness under processing time disturbance, generating a Pareto solution set balancing both performance and robustness, thereby offering a more comprehensive evaluation than solely relying on expected performance.",
      "chunk_id": 17294,
      "paper_id": "A two-stage assignment strategy for the robust scheduling of dual-resource constrained stochastic job shop scheduling problems",
      "paper_title": "A two-stage assignment strategy for the robust scheduling of dual-resource constrained stochastic job shop scheduling problems",
      "paper_year": "2019",
      "chunk_content": "In current research, the manufacturing systems constrained by both machines and heterogeneous workers are referred to as dual-resource constrained systems. Li et. al (2016) proposed a metaheuristic algorithm named Branch Population Genetic Algorithm to minimize the makespan and cost. Mencia et al. (2013) proposed a genetic algorithm to minimize the makespan of Job Shop Scheduling Problems (JSSP) with operator constraints. However, the dual-resource constrained scheduling problems with deterministic or finite compressible processing times do not consider the constraints of processing time randomness and the proficiency difference of workers in the job shop manufacturing environment. Robustness evaluation is the basis for the implementation of robust scheduling methods. Wu et al. (1999) proposed a graph theory-based decomposition method to achieve scheduling robustness and adopted the expected average weighted tardiness as a robustness measure for SJSSP. However, using the expected performance as the robustness measure only gives a performance expectation under the disturbance, it cannot provide an evaluation value of the influence degree of the random disturbance on the performance. When an initial scenario of the processing times is given, the schedule performance under the initial scenario and the robustness under the influence of processing time disturbance can both be evaluated. Thereafter, a Pareto optimization method can be employed to obtain a Pareto solution set with both performance and robustness.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:19",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What programming considerations should be taken into account when implementing an Estimation of Distribution Algorithm (EDA) for linear Genetic Programming (GP) to explicitly control program length?",
      "topics": [
        "control"
      ],
      "answer": "When implementing an EDA for linear GP, one should explicitly represent the program length distribution. This allows the user to limit the search to programs of manageable size without introducing bias, unlike tree-based representations where program length is controlled by the frequency of drawing terminals or by artificially enforced limits on program depth or length.",
      "chunk_id": 11878,
      "paper_id": "A linear estimation-of-distribution GP system",
      "paper_title": "A linear estimation-of-distribution GP system",
      "paper_year": "2008",
      "chunk_content": "The algorithm learns and samples the joint probability of 3-grams of instructions at the same time as it is learning and sampling a program length distribution. This work has several interesting features. Firstly, while several authors have extended EDAs to evolve computer programs, virtually all have done so for a tree representation. Here, for the first time, we explore the application of EDAs to a linear GP. The second distinctive feature of this work is that we explictly represent the program length distribution to be used during the search. With tree-based representations this is not used, since the primitive set always includes terminals, which, if drawn with sufficiently high frequency can terminate the construction of programs. A disadvantage of relying on terminal selection is that the probability of drawing terminals is totally under the control of evolution. Thus the user has no control over the size of the evolved programs, which, if unchecked, can easily become excessive. So, often hard limits on program depth or length must be artificially enforced, producing an undesirable bias. Here, by explicitly modelling the size distribution we instead have a natural way of limiting the search to programs of manageable size, without introducing any undesired bias. Finally, previous work has tended to use different probability distributions for different positions (or loci) in a tree, thereby expanding significantly the size of the parameter space in which the model lives.",
      "question_type": "implementation focused",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:21",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What statistical measures are proposed to quantify the performance of the EDA, specifically FSS-EBNA, in comparison to Genetic Algorithm (GA) approaches?",
      "topics": [
        "algorithm"
      ],
      "answer": "The $5 \\times 2 \\mathrm{cv} F$-test is used to determine the statistical significance of differences in the stop-generation among FSS-EBNA and GA approaches. A statistically significant difference over FSS-EBNA at the 0.05 confidence level is denoted by '[FORMULA120]', and significance at the 0.1 confidence level is denoted by '[FORMULA121]'. The stop-generation indicates when the algorithms stop based on the explained stopping criteria.",
      "chunk_id": 12412,
      "paper_id": "Feature subset selection by Bayesian networks  a comparison with genetic and sequential algorithms",
      "paper_title": "Feature subset selection by Bayesian networks:: a comparison with genetic and sequential algorithms",
      "paper_year": "2001",
      "chunk_content": "Table 4 reflects in which generation GA approaches and FSS-EBNA stop using the explained stopping criteria. The $5 \\times 2 \\mathrm{cv} F$-test is performed to determine the significance degree of the differences in the stopgeneration among each GA approach and FSS-EBNA. Thus, in Table 4 the symbol ' [FORMULA120] ' denotes a statistically significant difference over FSS-EBNA at the 0.05 confidence level; ' [FORMULA121] ', significance at the 0.1 confidence level. The results show that FSS-EBNA arrives faster to similar or better predictive accuracies (see also Table 2) than both GA approaches. GA-o and GAu show significant differences in the stop-generation respect to FSS-EBNA in all except one dataset. FSS-EBNA, by the use of BNs, captures the underlying structure of the problem faster than GAs, avoiding the disruption, as GAs, of the relationships among dependent variables. Only in the Image dataset, the domain of lowest dimensionality, the use of FSS-EBNA does not return an advantage respect to other algorithms: observing the accuracy and stopgeneration results in this domain, all FSS algorithms demonstrate a similar behaviour, finding similarly fitted subsets and population-based approaches needing nearly the same number of generations to stop.",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:22",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of the specific EDA discussed regarding spurious linkages and its effect on population sizing?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The crucial finding is that while spurious dependences might initially seem to have a limited impact on population size in Estimation of Distribution Algorithms, their effects become substantial when niching techniques are incorporated, as observed in the empirical analysis using onemax.",
      "chunk_id": 24006,
      "paper_id": "An introduction and survey of estimation of distribution algorithms",
      "paper_title": "An introduction and survey of estimation of distribution algorithms",
      "paper_year": "2011",
      "chunk_content": "It was also discovered that in order for the algorithm to reach a solution, the probability of an optimal solution must always exceed a certain threshold. Finally, the effects of spurious linkages on EDA performance were examined by Radetic and Pelikan [164]. The authors started by proposing a theoretical model to describe the effects of spurious (unnecessary) dependences on the population sizing of EDAs. This model was then tested empirically on onemax and the results showed that while it would be expected that spurious dependences would have little effect on population size, when niching was included the effects were substantial. ## 8. Additional information This section provides pointers to additional sources of information on EDAs. ### 8.1. Software The following list contains some free EDA implementations available online: - Implementations of BOA, hBOA and dtEDA: http://medal.cs.umsl.edu/software.php - Implementation of ECGA in Matlab and C++: http://illigal.org/category/source-code/ - Implementations of sequential and parallel mixed BOA and adaptive mixed BOA: http://jiri.ocenasek.com/\\#Download - Matlab toolbox for EDAs and several implementations: http://www.sc.ehu.es/ccwbayes/members/rsantana/software/ Software.html - Implementation of the Adapted Maximum-Likelihood Gaussian Model Iterated Density Estimation Evolutionary Algorithm (AMal.GaM-IDEA): http://homepages.cwi.nl/ bosman/source_code.php - Source code for RM-MEDA and several other EDAs and test problems: http://csswww.essex.ac.uk/staff/qzhang/mypublication.htm - Implementations of Real-coded BOA and multi-objective Realcoded BOA: http://www.evolution.re.kr/ ### 8.2.",
      "question_type": "evaluation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:24",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the crossover operator in NMBDE utilize both the target individual $px_{i}^{t}$ and the mutant individual $pu_{i}^{t+1}$ to produce the trial individual $pv_{i}$?",
      "topics": [
        "crossover"
      ],
      "answer": "The NMBDE algorithm uses a crossover operator to create a trial individual $pv_{i}$ by combining the target individual $px_{i}^{t}$ and its mutant individual $pu_{i}^{t+1}$. Specifically, for each component $j$ of the trial vector $pv_{ij}^{t+1}$, it takes the value from the mutant individual $pu_{ij}^{t+1}$ if either a random number $rand$ is less than or equal to the crossover rate $CR$, or if $j$ equals $r$ and $i$. Otherwise, $pv_{ij}^{t+1}$ takes the value from the target individual $px_{ij}^{t}$. This process is defined by the equation $pv_{i j}^{t+1}= \\begin{cases}pu_{i j}^{t+1}, & \\text { if }(r \\text { and } j \\leq C R) \\text { or }(j=r \\text { and } i) \\\\ px_{i j}^{t}, & \\text { otherwise }\\end{cases}$",
      "chunk_id": 10099,
      "paper_id": "A novel modified binary differential evolution algorithm and its applications",
      "paper_title": "A novel modified binary differential evolution algorithm and its applications",
      "paper_year": "2012",
      "chunk_content": "1. The operating of probability estimation operator in NMBDE. [IMAGE_1] Fig. 2. Flowchart of the NMBDE algorithm. individuals to construct the probability distribution model of the mutant individual to be bit \" 1 \". The bandwidth factor $p v_{i}=\\left(p v_{i, 1}, p v_{i, 2}, \\ldots, p v_{i, N}\\right)$ tunes the range and shape of the probability distribution model, and an appropriate $p v_{i}$ value can improve the search efficiency and maintain population diversity simultaneously. Then, the corresponding mutant individual $p u_{i}^{t+1}$ of the current target individual $p x_{i}^{t}$ is generated as Eq. (5) according to the probability estimation vector $r$, $p u_{i j}^{t+1}= \\begin{cases}1, & \\text { if } \\operatorname{rand}_{i} \\leq P\\left(p x_{i j}^{t+1}\\right) \\\\ 0, & \\text { otherwise }\\end{cases}$ where $\\operatorname{rand}_{i}^{t}$ ) is a random number; $P\\left(p x_{i j}^{t+1}\\right)$ is the $p u_{i j}^{r+1}$-th component of the probability vector of the $i$-th target individual. The mechanism of NMBDE to produce the mutant individual can be depicted in Fig. 1. ### 3.2. Crossover operator The crossover operator is used to produce the trail individual $p v_{i}=\\left(p v_{i, 1}, p v_{i, 2}, \\ldots, p v_{i, N}\\right)$ by mixing the target individual and its mutant individual. The trial vector $p v_{i}$ can be obtained according to the following equation: $p v_{i j}^{t+1}= \\begin{cases}p u_{i j}^{t+1}, & \\text { if }(r \\text { and } j \\leq C R) \\text { or }(j=r \\text { and } i) \\\\ p x_{i j}^{t}, & \\text { otherwise }\\end{cases}$ Table 1 Probability of being \" 1 \" for the mutant individual of NMBDE with different values of $f_{3}$ and $b=6$.",
      "question_type": "relationship",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:26",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm on Chimera instances, how should practitioners adjust the Tree-EDAr compared to Tree-EDA to improve computational time?",
      "topics": [
        "analysis"
      ],
      "answer": "To improve computational time when using Tree-EDAr compared to Tree-EDA on Chimera graph instances, practitioners should restrict the number of pair-wise dependencies to the edges of the Chimera graph. This restriction leads to important gains in computational time, although it does not necessarily improve the success rate.",
      "chunk_id": 25120,
      "paper_id": "Evolutionary Approaches to Optimization Problems in Chimera Topologies",
      "paper_title": "Evolutionary Approaches to Optimization Problems in Chimera Topologies",
      "paper_year": "2016",
      "chunk_content": "6. CONCLUSIONS In this paper we have investigated for the first time the behavior of EAs on problems defined of the Chimera instances used by D-Wave architectures. We have shown that a simple GA with one-point crossover is able to solve 991 of the 1000 instances considered although the success rate of the algorithm depends on the instances. Our results show that EAs that use probabilistic modeling of the solutions dot not produce an improvement over methods that do not incorporate any type of modeling. To some extent this was an unexpected result because different variants of EDAs had shown good results for Ising problems defined on other topologies. Using problem information does not provide improvements in terms of success rate, although by restricting the number pair-wise dependencies to the edges of the Chimera graph important gains in terms of computational time were achieved by Tree-EDAr over Tree-EDA. We have also identified a number of instance descriptors which are correlated with the behavior of the algorithms. This could serve as a first step for a more complete characterization of the impact that certain features of the Ising instances have in the performance of EAs. An analysis of the impact of the same features for other optimizers could help to understand how different methods explore the space of solutions to identify the optimum.",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:28",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the computational complexity of the specific DE/AEDA analyzed when determining the influence of the population size $\\Gamma$, the selection rate of SP population $r$, and the initial learning rate $\\alpha_{0}$ on the algorithm's performance?",
      "topics": [
        "design"
      ],
      "answer": "The computational complexity analysis of the DE/AEDA algorithm is implicitly addressed through a design of experiments (DOE) approach. Specifically, a medium-scale test case (60, 150) is selected, and an orthogonal array $L_{16}(4^3)$ is employed to explore the influence of the parameters $\\Gamma$, $r$, and $\\alpha_{0}$. Each factor ($\\Gamma$, $r$, $\\alpha_{0}$) consists of four levels. The average result obtained by the DE/AEDA across 20 independent runs for each test case is selected as the response variable (RV). While the text does not provide explicit complexity in Big O notation, the use of $L_{16}(4^3)$ indicates that 16 different parameter configurations are evaluated, and the algorithm is run 20 times for each configuration. Therefore, the total number of DE/AEDA executions is 320. This experimental approach aims to empirically assess the impact of parameter settings on the algorithm's performance, providing insights into the algorithm's behavior under different conditions, rather than providing a formal computational complexity analysis.",
      "chunk_id": 20360,
      "paper_id": "A self-adaptive estimation of distribution algorithm with differential evolution strategy for supermarket location problem",
      "paper_title": "A self-adaptive estimation of distribution algorithm with differential evolution strategy for supermarket location problem",
      "paper_year": "2020",
      "chunk_content": "Consequently, 24 test cases are constructed in total. The $y$-coordinates of each station and each candidate are set to $y_{s}=1, b_{g}=5$ respectively. While the supermarkets are randomly distributed among the stations, that is, $a_{g}=\\operatorname{rand}(1,6 \\cdot(|S|-1))$. And the distance between two consecutive stations is set to 6 , namely $x_{s+1}-x_{s}=6$, which guarantees the length of a car. The estimated demand for station $s$ per shift follows a uniform distribution $U(40,100)$. ### 4.2 Parameters setting The main parameters of the proposed DE/AEDA algorithm include the population size $\\Gamma$, the selection rate of SP population $r$ and the initial learning rate $\\alpha_{0}$. A mediumscale test case $(60,150)$, is selected to implement design of experiments (DOE) to explore the influence of these parameters on the performance of the DE/AEDA algorithm. Since each factor consists of four levels, the orthogonal array $L_{16}\\left(4^{3}\\right)$ is employed. The detailed configuration of these parameters and the orthogonal array are listed in Tables 2 and 3 respectively. For each test case, the DE/ AEDA runs 20 times independently. The average result obtained by the DE/AEDA is selected as response variable, denoted as RV.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:30",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How is the fitness of a solution $S$ determined in the context of the subproblem it is related to, according to the provided information?",
      "topics": [
        "fitness"
      ],
      "answer": "The fitness of a solution $S$ is calculated as fitness$(S)=\\max _{1 \\leq i \\leq m}\\left\\{\\lambda_{i}\\left|f_{i}(x)-z_{i}^{*}\\right|\\right\\}$, where $\\lambda_{i}$ represents a weight vector component, $f_{i}(x)$ is the objective value for objective $i$, and $z_{i}^{*}$ is the ideal point for objective $i$. A solution with a smaller fitness value is considered better.",
      "chunk_id": 16642,
      "paper_id": "MCEDA- A novel many-objective optimization approach based on model and clustering",
      "paper_title": "MCEDA: A novel many-objective optimization approach based on model and clustering",
      "paper_year": "2019",
      "chunk_content": "The initial population $P_{0}$, whose size is assigned with the number of the size of weights vector, are randomly generated and the ideal point $\\boldsymbol{z}^{*}$ are initialized. Then the normalization is applied to the population. For a solution $f_{j}$, the normalized objective value is defined as [21] $\\tilde{f}_{i}(\\boldsymbol{x})=\\frac{f_{i}(\\boldsymbol{x})-z_{i}^{*}}{z_{j}^{\\max }-z_{j}^{*}}, j=1,2, \\ldots, m$ Table 1 Mean and standard deviation values of IGD, obtained by MCEDA/B on DTLZ instances. The symbol \" + \" and \" - \" denote whether the IGD results of the algorithm with the compared parameter are better than or worse than that of the algorithms with the first parameter value according to the mean value and standard deviation. where $z_{i}^{\\max }$ is the maximum value for every objective $f_{j}$ in the population. The weight vectors are created recursively under the conditions as follows. $\\lambda^{j}=\\left(\\lambda_{1}^{j}, \\lambda_{2}^{j}, \\ldots, \\lambda_{m}^{j}\\right)^{T}, j=1,2, \\ldots, N$ $\\sum_{i=1}^{m} \\lambda_{i}^{j}=1, \\lambda_{i}^{j} \\in[0,1]$. Each weight vector $P_{t}$ is related to a subproblem and a solution $S$ which has the best fitness for the subproblem in terms of the solutions have been evaluated and compared. The fitness is represented as follows [12]. fitness $(S)=\\max _{1 \\leq i \\leq m}\\left\\{\\lambda_{i}\\left|f_{i}(x)-z_{i}^{*}\\right|\\right\\}$ The solution with the smaller fitness value is better.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:32",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To enhance the efficiency of EDAs, what are the two main computational bottlenecks that efficiency enhancement techniques should address, and what parallelization strategy is often advantageous in the case of EDAs?",
      "topics": [
        "optimization"
      ],
      "answer": "The two main computational bottlenecks that efficiency enhancement techniques for EDAs must address are fitness evaluation and model building. While parallelizing fitness evaluation is a common approach, it is often also advantageous to parallelize model building in EDAs. For instance, an efficient parallel implementation of the compact genetic algorithm has been successfully applied to noisy optimization problems.",
      "chunk_id": 23990,
      "paper_id": "An introduction and survey of estimation of distribution algorithms",
      "paper_title": "An introduction and survey of estimation of distribution algorithms",
      "paper_year": "2011",
      "chunk_content": "6. Efficiency enhancement techniques for EDAs While EDAs provide scalable solutions to many problems that are intractable with other techniques, solving enormously complex problems often necessitates that additional efficiency enhancement (EE) [19,17,42] techniques are used. There are two main computational bottlenecks that must be addressed by efficiency enhancement techniques for EDAs: (1) fitness evaluation and (2) model building. Efficiency enhancements for EDAs can be roughly divided into the following categories [42]: 1. Parallelization. 2. Evaluation relaxation. 3. Hybridization. 4. Time continuation. 5. Sporadic and incremental model building. 6. Incorporating problem-specific knowledge and learning from experience. In the remainder of this section we will briefly review each of these approaches, with an emphasis on efficiency enhancement techniques that are specific to EDAs. ### 6.1. Parallelization To enhance efficiency of any optimization technique, one may often parallelize the computation in some way. The most common approach to parallelization in EDAs and other metaheuristics is to parallelize fitness evaluation [118]. However, in the case of EDAs it is often also advantageous to parallelize model building. One of the most impressive results in parallelization of EDAs is the efficient parallel implementation of the compact genetic algorithm, which was successfully applied to a noisy optimization problem with over one billion decision variables [27].",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:33",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How is the frequency of controlling individuals in an Estimation of Distribution Algorithm (EDA) adjusted during the evolutionary process, and what factors determine which individuals are controlled in each generation?",
      "topics": [
        "search"
      ],
      "answer": "The frequency of controlling individuals in the EDA is high at the beginning of the search, when the approximate model is of poor quality, and decreases as the model becomes more reliable after a certain number of generations. The individuals controlled in each generation are determined by three factors: $\\alpha$, which is determined online by the performance of the model; $\\beta$, used jointly with $\\alpha$ to ensure a minimum number of individuals are controlled; and $\\eta$, which decreases with the evolutionary process and represents the ease with which individuals can be controlled.",
      "chunk_id": 26767,
      "paper_id": "Fitness approximation in estimation of distribution algorithms for feature selection",
      "paper_title": "Fitness approximation in estimation of distribution algorithms for feature selection",
      "paper_year": "2005",
      "chunk_content": "Due to the high dimensionality, ill distribution and limited number of training samples, it is very difficult to construct an approximate model that is globally correct. In fact, it is of more practical importance to build an approximate model that represents the promising individuals step by step. The individual control strategy described above can be viewed as a good filter for actively selecting the most informative individuals for model update. With no prior knowledge, the evolutionary process often starts with a population generated according to uniform distribution. So the approximate model is often of poor quality at the beginning of the search. With more promising individuals are found by exploration and exploitation, the model quality also improves as the evolutionary search proceeds. Thus, another general rule for model management is that the model should be updated with more controlled individuals at the beginning. After a certain number of generations, the model becomes much more reliable and the control frequency can reduce. The individuals that will be controlled in each generation are determined by three factors, $\\alpha, \\beta$ and $\\eta . \\alpha$ is determined online by the performance of the model. [FORMULA52] is used jointly with $\\alpha$ to ensure at least some individuals are controlled. Only [FORMULA54] can be controlled easily. We let it decreases with the evolutionary process.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:35",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "Based on the text, what is the main conclusion regarding the performance of the NMIEDA model compared to NMIEDA1 and NMIEDA2 in solving high-dimensional optimization problems?",
      "topics": [
        "convergence"
      ],
      "answer": "The NMIEDA model, utilizing a new updating and sampling mechanism, demonstrates superior performance in identifying high-quality solutions compared to NMIEDA1 and NMIEDA2, particularly on higher-order functions like Trap-5, Four Peaks, and Dec-3. This improved performance is attributed to enhanced optimization capability and avoidance of premature convergence, leading to greater convergence reliability in solving high-dimensional optimization problems.",
      "chunk_id": 18968,
      "paper_id": "NMIEDA- Estimation of distribution algorithm based on normalized mutual information",
      "paper_title": "NMIEDA: Estimation of distribution algorithm based on normalized mutual information",
      "paper_year": "2021",
      "chunk_content": "The convergence reliability results are shown in Figure 4. For convenience of comparison, the curve of NMIEDA1 in Figure 4(A) is moved vertically by +0.1 and the curve of NMIEDA2 in Figure 4(A) is moved vertically by +0.2 . According to Figure 4, NMIEDA and NMIEDA2 are able to identify high-quality solutions and outperform NMIEDA1 significantly on five problems. Moreover, NMIEDA outperforms NMIEDA2 for high-dimensional Trap-5, Four Peaks, and Dec-3 problems. According to the abovementioned analytical results, we conclude that the new updating mechanism plays a significant role in the performance of NMIEDA for higher-order functions, which improves the optimization capability of NMIEDA and avoids premature convergence. We also conclude that the new sampling mechanism significantly contributes to the convergence reliability of NMIEDA in solving high-dimensional optimization problems. [IMAGE_3] FIGURE 4 Convergence reliabilities of three algorithms # 3.3 | Comparisons of NMIEDA with four bivariate EDAs Next, the optimization results of NMIEDA are compared against four other bivariate EDAs: BMDA, MIMIC, COMIT, and SGMIEC.",
      "question_type": "application",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:36",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of EDA-PSO, as compared to PSEDA, contribute to the overall optimization process, given that both are mentioned as algorithms for comparison?",
      "topics": [
        "optimization"
      ],
      "answer": "The provided text mentions EDA-PSO and PSEDA as algorithms used for comparison with HMPSO in experimental evaluations. However, the text does not specify the algorithmic mechanisms or theoretical foundations of either EDA-PSO or PSEDA. Therefore, it is impossible to explain their specific contributions to the optimization process based solely on the provided text.",
      "chunk_id": 8710,
      "paper_id": "Composite Particle Swarm Optimizer With Historical Memory for Function Optimization",
      "paper_title": "Composite Particle Swarm Optimizer With Historical Memory for Function Optimization",
      "paper_year": "2015",
      "chunk_content": "11: end 12) It defines a new particle generation mechanism. The idea behind it is to generate a new particle through the competition among those generated based on historical memory, particles' current pbests, and the swarm's gbest. ## IV. EXPERIMENTAL EVALUATION Extensive simulations are carried out in order to compare HMPSO with CPSO-H6 [3], CLPSO [6], ALCPSO [7], FIPS [4], HPSO-TVAC [5], EDA-PSO [14], and PSEDA [15]. Further experimental evaluations with two HMPSO variants are then carried out to analyze and compare the performance of proposed HMPSO when it uses two different ways to build historical memory. All the mentioned algorithms are coded in MATLAB-R2010a and simulations are executed on a 2.4 GHz Xeon E5-2665 processor with 32 GB main memory running under Windows server 2008 environment. ## A. Benchmark Functions and Algorithm Configuration In order to study their performance deeply, 28 benchmark functions in CEC2013 [42] are used for the experimental tests here, which have been widely used for real-parameter optimization, as shown in Table I.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:38",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to improve an Extreme Learning Machine (EDA-ELM), how should practitioners leverage the EDA's probabilistic model building to enhance the ELM's performance in optimization problems?",
      "topics": [
        "search"
      ],
      "answer": "Practitioners should focus on using the EDA to explicitly build and sample probabilistic models of potential solutions, unlike individual evolutionary mechanism-based algorithms that implicitly describe such distributions. The EDA's strong global search capabilities and fast convergence speed should be exploited to solve optimization problems that are difficult for conventional evolutionary algorithms. This involves replacing or augmenting the ELM's existing optimization strategy with the EDA's probabilistic model-based approach to more effectively explore the solution space.",
      "chunk_id": 4311,
      "paper_id": "Drought prediction in the Yunnan-Guizhou Plateau of China by coupling the estimation of distribution algorithm and the extreme learning machine",
      "paper_title": "Drought prediction in the Yunnan-Guizhou Plateau of China by coupling the estimation of distribution algorithm and the extreme learning machine",
      "paper_year": "2022",
      "chunk_content": "2016; Zhu et al. 2020). Subsequently, the ELM model was improved by coupling with evolutionary algorithms to overcome the defects and has been successfully applied in various fields but seldom in drought prediction (Bui et al. 2019; Feng et al. 2019; Wu et al. 2020). Besides, the current optimization of the ELM model is mainly carried out by using individual evolutionary mechanism-based algorithms that implicitly describe the probability distribution of potential solutions. Different from the individual evolutionary mechanism-based algorithms, the estimation of distribution (EDA) algorithm (Muhlenbein et al. 1996; Larranaga and Lozano 2002) explores the optimal solution by building and sampling explicit probabilistic models of potential solutions. Its strong global search capabilities and fast convergence speed allow it to feasibly solve optimization problems, which are notoriously difficult to be handled by most conventional evolutionary algorithms (Hauschild and Pelikan 2011; Luo et al. 2015; Wang et al. 2020b). Therefore, it is necessary to improve the ELM model by coupling the EDA algorithm. Although several studies mentioned above confirmed the effectiveness and efficiency of the ELM in drought prediction, the applicability of improved ELM by coupling EDA algorithm (namely EDA-ELM model) for drought prediction is still to be investigated, particularly in the Yunnan-Guizhou Plateau, where few drought prediction models have been developed.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:39",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the combination of Evolutionary Programming (EP) and Estimation of Distribution Algorithm (EDA), how does EDA contribute to the convergence of EP, given that EP's convergence is described as relatively slow?",
      "topics": [
        "optimization"
      ],
      "answer": "The Estimation of Distribution Algorithm (EDA) uses the populations produced by Evolutionary Programming (EP) to estimate a probability distribution. This probability distribution is then used to predict the optimal individual, providing a search direction for the EP. This use of probability and statistics aims to accelerate the convergence of EP, which is otherwise considered relatively slow, particularly when using Gaussian variation within the classic Evolutionary Programming (CEP) for small neighboring search spaces.",
      "chunk_id": 13887,
      "paper_id": "Research and New Development of Application of the Estimation of Distribution Algorithms",
      "paper_title": "Research and New Development of Application of the Estimation of Distribution Algorithms",
      "paper_year": "2009",
      "chunk_content": "The new algorithm idea The intelligent mixture algorithm is the research hot spot of current intelligent optimization algorithm, may fuse many kinds of optimized algorithms' superiority, and enhances the algorithm the performance. Other intelligent method and Estimate of Distribution Algorithm's mixture will enhance search ability of the algorithm, better balanced astringency of algorithm and community multiplicity. I proposed the combination between Evolution Programming and Estimate of Distribution Algorithm. We also note that the optimization of the large data, there are many local optimum point, when the EP adopted to control the division nearest neighbor (that is, in the nearest neighbor division, the introduction of control factor, change the distance metric weights to achieve different division results. Plot out control factor by the Evolution Programming, then the coding space is relatively small.), its speed of convergence relatively slow. Based on probability and statistics idea of the EDA is a better method to make up. EDA can use the EP in the evolutionary process to produce populations , then use the probability distribution estimated to predict the optimal individual, provide search direction for the EP and accelerate their convergence. Evolutionary Programming (EP) has evolutionary trend, but its lack is that convergence is relatively slow. the classic Evolutionary Programming (CEP) using Gaussian variation, Gaussian variation suitable for the small neighboring search Space, that is, Gaussian mutation operator to provide a relatively small search step.",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:40",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of the [FORMULA200] algorithm in Case 2 compare to its performance in Case 4, based on the Sum of Squared Errors (SSE), Sum of Absolute Errors (SAE), and Mean Absolute Error (MAE) objective functions?",
      "topics": [
        "evolutionary"
      ],
      "answer": "In Case 2, the [FORMULA200] algorithm achieves minimum objective function values of SSE=5.72271, SAE=5.97704, and MAE=0.24353. In Case 4, the minimum objective function values are SSE=1.45053, SAE=3.07005, and MAE=0.16672. The SSE value in Case 4 is lower than in Case 2, suggesting that the results of the estimates can reproduce different operating conditions and with comparable values of the objective function with respect to the results of the parameter estimation cases.",
      "chunk_id": 19639,
      "paper_id": "Robust parameter estimation of a PEMFC via optimization based on probabilistic model building",
      "paper_title": "Robust parameter estimation of a PEMFC via optimization based on probabilistic model building",
      "paper_year": "2021",
      "chunk_content": "For this case, the minimum values of each objective function using by the [FORMULA200] were $S S E=5.72271, S A E=5.97704$ and $M A E=0.24353$, respectively, see Fig. 3-(a). In the same vein, if the estimated parameters for Case 2 are correct, they must reproduce also the polarization curve of Case 4, thus, Fig. 3-(b), shows that the estimations are correct and the respective minimum values of objective functions were $S S E=1.45053, S A E=3.07005$ and $M A E=0.16672$. We can observe that the SSE values of the validation cases are consistent with the SSE values of the estimation cases, this indicates that the results of the estimates can reproduce different operating conditions and with comparable values of the objective function with respect to the results of the parameter estimation cases. ### 4.5. Robustness and statistical analysis of 30 independent executions Bio-inspired algorithms such as evolutionary and swarm-intelligence algorithms are stochastic methods that delivers a different solution each time they are executed. Hence, quite often, they are compared using statistical measures and hypothesis tests, using a series of independent executions. The previous comparisons show the competitiveness of this proposal with reported solutions in the specialized literature. These comparisons are made using the best solutions from each algorithm.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:42",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To enhance the performance of the Estimation of Distribution Algorithm (EDA) described, the text mentions hybridization with local search metaheuristics. What programming considerations must be taken into account when integrating multipoint hill climbing (HC), multipoint simulated annealing (SA), and multipoint evolutionary gradient search (EGS) with the EDA?",
      "topics": [
        "search",
        "EDA",
        "local search",
        "optimization"
      ],
      "answer": "Integrating local search metaheuristics like multipoint hill climbing (HC), multipoint simulated annealing (SA), and multipoint evolutionary gradient search (EGS) with an EDA requires careful consideration of several programming aspects. First, the data structures used to represent the candidate solutions must be compatible between the EDA and the local search algorithms. This may involve creating a shared representation or implementing conversion routines. Second, the termination criteria for both the EDA and the local search algorithms must be clearly defined to avoid infinite loops or premature termination. Third, the frequency and intensity of local search application should be tunable parameters, allowing developers to experiment with different hybridization strategies. Finally, the implementation should allow for easy swapping of different local search metaheuristics to compare their effectiveness within the hybrid EDA framework.",
      "chunk_id": 11119,
      "paper_id": "A Hybrid Estimation of Distribution Algorithm with Decomposition for Solving the Multiobjective Multiple Traveling Salesman Problem",
      "paper_title": "A Hybrid Estimation of Distribution Algorithm with Decomposition for Solving the Multiobjective Multiple Traveling Salesman Problem",
      "paper_year": "2012",
      "chunk_content": "The performance of the algorithm is, then, enhanced by hybridizing EDA with local search. Three local search metaheuristics, including multipoint hill climbing (HC), multipoint simulated annealing (SA), and multipoint evolutionary gradient search (EGS), are explored in this paper. A new formulation of the objective functions for the mTSP is proposed and extended to the multiobjective framework. The proposed algorithms are, then, used to solve the formulated problem and simulation studies are carried out on various instances of the problems with different number of objective functions, salesmen, and cities. The proposed algorithms are, then, rigorously compared with several state-of-the-art evolutionary multiobjective optimizers. The remaining parts of this paper are organized as follows. The following section presents a literature review on the application of EC in mTSP, as well as a brief introduction of EDA and the local search metaheuristics that will be studied in this paper. Section III describes the problem formulation of the MmTSP, while the proposed algorithm is highlighted in Section IV. Section V presents the experimental results and discussions. Conclusions are drawn in Section VI. ## II. BACKGROUND INFORMATION In this section, a literature review, focusing on the application of evolutionary approaches to mTSP, is provided.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:44",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the time efficiency of the L-EDA compared to traditional EDA?",
      "topics": [
        "algorithm"
      ],
      "answer": "The time consumed by L-EDA and traditional EDA is compared, with L-EDA reported to save approximately 50-65% of the time compared to traditional EDA under various settings of the solution number.",
      "chunk_id": 24285,
      "paper_id": "Application of L-EDA in metabonomics data handling- global metabolite profiling and potential biomarker discovery of epithelial ovarian cancer prognosis",
      "paper_title": "Application of L-EDA in metabonomics data handling: global metabolite profiling and potential biomarker discovery of epithelial ovarian cancer prognosis",
      "paper_year": "2011",
      "chunk_content": "3 Times consumed by traditional EDA and L-EDA top-ranked variable can achieve satisfactory and stable classification results. The times consumed by traditional EDA and L-EDA are shown in Fig. 3. Under various settings of the solution number, L-EDA saved about $50-65 \\%$ time compared to traditional EDA. Note that, the solution capacity for L-EDA was set to 70 , if the solution capacity was set to a smaller number it was expected that more time could be saved. The discrimination ability of 78 variables was visualized by PCA model (Fig. 4). Recurrent EOC and EOC women were clustered in the same dimensional space, the nonrecurrent EOC group was separated from them, in the middle of the healthy women group and EOC groups. This is very different from Fig. 1a. With the original data, PCA cannot distinguish four groups. The data processed by L-EDA reasonably describe the metabolite profiling of recurrent and non recurrent EOC. Without L-EDA process, the metabolic differences of EOC prognosis are easily covered by the factors induced by chemotherapy and radiotherapy etc., non-recurrent EOC and recurrent EOC are not possible to be resolved. L-EDA is proved to be a powerful tool in metabonomics data mining.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:45",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs, specifically in comparison to GA and TS?",
      "topics": [
        "mutation"
      ],
      "answer": "The eEDA algorithm aims to improve convergence and solution quality by combining the exploration capabilities of Estimation of Distribution Algorithms with a permutation neighborhood search, enabling it to both explore and exploit candidate solutions. This contrasts with GA, which primarily focuses on exploration, and CW and TS, which primarily focus on exploiting solutions, leading to eEDA's superior performance.",
      "chunk_id": 16944,
      "paper_id": "Evolutionary Estimation of Distribution Algorithm for Agricultural Routing Planning in Field Logistics",
      "paper_title": "Evolutionary Estimation of Distribution Algorithm for Agricultural Routing Planning in Field Logistics",
      "paper_year": "2019",
      "chunk_content": "6 depicts the convergence process of GA, TS, and eEDA for problems 20(a) and 90(b). The X-axis is the generation, while the Y-axis is the objective function value. GA's solution to problem 20 is worse than the others, while the one for Problem 90 is better than TS after half of the generations. TS converges the fastest in problem 90; however, it seems to be trapped in a local-optima since the solution is not improved after approximately a quarter of the total iterations. For both problems, eEDA's solution is always better than those of other algorithms. The eEDA's objective function converges fast and ends with the best solution compared to GA and TS. Overall, the eEDA's convergence process is better than those of GA and TS. The eEDA combines EDA and the permutation neighborhood search in its structure, enabling the algorithm to both explore and exploit the candidate solution. Both CW and TS focus only on exploiting the solutions without exploring it, while GA focuses only on exploring the solution. These reasons enable eEDA to outperform CW, GA, and TS. [IMAGE_3] Fig. 4. The machine' route and solution for problem 12b.",
      "question_type": "process",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:47",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of the Cellular Compact Genetic Algorithm (CCGA), how does the algorithmic mechanism of converting the confident counter (cc) to its fractional representation and employing shift registers impact the hardware implementation of probability vector updates?",
      "topics": [
        "Estimation of Distribution Algorithms"
      ],
      "answer": "Within the CCGA, the confident counter (cc) is converted to a fractional number using the formula $(1 / \\mathrm{cc})$. Instead of directly using multipliers, which consume more hardware resources, the algorithm uses shift registers to multiply this fractional value, represented as [FORMULA46], with the probability vector. This shift register implementation scales down the [FORMULA47] value (equal to $1 / cc$) to a multiple of 2. Following this multiplication, the probability vectors are combined using an 8-bit adder, as described in the vector combination equation of the CCGA algorithm. This approach optimizes hardware resource utilization by replacing multipliers with shift registers and adders, while the FSM_CONTROL manages the datapath blocks to generate values A and B for tournament selection and updating probability vectors, requiring four clock cycles.",
      "chunk_id": 11281,
      "paper_id": "FPGA Implementation of a Cellular Compact Genetic Algorithm",
      "paper_title": "FPGA implementation of a cellular compact genetic algorithm",
      "paper_year": "2008",
      "chunk_content": "Hardware design of a cellular compact GA cell VCOMBIN is the hardware block that implements the step 6 of the pseudocode the Fig. 3. A hardware part of the block consists of comparators and multiplexers for comparing incoming confident counter. The best confident counter will be selected among the incoming confident counters of the neighbors. The confident counter (cc) is converted to [FORMULA44] by using fractional number $(1 / \\mathrm{cc})$. The multiplication of [FORMULA46] with the probability vector is implemented using shift register instead of using multipliers which occupy more hardware resource. With shift register implementation, the [FORMULA47] value which is equal to $1 / c c$, will be scaled down to multiple of 2 . From equation of vector combination of CCGA algorithm, after multiplication, the value of both probability vectors will be added using 8 -bit adder. FSM_CONTROL is a simple finite state machine that controls the three datapath blocks. The CCGA-bit module takes four clock cycles for generating the two values A and B for tournament selection and updating probability vectors. The COMM module takes sixteen clock cycles for sending and receiving probability vectors; however, the number of clock cycles depends on the size of the chromosome for a specific problem.",
      "question_type": "algorithmic mechanism",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:49",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "What specific implementation steps are required for the Estimation of Distribution Algorithm described to construct the probability matrix, given an initial population of N individuals?",
      "topics": [
        "fitness"
      ],
      "answer": "To construct the probability matrix, first generate N integer sequences as the initial population. Select D better individuals as the superior population. Calculate the frequencies of pair-wise neighboring operations using the formula \\(p_{ij}^{t}=(1-\\alpha) \\cdot \\bigcup_{n \\times m}^{1} \\sum_{k=1}^{1} \\delta_{ij}(x_{k}^{0})\\), where \\(\\delta_{ij}(x_{k}^{0})\\) equals 1 if operation \\(o_i\\) is followed by \\(o_j\\) in the kth individual's sequence, and 0 otherwise.  \\(n\\times m\\) represents the length of the operation sequence. These frequencies form the elements of the probability matrix \\(P^t\\), where t represents the iterative number and is initialized to 1.",
      "chunk_id": 15143,
      "paper_id": "An Efficient Estimation of Distribution Algorithm for Job Shop Scheduling Problem",
      "paper_title": "An Efficient Estimation of Distribution Algorithm for Job Shop Scheduling Problem",
      "paper_year": "2010",
      "chunk_content": "Let the population be N . N integer sequences are randomly generated and consist of the initial population. One sequence of operations denotes one individual. Evaluate the fitness (makespan) of individuals. The less makespan be, the larger the corresponding value of fitness. Some better individuals are selected as superior population according to some proportional. Let the superior population size be D. ### 3.3 Construct and Update Probability Model In this paper, we have constructed probability model according to the order of operations of individuals in superior population based on edge histogram[10]. Let the initial population be represented as $P(0)$, and the initial superior population be represented as $D^{0}$. The kth individual is as $x_{s}^{0}=\\left(\\pi_{s}^{0}(1), \\pi_{s}^{0}(2), \\ldots, \\pi_{s}^{0}(n \\times m)\\right)$, set $k \\in D$. The initial population is generated by uniform distribution. Through calculating the frequencies of pair-wise neighboring operations appearing in superior population $$\n\\begin{gathered}\np_{i j}^{t}=(1-\\alpha) \\cdot \\bigcup_{n \\times m}^{1} \\sum_{k=1}^{1} \\delta_{i j}\\left(x_{k}^{0}\\right), \\quad i, j=1,2, \\ldots n \\times m \\\\\n\\delta_{i j}\\left(x_{k}^{0}\\right)=\\left\\{\\begin{array}{lc}\n1 & \\pi_{k}^{0}(h) \\rightarrow o_{i} \\wedge \\pi_{k}^{0}(h+1) \\rightarrow o_{j} \\\\\n0 & \\text { otherwise }\n\\end{array}\\right.\n\\end{gathered}\n$$, the probability matrix is constructed as follows: $$\nP^{t}=\\left[\\begin{array}{cccc}\np_{11}^{t} & p_{12}^{t} & \\cdots & p_{1, n \\times m}{ }^{t} \\\\\np_{21}^{t} & p_{22}^{t} & \\cdots & p_{2, n \\times m}{ }^{t} \\\\\n\\cdots & \\cdots & \\cdots & \\cdots \\\\\np_{n \\times m, 1}{ }^{t} & p_{n \\times m, 2}{ }^{t} & \\cdots & p_{n \\times m, n \\times m}{ }^{t}\n\\end{array}\\right]\n$$ Where $t$ represents the iterative number, and $t=1$. $$\n\\begin{gathered}\np_{i j}^{t}=(1-\\alpha) \\cdot \\bigcup_{n \\times m}^{1} \\sum_{k=1}^{1} \\delta_{i j}\\left(x_{k}^{0}\\right), \\quad i, j=1,2, \\ldots n \\times m \\\\\n\\delta_{i j}\\left(x_{k}^{0}\\right)=\\left\\{\\begin{array}{lc}\n1 & \\pi_{k}^{0}(h) \\rightarrow o_{i} \\wedge \\pi_{k}^{0}(h+1) \\rightarrow o_{j} \\\\\n0 & \\text { otherwise }\n\\end{array}\\right.\n\\end{gathered}\n$$ where $h \\in\\{1,2, \\ldots,(n \\times m-1)\\}, \\delta_{i j}\\left(x_{k}^{0}\\right)$ equals to 1 denotes that the neighboring operation $o_{i} o_{j}$ appear in the structure of the kth individual. Otherwise $\\delta_{i j}\\left(x_{k}^{0}\\right)$ equals to 0 .",
      "question_type": "practical application",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:51",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the ABC-EDM approach, specifically the Estimation of Distribution Mechanism (EDM) within the scout bee phase, how are the normal distribution models $N(\\mu_{m_{x}}, \\delta_{m_{x}}^{2})$, $N(\\mu_{m_{y}}, \\delta_{m_{y}}^{2})$, and $N(\\mu_{m_{z}}, \\delta_{m_{z}}^{2})$ constructed and used to initialize new food sources, and what parameters determine their characteristics?",
      "topics": [
        "fitness"
      ],
      "answer": "In the scout bee phase of the ABC-EDM approach, the EDM relocates a new food source by first selecting [FORMULA113] promising solutions with better fitness values from the current population. Then, for each variable [FORMULA114], [FORMULA115], and [FORMULA116] (though [FORMULA116] is not explicitly defined), the means $(\\mu_{m_{x}}, \\mu_{m_{y}}, \\mu_{m_{z}})$ and standard deviations $(\\delta_{m_{x}}, \\delta_{m_{y}}, \\delta_{m_{z}})$ are calculated based on these [FORMULA118] promising solutions. These means and standard deviations are used to build three normal distribution models: $N(\\mu_{m_{x}}, \\delta_{m_{x}}^{2})$, $N(\\mu_{m_{y}}, \\delta_{m_{y}}^{2})$, and $N(\\mu_{m_{z}}, \\delta_{m_{z}}^{2})$. A new food source is initialized by sampling from these distributions for the $n+5$, $n+6$, and $n+7$ components, where 'n' relates to the number of active rotatable bonds ([FORMULA122]). The other components, $j \\in {1, 2, \\ldots, n+4}$, are set to the corresponding component of the best individual in the current population, denoted as $x_{j}^{(\\text {best })}$ ([FORMULA124]th component). The characteristics of the normal distributions are thus determined by the means and standard deviations calculated from the selected promising solutions.",
      "chunk_id": 5215,
      "paper_id": "Improving the artificial bee colony algorithm with a proprietary estimation of distribution mechanism for protein-ligand docking",
      "paper_title": "Improving the artificial bee colony algorithm with a proprietary estimation of distribution mechanism for protein-ligand docking",
      "paper_year": "2024",
      "chunk_content": "Intuitively, the normal distribution is not suitable for building probabilistic models for these variables. It is considered arbitrary to casually select probabilistic models for these variables. Since these variables are related to the conformation and orientation of the ligand, a promising solution is considered to have suitable conformation and orientation. As a result, it is a wise method to set these variables of a candidate solution by directly inheriting from a promising solution. In the scout bee phase of the proposed ABC-EDM approach, EDM works in three steps to relocate a new food source. First, [FORMULA113] promising solutions that have better fitness values are selected from the current population of food sources. Second, for each variable of [FORMULA114], and [FORMULA115], the means $\\left(\\mu_{m_{x}}, \\mu_{m_{y}}, \\mu_{m_{z}}\\right)$ and the standard deviations $\\left(\\delta_{m_{x}}, \\delta_{m_{y}}, \\delta_{m_{z}}\\right)$ are calculated based on these [FORMULA118] promising solutions. Consequently, three normal distribution models, i.e., $N\\left(\\mu_{m_{x}}, \\delta_{m_{x}}^{2}\\right), N\\left(\\mu_{m_{y}}, \\delta_{m_{y}}^{2}\\right)$, and $N\\left(\\mu_{m_{x}}, \\delta_{m_{x}}^{2}\\right)$, are built. Finally, a new food source is initialized as follows: $$\nx_{j}^{(i)}= \\begin{cases}x_{j}^{(\\text {best })} & \\text { if } j \\in\\{1,2, \\ldots, n+4\\} \\\\ \\text { sampled from } N\\left(\\mu_{m_{x}}, \\delta_{m_{x}}^{2}\\right) & \\text { if } j=n+5 \\\\ \\text { sampled from } N\\left(\\mu_{m_{y}}, \\delta_{m_{y}}^{2}\\right) & \\text { if } j=n+6 \\\\ \\text { sampled from } N\\left(\\mu_{m_{z}}, \\delta_{m_{z}}^{2}\\right) & \\text { if } j=n+7\\end{cases}\n$$ where [FORMULA122] is the number of active rotatable bonds. $x_{j}^{(\\text {best })}$ is the [FORMULA124] th component of the best individual in the current population.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:54",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the model-building GNG (MB-GNG) approach address the limitations of traditional methods in model building for Estimation of Distribution Algorithms (EDAs), specifically regarding outliers and resource allocation?",
      "topics": [
        "search"
      ],
      "answer": "The model-building GNG (MB-GNG) addresses limitations of traditional methods by treating outliers as newly discovered regions of the search space, unlike other algorithms that treat them as invalid data. Furthermore, MB-GNG avoids spending excess resources in finding the optimal size of the model, making it more suitable for high-dimensional problems compared to methods that do not account for this.",
      "chunk_id": 11709,
      "paper_id": "Scalable continuous multiobjective optimization with a neural network-based estimation of distribution algorithm",
      "paper_title": "Scalable continuous multiobjective optimization with a neural network-based estimation of distribution algorithm",
      "paper_year": "2008",
      "chunk_content": "However, the model building problem has specific requirements that those methods do not meet and even avoid. In particular, in those algorithms, outliers are treated as invalid data, where in the model building problem they represent newly discovered regions of the search space. Similarly, an excess of resources is spent in finding the optimal size of the model. In this work we propose a novel approach to model building in MOEDAs using an algorithm custom-made for the task. We modify the growing neural gas (GNG) network [5] to make it particularly suitable for model building. The resulting model-building GNG (MB-GNG) addresses the above described issues with success. Therefore it is capable of yielding better results when confronted to high-dimensional problems. The rest of this work first deals with the theoretical background that supports our proposal. We then proceed to describe MB-GNG and the EDA framework in which it is embedded. After that, a series of well-known test problems are solved with a set of other state-of-the-art algorithms and our approach. Each problem is scaled regarding the number of optimization functions in order to assess the behavior of the algorithms when subjected to extreme situations. As a conclusion some final remarks, comments and lines of future development are presented. # 2 Theoretical Background The concept of multiobjective optimization refers to the process of finding one or more feasible solutions of a problem that corresponds to the extreme values (either maximum or minimum) of two or more functions subject to a set of restrictions. More formally, a multiobjective optimization problem (MOP) can be defined as ## Definition 1 (Multiobjective Optimization Problem).",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:57:56",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the feature model described, what data structures are required to represent the model and its constituent features, according to the definitions provided?",
      "topics": [
        "search"
      ],
      "answer": "Based on the definitions, a feature model, denoted as M, can be represented as a collection (e.g., a list or set) of feature objects, f_i. Each feature object, f, should contain a collection (e.g., a list or set) of sketch objects, S_i, and a data structure to store the feature parameters, p. The parameters p could be implemented as a dictionary or a class, depending on the specific parameters associated with each sketch.",
      "chunk_id": 26074,
      "paper_id": "Quantitative optimization of interoperability during feature-based data exchange",
      "paper_title": "Quantitative optimization of interoperability during feature-based data exchange",
      "paper_year": "2016",
      "chunk_content": "3. The theory of feature-based interoperability ### 3.1. Formalization method for feature-based interoperability Although the research focus of feature interoperation in this paper is different from that of feature-based multi-resolution model in [42,44], this section takes advantage of the formal method to describe the problem. It is well known that the modeling procedure is a series of operations on features and sketches, but the issue of feature-based interoperability was not clearly defined in existing literatures. According to the characteristic of feature-based interoperability between heterogeneous CAD systems, a set of formal definitions related to feature-based interoperability is provided. - Definition 1: Feature model. The feature model consists of a series of features (such as: Extrusion, Pocket, Hole and Fillet); the definition is provided as follows: $$\nM=\\bigcup_{i=1}^{n} f_{i}\n$$ where $M$ describes a CAD feature model and $f_{i}$ is the $i$ th feature in $S_{i}$. - Definition 2: Feature. A feature consists of one or more sketches and the related feature parameters; the definition is provided as follows: $$\nf=\\left\\{\\bigcup_{i=1}^{n} S_{i}, p\\right\\}\n$$ where $S_{i}$ is the $i^{\\text {th }}$ sketch in $f$ and $p$ denotes the corresponding parameters related to $s_{i}$.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:57:57",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the EDA's output be measured using the criteria of performance when analyzing the behavior of a stock in a projection horizon?",
      "topics": [
        "genetic"
      ],
      "answer": "The performance of an action in a 12-week projection horizon can be analyzed using Estimation of Distribution Algorithms (EDAs). The output quality can be measured by evaluating the algorithms' behavior in solving problems that require the experience of an expert.",
      "chunk_id": 9828,
      "paper_id": "Daily Ecopertrol Stock Performance Estimate by Using Estimation of Distribution Algorithms (EDAs)",
      "paper_title": "Daily Ecopertrol Stock Performance Estimate by Using Estimation of Distribution Algorithms (EDAs)",
      "paper_year": "2012",
      "chunk_content": "En este articulo, se puede observar el efecto que las noticias economicas generan sobre la volatilidad en una serie de tiempo [8]. Desde el punto de vista de la Inteligencia Computacional, los anos 80 marcaron un gran avance en esta area del conocimiento, lo que fue fundamental para el desarrollo de modelos que explicaran la dinamica de los mercados financieros, empleando para ello tecnicas promisorias basadas en sistemas por adaptacion y aprendizaje. Es por esto que Mahfoud \\& Mani [13], desarrollaron un modelo que emplea los algoritmos geneticos integrados a las redes neuronales artificiales, para analizar el desempeno de una accion en un horizonte de proyeccion de 12 semanas. Los resultados arrojados por el modelo, mostraron el buen comportamiento de los algoritmos por evolucion, en la solucion de problemas que requieren la experiencia de un experto, tal y como sucede en los Sistemas Clasificadores Geneticos [5], [4]. De esta manera, se han desarrollado una serie de modelos para el pronostico y la prediccion de series temporales, mediante la utilizacion de algoritmos estadisticos por evolucion (EDA's) [9], [10], o la utilizacion de modelos borrosos para la identificacion del riesgo operacional en las actividades del negocio que involucra una entidad financiera [20], o en la asignacion de pantajes crediticios para clientes beneficiados por un credito de consumo [21].",
      "question_type": "evaluation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:57:59",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs involving the historical best individual ($W_b$) and the current generation's best individual ($C_b$) in updating the parameters $\\alpha_{i}^{j, g}$ and $\\beta_{i}^{j, g}$?",
      "topics": [
        "convergence"
      ],
      "answer": "The algorithmic approach aims to guide the EDA's search by leveraging information from both the historically best solution ($W_b$) and the current generation's best solution ($C_b$) to update the parameters $\\alpha_{i}^{j, g}$ and $\\beta_{i}^{j, g}$, which define the probability distributions used to generate new populations of binary strings. By comparing corresponding bits in $W_b$ and $C_b$, the algorithm adjusts $\\theta_i$ (and subsequently $\\theta_i^j$) to either $\\omega$, $-\\omega$, or 0, influencing the update of $\\alpha_{i}^{j, g}$ and $\\beta_{i}^{j, g}$ and biasing the generation of future populations towards regions of the search space that have previously yielded promising solutions. The Immune Quantum Inspired Evolutionary Algorithm (IQEA) further extends this by incorporating immune recognition and clonal selection to enhance the fitness of individuals.",
      "chunk_id": 1546,
      "paper_id": "MAX-SAT problem using evolutionary algorithms",
      "paper_title": "MAX-SAT Problem using Evolutionary Algorithms",
      "paper_year": "2014",
      "chunk_content": "Execution While (convergence criterion is not satisfied) Generate a population of [FORMULA142] binary strings with $\\alpha_{i}^{j, g}, \\beta_{i}^{j, g}$ with [FORMULA144] for [FORMULA145], and [FORMULA146],. Evaluate the fitness values of [FORMULA147] individuals, Store the best solution among [FORMULA148], Save best individual from [FORMULA149]. Get the historically best individual, $W_{b}=\\left[w_{b, 1}, w_{b, 2, \\ldots}, w_{b, m}\\right]$, and the best individual from the current generation is $C_{b}=\\left[c_{b, 1}, c_{b, 2, \\ldots}, c_{b, m}\\right]$, for [FORMULA152] $$\n\\begin{aligned}\n& \\text { if } w_{b, i}=1 \\text { and } c_{b, i}=1 \\\\\n& \\quad \\theta_{i}=\\omega \\\\\n& \\text { elseif } w_{b, i}=0 \\text { and } c_{b, i}=0 \\\\\n& \\quad \\theta_{i}=-\\omega \\\\\n& \\text { else } \\\\\n& \\quad \\theta_{i}=0 \\\\\n& \\text { Set } \\theta_{i}^{j}=\\theta_{i} \\text { for } j=1,2 \\ldots, N \\text {, } \\\\\n& \\text { end for } \\\\\n& \\text { Update } \\alpha_{i}^{j, g} \\text {, and } \\beta_{i}^{j, g} \\text { using } \\theta_{i}^{j} \\text {, } \\\\\n& g=g+1 \\text {, }\n\\end{aligned}\n$$ ## End while ## C. Immune Quantum Inspired Evolutionary Algorithm Immune quantum evolutionary algorithm (IQEA) is an extension of QEA by adding an immune operator into QEA. Immune algorithms are evolutionary algorithms based on physiological immune systems. Physiological immune systems have mechanisms that enable cells to exhibit and recognize foreign substances and develop an immune operation. Immune operation based immune operator is employed in IQEA to improve QEA. Further study regarding the immune algorithm and its functionality is presented in [6]. The immune operator is based on two major operations: immune recognition and clonal selection. Immune recognition is used for raising fitness. For an individual [FORMULA154], an immune recognition means modifying the genes on some bits in accordance with prior knowledge, to get higher fitness with greater probability. Suppose a population $\\Omega=i_{1}, i_{2}, \\ldots, i_{k}, \\ldots, i_{N}$, the immune recognition on $\\Omega$ means the operation carried out on $\\psi=\\mathrm{n} \\alpha$ individuals.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:01",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms and the Additive Drift Theorem, what role does the variable $Z_t$ play in analyzing the runtime, and how does the condition $E[Z_{t+1}-Z_t + \\varepsilon; t < T_a | \\mathscr{F}_t] \\leq 0$ for all $x > 0$ contribute to determining the expected time $E[T_a]$?",
      "topics": [
        "search"
      ],
      "answer": "In the Additive Drift Theorem, $Z_t$ represents a discrete-time stochastic process used to analyze the runtime of an algorithm. The condition $E[Z_{t+1}-Z_t + \\varepsilon; t < T_a | \\mathscr{F}_t] \\leq 0$ for all $x > 0$ implies that the expected drift, which is the change in $Z_t$ at each time step, is negative. Here, $\\varepsilon$ is a positive constant, $T_a$ is the time when $Z_t$ becomes less than or equal to $a$, and $\\mathscr{F}_t$ represents the filtration. If this condition holds, along with other conditions, then the expected time $E[T_a]$ is bounded by $Z_0 / \\varepsilon$, where $Z_0$ is the initial value of $Z_t$.",
      "chunk_id": 22653,
      "paper_id": "Level-Based Analysis of Genetic Algorithms and Other Search Processes",
      "paper_title": "Level-Based Analysis of Genetic Algorithms and Other Search Processes",
      "paper_year": "2018",
      "chunk_content": "Hence, we have $\\operatorname{Pr}_{y \\sim D\\left(P_{y}\\right)}\\left(y \\in A_{\\geq j+1}\\right) \\geq 1(1 / 3)$, and we can satisfy condition (G1) by defining $z_{j}:=1 / 3$ for all $$\np_{t+1}^{\\left(Y_{t}\\right)}=\\operatorname{Pr}_{y \\sim D\\left(P_{t}\\right)}\\left(y \\in A_{\\geq Y_{t}}\\right) \\geq(1+\\delta) \\gamma_{0}\n$$. Step 4: For the parameters we have chosen, it is easy to see by numerical calculation that the population size $Z_{t}:=0$ satisfies condition (G3). Step 5: Using that $\\ln ([(6 \\delta \\lambda) /\\left(4+\\delta \\lambda z_{j}\\right)])<\\ln \\left(6 / z_{j}\\right)$, the expected time to discover the point $Z_{t}:=g\\left(X_{t}^{\\left(Y_{t}+1\\right)}, Y_{t}\\right)$ is no more than $$\n\\begin{aligned}\n\\frac{8}{(1 / 3)^{2}} & \\sum_{j=1}^{m-1}\\left(\\lambda \\ln \\left(\\frac{6}{1 / 3}\\right)+\\frac{1}{1 / 3}\\right) \\\\\n& <216(m-1)(\\lambda+1)\n\\end{aligned}\n$$ ## C. Proof of the Level-Based Theorem Theorem 1 will be proved using drift analysis [28], [29], a standard tool in theory of randomized search heuristics. We a variant of the additive drift theorem [29], which is proved in the supplementary materials. The lower bound statement will be used in Section VI to evaluate the tightness of the theorem. In what follows, \" $\\left(Z_{t+1}-Z_{t}+\\varepsilon\\right) ; t<T_{a}$ \" denotes \" $\\left(Z_{t+1}-Z_{t}+\\varepsilon\\right) \\cdot \\mathbb{1}_{\\left[t<T_{a}\\right]}$ \" (see [55, 6.3]). Theorem 2 (Additive Drift Theorem): Let $\\left(Z_{t}\\right)_{t \\in \\mathbb{N}}$ be a discrete-time stochastic process in $[0, \\infty)$ adapted to any filtration $\\left(\\mathscr{F}_{t}\\right)_{t \\in \\mathbb{N}}$. Define $T_{a}:=\\min \\left[t \\in \\mathbb{N} \\mid Z_{t} \\leq a\\right]$ for any $a \\geq 0$. For some $\\varepsilon>0$ and constant $0<b<\\infty$, define the conditions 1.1) $E\\left[Z_{t+1}-Z_{t}+\\varepsilon ; t<T_{a} \\mid \\mathscr{F}_{t}\\right] \\leq 0$ for all $x>0$; 1.2) $E\\left[Z_{t+1}-Z_{t}+\\varepsilon ; t<T_{a} \\mid \\mathscr{F}_{t}\\right] \\geq 0$ for all $z_{i} \\geq z_{*}$; 2) $Z_{t}<b$ for all $$\n<\\frac{m}{z_{*}}\\left(2+\\frac{1}{\\lambda}\\right) \\leq \\frac{41 m}{20 z_{*}}\n$$; 3) $E\\left[T_{a}\\right]<\\infty$. If 1.1), 2), and 3) hold, then $E\\left[T_{a} \\mid \\mathscr{F}_{0}\\right] \\leq Z_{0} / \\varepsilon$. If 1.2), 2), and 3) hold, then $E\\left[T_{a} \\mid \\mathscr{F}_{0}\\right] \\geq\\left(Z_{0}-a\\right) / \\varepsilon$.",
      "question_type": "definition",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:03",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm using symmetric non-negative matrix factorization (SNMF), what specific steps are involved in updating the matrix H using the gradient descent method, and how is the bipartite graph adjacency matrix B then calculated?",
      "topics": [
        "optimization",
        "Estimation of Distribution Algorithms",
        "matrix factorization"
      ],
      "answer": "To implement the gradient descent method for SNMF, the matrix H is updated iteratively using the rule:  $\\tilde{h_{i p}}=\\frac{h_{i p}}{\\sum_{j} \\tilde{h}_{j p}} \\sum_{j} \\frac{\\operatorname{mim}_{i j}}{\\left(H H^{T}\\right)_{i j}} h_{j p}$, where $mim_{ij}$ represents the elements of the Mutual Information Matrix (MIM). After convergence, the bipartite graph adjacency matrix B is calculated as $B = H \\Lambda^{1/2}$, where $\\Lambda$ is a diagonal matrix with elements $\\lambda_{p} = \\sum_{i=1}^{n} h_{i p}$.",
      "chunk_id": 1255,
      "paper_id": "Estimation of distribution algorithm using factor graph and Markov blanket canonical factorization",
      "paper_title": "Estimation of Distribution Algorithm using Factor Graph and Markov Blanket Canonical Factorization",
      "paper_year": "2014",
      "chunk_content": "When $b_{i p}=p\\left(v_{i}, u_{p}\\right)$, equation (5) and equation (6) are the same. $p\\left(u_{p} \\mid v_{i}\\right)=\\frac{b_{i} p}{d_{i}}$ is the conditional probability of transitions from $\\lambda_{p}$ to $u_{p}$ and indicates how likely variable $i$ belongs to factor nodes p. Based on equation (5), the bipartite graph can be approximated by minimizing the distance $\\left(M I M, B \\Lambda^{-1} B^{T}\\right)$. To make the problem easy we use $H=B \\Lambda^{-1 / 2}$, then we have $$\n\\min _{H \\in \\Re_{n}^{n+m}} \\operatorname{distance}\\left(M I M, H H^{T}\\right), \\quad \\text { s.t. } h_{i p} \\geq 1\n$$ This problem is a symmetric non-negative matrix factorization (SNMF) [15]. There are different numerical methods to find the local minima of this problem. Here we use the gradient descent method to minimize the divergence distance between the two adjacency matrices (equation (8)). $$\nD D(X, Y)=\\sum_{i j}\\left(x_{i j} \\log \\frac{x_{i j}}{y_{i j}}-x_{i j}+y_{i j}\\right)\n$$ Theorem 4.1 The distance is non-increasing under the following update rule $$\n\\tilde{h_{i p}}=\\frac{h_{i p}}{\\sum_{j} \\tilde{h}_{j p}} \\sum_{j} \\frac{\\operatorname{mim}_{i j}}{\\left(H H^{T}\\right)_{i j}} h_{j p}\n$$ The distance is invariant under the update rule if and only if $\\mathbf{D}=\\left\\langle X_{i_{1}}, \\ldots, X_{i_{|\\mathbf{D}|}}\\right\\rangle$ is at stationary point of the distance [16]. After $\\mathbf{d}=\\left\\langle x_{i_{1}}, \\ldots, x_{i_{|\\mathbf{D}|}}\\right\\rangle$ is calculated, $B$ can be calculated using $B=H \\Lambda^{1 / 2}$ and $\\Lambda=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{m}\\right)$ where $\\lambda_{p}=\\sum_{i=1}^{n} h_{i p}$.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:05",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the computational complexity of the NSGA-II non-dominated sorting algorithm, used for fitness assignment within the described Estimation of Distribution Algorithm, considered in the context of its effectiveness and the overall algorithm's performance?",
      "topics": [
        "complexity"
      ],
      "answer": "The NSGA-II non-dominated sorting scheme is selected for fitness assignment due to its 'proven effectiveness' and 'relative low computational cost'. The algorithm extracts a set $\\hat{\\mathcal{P}}_{t}$ containing the best $\\left\\lfloor\\alpha \\mid \\mathcal{P}_{t}\\right\\rfloor$ elements from the sorted population, where $\\mathcal{P}_{t}$ represents the population at iteration t, and $\\alpha$ is a parameter controlling the proportion of elements selected. The MB-GNG network's maximum size, $N_{\\max }$, is bound by $N_{\\max }=\\left\\lceil\\gamma \\mid \\hat{\\mathcal{P}}_{t}\\right\\rceil$, where $\\gamma$ controls the relationship between the size of $\\hat{\\mathcal{P}}_{t}$ and $N_{\\max }$. The computational cost of NSGA-II sorting contributes to the overall efficiency of the EDA, influencing the speed at which the algorithm iterates and converges to a set of non-dominated solutions $\\mathcal{P}_{t}^{*}$.",
      "chunk_id": 13544,
      "paper_id": "On the Computational Properties of the Multi-Objective Neural Estimation of Distribution Algorithm",
      "paper_title": "On the Computational Properties of the Multi-Objective Neural Estimation of Distribution Algorithm",
      "paper_year": "2009",
      "chunk_content": "It then proceeds to sort the individuals. The NSGA-II non-dominated sorting [5] was the scheme selected for fitness assignment. It was chosen because of its proven effectiveness and its relative low computational cost. A set $\\hat{\\mathcal{P}}_{t}$ containing the best $\\left\\lfloor\\alpha \\mid \\mathcal{P}_{t}\\right\\rfloor$ elements is extracted from the sorted version of [FORMULA90]. A MB-GNG network is then trained using [FORMULA91] as training data set. In order to have a controlled relation between size of [FORMULA92] and the maximum size of the network, [FORMULA93], these two sizes are bound as $N_{\\max }=\\left\\lceil\\gamma \\mid \\hat{\\mathcal{P}}_{t}\\right\\rceil$. The resulting Gaussian kernels are sampled to produce an amount $\\left\\lfloor\\omega \\mid \\mathcal{P}_{t}\\right\\rfloor$ of new individuals. Each one of these individuals substitute a randomly selected ones from the section of the population not used for model-building $\\mathcal{P}_{t} \\backslash \\hat{\\mathcal{P}}_{t}$. The set obtained is then united with best elements, [FORMULA97], to form the population of the next iteration [FORMULA98]. Iterations are repeated until a given stopping criterion is met. The output of the algorithm is the set of non-dominated solutions of $\\mathcal{P}_{t}, \\mathcal{P}_{t}^{*}$. # 20.4 Measuring Complexity The assessment of an optimizer performance is one of the most vibrant areas of research in the MOEA context.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:07",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "According to the details in the chunk, what advantages does RM-MEDA have over NSGA-II and MISA in the context of multi-objective optimization problems?",
      "topics": [
        "convergence"
      ],
      "answer": "The chunk mentions that RM-MEDA, NSGA-II, and MISA are compared using inverted generation distance (IGD) for measuring diversity and convergence, and the Coverage of two sets metric for comparing solutions. However, specific advantages of RM-MEDA over NSGA-II and MISA are not detailed in the chunk, only that they are algorithms used for comparison on test problems F1-F8.",
      "chunk_id": 13387,
      "paper_id": "Hybrid multiobjective estimation of distribution algorithm by local linear embedding and an immune inspired algorithm",
      "paper_title": "Hybrid Multiobjective Estimation of Distribution Algorithm by Local Linear Embedding and an Immune Inspired Algorithm",
      "paper_year": "2009",
      "chunk_content": "The results will be compared with a fast and elitist multi-objective genetic algorithm (NSGA-II) [15], an immune inspired multiobjective algorithm (MISA) [8], and a regularity model-based multiobjective estimation distribution algorithm (RM-MEDA) [1]. Besides, inverted generation distance (IGD) [21] is adopted for measuring both the diversity and convergence of an approximation to the true Pareto optimal front. A binary quality metric, the Coverage of two sets [22] is employed to compare obtained solutions by different algorithms. The eight problems are defined in the Table 2. The first two problems (F1 and F2) were developed by Zitzler, Deb, and Thiele [23], of which there are not linkage mappings among variables. The next three problems are from [1], and there are linear or near linear mappings among variables. Last three problems are modified by introducing complicated nonlinear mappings in the form of $\\sin ($ ), circle, and $S$-curve. Therefore, the last three problems are more complicated. In this study, all the simulations run at a personal computer with P-IV 3.0G CPU and 2G RAM. By reference [1] and [15], the maximal number of function evaluations of the eight test instances is shown in Table 2.",
      "question_type": "advantage",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:09",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the feature extraction stage when computing the fitness function for a channel selection in this EDA implementation?",
      "topics": [
        "Estimation of Distribution Algorithms (EDAs)"
      ],
      "answer": "The feature extraction stage involves applying the CSP method to channel-reduced EEG data, generating a reduced set of 5 projection signals for each class versus the others. Subsequently, three frequency band filters (8-12 Hz, 21-20 Hz, and 20-30 Hz) are applied. Finally, seven features are extracted: max, min, and mean voltage values, voltage range, number of samples above zero volts, zero voltage crossing rate, and average signal power. This results in 420 numeric features (4 classes x 5 projections x 3 filters x 7 features) for each case in the dataset.",
      "chunk_id": 25239,
      "paper_id": "User Adapted Motor-Imaginary Brain-Computer Interface by means of EEG Channel Selection Based on Estimation of Distributed Algorithms",
      "paper_title": "User Adapted Motor-Imaginary Brain-Computer Interface by means of EEG Channel Selection Based on Estimation of Distributed Algorithms",
      "paper_year": "2016",
      "chunk_content": "For this, a classifier using the corresponding subset of channels is built and its mean accuracy is used as a fitness function. For comparison reasons, we have applied the same methodology used by AlZoubi et al. in [25]. These are the steps followed to compute the fitness function for a given channel selection: (i) Feature Extraction. Firstly, the CSP method is applied to the channel-reduced raw EEG data. For each class, applying CSP for that class versus the others, a reduced set of 5 projection signals is obtained. Then 3 frequency band filters (for $8-12 \\mathrm{~Hz}, 21-20 \\mathrm{~Hz}$, and $20-30 \\mathrm{~Hz}$ ) are applied, and, finally, 7 features are extracted: max, min, and mean voltage values, voltage range, number of samples above zero volts, zero voltage crossing rate, and average signal power. This process gives 420 ( 4 classes $\\times 5$ projections $\\times 3$ filters $\\times 7$ features) numeric features for each case of the dataset. (ii) Training and Cross-Validation. In [25] twelve classification paradigms are compared, and for this research we have chosen Support Vector Machines (SVM) [29, 30] because their results are among the best.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:11",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the EDA's model estimation be assessed by comparing the average BAVs when using RegEDA-LARS-AND versus UMDA on separable problems?",
      "topics": [
        "optimization"
      ],
      "answer": "The quality of the model estimation in RegEDA-LARS-AND can be assessed by comparing its average BAVs (presumably Bayesian Assessment Values) to those of UMDA on separable problems, such as the Tablet function. If RegEDA-LARS-AND behaves similarly to UMDA and RegEDA-Shr on separable problems, it indicates that the sparsity pattern of their structures is similar to diagonal matrices. However, for non-separable problems, such as the Sum Cancellation function, the ability of RegEDA-LARS-AND to outperform other algorithms by concentrating the search via regularized model estimation, including only the more important links based on the regularization technique, indicates a higher quality model estimation.",
      "chunk_id": 367,
      "paper_id": "Regularized continuous estimation of distribution algorithms",
      "paper_title": "Regularized continuous estimation of distribution algorithms",
      "paper_year": "2013",
      "chunk_content": "13), clearly shows that the model estimation employed in RegEDA-Shr is more efficient. This leads to a regularized combination of the models used in EMNA and UMDA. The conservative merging strategy used in RegEDA-LARS-AND model estimation causes fewer dependencies to be added to the model, leading to sparser structures. The fact that this algorithm behaves similarly to UMDA, and therefore RegEDA-Shr, for many of the functions, shows that the sparsity pattern of these structures is very similar to diagonal matrices in the presence of problem [IMAGE_14] Fig. 15. Average BAVs for Tablet function. separability. When the problem is not separable (like the Sum Cancellation function - Fig. 18), RegEDA-LARS-AND can, thanks to this regularized model estimation method, perform a more concentrated search by including only what are, according to the regularization technique, the more important links. Therefore this algorithm is able to outperform other algorithms with the increase in problem dimensionality. The optimization performance of RegEDA-GL compared with EMNA is especially interesting in these experiments. The model estimation method in the two algorithms differs only as to the regularization term added to ML estimation of MGD in the graphical LASSO method.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:12",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the DE-EIG methodology described in the context of EDAs?",
      "topics": [
        "optimization"
      ],
      "answer": "The primary objective of DE-EIG is to enhance the crossover operation in Differential Evolution by incorporating statistical information from the population. It achieves this by utilizing the eigenvectors of the solutions to transform the coordinate system, making the crossover rotationally invariant. This aims to improve performance on benchmark functions by exploiting population diversity and preventing premature convergence through random selection of original or rotated coordinate systems.",
      "chunk_id": 26118,
      "paper_id": "A hybrid estimation of distribution algorithm with differential evolution for global optimization",
      "paper_title": "A Hybrid Estimation of Distribution Algorithm with Differential Evolution for Global Optimization",
      "paper_year": "2016",
      "chunk_content": "Meanwhile, for the further improvement, expensive LS [20] is applied. This section will introduce the framework of DE/EDA, DE-EIG and expensive LS respectively. And the algorithm framework of EDA/DE-EIG is presented finally. ## A. DE-EIG As traditional DE operates the crossover in the original coordinate, it is inevitable to lose some statistical information. To utilize the statistical information of the population, the eigenvectors of the solutions have been applied to the crossover in DE [19], [21]. DE-EIG makes the crossover rotationally invariant by transform the coordinate system of the solutions in the population [19]. It has shown impressive advantages over BBOB 2012 [22] and CEC 2013 [23] benchmark functions and two real-world optimization problems in CEC 2011 [24] compared other six algorithms. In DE-EIG, an alternate coordinate system is employed by the individuals during the crossover. The eigenvector of the population is utilized to transform the coordinate system. Meanwhile, to exploit the diversity of the population and prevent the premature of the solutions in the population, the candidate solutions are generated with the original coordinate system or the rotated coordinated system randomly by a parameter. The Algorithm 3 is the main framework of DE-EIG.",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:14",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the update of the probability model using a Gaussian distribution, and how are the mean and covariance parameters estimated?",
      "topics": [
        "search"
      ],
      "answer": "In Estimation of Distribution Algorithms, the probability model is updated in each generation, assuming a Gaussian distribution to model the solutions. The probability distribution \\(P(x_{1}, x_{2} \\cdots x_{m})\\) of the vector \\((x_{1}, x_{2} \\cdots x_{m})\\) of \\(m\\) variables is a product of the distributions of individual variables, expressed as \\(P(x_{1}, x_{2} \\cdots x_{m})=\\prod_{i=1}^{m} P(x_{i})\\). The mean \\(\\widehat{\\mu}_{i}(k)\\) is estimated as \\(\\frac{1}{N} \\sum_{s=1}^{N} x_{i}^{s}(k)\\), where \\(N\\) represents the number of promising individuals and \\(x_{i}^{s}(k)\\) is the \\(i\\)th variable of the \\(s\\)th individual in the \\(k\\)th iteration. The covariance \\(\\sigma_{i}^{2}(k)\\) is estimated as \\(\\frac{1}{N} \\sum_{n=1}^{B N}\\left(x_{i}^{n}(k)-\\bar{\\mu}_{i}(k)\\right)\\left(x_{i}^{n}(k)-\\bar{\\mu}_{i}(k)\\right)^{T}\\), where \\(\\bar{\\mu}_{i}(k)\\) is the mean of the \\(i\\)th variable in the \\(k\\)th iteration, and \\(BN\\) is the selected individuals size.",
      "chunk_id": 17429,
      "paper_id": "EDA based Deep Neural Network Parameter Optimization",
      "paper_title": "EDA based Deep Neural Network Parameter Optimization",
      "paper_year": "2019",
      "chunk_content": "Meanwhile, the probability model is updated in each generation according to the new population. therefore, the population evolves, and finally search the optimal solutions. The diagram of the EDA is shown in Figure 3. The most important step of EDAs is the construction of probabilistic model, and the Gaussian distribution of individuals is assumed to model and estimate the distribution of solutions[9, 10]. Therefore, mean and variance of promising individuals are computed according to the maximum likelihood. There the probability distribution $P\\left(x_{1}, x_{2} \\cdots x_{m}\\right)$ of the vector $\\left(x_{1}, x_{2} \\cdots x_{m}\\right)$ of m variables is a product of the distributions of individual variables: $$\nP\\left(x_{1}, x_{2} \\cdots x_{m}\\right)=\\prod_{i=1}^{m} P\\left(x_{i}\\right)\n$$ The mean and covariance parameters of the normal pdf can be estimated according to the promising individuals[11]. $$\n\\widehat{\\mu}_{i}(k)=\\frac{1}{N} \\sum_{s=1}^{N} x_{i}^{s}(k)\n$$ $$\n\\sigma_{i}^{2}(k)=\\frac{1}{N} \\sum_{n=1}^{B N}\\left(x_{i}^{n}(k)-\\bar{\\mu}_{i}(k)\\right)\\left(x_{i}^{n}(k)-\\bar{\\mu}_{i}(k)\\right)^{T}\n$$ $\\bar{\\mu}_{i}(k)$ is the mean of $i$ th variable in kth iteration, BN is the selected individuals size. $\\sigma_{i}^{2}(k)$ is the covariance of [FORMULA34] th variable in $k$ th iteration. [IMAGE_2] Figure 3: Flowchart of Traditional EDA. The normal pdf $N\\left(\\mu_{i}, \\sigma_{i}\\right)$ is defined as equation (8). $$\nN\\left(x_{i}, \\mu_{i}, \\sigma_{i}\\right)=\\frac{1}{\\sigma_{i} \\sqrt{2 \\pi}} e^{-\\frac{\\left(x_{i}-\\mu_{i}\\right)^{2}}{2 \\sigma_{i}^{2}}}\n$$ The probability distribution [FORMULA38] can be described as equation (9).",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:16",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm in a dynamic environment, how should practitioners incorporate online learning of a mixture model to maintain population diversity when the fitness landscape is multimodal and time-varying?",
      "topics": [
        "genetic"
      ],
      "answer": "Practitioners should employ an online learning version of the Expectation-Maximization (EM) algorithm to adapt the mixture model to the changing environment. This involves augmenting the mixture model with an automatic control of the number of components and an operator specifically designed to promote population diversity. The goal is to create a concise model of information from previous iterations to effectively search multimodal and time-varying fitness landscapes.",
      "chunk_id": 23873,
      "paper_id": "Online Learning in Estimation of Distribution Algorithms for Dynamic Environments",
      "paper_title": "Online Learning in Estimation of Distribution Algorithms for Dynamic Environments",
      "paper_year": "2011",
      "chunk_content": "Trying to implement at least part of the aforementioned functionalities, several evolutionary approaches have been developed and extended to deal with dynamic environments. Among them, we can cite genetic algorithms [2], [3], particle swarm optimization [4], [5], artificial immune systems [6], [7] and estimation of distribution algorithms (EDAs) [8]-[10]. In this paper, we introduce an inexpensive EDA to the optimization of dynamic environments in continuous domains. This approach employs an online learning version of a mixture model, augmented by an automatic control of the number of components and by an operator to promote population diversity. The purpose is to make the search mechanism effective even in multimodal and time-varying fitness landscapes, by properly applying a concise model of the information from the previous iterations. This paper is organized as follows: Section II presents a brief introduction to estimation of distribution algorithms. In Section III, mixture models, the ExpectationMaximization (EM) algorithm and an online version of EM are described. The details of the proposed algorithm are presented in Section IV. A generator of dynamic environments in continuous search spaces, to be considered in the experiments, is depicted in Section V. Section VI discusses the obtained results on a number of dynamic optimization problems.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:17",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the number of parameters to be coded in the Local Linear Wavelet Neural Network (LLWNN) determined, given that a float point coding scheme is used and there are $M$ nodes in the hidden layer and [FORMULA39] input variables?",
      "topics": [
        "optimization"
      ],
      "answer": "The total number of parameters to be coded for the LLWNN is calculated as $(2n + n + 1) * M = (3n + 1)M$, where $M$ represents the number of nodes in the hidden layer and $n$ (in the expression $3n+1$) is related to the number of input variables [FORMULA39]. The coding represents the parameters $a_{ij}$, $b_{ij}$, and $\\omega_{ij}$ for each node.",
      "chunk_id": 26329,
      "paper_id": "Stock index modeling using EDA based Local Linear Wavelet Neural Network",
      "paper_title": "Stock index modeling using EDA based Local Linear Wavelet Neural Network",
      "paper_year": "2005",
      "chunk_content": "A float point coding scheme is adopted here. For LLWNN coding, suppose there are $M$ nodes in hidden layer and [FORMULA39] input variables, then the total number of parameters to be coded is $(2 n+n+1) * M=(3 n+1) M$. The coding of a LLWNN into an individual or particle is as follows: $$\n\\begin{aligned}\n& \\left|a_{11} b_{11} \\ldots a_{1 n} b_{1 n} \\omega_{10} \\omega_{11} \\ldots \\omega_{1 n}\\right| a_{21} b_{21} \\\\\n& \\ldots a_{2 n} b_{2 n} \\omega_{20} \\omega_{21} \\ldots \\omega_{2 n} \\mid \\ldots \\\\\n& \\left|a_{n 1} b_{n 1} \\ldots a_{n n} b_{n n} \\omega_{n 0} \\omega_{n 1} \\ldots \\omega_{n n}\\right|\n\\end{aligned}\n$$ The simple loop of the proposed training algorithm for local linear wavelet neural network is as follows. S1 Initial population is generated randomly. S2 Parameter optimization with EDA; S3 If the satisfactory solution is found or maximum number of generations is reached then stop; otherwise goto step S2. TABLE I THE RMSE RESULTS OF LLWNN AND WNN MODELS FOR TEST DATA SETS TABLE II STATISTICAL ANALYSIS OF THE LEARNING METHODS (TEST DATA) ## IV. EXPERIMENTS We considered 7-year stock data for the Nasdaq-100 Index and 4-year for the NIFTY index. Our target is to develop efficient forecast models that could predict the index value of the following trade day based on the opening, closing and maximum values of the same on a given day. The assessment of the prediction performance of the different ensemble paradigms were done by quantifying the prediction obtained on an independent data set.",
      "question_type": "relationship",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:19",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "In the context of scheduling operations on machines, how does the left-shift scheme impact the earliest available time for an operation $L_{16}(4^4)$ on a machine $P$, and what condition must be met to allow this left-shift?",
      "topics": [
        "representation"
      ],
      "answer": "The left-shift scheme aims to allocate operations earlier on their assigned machine by checking idle time intervals from left to right. For operation $L_{16}(4^4)$ on machine $P$, the scheme checks if there's enough time between operations already scheduled to complete the operation. Specifically, if the condition $\\max \\{t_{k}^{S}, C_{i, j-1}\\} + t_{i, j, k} \\leq t_{k}^{E}$ is met, where $t_{k}^{S}$ is the start time, $C_{i, j-1}$ represents a value related to the operation, $t_{i, j, k}$ is the time, and $t_{k}^{E}$ is the end time, then the time interval is considered available, and the operation can be left-shifted.",
      "chunk_id": 515,
      "paper_id": "An effective estimation of distribution algorithm for the flexible job-shop scheduling problem with fuzzy processing time",
      "paper_title": "An effective estimation of distribution algorithm for the flexible job-shop scheduling problem with fuzzy processing time",
      "paper_year": "2013",
      "chunk_content": "So, the starting time of $\\mathrm{C}++$ can be described as follow. If the operation has no predecessor, $C_{i, j-1}$ is replaced with $(0,0,0)$. $$\nS_{i, j}=\\max \\left\\{t_{k}^{S}, C_{i, j-1}\\right\\}\n$$ When operation $L_{16}\\left(4^{4}\\right)$ is assigned on machine $P=150, \\eta=20, \\alpha=0.3$, it checks the idle time intervals between operations that have already been scheduled on machine $\\beta=0.1$ from left to right to find the earliest available time $(24.8,37.2,51.9)$. If there is enough time span from $(21,36,50)$ until $(24,39,57)$ to complete $1-5$, i.e. [IMAGE2] Figure 3. Fuzzy Gantt chart of the solution shown in Figure 2. [IMAGE3] Figure 4. Fuzzy Gantt chart after left shifting to the schedule in Figure 2. $$\n\\max \\left\\{t_{k}^{S}, C_{i, j-1}\\right\\}+t_{i, j, k} \\leq t_{k}^{E}\n$$ Then we say that time interval ${ }^{\\mathrm{b}} 1.7 \\mathrm{GHz}$ is available for [FORMULA103]. That is, [FORMULA104] can be left shifted. For each operation following the operation sequence vector of the representation, the above left-shift scheme is used to allocate it on its assigned machine from left to right. In this way, a representation is decoded to a detailed schedule.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:21",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing RV-GOMEA, what algorithmic steps are involved in combining GOMEA with another real-valued EA, such as CMA-ES, to leverage GOMEA's model-building capabilities?",
      "topics": [
        "optimization"
      ],
      "answer": "RV-GOMEA maintains a population of solutions and uses a linkage model to describe the linkage structure of the optimization problem. To combine GOMEA with another real-valued EA, such as CMA-ES, the chosen EA replaces AMaLGaM as the method for sampling new problem-variable values. The GOMEA component then utilizes its model-building capabilities, as described in Section 4, to guide the search process in conjunction with the sampling strategy of the chosen real-valued EA.",
      "chunk_id": 18641,
      "paper_id": "Achieving Highly Scalable Evolutionary Real-Valued Optimization by Exploiting Partial Evaluations",
      "paper_title": "Achieving Highly Scalable Evolutionary Real-Valued Optimization by Exploiting Partial Evaluations",
      "paper_year": "2021",
      "chunk_content": "Both of these problems are non-separable, multi-objective and multi-modal, making them very well suited for optimization with an EA. The possibility of applying partial evaluations to these problems then gives GOMEA a substantial advantage compared to different algorithms that cannot benefit as much from partial evaluations. ## 6 Real-Valued GOMEA RV-GOMEA (Bouter et al., 2017b) was introduced as a combination of GOMEA (Thierens and Bosman, 2011) and AMaLGaM (Bosman et al., 2013). The use of AMaLGaM as the method of sampling new problem-variable values is however not an absolute necessity. Instead, other real-valued EAs could be combined with GOMEA in order to benefit from its model-building capabilities. In this section, we present a general outline of RV-GOMEA, and we show how a different EA can be combined with GOMEA to form variations of RV-GOMEA. In Section 7.2 we then apply this to combine GOMEA with CMA-ES. In RV-GOMEA, a population of solutions is maintained, and a linkage model is used to describe the linkage structure of the optimization problem (see Section 4).",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:23",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What benchmarking approach is outlined to assess the EDA component within the C-Multi algorithm?",
      "topics": [
        "search"
      ],
      "answer": "The C-Multi algorithm, which incorporates an estimation of distribution algorithm (EDA), was assessed using the DTIZ family of benchmark problems. Its performance was compared against the I-Multi algorithm, a multi-swarm algorithm without probabilistic modeling. The comparison focused on whether C-Multi achieved good results, with specific attention paid to its convergence and diversity on the Pareto front.",
      "chunk_id": 24430,
      "paper_id": "C-Multi  A competent multi-swarm approach for many-objective problems",
      "paper_title": "C-Multi: A competent multi-swarm approach for many-objective problems",
      "paper_year": "2016",
      "chunk_content": "Since, several studies pointed that Pareto based algorithms scale poorly in MaOPs [2-4] because of the increase in the number of non-dominated solutions which deteriorates the selection pressure compromising the convergence to the Pareto front and diversity of the solutions. In a previous work [5] we presented a hybrid algorithm called C-Multi to deal with this challenge. The project of C-Multi is based on two phases: the first uses a unique particle swarm optimization algorithm (PSO) [6] to discover the different regions of the Pareto front. The second phase uses multiple swarms to specialize on a dedicate part. On each swarm, an estimation of distribution algorithm (EDA) [7] is used to focus on convergence to its allocated region. The study featured a comparative study involving the [^0]C-Multi and the I-Multi algorithms using the DTIZ [8] family of benchmark problems. I-Multi is a multi-swarm algorithm that has a similar project to C-Multi but does not incorporate probabilistic modeling to the search as C-Multi does. The result of the comparison was that C-Multi achieved good results in some problems, but performed poorly in general. Here, in this study, our goal is to investigate the following hypothesis: $H_{1}$ the performance of C-Multi can be further improved by an appropriate adjustment of two critical components of the algorithm; the type of archiving method, and the number of swarms.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:24",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of the UMDA on the LeadingOnes problem, specifically concerning the probability of sampling the optimum within a certain number of iterations?",
      "topics": [
        "selection"
      ],
      "answer": "The key takeaway is that, with a probability of at least a specified value derived from union bounds and error probabilities, the UMDA does not sample the optimum of the LeadingOnes problem within the first \\(\\left\\lfloor\\frac{n-\\xi}{d+1}\\right\\rfloor\\) iterations, where \\(n\\) represents the problem size, \\(\\xi\\) relates to the relevant positions, and \\(d\\) is a parameter influencing the iteration count. This result combines upper and lower bound analysis to provide a probabilistic guarantee on the UMDA's performance.",
      "chunk_id": 17870,
      "paper_id": "A simplified run time analysis of the univariate marginal distribution algorithm on LeadingOnes",
      "paper_title": "A simplified run time analysis of the univariate marginal distribution algorithm on LeadingOnes",
      "paper_year": "2021",
      "chunk_content": "We now assume that $n-\\xi \\geq 1$, as Theorem 6 is trivial otherwise. Our claim above then yields that, up to iteration $t^{\\prime}:=\\left\\lfloor\\frac{n-\\xi}{d+1}\\right\\rfloor-1$, with a probability of at least $1-\\frac{n-\\xi}{d+1} n^{-2} \\geq 1-n^{-1}$, each position greater than $1+n-\\xi$ was never selectionrelevant. This means that by Lemma 3, all such frequencies are at most [FORMULA371]. By Lemma 5 with $i=1+n-\\xi$, with a probability of at least $1-\\lambda\\left(\\frac{3}{4}\\right)^{n-i}=1-\\lambda\\left(\\frac{3}{4}\\right)^{\\xi-1}=1-n^{-2}$, the UMDA does not sample the optimum of LeAdingones within a single iteration. Applying a union bound over the first $t^{\\prime}+1 \\leq n$ iterations, with a probability of at least $1-n^{-1}$, the UMDA does not sample the optimum up to iteration $t^{\\prime}$ (which are $t^{\\prime}+1$ iterations). Overall, by a union bound over all error probabilities, with a probability of at least [FORMULA378], the UMDA does not sample the optimum within the first $t^{\\prime}+1=\\left\\lfloor\\frac{n-\\xi}{d+1}\\right\\rfloor$ iterations, which concludes the proof. For the sake of completeness, we state the combined result of our upper and lower bound. [^0] [^0]: ${ }^{1}$ Note that such frequencies are at most [FORMULA381] with a probability of 1 , as we condition on this event throughout the proof, as stated at the beginning of the proof.",
      "question_type": "synthesis summary",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:26",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the calculation of 'influence' in a decentralized continuous EDA, and how does this influence calculation relate to the algorithm's computational complexity?",
      "topics": [
        "complexity"
      ],
      "answer": "In a decentralized continuous EDA, the 'influence' of an individual \\( \\mathbf{x}_{i} \\) is calculated using the formula \\( \\text { influence }\t(\t\\mathbf{x}_{i}\t)\t=\t\\sum_{j \\in B(i)}\t(\\sum_{k} f_{j}\t(\\mathbf{x}_{j}\t)\t/\t k\t) \\), where \\( B(i) \\) represents the neighbors of individual \\( i \\), \\( f_{j}(\\mathbf{x}_{j}) \\) is the loss function value for individual \\( j \\), and \\( k \\) is a parameter not further specified in the provided content. The computational complexity of the decentralized continuous EDA is stated to be similar to that of a centralized continuous EDA in one iteration, assuming they have the same parameters. This assessment considers that while decentralized EDAs require finding individuals and calculating the average of their loss function values, operations needing less computational resource than calculating loss functions, and information transfer between neighbors needing extra operations, the computing resource is distributed on each subsystem. The formula for \\( \\min f_{5}(\\mathbf{x})=x_{51}^{2}+x_{52}^{2}+x_{2}^{2}+x_{4}^{2}+x_{6}^{2}+x_{8}^{2} \\) is given as an example of a loss function calculation in a subsystem.",
      "chunk_id": 17470,
      "paper_id": "A Decentralized Continuous Estimation of Distribution Algorithm for Networked Systems",
      "paper_title": "A Decentralized Continuous Estimation of Distribution Algorithm for Networked Systems",
      "paper_year": "2019",
      "chunk_content": "Then we can use the average loss values of these individuals to represent the influence as shown in (5). $$\n\\text { influence }\\left(\\mathbf{x}_{i}\\right)=\\sum_{j \\in B(i)}\\left(\\sum_{k} f_{j}\\left(\\mathbf{x}_{j}\\right) / k\\right)\n$$ Now we discuss the computational complexity of the decentralized continuous EDA. We ignore the cost of data exchange between neighbors because it depends on the communication network. For each subsystem, we need to calculate $$\n\\min f_{5}(\\mathbf{x})=x_{51}^{2}+x_{52}^{2}+x_{2}^{2}+x_{4}^{2}+x_{6}^{2}+x_{8}^{2}\n$$ loss function values in an iteration. Compared with centralized EDA, we needn't calculate extra loss function values in an iteration. However, we need find some individuals and calculate the average of their loss function values in an iteration. These operations need much less computational resource than calculating loss functions. Meanwhile, the information transfer between neighbors also need extra operations. So the computational complexity of a decentralized continuous EDA and a centralized continuous EDA is similar in an iteration if they have the same parameters. Considering that the computing resource is distributed on each subsystem, the decentralized EDA can perform better. The decentralized continuous EDA also has the advantage of flexibility. That is, if the decentralized system loses a subsystem, we only need change the codes of its neighboring subsystems other than of the whole system. ## 4 Experimental Results Decentralized systems have a variety of topologies. Different topologies may affect the effectiveness of decentralized algorithms.",
      "question_type": "conceptual deep",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:29",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm (EDA) within an Immune Genetic Algorithm (IGA) for gas emission quantity prediction, how should practitioners establish a probability model to generate new populations?",
      "topics": [
        "genetic"
      ],
      "answer": "Practitioners should establish a probability model by analyzing and comparing excellent groups, constructing a distribution probability model of the gas emission quantity prediction characteristic variable. This model is then used to generate new populations in the IGA, iteratively improving the population quality and controlling the convergence direction of the algorithm.",
      "chunk_id": 7112,
      "paper_id": "Research on gas emission quantity prediction model based on EDA-IGA",
      "paper_title": "Research on gas emission quantity prediction model based on EDA-IGA",
      "paper_year": "2023",
      "chunk_content": "Therefore, the Estimation of Distribution Algorithm (EDA)is introduced into the IGA, by establishing a probability model, analyzing and comparing excellent groups, constructing a distribution probability model of the gas emission quantity prediction characteristic variable [6], and generating new populations, and through repeated iterations, the quality of the population is continuously improved. Optimizing updates, thereby controlling the convergence direction and memory properties of the algorithm. Under the optimization of the EDA, through the method of probability distribution, the high-affinity antibodies produced by IGA can be quickly retained and distributed, occupying the space of low-affinity antibodies. Compared with the single use of IGA, the model runs faster speed up, and the direction of convergence is controlled. Based on this, a gas emission quantity prediction model based on the Immune Genetic Algorithm and the Estimation of Distribution Algorithm was established, and a working face with gas emission risk in a coal mine from Shandong Province in China was used as the experimental object to verify the prediction accuracy of the model. It is found that the model has higher prediction accuracy, which shows that the model can be used as an effective prediction method for the prediction of gas emission quantity in coal mines and can be used to guide the engineering practice of coal mines.",
      "question_type": "application",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:30",
      "generation_style": "practical_application"
    },
    {
      "question": "In the described Estimation of Distribution Algorithm, what role does the parameter $\\tau_{i}$, representing the number of iterations where [FORMULA230] remains the same, play in the overall control strategy, and how does it influence the selection between different prediction and mutation strategies?",
      "topics": [
        "control"
      ],
      "answer": "The parameter $\\tau_{i}$ is used to detect changes in the environment. When a change is detected, indicated by $\\tau_{i}$ iterations where [FORMULA230] remains the same, the algorithm executes a change response mechanism. This mechanism calculates a center point $C_{t}$ of [FORMULA238] at time period [FORMULA239] and then employs three sub-strategies: Center Point-based Prediction, Guiding Individual-based Prediction, and Precision Controllable Mutation. If no change is detected based on $\\tau_{i}$, the algorithm uses the reproduction operator of RM-MEDA to generate offspring $P_{off}$ and performs environmental selection using $P \\cup P_{off}$. Therefore, $\\tau_{i}$ effectively acts as a trigger that determines whether the algorithm should adapt to potential environmental changes using specific prediction and mutation operators or continue with the standard RM-MEDA reproduction operator.",
      "chunk_id": 3456,
      "paper_id": "Combining a hybrid prediction strategy and a mutation strategy for dynamic multiobjective optimization",
      "paper_title": "Combining a hybrid prediction strategy and a mutation strategy for dynamic multiobjective optimization",
      "paper_year": "2022",
      "chunk_content": "Input: [FORMULA228] (population size), $\\tau_{i}$ (the number of iterations where [FORMULA230] remainsthe same), [FORMULA231] (the severity of change), the stopping criterion Output: [FORMULA232] 1: Initialize a population [FORMULA233], the discrete time parameter $t \\leftarrow 0$, iteration counter Iter [FORMULA235]; 2: while the stopping criterion is not met do 3: if change is detected then 4: $\\quad t=t+1$; 5: $\\quad C_{t} \\leftarrow$ calulate the center point of [FORMULA238] at time period [FORMULA239]; // the sub-strategies in HPPCM (change response mechanism) are the center point-based prediction strategy (line 6), the guiding individualbased prediction strategy (line 7) and the precision controllable mutation strategy (line 8) 6: $\\quad P_{1} \\leftarrow$ Center Point-based Prediction $\\left(P^{t}:, \\operatorname{dir}, C_{t}, t\\right) ; \\quad / /$ refer to Algorithm-1 $7: \\quad P_{2} \\leftarrow$ Guiding Individual-based Prediction $\\left(P, C_{t}, V_{t}, t\\right) ; \\quad / /$ refer to Algorithm-A2 8: $\\quad P_{3} \\leftarrow$ Precision Controllable Mutation $(P, t) ; \\quad / /$ refer to Algorithm-A3 9: $\\quad P_{\\text {in. }}$ Enice [FORMULA247] BoundaryCorrection $\\left(P_{1} \\cup P_{2} \\cup P_{3}\\right)$ 10: $\\quad P \\leftarrow$ EnvironmentalSelection $\\left(P_{\\text {in. } \\text { bice }} \\cup P\\right) ; \\quad / /$ use the environmental selection operator of RM-MEDA 11: else 12: $\\quad P_{o f f} \\leftarrow$ GetOffspring $(P) ; \\quad / /$ use the reproduction operator of RMMEDA 13: [FORMULA253] EnvironmentalSelection $\\left(P \\cup P_{o f f}\\right) ; \\quad / /$ perform environment selection operation of RM-MEDA 14: end if 15: Iter [FORMULA255] Iter +1 16: end while 17: Return [FORMULA256]; other evolutionary solvers can also be employed.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:32",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What computational trade-offs are described between applying Sequential Quadratic Programming (SQP) only to the best solution after an evolutionary search versus using gradient-based mutation operators on all newly generated individuals?",
      "topics": [
        "optimization"
      ],
      "answer": "Applying SQP only to the best solution after an evolutionary search, as done with the feasible SQP (FSQP) method after simulated annealing (SA) has converged, may not be sufficient to locate the global optimum. In contrast, gradient-based mutation operators applied to all newly generated individuals are noted as not being as efficient as classical approaches like SQP, implying a trade-off where SQP offers potentially better optimization of a single solution but may miss the global optimum, while gradient-based mutation provides broader exploration at the cost of per-individual optimization efficiency.",
      "chunk_id": 14885,
      "paper_id": "A novel memetic algorithm for constrained optimization",
      "paper_title": "A Novel Memetic Algorithm for Constrained Optimization",
      "paper_year": "2010",
      "chunk_content": "The sequential quadratic programming (SQP) technique has been applied to the best individual in every ten generations [32]. In an alternative strategy, the classical optimization approach is only applied to the best individuals after the evolutionary search has finished. For examples, in [10] and [33], the best solution found after a whole EP search is optimized by a deterministic optimization neural network algorithm using a Lagrange multiplier method. In the hybrid simulated annealing (SA) method developed in [34], the best solution found by the SA is improved by the feasible SQP (FSQP) method after the SA has converged. The disadvantage of these prudent strategies is that the application of classical approaches to only one solution may be not sufficient to locate the global optimum. Apart from the application of classical optimization approaches, some work adopts crossover or mutation operators as alternatives to the classical approaches to exploit local areas of the search space. Such operators include the gradient-based mutation operator [31], [35], [11], the simplex crossover [36] and some others. These operators are usually applied to all the newly generated individuals. The problem with these operators is that they are not as efficient as the classical approaches. In this paper, we present a new strategy which can reduce the computational cost in terms of number of fitness evaluations, and increase the possibility of finding the globallyoptimal feasible solutions.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:34",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model, when considering that Type D cannot handle probabilistic dependencies between nodes in a program tree?",
      "topics": [
        "search"
      ],
      "answer": "The text indicates that Type D fails to account for probabilistic dependencies when estimating the global structure of a program. XEDP, unlike Type D, incorporates these dependencies using a conditional probability tree. Therefore, when implementing an EDA model, the algorithmic steps must explicitly include the calculation and utilization of conditional probabilities between variables within the program tree. This involves constructing and traversing the conditional probability tree to guide the search process, something Type D lacks.",
      "chunk_id": 26657,
      "paper_id": "Probabilistic distribution models for EDA-based GP",
      "paper_title": "Probabilistic distribution models for EDA-based GP",
      "paper_year": "2005",
      "chunk_content": "Type D can estimate both global structure and substructures of a program. However, Type D can not deal with probabilistic dependencies between nodes in a program tree when estimating global structure, while XEDP can involve probabilistic dependencies by the conditional probability tree. Search by Type D also failed and therefore the probabilistic dependencies between variables in the conditional probability tree is thought to play an important role in program search.",
      "question_type": "implementation focused",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:36",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How is the initial processing sequence determined when scheduling jobs in the first stage, specifically considering sequence-dependent setup times?",
      "topics": [
        "optimization"
      ],
      "answer": "The initial processing sequence is determined by first identifying the maximum setup time, such as \\$st_{k28}\\, which represents the setup time from Job 2 to Job 8. Job 8, the successor job, is then chosen as the first machining job. Subsequent machining jobs are determined by finding the Rth maximum value from the remaining sequence-dependent setup times (SDST), such as \\$st_{k36}\\. The successor jobs of these setup times, such as Job 6, are selected as the first machining job in the remaining machines of the first stage. This process repeats until all jobs are scheduled, and the job with the lowest setup time is identified as the subsequent machining job.",
      "chunk_id": 8059,
      "paper_id": "An estimation of distribution algorithm with multiple intensification strategies for two-stage hybrid flow-shop scheduling problem with sequence-dependent setup time",
      "paper_title": "An estimation of distribution algorithm with multiple intensification strategies for two-stage hybrid flow-shop scheduling problem with sequence-dependent setup time",
      "paper_year": "2023",
      "chunk_content": "Such as the maximum setup time in Table 3 is setup time from Job 2 to Job 8, i.e. $s t_{k 28}$. Then the successor job of this setup time, which is Job 8 , is chosen as the first machining job. Assuming that there are $C_{i}$ machines in the first stage. In this example, $i, C_{\\text {opt }}$ is 2 . The $R$ th maximum value is found in turn from the remaining SDST, the $s t_{k 36}$ of Table 3 is picked up. The successor jobs of these setup times, Jobs 6, are chosen as the first machining job in the remaining machines of the first stage. According to the above process, the first machining jobs in the machines of the first stage are determined. The job with the lowest setup time is identified as the subsequent machining job. This operation is repeated until all jobs are scheduled for processing in the first stage. The final processing sequence is obtained, the result of Table 3 is $\\{8,6,4,10,2,7,3,9,5,1\\}$.  2.",
      "question_type": "relationship",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:37",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of considering conflicting objectives, such as profit maximization and cost minimization, when formulating objective functions for order acceptance problems solved using Estimation of Distribution Algorithms (EDAs)?",
      "topics": [
        "objectives"
      ],
      "answer": "The overall purpose is to efficiently select orders by aligning objective functions with manufacturing objectives, recognizing that maximizing capacity utilization does not always equate to maximum profit. By considering both profit maximization (accepting perspectives) and cost minimization (rejecting perspectives), manufacturers can balance overtime (OT) and outsourcing costs while retaining profit levels, even when capacity constraints limit order support. The EDA's objective function should reflect this balance to optimize order selection.",
      "chunk_id": 972,
      "paper_id": "Overtime capacity expansion in order acceptance with node based estimation of distribution algorithms",
      "paper_title": "Overtime Capacity Expansion in Order Acceptance with Node Based Estimation of Distribution Algorithms",
      "paper_year": "2014",
      "chunk_content": "Effective OA appears to be one of the most important key performance indicators as it reflects the incoming profits, and determines costs of the future manufacturing processes. Recent papers [2][3] proposed to include in-house OT and outsourcing as additional capacity into OA problems in order to gain more profits. They suggested that maximizing capacity utilization does not always result in maximum profit. Conversely, from the maximum profit, manufacturers can compensate the cost of OT and outsourcing while retaining the profits level. In order to efficiently select orders, using the appropriate objective functions is important. The objective functions of OA should be consistent with the manufacturing objectives. However, these objectives can be conflicted to each other. The objective functions in OA can be divided into two points of view. From the accepting perspectives, the main objectives rather depend on the maximization of profits. On the other hand, from the rejecting perspectives, the main objectives rather depend on the minimization of costs. Profits maximization can reflect the costs minimization. For example, minimizing make span can result in increasing of capacities utilization. Practically, it is not necessary to calculate both profits and costs consecutively. However, in many situations, manufacturers could not raise the capacity level to support more orders to gain more profits.",
      "question_type": "definition",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:39",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of using minimum entropy as a performance index differ from using mixture correntropy, specifically regarding the termination conditions of the cycle?",
      "topics": [
        "performance"
      ],
      "answer": "Within the described EDA framework, both minimum entropy and mixture correntropy are used as performance indices to determine when the algorithm should terminate its cycle. For the minimum entropy performance index, the cycle stops when the difference between two adjacent rational entropies is less than 0.001. In contrast, for the mixture correntropy performance index, the cycle terminates when the difference between two adjacent mixture correntropies is less than 0.0001, or when the mixture correntropy reaches its maximum value of 1.",
      "chunk_id": 16117,
      "paper_id": "Performance Assessment of Non-Gaussian Control Systems Based on Mixture Correntropy",
      "paper_title": "Performance Assessment of Non-Gaussian Control Systems Based on Mixture Correntropy",
      "paper_year": "2019",
      "chunk_content": "In the EDA, the initial population is 1000 groups of data, and the probability model of the parameters is established. For the minimum entropy performance index, the termination condition of the cycle is that the difference between two adjacent rational entropies is less than 0.001 . For the performance index of mixture correntropy, the cycle termination condition is that the difference between two adjacent mixture correntropies is less than 0.0001 , or the mixture correntropy reaches the maximum value 1 . To improve the accuracy and speed of the algorithm, the RELS algorithm is chosen to determine the approximated range of the initial population. The following Algorithm 1 is the program steps for performance evaluation. ``` Algorithm 1 Program steps for the performance evaluation 1: Use the CARMA model to represent the system; estimate the delay \\(\\tau\\) by analyzing the correlation between \\(u(t)\\) and \\(y(t)\\); use the Akaike information criterion to obtain the order of the model \\(\\left(n{a}, n{b}, n{c}\\right)\\). 2: \\(a{n}, b{n}\\), and \\(c{n}\\) in Equation (2) and the estimation of noise variance \\(\\sigma_{v}\\) are obtained by combining data in step 1 with RELS.",
      "question_type": "comparison",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:40",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners generate a new population, $D_l$, given a probability distribution $p_l(\\mathbf{x})$?",
      "topics": [
        "search"
      ],
      "answer": "To generate the new population $D_l$ in an EDA, sample $N$ individuals from the probability distribution $p_l(\\mathbf{x})$. Each individual, $x_i$, is defined such that if the $i$-th predictor variable is selected, $x_i = 0$, otherwise $x_i = 1$. This sampling process replaces genetic crossover and mutation operators, using the induced probabilistic model to generate new solutions.",
      "chunk_id": 12458,
      "paper_id": "Prototype Selection and Feature Subset Selection by Estimation of Distribution Algorithms. A Case Study in the Survival of Cirrhotic Patients Treated with TIPS",
      "paper_title": "Prototype selection and feature subset selection by estimation of distribution algorithms.: A case study in the survival of cirrhotic patients treated with TIPS",
      "paper_year": "2001",
      "chunk_content": "EDA $D_{0} \\leftarrow$ Generate $N$ individuals (the initial population) randomly. Repeat for $l=1,2, \\ldots$ until a stop criterion is met. $D_{l-1}^{s} \\leftarrow$ Select $S \\leq N$ individuals from $D_{l-1}$ according to a selection method. $p_{l}(\\mathbf{x})=p\\left(\\mathbf{x} \\mid D_{l-1}^{s}\\right) \\leftarrow$ Estimate the joint probability distribution of an individual being among the selected inviduals. $D_{l} \\leftarrow$ Sample $$\nx_{i}=\\left\\{\\begin{array}{l}\n0 \\text { iff the } i \\text {-th predictor variable is selected } \\\\\n1 \\text { in other case }\n\\end{array}\\right.\n$$ individuals (the new population) from $p_{l}(\\mathbf{x})$. Fig. 2. Main scheme of the EDA approach. In this way, a randomized, evolutionary, population-based search can be performed using probabilistic information to guide the search. In this way, both approaches (GAs and EDAS) do the same except that EDAs replaces genetic crossover and mutation operators by means of the following two steps: 1. a probabilistic model of selected promising solutions is induced, 2. new solutions are generated according to the induced model. The main problem of EDA resides on how the probability distribution $$\nx_{i}=\\left\\{\\begin{array}{l}\n0 \\text { iff the } i-n \\text {-th case is selected } \\\\\n1 \\text { in other case }\n\\end{array}\\right.\n$$ is estimated. Obviously, the computation of $2^{n}$ probabilities (for a domain with [FORMULA39] binary variables) is impractical. This has led to several approximations where the probability distribution is assumed to factorize according to a probability model (see Larranaga et al.",
      "question_type": "process",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:42",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the computational complexity of the specific ELALO algorithm analyzed, considering that in the original ALO, the population size is denoted as $d$, the individual dimension as $D$, and the maximum number of iterations as $\\theta$?",
      "topics": [
        "convergence"
      ],
      "answer": "The provided text states that in ALO, the population size is $d$, the individual dimension is $D$, and the maximum number of iterations is $\\theta$. It also states that time complexity is a key factor of algorithm performance. However, the text does not provide a specific time complexity analysis or formula for ELALO or ALO in terms of $d$, $D$, and $\\theta$. The text only mentions that time complexity reflects the operation efficiency of the algorithm and that ELALO has a good convergence.",
      "chunk_id": 4468,
      "paper_id": "Bearing Fault Classification Using Improved Antlion Optimizer and Extreme Learning Machine",
      "paper_title": "Bearing Fault Classification Using Improved Antlion Optimizer and Extreme Learning Machine",
      "paper_year": "2022",
      "chunk_content": "In Figure 6, the abscissa is the number of iterations of the algorithm, and the maximum number of iterations is 2000 generations. In order to make a clear distinction between the algorithm iteration curves, the logarithm of the fitness value with base 10 is taken as the ordinate. The optimization accuracy of ELALO has been significantly improved. When the algorithm reaches the same accuracy in the early stage, Table 2: Comparison results in four benchmark functions. [IMAGE_5] Figure 6: Comparison of convergence curve. (a) Curve comparison on $F_{1}$, (b) curve comparison on $$\n\\bar{\\xi}=H^{*} Y\n$$, (c) curve comparison on $H^{*}$, and (d) curve comparison on $k$. the convergence speed of the ELALO is also higher than the initial ALO. Hence, one can see that the proposed ELALO algorithm has a good convergence. 2.4. Time Complexity Analysis. Time complexity is the key factor of algorithm performance and reflects the operation efficiency of the algorithm. In ALO, the population size is $d$, the individual dimension is $D$, and the maximum number of iterations is $\\theta$.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:43",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the selection pressure in an Estimation of Distribution Algorithm differ between subproblems with evenly distributed weights versus those with unevenly distributed weights, based on the weight vector W?",
      "topics": [
        "selection"
      ],
      "answer": "Subproblems with evenly distributed weights, represented by the weight vector W, maintain a relatively large selection pressure but an inferior diversity. Conversely, subproblems with unevenly distributed weights exhibit the opposite characteristic: a relatively small selection pressure and a superior diversity. This is based on the equation relating \\theta, w1i, and the sum of w1j^2.",
      "chunk_id": 16908,
      "paper_id": "Dynamic deployment optimization of near space communication system using a novel estimation of distribution algorithm",
      "paper_title": "Dynamic deployment optimization of near space communication system using a novel estimation of distribution algorithm",
      "paper_year": "2019",
      "chunk_content": "(15) and (16), we have: $F_{d}\\left(F_{1}\\right)<F_{d}\\left(F_{2}\\right)$ which contradicts Eq. (14). The proof is completed. Given a reference point $Z=\\left(z_{1}, \\ldots, z_{N}\\right)$, the current and its equivalent solution to a certain subproblem are $F_{1}=\\left(f_{11}, \\ldots, f_{1 N}\\right)$ and $F_{2}=\\left(f_{21}, \\ldots, f_{2 N}\\right)$, respectively and $\\left\\{\\begin{array}{l}F_{3}=\\left(f_{31}, \\ldots, f_{3 i}, \\ldots, f_{3 k}, \\ldots, f_{3 j}, \\ldots, f_{3 N}\\right) \\\\ f_{3 i}=\\xi f_{1 i} \\\\ f_{3 j}=\\zeta\\left(\\frac{\\left(1-\\xi f_{1 i}\\right.}{w_{2}}+f_{1 j}\\right) \\\\ f_{3 k}=f_{1 k}, k \\in\\{1, \\ldots, N\\} \\backslash\\{i, j\\}\\end{array}\\right.$ is the number of objectives. The projection of $\\xi, \\zeta \\sim(0,1), F_{3}$ on vector $\\overrightarrow{F_{1} Z}$ is $F_{3}=\\left(f_{31}, \\ldots, f_{3 N}\\right)$. The PBI decomposition with penalty factor $\\theta$ can be described as follows: $\\theta\\left|\\overrightarrow{F_{2} F_{3}}\\right|+\\left|\\overrightarrow{F_{3} Z}\\right|=\\left|\\overrightarrow{F_{1} Z}\\right|$ Then the angle between $$\n\\left\\{\\begin{array}{l}\n\\left|\\left(z_{i}-f_{3 i}\\right) w_{i}\\right|>\\left|\\left(z_{i}-f_{1 i}\\right) w_{i}\\right| \\\\\n3 k \\in 1, \\ldots, N,\\left|\\left(z_{k}-f_{3 k}\\right) w_{k}\\right|<\\left|\\left(z_{k}-f_{1 k}\\right) w_{k}\\right|\n\\end{array}\\right.\n$$ and $\\overrightarrow{F_{1} F_{2}}$ is: $\\left(\\overrightarrow{F_{1} Z}, \\overrightarrow{F_{1} F_{2}}\\right)=\\arctan \\frac{1}{\\theta}$ Given the unit vector $X_{i}=(0, \\ldots, 1, \\ldots, 0)$ along axis $N^{2}$, we have: $\\left\\langle X_{i}, \\overrightarrow{F_{1} Z}\\right\\rangle=\\arccos \\frac{f_{1 i}-z_{i}}{\\sqrt{\\sum_{j=1}^{N}\\left(f_{1 j}-z_{j}\\right)^{2}}}=\\arccos \\frac{w_{1 i}}{\\sqrt{\\sum_{j=1}^{N}\\left(w_{1 j}\\right)^{2}}}$ To investigate the selection pressure of the $\\frac{N^{2}}{2}-1$ th objective, the following equation is solved: $\\arctan \\frac{1}{\\theta}=\\arccos \\frac{w_{1 i}}{\\sqrt{\\sum_{j=1}^{N}\\left(w_{1 j}\\right)^{2}}}$ And we have: $$\n\\left\\{\\begin{array}{l}\n\\theta=\\frac{w_{1 i}}{\\sqrt{\\sum_{1, j, j e i}^{N}\\left(w_{1 j}^{2}\\right)}}>\\frac{w_{1 i}^{\\prime}}{\\sqrt{\\sum_{1, j, j e i}^{N}\\left(w_{1 j}^{2}\\right)}} \\\\\nw_{1 i}^{\\prime}=\\epsilon w_{1 i}, \\epsilon \\sim(0,1), i \\neq j \\\\\nw_{1 j}^{\\prime}=w_{1 j}, 3 k \\in\\{1, \\ldots, N\\}, k \\neq j \\neq i, w_{1 k}^{\\prime}=w_{1 k}+(1-\\epsilon) w_{1 i}\n\\end{array}\\right.\n$$ where $W_{h}=\\left(w_{h 1}, \\ldots, w_{h N}\\right)$ is the weight vector of subproblem $h$. According to Eq. (22), the selection pressure differs from different objectives. The subproblems with evenly distributed weights maintain a relatively large selection pressure and an inferior diversity, and it is the opposite for those with unevenly distributed weights.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:45",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the algorithmic steps involved in augmenting the population size within the EDA when convergence is not achieved, as described in the context of minimizing B-functions?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "convergence",
        "population size",
        "B-functions"
      ],
      "answer": "When a certain Estimation of Distribution Algorithm does not converge during the minimization of B-functions, the population size is iteratively augmented. The initial population size is 100 individuals. If convergence is not reached, the population is increased by increments of 100, up to a maximum of 500 individuals. For the high entropy B-function, further augmentation is performed to population sizes of 1000, 2000, 4000, and 8000 if necessary.",
      "chunk_id": 21694,
      "paper_id": "Evaluating the Max-Min Hill-Climbing Estimation of Distribution Algorithm on B-Functions",
      "paper_title": "Evaluating the Max-Min Hill-Climbing Estimation of Distribution Algorithm on B-Functions",
      "paper_year": "2018",
      "chunk_content": "The aim of the experiment was to find the critical (minimum) population size that guarantees $100 \\%$ of success in 30 runs. We used the bisection method to compute this critical value. Figure 1 shows the average number of function evaluations for the critical population size found. From this figure is easy to conclude that in this problem, MMHCEDA scales well with the number of variables. [IMAGE_0] Fig. 1. Scalability of the $C B E D A_{M M H C}$ for the function [FORMULA78]. ### 3.3 Minimization of BF2B30s4-2312 and BF2B30s4-1245 In this experiment two B-functions, one with low (BF2B30s4-2312) and another with high univariate entropies (BF2B30s4-1245) were chosen. Later on, the percentage of convergence of each algorithm was obtained for a population size of 100 individuals. When a certain algorithm didn't converge [FORMULA79], its population was augmented with 100 individuals until arriving to a population of 500 . Note that with the high entropy Bfunction were necessary population sizes of 1000, 2000, 4000 and 8000.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:46",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of optimizing the Permutation Flowshop Scheduling Problem (PFSP) using Estimation of Distribution Algorithms (EDAs), how is the makespan objective function, denoted as \\(F(\\sigma)\\), specifically calculated to evaluate the performance of a given permutation \\(\\sigma\\)?",
      "topics": [
        "optimization"
      ],
      "answer": "The makespan objective function, \\(F(\\sigma)\\), for the Permutation Flowshop Scheduling Problem (PFSP) is calculated as \\(C_{\\sigma_{n}, m}\\), which represents the finishing time of the last task \\(\\sigma_{n}\\) on the last machine \\(m\\). This finishing time depends on the processing times of all preceding tasks \\(\\sigma_{1}\\) through \\(\\sigma_{n-1}\\). Therefore, minimizing \\(F(\\sigma) = C_{\\sigma_{n}, m}\\) involves finding a permutation \\(\\sigma\\) that minimizes the completion time of the final task on the final machine, considering the dependencies imposed by the processing order on all machines.",
      "chunk_id": 3492,
      "paper_id": "Introducing Generative adversarial networks on Estimation of distribution algorithm to solve permutation-based problems",
      "paper_title": "Introducing Generative adversarial networks on Estimation of distribution algorithm to solve permutation-based problems",
      "paper_year": "2022",
      "chunk_content": "This is due to the fact that $2 n$ distinct permutations can represent each tour for symmetric instances, whereas $m$ distinct permutations can represent each tour for asymmetric instances. For example, $\\sigma_{1}=\\{1,3,2,4,5\\}$ represents the same solution of $\\sigma_{2}=\\{5,4,2,3,1\\}$, because $f\\left(\\sigma_{1}\\right)=f\\left(\\sigma_{2}\\right)$. 2) Permutation Flowshop Scheduling Problem (PFSP): The PFSP is a problem of scheduling $$\nF(\\sigma)=C_{\\sigma_{n}, m}\n$$ tasks on a set of $m$ machines. Each task is composed of $\\sigma_{n}$ operations, the $j^{\\text {th }}$ operation must be executed by machine $$\nF(\\sigma)=\\sum_{i=1}^{n} C_{\\sigma_{i}, m}\n$$. A job can take her turn on the machine $(n * n)$, if the machine $\\pi(2,3,1,4)$ is available, and its $j-1^{\\text {th }}$ operation has been completely processed by machine $j-1$. The objective of the optimization of this problem is to find a permutation of jobs that minimizes the performing time of all $\\leftarrow$ jobs, it means the solution is represented by a permutation of length $N<M$, and this order is the same in all machines. The minimization of the makespan objective function can be formulated as follows: $$\nF(\\sigma)=C_{\\sigma_{n}, m}\n$$ In equation above, $C_{\\sigma_{n}, m}$ signifies the finishing time of last task $\\sigma_{n}$ on last machine $L \\in R^{N * N}$, which depends on processing time of previous jobs $\\sigma_{1}, \\ldots, \\sigma_{n-1}$.",
      "question_type": "theory",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:48",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs for bio-control in mushroom farming?",
      "topics": [
        "search"
      ],
      "answer": "The primary objective is to apply an Estimation of Distribution Algorithm (EDA) that uses a Markov network probabilistic model, specifically the Distribution Estimation Using Markov networks (DEUM${_d}$), to the problem of bio-control in mushroom farming, which involves multiobjective optimization with a weighted fitness function and bang-bang-control solutions; the approach is compared to genetic algorithms (GA) with directed intervention crossover schemes.",
      "chunk_id": 11582,
      "paper_id": "Bio-control in mushroom farming using a Markov network EDA",
      "paper_title": "Bio-control in Mushroom Farming using a Markov Network EDA",
      "paper_year": "2008",
      "chunk_content": "Bio-control in Mushroom Farming using a Markov Network EDA Yanghui Wu, John McCall, Paul Godley, Alexander Brownlee, David Cairns and Julie Cowie #### Abstract In this paper we present an application of an Estimation of Distribution Algorithm (EDA) that uses a Markov network probabilistic model. The application is to the problem of bio-control in mushroom farming, a domain which admits bang-bang-control solutions. The problem is multiobjective and uses a weighted fitness function. Previous work on this problem has applied genetic algorithms (GA) with directed intervention crossover schemes aimed at effective biocontrol at an efficient level of intervention. Here we compare these approaches with the EDA Distribution Estimation Using Markov networks (DEUM $\\mathbf{M}_{\\mathbf{d}}$ ). DEUM ${ }_{\\mathrm{d}}$ constructs a probabilistic model using Markov networks. Our experiments compare the quality of solutions produced by DEUM ${ }_{d}$ with the GA approaches and also reveal interesting differences in the search dynamics that have implications for algorithm design. ## I. INTRODUCTION ESTIMATION of Distribution Algorithms (EDA) are a sub-class of Evolutionary Algorithms that use information gained from a population of solutions to construct a probabilistic model.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:50",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of Estimation of Distribution Algorithms (EDAs) for control loop performance assessment in non-Gaussian systems, what mathematical principles govern the use of probability density functions (PDFs) and rational entropy (RE) to describe the characteristics and uncertainty of random variables when estimating system parameters and noise PDF?",
      "topics": [
        "search"
      ],
      "answer": "Within the framework described, the Estimation of Distribution Algorithm (EDA) utilizes a probability density function (PDF) to represent the characteristics of random variables within the non-Gaussian system. Simultaneously, rational entropy (RE) is employed to quantify the uncertainty associated with these random variables. The improved EDA algorithm leverages these two components, PDF and RE, to estimate both the system parameters and the noise PDF, aiming to enhance the performance benchmark estimation in control loop performance assessment. The effectiveness of this approach is demonstrated through case studies on an ARMAX system.",
      "chunk_id": 21205,
      "paper_id": "Non-Gaussian Systems Control Performance Assessment Based on Rational Entropy",
      "paper_title": "Non-Gaussian Systems Control Performance Assessment Based on Rational Entropy",
      "paper_year": "2018",
      "chunk_content": "In this method, probability density function (PDF) and rational entropy (RE) are respectively used to describe the characteristics and the uncertainty of random variables. To better estimate the performance benchmark, an improved EDA algorithm, which is used to estimate the system parameters and noise PDF, is given. The effectiveness of the proposed method is illustrated through case studies on an ARMAX system. Keywords: Control loop Performance Assessment; non-Gaussian system; estimation of distribution algorithm (EDA); rational entropy ## 1. Introduction Poor control loop performance will reduce the effectiveness of the control loop, which may lead to product quality degradation, increase product costs and other issues. There are many factors in the chemical process, which include inadequate parameter tuning and maintained controllers, equipment failure, without or insufficient feedforward compensation, inappropriate control structure design and so on, that can affect control loop performance [1]. The purpose of Control loop Performance Assessment (CPA) is to provide a comprehensive health assessment framework for control loops. Such framework includes assessing, detecting and diagnosing as well as suggesting improvement measures [2]. Harris (1989) proposed a performance indicator based on minimum variance control (MVC) [3], which laid the foundation for the development of CPA research field.",
      "question_type": "conceptual deep",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:51",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm using Backward Scheduling based on Insertion Policy (BSIP), how should practitioners calculate the actual start and finish times of a task $\\tau_{i}^{s}$ and $\\tau_{i}^{f}$ relative to the makespan?",
      "topics": [
        "scheduling",
        "Estimation of Distribution Algorithms"
      ],
      "answer": "When using BSIP, the actual start time of task $i$, denoted as $\\tau_{i}^{s}$, is calculated as $\\max \\{\\tau_{1}^{f}, \\ldots, \\tau_{I}^{f}\\} - \\tau_{i}^{f}$, where $\\max \\{\\tau_{1}^{f}, \\ldots, \\tau_{I}^{f}\\}$ represents the makespan (the maximum finish time among all tasks from 1 to $I$). Similarly, the actual finish time of task $i$, denoted as $\\tau_{i}^{f}$, is calculated as $\\max \\{\\tau_{1}^{f}, \\ldots, \\tau_{I}^{f}\\} - \\tau_{i}^{s}$. This reverses the scheduling direction, starting from the latest possible time.",
      "chunk_id": 8453,
      "paper_id": "A Two-Stage Estimation of Distribution Algorithm With Heuristics for Energy-Aware Cloud Workflow Scheduling",
      "paper_title": "A Two-Stage Estimation of Distribution Algorithm With Heuristics for Energy-Aware Cloud Workflow Scheduling",
      "paper_year": "2023",
      "chunk_content": "In addition, tasks can be scheduled forward or backward with respect to the TSO. In the Forward Scheduling based on Insertion Policy (FSIP) only those tasks without unscheduled parent tasks can be scheduled to start as early as possible, starting from a task without parent tasks. Whereas, in the Backward Scheduling based on Insertion Policy (BSIP) only those tasks without unscheduled child tasks can be scheduled to start as late as possible, starting from a task without child tasks. Let $\\tau_{i}^{s}$ and $\\tau_{i}^{f}$ be the start time and finish time of $c_{1}^{h t}=10, c_{1}^{v m}=2, c_{2}^{v m}=4$. Thus, the ready time of $c_{3}^{v m}=4$, denoted by $\\tau_{i}^{r}$, is the earliest time at which $0.4,0.6,0.8,1.0$ can be executed without considering resource constraints, and can be is calculated in the FSIP and BSIP respectively by (6) and (7). $$\n\\begin{aligned}\n& \\tau_{i}^{r}=\\left\\{\\begin{array}{cc}\n0 & P R_{i}=\\emptyset \\\\\n\\max _{\\vec{i} \\in P R_{i}}\\left\\{\\tau_{i}^{f}\\right\\} & P R_{i} \\neq \\emptyset\n\\end{array}\\right. \\\\\n& \\tau_{i}^{r}=\\left\\{\\begin{array}{cc}\n0 & S C_{i}=\\emptyset \\\\\n\\max _{\\vec{i} \\in S C_{i}}\\left\\{\\tau_{\\vec{i}}^{f}\\right\\} & S C_{i} \\neq \\emptyset\n\\end{array}\\right.\n\\end{aligned}\n$$ Let $I T L_{j}=\\left[u_{j}^{1}, v_{j}^{1}\\right] \\cup \\cdots \\cup\\left[u_{j}^{Q_{j}}, v_{j}^{Q_{j}}\\right]=\\cup_{q=1}^{Q_{j}}\\left[u_{j}^{q}, v_{j}^{q}\\right]$ be the idle time-slot lists of $p_{1}(0)=15.9, p_{1}(0.2)=22.4, p_{1}(0.4)=27.2, p_{1}(0.6)=33.0$ that is its current available time lists for tasks, where $u_{j}^{1}<v_{j}^{1}<\\cdots<u_{j}^{Q_{j}}<v_{j}^{Q_{j}}=\\infty,\\left[u_{j}^{q}, v_{j}^{q}\\right]$ is the $q$ th idle time-slot and $Q_{j}$ is the number of idle time-slots in $I T L_{j}$. The VM available time for $4 \\mathrm{Gbit} / \\mathrm{s}$, denoted by $\\tau_{i}^{\\text {in }}$, is the start time of the earliest idle time-slot which can be used to execute $2, b_{2}^{v m}=4, b_{3}^{v m}=4$, and can be calculated by: $$\n\\begin{aligned}\n& \\tau_{i}^{a}=\\min \\left\\{u_{v i d_{i}}^{q} \\mid v_{v i d_{i}}^{q}-\\max \\left\\{u_{v i d_{i}}^{q}, \\tau_{i}^{r}\\right\\}\\right. \\\\\n& \\left.\\geq e t_{i}, q=1, \\ldots, Q_{v i d_{i}}\\right\\}\n\\end{aligned}\n$$ Thus, the start time and the finish time of $v m_{2}, v m_{1}, v m_{3}, v m_{2}$ can be calculated by (9) and (10). $$\n\\begin{aligned}\n& \\tau_{i}^{s}=\\max \\left\\{\\tau_{i}^{r}, \\tau_{i}^{a}\\right\\} \\\\\n& \\tau_{i}^{f}=\\tau_{i}^{s}+e t_{i}\n\\end{aligned}\n$$ It should be noted that the actual start and finish time of $\\tau_{1}^{s}=0, \\tau_{1}^{I}=27, \\tau_{2}^{s}=27$ are $\\max \\left\\{\\tau_{1}^{f}, \\ldots, \\tau_{I}^{f}\\right\\}-\\tau_{i}^{f}$ and $\\max \\left\\{\\tau_{1}^{f}, \\ldots, \\tau_{I}^{f}\\right\\}-\\tau_{i}^{s}$ respectively when the BSIP is used.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:54",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, what convergence guarantees are established if the selection method is elitist, and how does this affect the population's composition?",
      "topics": [
        "selection"
      ],
      "answer": "If the selection is elitist, the algorithm converges to a population that contains the global optimum because the global optimum will never be lost once it is found. This ensures that the best solution encountered so far is always retained in the population, guaranteeing convergence to the optimal solution.",
      "chunk_id": 13877,
      "paper_id": "High-order EDA",
      "paper_title": "HIGH-ORDER EDA",
      "paper_year": "2009",
      "chunk_content": "The probability of going from population $D_{p}$ (at step $X=\\bigcup_{i=1}^{l} S_{i}$ ) and $D_{q}$ (at step $d_{0}=\\phi$ ) to $D_{v}$ at step $$\n\\begin{aligned}\n& b_{i} \\neq \\phi, \\quad \\forall i=1, \\cdots l ; \\quad d_{i}=X \\\\\n& \\forall i \\geq 2, \\exists j<i \\text { such that } c_{i} \\subseteq S_{j}\n\\end{aligned}\n$$ is given by $P\\left(D_{c} \\mid D_{p} D_{q}\\right)=$ $\\sum_{D_{p}^{+} D_{q}^{+}} P^{x}\\left(D_{p}^{u d} D_{q}^{u d}\\right) \\frac{M!}{d_{c 1}!\\cdots d_{c m}!} \\prod_{n=1}^{m} \\frac{\\left(\\lambda P^{x}\\left(x^{(i)}\\right)+\\left(1-\\lambda\\right) P^{q}\\left(x^{(i)}\\right)\\right)^{d_{c}}}{c m}$ $>0$ where $P^{x}\\left(D_{p}^{u d} D_{q}^{u d}\\right)$ is the probability to select $D_{p}^{u d} D_{q}^{u d}$ from $k$ and $D_{q} . P(x, t-1)$ coincides with some $P^{p}\\left(x^{i}\\right)$ and $P(x, t-2)$ coincides with some $P^{q}\\left(x^{i}\\right)$. Hence the Markov chain is irreducible, i.e., all the states are intercommunicated. And the chain will visit $$\n\\text { Eg3 } \\max \\sum_{i=1}^{n} x_{i}\n$$ infinitely often with probability 1 . If the selection is elitist, then the global optimum will never be lost when it is found. Therefore the algorithm converges to a population that contains the global optimum. # 3.2. Convergence speed Here we again use OneMax problem to show the relation between $$\n\\text { s.t. } 2 \\leq x_{2 j-1}^{2}+x_{2 j}^{2}+x_{2 j+1}^{2} \\leq 8\n$$ and convergence speed of $n=2 m+1, x_{i} \\in\\{0, \\pm 1, \\pm 2\\} \\quad i=1, \\cdots n \\quad j=1, \\cdots, m$-order EDA. Because the variables are independent with each other and the initial value is identical, we can give the following assumption without loss of generality $$\n\\begin{aligned}\n& P\\left(x_{i}=1, t\\right) \\equiv p(t), \\quad P\\left(x_{i}=0, t\\right) \\equiv q(t) \\\\\n& P^{x}\\left(x_{i}=1, t\\right) \\equiv r(t) \\quad i=1, \\ldots, n\n\\end{aligned}\n$$ Obviously $$\np(t)+q(t) \\equiv 1\n$$ After the Roulette wheel selection, $$\nr(t)=p(t)+\\frac{1-p(t)}{n}=\\frac{1}{n}+\\left(1-\\frac{1}{n}\\right) p(t)\n$$ So $$\n\\begin{aligned}\n& p(t)=\\lambda r(t-1)+(1-\\lambda) r(t-2) \\\\\n& =\\lambda\\left[\\frac{1}{n}+\\frac{1}{n} p(t-1)\\right]+(1-\\lambda)\\left[\\frac{1}{n}+\\frac{1}{n} p(t-2)\\right] \\\\\n& =\\frac{1}{n}+\\left(1-\\frac{1}{n}\\right)[\\lambda p(t-1)+(1-\\lambda) p(t-2)] \\\\\n& q(t)=\\left(1-\\frac{1}{n}\\right)[\\lambda q(t-1)+(1-\\lambda) q(t-2)]\n\\end{aligned}\n$$ According to the theory of difference equation, we have $$\nq(t)=c_{1} x_{1}^{t}+c_{2} x_{2}^{t}\n$$ where $c_{1}, c_{2}$ are constants and $$\nx_{1}=\\frac{\\left(1-\\frac{1}{n}\\right) \\lambda+\\sqrt{\\left(1-\\frac{1}{n}\\right)^{2} \\lambda^{2}+4\\left(1-\\frac{1}{n}\\right)(1-\\lambda)}}{2}>0\n$$ $$\nx_{2}=\\frac{\\left(1-\\frac{1}{n}\\right) \\lambda-\\sqrt{\\left(1-\\frac{1}{n}\\right)^{2} \\lambda^{2}+4\\left(1-\\frac{1}{n}\\right)(1-\\lambda)}}{2} \\leq 0\n$$ Obviously $\\left|x_{2}\\right| \\ll|x_{1}|<1$ and $\\lim _{|x_{1}| \\rightarrow|x_{0}|} q(t)=0$.",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:55",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of the multi-objective univariate marginal probability algorithm (MOUMDA) compare to the multi-objective population based incremental learning (MOPBIL) in the context of binary representation EDAs, based on the metrics provided?",
      "topics": [
        "convergence"
      ],
      "answer": "The provided text does not offer a direct comparison of performance metrics between MOUMDA and MOPBIL. It only identifies them as multi-objective EDAs in binary representation, alongside the multi-objective restricted Boltzmann machine (MORBM). Therefore, a specific performance comparison based on metrics cannot be derived from the text.",
      "chunk_id": 23275,
      "paper_id": "Evolutionary algorithms for solving multi-objective travelling salesman problem",
      "paper_title": "Evolutionary algorithms for solving multi-objective travelling salesman problem",
      "paper_year": "2011",
      "chunk_content": "The algorithm adapted guided mutation to enhance the exploitation capability of the algorithm. In another study (Lin 2009), a new EDA based on maximum entropy was proposed to deal with the job shop scheduling problem. In Xu et al. (2009), EDA was used to estimate the distribution of the state transitions and the global updating rules of ant colony system. In this case, EDA is not applied directly to deal with the TSP; but acted as an optimization tool to update the parameter in the ant colony system. Most of the mentioned studies carried out the performance comparison between EDA and GA, and the experimental results indicated that the performance of EDA, when hybridized with local search, is better than GA in terms of solution quality and convergence speed. While a great deal of efforts was being put in to study the single-objective permutation-based problems (specifically the TSP) using EDAs, there is still no research which studies multi-objective permutation-based problems (specifically the MOTSP) using EDAs. In this paper, three of the MOEDAs in binaryrepresentation, namely multi-objective univariate marginal probability algorithm (MOUMDA) (Muhlenbein and Paass 1996), multi-objective population based incremental learning (MOPBIL) (Baluja 1994), and multi-objective restricted Boltzmann machine (MORBM) (Tang et al.",
      "question_type": "comparison",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:58:56",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model, specifically focusing on how the critical difference (CD) is calculated and used in a post-hoc test to determine the significance between various algorithms' performance?",
      "topics": [
        "convergence",
        "Estimation of Distribution Algorithms"
      ],
      "answer": "The provided text mentions the introduction of a critical difference (CD) via a post-hoc test to address the unknown degree of significance between various algorithms, which arises as a disadvantage of the Friedman test. However, the precise algorithmic steps for calculating the CD and its integration within the post-hoc test are not detailed. The text only states that CD is introduced to determine the significance of the difference between algorithms, implying that it would involve statistical calculations based on the Friedman test results and sample sizes to establish a threshold for statistically significant differences in performance, considering the mean ranks obtained for each algorithm.",
      "chunk_id": 8161,
      "paper_id": "A co-evolutionary migrating birds optimization algorithm based on online learning policy gradient",
      "paper_title": "A co-evolutionary migrating birds optimization algorithm based on online learning policy gradient",
      "paper_year": "2023",
      "chunk_content": "The algorithm results are verified by both statistical analyses within the [FORMULA230] and $90 \\%$ confidence interval, where CMBO-PG is the control algorithm. The mean rank of CMBO-PG is smaller than the comparison algorithms (except for 10D), which means the performance of CMBO-PG is better than other algorithms. The disadvantage of the Friedman test is that the degree of significance is still unknown although obtained the difference between various algorithms. In this regard, a critical difference $\\langle C D\\rangle$ is introduced via a post-hoc test called the Table 4 Mean and Standard Deviation of CMBO-PG and Comparison Algorithm on CEC 17 Benchmark Test Suite (10D). Table 5 Mean and Standard Deviation of CMBO-PG and Comparison Algorithm on CEC 17 Benchmark Test Suite (30D). [IMAGE6] [IMAGE7] [IMAGE8] [IMAGE9] Fig. 7. The convergence graphs of CMBO-PG and comparison algorithm (10D). [IMAGE10] [IMAGE11] [IMAGE12] [IMAGE13] Fig.",
      "question_type": "process",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:58:58",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the EDA's output be measured using the criteria of approximate optimal switching sequence and switching times?",
      "topics": [
        "search"
      ],
      "answer": "The quality can be measured by obtaining the approximate optimal switching sequence and switching times, such as those represented by values for $D^{1}(t)$ and the associated numerical sequences, as shown in the provided data, to assess the performance of the EDA.",
      "chunk_id": 7167,
      "paper_id": "Process optimization of microbial fermentation with parameter uncertainties via distributionally robust discrete control",
      "paper_title": "Process optimization of microbial fermentation with parameter uncertainties via distributionally robust discrete control",
      "paper_year": "2023",
      "chunk_content": "In addition, we are also interested in finding a better way to deal with semi infinite constraints and min-max problem, maybe bilevel optimization is a promising research direction. ## CRediT authorship contribution statement Juan Wang: Conceptualization, Methodology, Reviewing and editing. Chihua Chen: Data curation, Methodology, Writing - original draft. Feiyan Zhao: Software, Data curation. Jichao Wang: Visualization, Investigation, Software. An Li: Supervision, Formal analysis, Reviewing and editing. ## Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. ## Data availability Data will be made available on request. ## Acknowledgments This work was supported by the National Natural Science Foundation of China (Grant No. 42176011) and the Natural Science Foundation of Shandong Province of China (Grant No. ZR2020MD060). ## Appendix See Table 3. Table 3 The obtained approximate optimal switching sequence and switching times of $D^{1}(t)$. (a) ${ }^{1}, a^{1}, a^{2}, a^{3}, a^{4}, a^{5}, a^{6}, a^{7}, a^{8}, a^{9}, a^{10}, a^{11}, a^{12}, a^{13}, a^{14}, a^{15}, a^{16}, a^{17}, a^{18}, a^{19}, a^{20}$, $a^{2}, a^{2}, a^{3}, a^{4}, a^{5}, a^{6}, a^{7}, a^{8}, a^{9}, a^{10}, a^{11}, a^{12}, a^{13}, a^{14}, a^{15}, a^{16}, a^{17}, a^{18}$, [FORMULA314], $a^{2}, a^{2}, a^{3}, a^{4}, a^{5}, a^{6}, a^{7}, a^{8}, a^{9}, a^{10}, a^{11}, a^{12}$, $(4.0000,4.6882,5.7892,6.6564,7.0400,7.6355,8.5600,9.5610,10.0800,11.3347$, $11.6000,12.0927,12.5553,12.8706,13.1200,13.9528,14.6400,15.1928,16.8959$, $17.6800,18.0132,18.2687,18.6898,19.2000,19.8619,20.7200,21.1584,22.2400$, $22.7400,23.7600,24.0527,24.6613,25.2800,26.2671,26.8000,27.3918,28.3200$, $29.1254,29.3283,29.8400,30.0756,30.3658,31.3600,32.4541,32.8800,34.4000$, $34.7418,35.1129,35.9200,37.1083,37.4400,37.7791,38.3149,38.7133,38.9600$, $39.6533,40.4800,40.7852,41.1794,41.5906,42.0000,42.9045,43.5200,44.4187$, $45.0400,45.5412,46.5600,47.0623,48.0800,48.8115,49.3143,49.6000,49.9541$, $51.1200,51.3792,52.6400,53.2853,53.5035,54.1600,54.7127,55.6800,56.8510$, $57.2000,57.7767,58.7200,59.4446,61.2486,61.7600,62.7026,63.2800,64.2990$, $64.8000,65.1981,65.6404,66.3200,67.3203,67.8400,68.2901,69.3600,70.4141$, $70.6243,70.8800,71.3989,72.4000,73.3535,73.9200,74.2728,74.5119,75.4400$, $76.0509,76.9600,77.5285,78.4800$ ).",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:58:59",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of utilizing stochastic process models, such as kriging, as response surfaces within the design of Estimation of Distribution Algorithms?",
      "topics": [
        "design"
      ],
      "answer": "The primary objective is to create a surrogate model of the objective function, incorporating a measure of confidence in the model's predictions, which, when combined with information about the current best solution, allows for defining criteria that assess the expected utility of exploring new points in the search space. Techniques like Newton's method, response surface methodology, and kriging models are employed to approximate the objective function and guide the search process in EDAs.",
      "chunk_id": 13259,
      "paper_id": "Bayesian inference in estimation of distribution algorithms",
      "paper_title": "Bayesian inference in estimation of distribution algorithms",
      "paper_year": "2007",
      "chunk_content": "Newton's method is a simple and well-known example of this class of techniques, fitting a local quadratic model at each iteration of the algorithm and directing the search using the optimum of the model (Sequential quadratic programming techniques generalise this idea) [12]. Response surface methodology [13] also utilizes optimization procedures that fit a low-order polynomial regression model to the $\\left(\\mathrm{x}_{t i}, S\\left(\\mathrm{x}_{t i}\\right)\\right.$ data and use simple calculus to estimate the optima from the polynomial. Experimental design techniques are an important part of response surface approaches. Stochastic process models of the objective function have also been widely considered as response surfaces and in model-based optimization, dating back to an algorithm introduced by Kushner in 1964 based on a 1-D Weiner process [14]. Subsequent work includes that of Stuckman [15], the Bayesian approach to global optimization of Mockus [16] and the P-algorithm of Zilinskas [17]. In engineering design, more sophisticated stochastic process models have been employed as response surfaces for model-based optimization. In particular, kriging models have received attention [18]. These techniques produce a surrogate model of the objective function, together with a value for the confidence of the model at any point [FORMULA162]. Together with information about the current best solution found, this confidence information is used to define criteria that indicate the expected utility of searching future points in the search space [19].",
      "question_type": "synthesis summary",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:01",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the use of Normalized Mutual Information (NMI) to quantify the relationship between variables, specifically explaining how the formula for $NMI(X, Y)$ relates to the underlying entropies and mutual information?",
      "topics": [
        "algorithm"
      ],
      "answer": "In Estimation of Distribution Algorithms, Normalized Mutual Information (NMI) is used as a correlation measurement to identify relationships among variables. The mutual information $MI(X, Y)$ between two random discrete variables $X$ and $Y$ is defined as $MI(X, Y) = H(X) + H(Y) - H(X, Y)$, where $H(X)$ and $H(Y)$ are the marginal entropies, and $H(X, Y)$ is the joint entropy. However, because there is no upper limit on mutual information, NMI is used to normalize the mutual information to a scale between 0 (no correlation) and 1 (perfect correlation). The $NMI(X, Y)$ is calculated as $NMI(X, Y) = 2 * MI(X, Y) / (H(X) + H(Y))$. This formula scales the mutual information by the sum of the individual entropies of $X$ and $Y$, ensuring that the resulting NMI value falls within the range of 0 to 1, where a higher value indicates a closer relationship between the variables.",
      "chunk_id": 18953,
      "paper_id": "NMIEDA- Estimation of distribution algorithm based on normalized mutual information",
      "paper_title": "NMIEDA: Estimation of distribution algorithm based on normalized mutual information",
      "paper_year": "2021",
      "chunk_content": "2.1.1 | Correlation measurement In this article, we adopt the normalized mutual information (NMI) as the correlation measurement to identify the relationships among the variables. The definitions of mutual information and NMI are as follows: Definition 1. Given two random discrete variables $X$ and $Y, M I(X, Y)$ is the mutual information of $H(Y)$ and $Y$ : $$\nM I(X, Y)=H(X)+H(Y)-H(X, Y)\n$$ where $H(X)$ and $H(Y)$ are the marginal entropies and $H(X, Y)$ is the joint entropy of $N M I(X, Y) \\in[0,1]$ and ${ }^{41}$. Since there is no upper limit on the mutual information, the correlation between $F$ and $F\\left[x_{i}=u, x_{j}=v\\right]$ is difficult to determine. Definition 2. Normalized mutual information (NMI) is a normalization of the mutual information (MI) score to scale the relations between 0 (no correlation) and 1 (perfect correlation). The $N M I(X, Y)$ (normalized mutual information of $x_{j}$ and $u$ ) can be calculated by $H(X), H(Y)$, and $M I(X, Y)^{40}$ $$\nN M I(X, Y)=2 \\frac{M I(X, Y)}{H(X)+H(Y)}\n$$ where $N M I(X, Y) \\in[0,1]$, and a higher value corresponds to a closer relation between variables $P\\left(x_{i}, x_{j}\\right)$ and $H\\left(x_{i}\\right)$.",
      "question_type": "conceptual deep",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:03",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "In which practical scenarios does the EDA model UMDA outperform a Genetic Algorithm (GA) in terms of the number of evaluations required to find a solution?",
      "topics": [
        "optimization"
      ],
      "answer": "UMDA, an EDA strategy, outperforms GA, particularly in NK landscapes, by requiring fewer evaluations to find a solution. hBOA, an advanced EDA, also demonstrates this advantage over GA in similar scenarios.",
      "chunk_id": 19436,
      "paper_id": "EDA-based optimized global control for PV inverters in distribution grids",
      "paper_title": "EDA-based optimized global control for PV inverters in distribution grids",
      "paper_year": "2021",
      "chunk_content": "The EDA will converge faster to the near-optimal solutions than other EC algorithms. Several references in the literature evaluate the EDA performance against other algorithms, mainly genetic algorithms (GA) due to its similarities. In [19], EDA performance is contrasted with other metaheuristics in an electric vehicle charging problem. In that study, EDA performs best in terms of computational time and quality of the solution. In [20], a node allocation problem in a network is solved using GA and EDA. Again, EDA outperforms GA with these settings, especially as the population grows. Finally, in [21], an in-depth analysis of different metaheuristics is made for a series of NK landscapes. In this study, different strategies for EDA implementation are analyzed. UMDA and an advanced EDA (hierarchical Bayesian optimization algorithm, hBOA) are contrasted with GA. EDAs strategies prove to be better in terms of the number of evaluations. EDAs have been used in various applications: as an optimizer for a controller for networked dc motor systems [22], systemlevel synthesis for chip designs [23], carbon emissions reduction and project makespan minimization [24], dynamic deployment of near space communication systems [25], energy-efficient scheduling of cloud computing services [26], or calibrating the parameters for microscopic traffic models [27].",
      "question_type": "practical application",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:04",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, how does the bin width, influenced by parameters such as [FORMULA393] and [FORMULA395], affect the probability of sampling the optimal solution, and what trade-offs exist between bin width and the number of bins?",
      "topics": [
        "search"
      ],
      "answer": "The EDA's performance is affected by the bin width, which is influenced by parameters such as [FORMULA393] and [FORMULA395]. A smaller [FORMULA393] increases the probability of selecting the bin containing the optimal solution, but the larger width of the bin makes it more difficult to sample the optimal solution. Conversely, a larger [FORMULA395] partitions the search space into narrower bins, increasing the probability of sampling the optimal solution in the later random sampling process, even though many bins may be empty and cannot be selected. Therefore, there is a trade-off between the width of the bins and the number of bins when searching for high-quality solutions.",
      "chunk_id": 20885,
      "paper_id": "An Estimation of Distribution Algorithm for Mixed-Variable Newsvendor Problems",
      "paper_title": "An Estimation of Distribution Algorithm for Mixed-Variable Newsvendor Problems",
      "paper_year": "2020",
      "chunk_content": "The results are shown in Fig. 6. It is obvious that the quality of the optimal solution increases according to the growth of [FORMULA392]. In the AWH model, since the bins in the promising area is shrinking as the search goes, the bin that contain the optimal solution is also becoming narrow. Therefore, it is very promising to find high-quality solutions. Although a smaller [FORMULA393] can make the bin that contains the optimal solution have a higher probability to be selected, the larger width of the bin will make it more difficult to sample the optimal solution in [IMAGE_5] Fig. 6. Mean profit for [FORMULA394] with different number of bins. this bin. A larger [FORMULA395] can partition the search space into lots of narrower bins. In this case, although the number of bins will increase, many of these bins are empty and cannot be selected. Furthermore, a narrower width can increase the probability of sampling the optimal solution in the later random sampling process and it is very helpful to find high precise solutions. ## VI. CONCLUSION Since the newsvendor model has a significant practical application in the field of production, service and management, various variants of the newsvendor model have been proposed.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:06",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of Estimation of Distribution Algorithms (EDAs) compare to that of Non-dominated Sorting Genetic Algorithm (NSGA-II), Strength Pareto Evolutionary Algorithm (SPEA2), and Fast Pareto Genetic Algorithm (FastPGA) in solving multi-objective optimization problems (MOOPs), considering that all aim to find a Pareto set of trade-off optimal solutions?",
      "topics": [
        "genetic"
      ],
      "answer": "Estimation of Distribution Algorithms (EDAs) are evolutionary algorithms that use a probability distribution model, whereas Non-dominated Sorting Genetic Algorithm (NSGA-II), Strength Pareto Evolutionary Algorithm (SPEA2), and Fast Pareto Genetic Algorithm (FastPGA) are multi-objective evolutionary algorithms (MOEAs) based on evolutionary principles. While all aim to approximate the Pareto set for NP-hard multi-objective optimization problems (MOOPs), EDAs differ algorithmically by explicitly estimating and sampling from a probability distribution, unlike NSGA-II, SPEA2, and FastPGA which rely on genetic operators like crossover and mutation.",
      "chunk_id": 10824,
      "paper_id": "Pareto-Based Multi-objective Estimation of Distribution Algorithm with Gaussian Copulas and Application in RFID Network Planning",
      "paper_title": "Pareto-Based Multi-objective Estimation of Distribution Algorithm with Gaussian Copulas and Application in RFID Network Planning",
      "paper_year": "2012",
      "chunk_content": "INTRODUCTION MULTI-OBJECTIVE optimization problems(MOOPs) are widely encountered in various fields of science and technology. The solution of a multi-objective optimization problem is usually a set of acceptable trade-off optimal solutions. This solution set is called a Pareto set[1]. Multi-objective optimization algorithms, especially those based on evolutionary principles, have seen wide acceptability because most engineering problems are NP-hard and therefore a quick computation of approximate solutions is often desirable. In recent years, a considerable amount of interest has been shown in multi-objective evolutionary algorithm (MOEA) and a number of different MOEAs have been suggested, such as Strength Pareto Evolutionary Algorithm (SPEA2)[2], Non-dominated Sorting Genetic Algorithm (NSGA-II) [3], and Fast Pareto Genetic Algorithm (FastPGA)[4]. Estimation of distribution algorithms(EDAs)[5] are a class of evolutionary algorithms based on probability distribution model. It has been proven that EDA has some Manuscript received May 28, 2012. This work was supported in part by the Scientific and Technological Innovation Projects of Department of Education of Guangdong Province, P.R.C. and Guangzhou Science and Technology Projects under Grant No.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:08",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the specific EDA mentioned, what data structures are required to maintain multiple probability models in the Layered Learning Estimation of Distribution Algorithm (LLEDA)?",
      "topics": [
        "genetic"
      ],
      "answer": "The Layered Learning Estimation of Distribution Algorithm (LLEDA) requires data structures to maintain multiple probability models. Specifically, it needs a mechanism to store the mean position of individuals in each layer and the covariance of each layer. These are used to sample promising offspring separately, suggesting the need for data structures that can efficiently store and update means and covariance matrices, likely arrays or matrix objects.",
      "chunk_id": 3260,
      "paper_id": "A Layered Learning Estimation of Distribution Algorithm",
      "paper_title": "A Layered Learning Estimation of Distribution Algorithm",
      "paper_year": "2022",
      "chunk_content": "At last, the covariance of each layer is estimated based on the generated new mean position and the individuals in this layer. By this means, multiple probability models with high quality are maintained and then are used to sample promising and diversified offspring separately. Comparative experiments conducted on a widely used benchmark problem set demonstrate that the proposed LLEDA achieves competitive or even much better performance than several state-of-the-art and representative EDAs. ## CCS CONCEPTS - Mathematics of computing $\\rightarrow$ Evolutionary algorithm; Bioinspired optimization; ## KEYWORDS Estimation of Distribution Algorithms, Multivariate Gaussian Distribution, Layered Learning, Multiple Distribution Models, Global Numerical Optimization ## ACM Reference format: Yong Li, Qiang Yang, Xu-Dong Gao, Zhen-Yu Lu and Jun Zhang. 2022. A Layered Learning Estimation of Distribution Algorithm. In Proceedings of Genetic and Evolutionary Computation Conference Companion (GECCO '22 Companion), July 9-13, 2022, Boston, MA, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3520304.3528904 [^0] ## 1 INTRODUCTION Estimation of distribution algorithms (EDAs) [1-4] have been regarded as a special branch of evolutionary algorithms (EAs) since it was proposed in 2001 [5]. Different from traditional EAs, like particle swarm optimization (PSO) [6-8], which generate offspring based on individuals, EDAs generate new solutions via a specific probability distribution model, which is estimated based on highquality solutions in the current population.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:10",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the encoding process in a multicast system be assessed in terms of achievable throughput?",
      "topics": [
        "encoding"
      ],
      "answer": "The achievable throughput in multicasting from a source to sinks can be used to assess the encoding process. Specifically, the throughput can be measured by the minimum of the maximum flow from the source $s$ to each sink $dl$, represented as $h=\\min _{s} \\max _{\\text {max }} \\text {flow }(s, d l)$.",
      "chunk_id": 15410,
      "paper_id": "Efficient resource minimization scheme for network coding-assisted multicast system",
      "paper_title": "Efficient Resource Minimization Scheme for Network Coding-Assisted Multicast System",
      "paper_year": "2010",
      "chunk_content": "al. [1]. It has been shown that the throughput of the information flow from the source to sinks in the multicast system can be improved by intermediate nodes' activity of encoding the outgoing messages on the basis of the incoming messages. In [5], Li et al. further showed that linear network coding is sufficient to achieve the optimal throughput in the multicast system. A classic example that illustrates the power of network coding is shown in Fig. 1, where each link has unit capacity. The achievable throughput in multicasting from source $s$ to sinks d 1 and d 2 in the network A of Fig. 1, with network coding, is 2. Fig. 1 illustrates that node $a$ in network A encodes the message that is transmitted from node $b$ to node $b$ through linear operation $\\alpha+\\beta$. Fig. 1 also illustrates that sink d1 can obtain both messages $\\alpha$ and $\\beta$ by performing linear operations from messages $(b, d 1)$ and $d 2$, which come through links $(m, d 1)$ and $(b, d 1)$, respectively. [^0]Similarly, sink $d 2$ can obtain both messages $d 1, d 2 \\ldots d l$ and $$\nh=\\min _{s} \\max _{\\text {max }} \\text { flow }(s, d l)\n$$ by performing linear operations from message $(s, d l)$, which comes through link ( $n, d 2$ ) and message $(a, b)$, which comes through link ( $b, d 2$ ).",
      "question_type": "definition",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:11",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "Based on the text, what is the main conclusion regarding the performance of the DHNN-EDA model when compared to SA-B and TA-B on benchmark instances of the unconstrained binary quadratic programming problem?",
      "topics": [
        "comparison"
      ],
      "answer": "The DHNN-EDA generally achieves superior solutions compared to SA-B across all instances of the glov500, bqp1000, and bqp2500 problem sets. When compared to TA-B, the DHNN-EDA obtains better solutions for most instances, with equivalent performance on glov500-1 and glov500-4, and slightly worse performance on bqp2500-5, bqp2500-8 and bqp2500-9. Overall, the average results of DHNN-EDA surpass the best results of both SA-B and TA-B, particularly for the bqp2500 instances.",
      "chunk_id": 15113,
      "paper_id": "Discrete Hopfield network combined with estimation of distribution for unconstrained binary quadratic programming problem",
      "paper_title": "Discrete Hopfield network combined with estimation of distribution for unconstrained binary quadratic programming problem",
      "paper_year": "2010",
      "chunk_content": "For the TS-B and SA-B, only the best solution value for each instance of set glov500 and the problem sets bqp1000 and bqp2500 are available. The two sets of bqp1000 and bqp2500 are first provided in Beasley (1998) as new benchmark instances contained in the ORLIB (Beasley, 2008). In those days, the values of the solutions obtained by these algorithms were reported as the bestknown solutions for each instance of the two sets. Comparing with SA-B, the DHNN-EDA can obtain better solution for all instances. Further, the average results of the DHNN-EDA are better than Table 4 Comparison of average computational time of the multi-start DHNN, DHNN with random flipping perturbation, and the DHNN-EDA for the 35 test problem instances. the best results of SA-B in the whole. Compare with the TA-B, the DHNN-EDA also can obtain better solutions for all instances except that the DHNN-EDA get the same solutions for glov500-1 and glov500-4 instances, and get worse solution for bqp2500-5, bqp2500-8 and bqp2500-9 instances. Further, the average results of the DHNN-EDA are better than the best results of the TA-B for bqp2500 instances in the whole.",
      "question_type": "comparison",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:13",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms (EDAs) that utilize undirected graphical models, what fundamental properties of chordal graphs, specifically the running intersection property, are leveraged to represent probability dependencies among variables?",
      "topics": [
        "search"
      ],
      "answer": "Chordal graphs in EDAs possess the property that their maximal cliques can be joined to form a tree, called the junction tree. This junction tree satisfies the running intersection property, meaning that for any two cliques containing a node alpha, they are either adjacent in the junction tree or connected by a chain made entirely of cliques that contain alpha. In the context of independence graphs, used to display probability dependencies, two vertices are joined if the corresponding variables are not conditionally independent given the rest of the variables. Each variable $X_{i}$ is associated with a vertex $V_{i}$ in the graph.",
      "chunk_id": 26404,
      "paper_id": "Estimation of Distribution Algorithms with Kikuchi Approximations",
      "paper_title": "Estimation of distribution algorithms with Kikuchi approximations",
      "paper_year": "2005",
      "chunk_content": "An undirected graph is said to be chordal if every cycle of length four or more has a chord. A fundamental property of chordal graphs is that their maximal cliques can be joined to form a tree, called the junction tree, such that any two cliques containing a node $\\alpha$ are either adjacent in the junction tree, or connected by a chain made entirely of cliques that contain $2 b$. This property is called the running intersection property (Lauritzen, 1996). A junction tree is an acyclic junction graph. Independence graphs serve to display the probability dependencies that exist among the variables of a given probability distribution. Two vertices are joined in the independence graph if the corresponding variables are not conditionally independent given the rest of the variables. Throughout this paper we will only consider independence graphs defined on undirected graphs. Each variable $X_{i}$ is associated to vertex $V_{i}$ in the corresponding graph. To emphasize this relationship, vertices may be named as the variables $V=\\left\\{X_{1}, \\ldots, X_{n}\\right\\}$. Whether we refer to a variable or a vertex will be made clear from the context. Now, the problem of probability approximation using factorizations will be discussed. The next section briefly reviews EDAs that are based on undirected graphical models and that are relevant to our research.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:14",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to infer node infection probability, what specific implementation steps are required for learning the Bayesian network structure?",
      "topics": [
        "comparison"
      ],
      "answer": "To build a model to infer the status of nodes using EDA algorithms for learning the Bayesian network structure, the process involves using historical feature data to build a discrete time probabilistic model. The probability of node infection is then determined based on the observed features at each time step. This approach aims to reduce required storage space and computational complexity compared to methods that assume access to the infection graph or propagation graph.",
      "chunk_id": 15856,
      "paper_id": "A probability distribution function for investigating node infection and removal times",
      "paper_title": "A probability distribution function for investigating node infection and removal times",
      "paper_year": "2019",
      "chunk_content": "They also assumed to have access to the contact network, ie, the spreading rate, start time of infection, and location of information source through a four-step method. The aim of the present study was to investigate the node infection probability while reducing the limitations and weaknesses of the state-of-the-art methods. For instance, current methods are time consuming and nonoperational with high error rate. They are associated with many open problems in topology, number of origins, number of networks, temporal dynamics, complexity, and scalability. $(i+1, r-1)$ Most of these methods assume to have the infection graph or propagation graph, which is not a realistic assumption. This paper presents a method for inferring the infection probability of a node at each time step back-in-time. To achieve this, we have built a discrete time probabilistic model with EDA algorithms using historical feature data. Then, the probability of node infection was determined based on the observed features at each time step. The nodes with highest probabilities of infection at earlier time points could be the origin of propagation. The proposed method reduces the required storage space and computational complexity in comparison with other methods. In order to build the model to infer the status of the nodes, we used EDA algorithms for learning the Bayesian network structure.",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:16",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the optimization of initial core loading patterns, what specific parameters within the Estimation of Distribution Algorithm (EDA) are identified as having a significant impact on algorithm performance, and how are these parameters mathematically related to the objective of minimizing fissile inventory?",
      "topics": [
        "performance"
      ],
      "answer": "The content indicates that the choice of internal parameters is more important for algorithms performance. While specific mathematical relationships are not explicitly provided, the parameters implicitly influence the objective function related to fuel utilization, as evidenced by the observation that the best case (case VI with N=240, M=20%, and α=0.5) achieves -10% less fissile inventory compared to the benchmark case. Here N, M, and alpha are internal parameters. The parameters influence flux shape and leakage, ultimately impacting the required fissile inventory and cycle length. The goal is achieving higher cycle length of -340 Full Power Days (FPDs) compared to 330 FPDs.",
      "chunk_id": 6454,
      "paper_id": "Initial core loading pattern optimization studies using estimation of distribution algorithm for VVER type cores",
      "paper_title": "Initial core loading pattern optimization studies using estimation of distribution algorithm for VVER type cores",
      "paper_year": "2023",
      "chunk_content": "[IMAGE22] Fig. 23. Normalized critical Boron Vs Full Power Days (FPDs). solution. The choice of internal parameters is more important for algorithms performance. The initial fissile requirement for all optimized LPs is lower than the benchmark case. All the optimized LPs have shown a better initial fissile content (Table 5) compared to benchmark case along with safe and continues operation at rated power during initial phase. All the cases except case I \\& V have shown longer or same cycle length compared to benchmark case. Considering all the factors, the best case observed w.r.t fuel utilization is case VI $(\\mathrm{N}=240, \\mathrm{M}=20 \\%$ and $\\alpha=0.5)$. Here $-10 \\%$ less fissile inventory is required compared to benchmark case for achieving higher cycle length of -340 FPDs compared to 330 FPDs. This has been achieved by changing the flux shape and reducing leakage. However, to prove that the fuel utilization has improved (if at all) from initial phase to equilibrium phase, a comprehensive study for full fuel [IMAGE23] Fig. 24. Normalized Radial Peaking Factor (RPF) Vs Full Power Days (FPDs).",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:18",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "Based on the provided information, how do the termination criteria and scalability results compare between [FORMULA106]-PBIL and BOA, specifically regarding the coefficient of determination?",
      "topics": [
        "termination",
        "scalability",
        "Estimation of Distribution Algorithms"
      ],
      "answer": "Both [FORMULA106]-PBIL and BOA terminate when all \\(\\hat{p}_{i, j} \\mathrm{~s}\\) are > 0.95 or < 0.05, indicating a loss of diversity inside PVs. Regarding scalability, both algorithms exhibit approximately linear scalability. This is supported by the coefficient of determination (R²) values obtained from linear models fitted to their performance curves: R²=0.9992 for [FORMULA106]-PBIL and R²=0.9846 for BOA.",
      "chunk_id": 13070,
      "paper_id": "An Incremental Approach for Niching and Building Block Detection via Clustering",
      "paper_title": "An Incremental Approach for Niching and Building Block Detection via Clustering",
      "paper_year": "2007",
      "chunk_content": "The population size [FORMULA97] was set to be proportional to the problem size [FORMULA98], as $N=5 \\lambda$. The number of clusters [FORMULA100] was also proportional to $\\lambda ; k=\\lambda / 5$. Termination criterion was set to be the loss of diversity inside PVs: the algorithm finishes when all $\\hat{p}_{i, j} \\mathrm{~s}$ are $>0.95$ or $<0.05$. Figure 3 shows the result of this experiment. In the same graph, a similar verification for BOA is also shown (from [14]), with the same intention of checking scalability. Results show that both algorithms scale up very well; actually, the hypothesis of (at most) linear scalability is supportable, since linear models for both curves resulted coefficient of determination $R^{2}=0.9992$ for [FORMULA106]-PBIL and 0.9846 for BOA. ## 6. Discussion and conclusion Linkage learning has revealed a challenging goal for evolutionary computation, and several approaches have been proposed. Some of them are highly explicit on this intention: messy GA [6], for example, performs the identification and recombination of BBs in two separate phases. An EDA follows a diverse tactic, attempting to a learn a (potentially) complex statistical model in each generation.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:20",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing RM-MEDA, how can a developer assess the quality of the Pareto Front (PF) approximation achieved by the algorithm?",
      "topics": [
        "algorithm"
      ],
      "answer": "To assess the Pareto Front (PF) approximation quality in RM-MEDA, a developer can calculate the Inverted Generational Distance metric in the decision space (IGDX) and the Inverted Generational Distance metric in the objective space (IGDF). Lower values of IGDF indicate a better approximation of the PF. The statistical significance of the IGDX values can be evaluated using a one-sided t-test to compare RM-MEDA's performance against other algorithms.",
      "chunk_id": 11792,
      "paper_id": "Approximating the Set of Pareto-Optimal Solutions in Both the Decision and Objective Spaces by an Estimation of Distribution Algorithm",
      "paper_title": "Approximating the Set of Pareto-Optimal Solutions in Both the Decision and Objective Spaces by an Estimation of Distribution Algorithm",
      "paper_year": "2009",
      "chunk_content": "6-10: on F3-F6, that the final solutions with the lowest IGDF values obtained by MMEA and RM-MEDA approximate the PFs very well while Omni-Optimizer and KP1 always miss part of the PFs; on [FORMULA298], it is clear that MMEA and RM-MEDA provide better approximations than Omni-Optimizer and KP1, although none could approximate the PF very well. In terms of the IGDX metric, it is evident from Table III that MMEA outperforms the three other algorithms on [FORMULA299], except RM-MEDA on F7. Figs. 6-9 also reveal that the solutions generated by MMEA, are distributed more uniformly in the decision space than those obtained by the three other ones. Table III shows TABLE II EXPERIMENTAL SETTINGS FOR $F 1-F 7$ TABLE III Statistical Results on F1-F7 (mean $\\pm$ std.) \" + \" in Parenthesis: The One-Side t-Test Indicates That the Metric Value Is Larger Than That Obtained by MMEA at the 95\\% Significance Level. \"-\" in Parenthesis Means a Failure in the t-Test to Reject the Null Hypothesis.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:21",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metric is used to evaluate the effectiveness of the EDA model MemPR, and how is this metric quantified when comparing it to other algorithms such as VNS, ES, OSGA, and ALPS?",
      "topics": [
        "search"
      ],
      "answer": "The primary metric for evaluating the algorithms, including the EDA model MemPR, is the 'average best quality' reached after ten repetitions with an evaluation budget of $10^{7}$. This metric represents the best solution quality found by each algorithm within the given computational budget. The comparison is performed by tuning the parameters of all algorithms by hand and then observing the average best quality achieved across multiple runs.",
      "chunk_id": 22188,
      "paper_id": "Analysing a Hybrid Model-Based Evolutionary Algorithm for a Hard Grouping Problem",
      "paper_title": "Analysing a Hybrid Model-Based Evolutionary Algorithm for a Hard Grouping Problem",
      "paper_year": "2018",
      "chunk_content": "5 Algorithm Comparison We compare MemPR with four algorithms available in the HeuristicLab optimisation environment [8] on real-world problem instances of various sizes. The algorithms we compare against are Variable Neighbourhood Search (VNS) [11], Evolution Strategy (ES), Offspring Selection Genetic Algorithm (OSGA) [10] and Age Layered Population Structure Genetic algorithm (ALPS) [9]. All algorithms use linear linkage encoding and randomly choose between greedy partition crossover, group crossover, lowest index max crossover and lowest index first crossover [12,13]. MemPR and VNS both need a definition of neighbourhood and the both use the same definition in our tests. The neighbourhood of a solution are all solutions reached by applying any move out of four different types of moves. The first two types are extracting an item from a group and either inserted into an existing or a new group. The remaining moves either split a group into two smaller groups, or merge two groups into one. Table 1. Best quality found by different algorithms Table 1 shows the average best quality reached in ten repetitions with an evaluation budget of $10^{7}$. The parameters of the algorithms were tuned by hand. On the smallest instances ALPS and ES achieves comparable solutions, but MemPR consistently outperforms the other algorithms on the bigger instances. While the differences between MemPR and ALPS are numerically not very big, in the real world application saving a single transport is significant.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:23",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "Based on the text, what is the main conclusion regarding the performance of the EDA model using the algorithms [FORMULA155] and BayEDA [FORMULA156] on the test functions, considering the selection pressure and population range?",
      "topics": [
        "performance"
      ],
      "answer": "The algorithms [FORMULA155] and BayEDA [FORMULA156] exhibit highly similar performance across the test functions, with [FORMULA155] showing slightly better mean performance, while BayEDA [FORMULA156] demonstrates a larger standard deviation, potentially due to model variability, under soft selection pressure and an initial population range of $[-15,5]$.",
      "chunk_id": 13257,
      "paper_id": "Bayesian inference in estimation of distribution algorithms",
      "paper_title": "Bayesian inference in estimation of distribution algorithms",
      "paper_year": "2007",
      "chunk_content": "As a consequence, models of such shape and location will tend to appear more frequently in samples from the posterior distribution. The influence of this effect will however depend on the test problem and the algorithm parameters. For this experiment, a very soft selection pressure was used and the initial population was uniform in the range $[-15,5]$. This causes the model to evolve more slowly over the objective function surface and causes it to encounter more locally optimal basins of attraction before locating the region around the global optimum. Experiments were also conducted on 10-D versions of the test functions given in Table IV. For each problem, 30 trials were conducted. Following the experiments in Chapter 8 of [2], we used $M=2000$ and $\\tau=0.5$. For $f_{S p h}$, runs were for 100 generations and for all other functions runs were for 200 generations The results are summarized in Table V. Overall the results show highly similar performance for the two algorithms. [FORMULA155] showed slightly better mean performance in most cases, although the standard deviation of the results for BayEDA [FORMULA156] was almost always larger that of [FORMULA157] (possibly a result of the model variability [IMAGE_4] Fig.",
      "question_type": "evaluation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:24",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of using Voronoi configurations for initialization relate to the subsequent exchange of individuals between islands, and what effect does this have on the overall performance compared to uniform configurations?",
      "topics": [
        "convergence"
      ],
      "answer": "The algorithmic mechanism of using Voronoi configurations for initialization in EDAs involves partitioning the search space to reduce the modality of constrained regions, emphasizing search in the starting regions, and making the search for optimal solutions more effective. The text indicates that the best Voronoi configurations need less interaction between the islands than the best uniform configuration. With a greater exchange of individuals, the beneficial properties of the proposed initialization get diluted. This suggests that the initial partitioning provided by Voronoi configurations allows for more focused exploration within each island, reducing the need for extensive communication compared to uniform configurations where individuals are more broadly distributed from the start. Voronoi configurations also outperformed their equivalent population-sized sequential EDAs.",
      "chunk_id": 23869,
      "paper_id": "A new initialization procedure for the distributed estimation of distribution algorithms",
      "paper_title": "A new initialization procedure for the distributed estimation of distribution algorithms",
      "paper_year": "2011",
      "chunk_content": "Fig. 3 Evolution of the average score for the [FORMULA53] function [IMAGE_3] Table 6 Number of individuals exchanged between the best configurations obtained with Voronoi configurations and that, in general, the Voronoi configurations tend to improve the results of the traditional initialization method. The partition of the search space reduces the modality of the constrained regions and offers the possibility, at least in the earliest generations, to emphasize the search in the starting regions. Thus, the search for the optimal solutions is more effective. The analysis also discovered that the best Voronoi configurations need less interaction between the islands than the best uniform configuration. With a greater exchange of individuals, the beneficial properties of the proposed initialization get diluted. The Voronoi configurations also outperformed their equivalent population-sized (both global and island population sizes) sequential EDAs. This approach has been proposed for EDAs, since they are more influentiated by the initial population than other evolutionary algorithms with more explorative mechanisms. However, this procedure could also improve the performance of other evolutionary algorithms by reaching a higher score or helping them to improve their convergence speed. Acknowledgments This work was supported by the Madrid Regional Education Ministry and the European Social Fund and financed by the Spanish Ministry of Science TIN2007-67148.",
      "question_type": "relationship",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:26",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm (EDA) for feature selection, specifically the GEFeS\n_{\\text{EDA}} approach, what practical considerations should practitioners keep in mind to reduce the number of features necessary for recognition?",
      "topics": [
        "search"
      ],
      "answer": "When implementing the GEFeS\n_{\\text{EDA}} approach, practitioners should prioritize its demonstrated effectiveness in reducing the number of features required for recognition. The approach has been shown to perform better at feature reduction across different experiments, including unimodal (Face-Only and Periocular-Only) and multimodal (Face + Periocular) biometric identification scenarios. Therefore, it is particularly suitable when minimizing the feature set is a key objective.",
      "chunk_id": 23808,
      "paper_id": "A comparison of GEC-based feature selection and weighting for multimodal biometric recognition",
      "paper_title": "A Comparison of GEC-Based Feature Selection and Weighting for Multimodal Biometric Recognition",
      "paper_year": "2011",
      "chunk_content": "For the percentage of features used, the [FORMULA57] instances used the least amount of features, while the [FORMULA58] instances used the highest percentage of features for each experiment performed. ## VI. CONCLUSION Our results show that SSGA-based feature weighting $\\left(\\mathrm{GEFeW}_{\\text {SSGA }}\\right)$ produces the best recognition accuracy for biometric identification when both the face and periocular modalities are used. In general, the multimodal (Face + Periocular) experiment performs better in terms of accuracy when compared to the uni-modal (Face-Only and Periocular-Only) experiments. EDA-based feature selection $\\left(\\mathrm{GEFeS}_{\\text {EDA }}\\right)$ performed better at reducing the number of features necessary for recognition for all of our experiments. Our future work will include investigating the usage of different optimization techniques to further improve the performance of multi-biometric identification. ## ACKNOWLEDGMENT This research was funded by the Office of the Director of National Intelligence (ODNI), Center for Academic Excellence (CAE) for the multi-university Center for Advanced Studies in Identity Sciences (CASIS) and by the National Science Foundation (NSF) Science \\& Technology Center: Bio/computational Evolution in Action CONsortium (BEACON). The authors would like to thank the ODNI and the NSF for their support of this research.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:28",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the computational complexity of the MONEDA algorithm analyzed when applied to Multi-Objective Problems (MOPs) with a large number of objectives, specifically focusing on how the algorithm's computational requirements scale?",
      "topics": [
        "performance"
      ],
      "answer": "The analysis of MONEDA's computational complexity focuses on experimentally assessing how the computational requirements of MONEDA scale when dealing with MOPs that have a relatively large amount of objectives. The goal is to establish the advantages of MONEDA with regard to similar methods in terms of computational requirements, in addition to optimization efficiency. The analysis includes comparing MONEDA's computational properties to other Multi-Objective Estimation of Distribution Algorithms (MOEDAs) and Multi-Objective Evolutionary Algorithms (MOEAs).",
      "chunk_id": 13536,
      "paper_id": "On the Computational Properties of the Multi-Objective Neural Estimation of Distribution Algorithm",
      "paper_title": "On the Computational Properties of the Multi-Objective Neural Estimation of Distribution Algorithm",
      "paper_year": "2009",
      "chunk_content": "In particular, MONEDA introduces a novel model-building algorithm that addresses the particular requirements of MOPs. The advantages of MONEDA in terms of optimization efficiency with regard to other MOEDAs and MOEAs have been experimentally established [17, 15]. However, one very important area remains not properly dealt with: the computational characteristics of MONEDA. MONEDA was devised with high-dimensional MOPs in mind therefore it was supposed to not only to be able to find adequate solutions in those situations but to be capable to scale its performance and making feasible to address MOPs with many objectives. Previous experiments produced some evidences that pointed out that MONEDA had a better computational properties when compared to other similar preexisting approaches, but there is not an exhaustive study on the subject. The leitmotif of this paper is to explore the behavior of MONEDA in terms of its computational requirements and to assess how that quantity scales when dealing with problems with relatively large amounts of objectives. In order to properly comprehend this matters other MOEDAs and MOEAs will be included in the analysis. The main contribution of this paper is to experimentally establish the advantages of MONEDA with regard to similar methods not only in terms of optimization efficiently but also in terms of computational requirements.",
      "question_type": "performance",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:30",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What advantages does the wrapper-type EDA approach have over filter-based methods in the context of channel selection for BCI-EEG interfaces?",
      "topics": [
        "complexity"
      ],
      "answer": "The wrapper-type EDA approach can cope with intricate and higher order dependencies between channels, whereas filter-based methods cannot. The EDA approach identifies important interacting features to generate new solutions, and the probabilistic models generated by the EDA can provide information about relevant interactions for the problem.",
      "chunk_id": 25233,
      "paper_id": "User Adapted Motor-Imaginary Brain-Computer Interface by means of EEG Channel Selection Based on Estimation of Distributed Algorithms",
      "paper_title": "User Adapted Motor-Imaginary Brain-Computer Interface by means of EEG Channel Selection Based on Estimation of Distributed Algorithms",
      "paper_year": "2016",
      "chunk_content": "As recognized by some of the authors, the potential of the wrapper approach [24] is its capacity for considering the synergies between the features during the classification process. But in order to exploit these synergies, the EAs should be able to identify the most important interacting features and use this information at the time of generating new solutions. This is exactly what EDAs do. While filter-based methods could in principle be applied for channel selection, these methods cannot cope with intricate and higher order dependencies between the channels, as our wrapper-type EDA approach does. In addition, the analysis of the probabilistic models generated by the EDA can provide information about what the relevant interactions for the problem are. However, in this paper, we use probabilistic models only to make a more efficient generation of solutions and leave the analysis of the potential information captured by the model as a topic of future research. ## 3. Proposed Approach Having to personalize BCI-EEG interfaces to each user as main goal, this paper proposes a novel two-step method for personalized optimal channel selection. Firstly, the system selects an adequate range of number of channels to be explored in the second step. This selection is done by applying a simple greedy algorithm. Thereafter, a more computationally expensive EDA is applied to find the final channel selection. The main reason to choose EDA for the second step is its ability to capture the relevant relationships between the variables of the optimization problem, and, thus, the selection of the channels will be more accurate, reducing the complexity of the BCI system.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:31",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing a Compact Genetic Algorithm (cGA) to optimize a function, and given the event 'B_0' where the selected individuals 'x^1' and 'x^2' have identical values in the set 'L' (i.e., 'x_{|L}^1 = x_{|L}^2'), how does this event influence the update of the frequency vector 'f_t' and what programming considerations should be taken to efficiently calculate the updated vector 'f_{t+1}'?",
      "topics": [
        "optimization"
      ],
      "answer": "If event 'B_0' occurs, the frequency vector 'f_{t+1}' is updated such that 'f_{t+1} <= f_{t+1}' (component-wise). This leads to '||f_{t+1}||_1 <= ||f_{t+1}'||_1 = ||f_t + (1/μ)(y^1 - y^2)||_1 = ||f_t||_1 + (1/μ)(||y^1||_1 - ||y^2||_1)', where 'μ' is a parameter, 'y^1' and 'y^2' are offspring, and '||.||_1' denotes the L1 norm. In practical implementation, a developer should check for the occurrence of 'B_0' by comparing the relevant components of 'x^1' and 'x^2'. If 'B_0' holds, the update of 'f_{t+1}' can be simplified to reflect that 'f_{t+1}' is less than or equal to 'f_{t+1}' component-wise, potentially saving computational cost by avoiding unnecessary updates if the condition '||y^1||_1 <= ||y^2||_1' is already met. Optimizing the calculation of the L1 norms and the vector subtraction is crucial for performance, especially when dealing with large frequency vectors.",
      "chunk_id": 18542,
      "paper_id": "The Runtime of the Compact Genetic Algorithm on Jump Functions",
      "paper_title": "The Runtime of the Compact Genetic Algorithm on Jump Functions",
      "paper_year": "2021",
      "chunk_content": "By Lemma 5 again, we have $k>n-\\left\\|x^{i}\\right\\|_{1}>0$ with probability [FORMULA1273] for both $i \\in\\{1,2\\}$. Let us call this event [FORMULA1275]. Note that if [FORMULA1276] holds, then both offspring lie in the gap region. Consequently, $\\left\\|y^{1}\\right\\|_{1} \\leq\\left\\|y^{2}\\right\\|_{1}$ and thus $\\left\\|f_{t+1}^{\\prime}\\right\\|_{1} \\leq\\left\\|f_{t}\\right\\|_{1}$. Let $L=\\left\\{i \\in\\{1 \\ldots n\\} \\mid f_{\\mu}=\\frac{1}{n}\\right\\}, \\ell=|L|$, and [FORMULA1280] as in Lemma 8. Note that by definition, $D_{t} \\geq\\left(1-\\frac{1}{n}\\right) \\ell$, hence from $D_{t}<\\frac{2}{4} k$ and $n \\geq 4$ we obtain $\\ell<k$. Let $B_{0}$ be the event that $|M|=0$, that is, $x_{|L}^{1}=x_{|L}^{2}$. Note that in this case, we have $f_{t+1} \\leq f_{t+1}^{\\prime}$ (component-wise) and thus $$\n\\left\\|f_{t+1}\\right\\|_{1} \\leq\\left\\|f_{t+1}^{\\prime}\\right\\|_{1}=\\left\\|f_{t}+\\frac{1}{\\mu}\\left(y^{1}-y^{2}\\right)\\right\\|_{1}=\\left\\|f_{t}\\right\\|_{1}+\\frac{1}{\\mu}\\left(\\left\\|y^{1}\\right\\|_{1}-\\left\\|y^{2}\\right\\|_{1}\\right)\n$$ By Lemma 8, Bernoulli's inequality, and $\\ell \\leq k$, we have $$\n\\operatorname{Pr}\\left[B_{0}\\right]=\\left(1-2 \\frac{1}{n}\\left(1-\\frac{1}{n}\\right)\\right)^{\\ell} \\geq 1-\\frac{2 \\ell}{n} \\geq 1-\\frac{2 k}{n}\n$$ Since $\\ell<k \\leq \\frac{n}{320}<\\frac{n}{2}$, by Lemma 12, we have $\\left\\|x_{|\\{n\\}| L}^{1}\\right\\|_{1} \\neq\\left\\|x_{|\\{n\\}| L}^{2}\\right\\|_{1}$ with probability at least $\\frac{1}{16}$. This event, called [FORMULA1295] in the following, is independent of [FORMULA1296]. We have $$\n\\operatorname{Pr}\\left[A \\cap B_{0} \\cap C\\right] \\geq \\operatorname{Pr}\\left[B_{0} \\cap C\\right]-\\operatorname{Pr}[\\widetilde{A}] \\geq\\left(1-\\frac{2 k}{n}\\right) \\frac{1}{16}-\\exp (-\\Omega(k))\n$$ If $A \\cap B_{0} \\cap C$ holds, then $\\left\\|f_{t+1}\\right\\|_{1} \\leq\\left\\|f_{t+1}^{\\prime}\\right\\|_{1} \\leq\\left\\|f_{t}\\right\\|_{1}-\\frac{1}{\\mu}$. If $A \\cap B_{0} \\cap \\bar{C}$ holds, then we still have $\\left\\|f_{t+1}\\right\\|_{1} \\leq\\left\\|f_{t+1}^{\\prime}\\right\\|_{1} \\leq\\left\\|f_{t}\\right\\|_{1}$.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:34",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific computational challenge is highlighted regarding bioinformatic tools for next generation sequencing (NGS) data?",
      "topics": [
        "analysis"
      ],
      "answer": "The highlighted challenge is that most bioinformatic tools for next generation sequencing (NGS) data are computationally intensive, requiring a large amount of computational power for processing and analysis.",
      "chunk_id": 16375,
      "paper_id": "Fast single individual haplotyping method using GPGPU",
      "paper_title": "Fast single individual haplotyping method using GPGPU",
      "paper_year": "2019",
      "chunk_content": "Fast single individual haplotyping method using GPGPU Joong Chae $\\mathrm{Na}^{\\mathrm{a}}$, Inbok Lee ${ }^{\\mathrm{b}}$, Je-Keun Rhee ${ }^{\\mathrm{c}, *}$, Soo-Yong Shin ${ }^{\\mathrm{d}, \\mathrm{e}, * *}$${ }^{a}$ Department of Computer Science and Engineering, Sejong University, Seoul, 05006, South Korea${ }^{b}$ Department of Software, Korea Aerospace University, Goyang, 10540, South Korea${ }^{c}$ School of Systems Biomedical Science, Soongsil University, Seoul, 06978, South Korea${ }^{d}$ Department of Digital Health, SAHIST, Sungkyunkwan University, Seoul, 06351, South Korea${ }^{e}$ Big Data Research Center, Samsung Medical Center, Seoul, 06351, South Korea ## A R T I C L E I N F O Keywords: Single individual haplotyping PEATH/G GPGPU CUDA Next generation sequencing A B STR ACT Background: Most bioinformatic tools for next generation sequencing (NGS) data are computationally intensive, requiring a large amount of computational power for processing and analysis.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:35",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs that utilizes Principal Component Analysis (PCA) and Multi-Layer Perceptron (MLP)?",
      "topics": [
        "testing"
      ],
      "answer": "The primary objective is to reduce the number of variables by using PCA to extract the main characteristics of the dataset, and then train an MLP to build a calibration module. This involves transforming correlated variables into uncorrelated principal components and then testing different MLP structural configurations, specifically the input and hidden layers, to select the best one based on the mean square error of prediction (MSEP) as the fitness function, repeating the process multiple times to account for the non-deterministic nature of MLP training.",
      "chunk_id": 12487,
      "paper_id": "Parallel EDA to create multivariate calibration models for quantitative chemical applications",
      "paper_title": "Parallel EDA to create multivariate calibration models for quantitative chemical applications",
      "paper_year": "2006",
      "chunk_content": "As fitness function, a global error value is given, defined as the average of the square difference between the predicted value and the real value for each variable, that is, mean square error of prediction (MSEP). Furthermore, as training a MLP is a non-deterministic process, we need to repeat all the process several times; we fixed this value to 10 . In the following sections we explain how PCA and a filtering technique are used to reduce the number of variables, obtaining a subset with the most significant ones. ### 4.1. PCA approach The initial approach to the problem was to use PCA to extract the main characteristics of the dataset and, afterwards, train the MLP to build the calibration module. PCA involves a mathematical procedure that transforms a number of (possibly) correlated variables into a (smaller) number of uncorrelated variables called principal components. Once a dataset with the principal components is available, we can test different MLP structural configurations in order to select the best one. We do that using a brute-force approach: testing a large range of possibilities for the input and hidden layers of the MLP. Obviously, we need to put a limit to this trial-and-error process, due to the huge number of possible configurations for the MLP.",
      "question_type": "application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:37",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the proliferation of building blocks, specifically when considering the impact of variation operators like uniform crossover on population size?",
      "topics": [
        "optimization"
      ],
      "answer": "The success of Genetic Algorithms, a precursor to Estimation of Distribution Algorithms, hinges on the proliferation of important building blocks, which are partially defined solutions where averaging over all solutions they are part of yields a fitness better than the population average. A key principle is the proper mixing of these partial solutions, minimizing disruption by copying them entirely and avoiding the copying of (semi)-independent parts. If variation, such as uniform crossover, destroys these building blocks with high probability, the population size required to find the optimum may grow exponentially with the problem size. Conversely, polynomial population size growth can be achieved if the partial solutions are properly mixed, exemplified by the sum of additively decomposable, non-overlapping, deceptive trap functions.",
      "chunk_id": 5935,
      "paper_id": "Parameterless Gene-pool Optimal Mixing Evolutionary Algorithms",
      "paper_title": "Parameterless Gene-pool Optimal Mixing Evolutionary Algorithms",
      "paper_year": "2024",
      "chunk_content": "Then, we present our benchmark problems and the design of experiments in Section IV, followed by the results in Section V. The article ends with a discussion in Section VI and conclusions in Section VII. ## II. From Genetic Algorithms TO ESTIMATION-OF-DISTRIBUTION ALGORITHMS AND BACK AGAIN It was already hypothesized by John Holland himself that the simple Genetic Algorithm succeeds at optimization if it can proliferate important building blocks [23], i.e., partially defined solutions for which it holds that, when averaging over all the solutions that it is part of in a population, the fitness is better than the average fitness of the population. Key is the proper mixing of these partial solutions, which means disrupting them as little as possible (i.e., copying them entirely from one solution to the next) and not copying other parts of the solution that they are (semi)-independent from. If these important partial solutions have a large probability of being destroyed during variation, for instance by using uniform crossover, the population size that is required to find the optimum may grow exponentially with the problem size. Conversely, polynomial population size growth can be achieved if the partial solutions are properly mixed. A wellknown example of this is represented by the sum of additively decomposable, non-overlapping, deceptive trap functions [12].",
      "question_type": "theory",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:38",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm (EDA) for AutoClustering, what specific data structure is used to represent valid sequences of building blocks for density-based clustering algorithms, and what is its purpose?",
      "topics": [
        "search"
      ],
      "answer": "The EDA for AutoClustering uses a directed acyclic graph (DAG), referred to as a Building Blocks DAG. This DAG implicitly represents all valid sequences of building blocks, specifying the generic structure of density-based clustering algorithms. AutoClustering searches this Building Blocks DAG after the execution of each building block to recall which blocks are valid next.",
      "chunk_id": 10616,
      "paper_id": "AutoClustering  An estimation of distribution algorithm for the automatic generation of clustering algorithms",
      "paper_title": "AutoClustering An Estimation of Distribution Algorithm for the Automatic Generation of Clustering Algorithms",
      "paper_year": "2012",
      "chunk_content": "Also, we propose an EDA for our target task, whilst [16] proposed a very different genetic programming system. In order to increase the chances for a good clustering algorithm to be generated by AutoClustering, the alphabet is defined as a number of carefully-selected typical clustering procedures (building blocks of clustering algorithms). Additionally, the EDA uses a pre-defined data structure that implicitly represents all the valid sequences of building blocks. This data structure is a directed acyclic graph (DAG), which specifies the generic structure of density-based clustering algorithms. We will call this a Building Blocks DAG. AutoClustering searches the Building Blocks DAG after the execution of each building block to recall which blocks are valid next. We have analyzed in detail many density-based clustering algorithms, in order to identify the most significant densitybased clustering procedures. Many recently proposed algorithms present relatively small contributions in relation to previous algorithms. We have therefore selected a small number of basic density-based clustering algorithms - those presenting a particularly different approach to the task. Any subsequent version of these algorithms proposed later for slightly improving performance or flexibility was therefore not considered. As expected, the selected basic algorithms had some building blocks in common. A set of typical density-based clustering procedures was then extracted from these basic algorithms.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:39",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, what mechanism is proposed to address the issues of premature convergence and the missing of multimodality-specific mechanisms when applied to multimodal problems?",
      "topics": [
        "search"
      ],
      "answer": "The cooperative clustering based multimodal EDA (CMEDA) is proposed. It adopts a fast cooperative clustering strategy to locate multiple optima based on both decision and target space information. This strategy is integrated with the EDA to divide multimodal problems into promising regions automatically, leveraging the EDA's ability to preserve high diversity.",
      "chunk_id": 21418,
      "paper_id": "Multimodal estimation of distribution algorithm based on cooperative clustering strategy",
      "paper_title": "Multimodal Estimation of Distribution Algorithm Based on Cooperative Clustering Strategy",
      "paper_year": "2018",
      "chunk_content": "Multimodal Estimation of Distribution Algorithm Based on Cooperative Clustering Strategy Shanshan Huang ${ }^{1}$, Houming Jiang ${ }^{2}$1. Nari Group Corporation (State Grid Electric Power Research Institute), Nanjing, 210003E-mail: huangshanshan@sgepri.sgcc.com.cn2. Nari Group Corporation (State Grid Electric Power Research Institute), Nanjing, 210003E-mail: jianghouming@sgepri.sgcc.com.cn #### Abstract Multimodal optimization is becoming more and more important. Among all the optimization algorithms, estimation of distribution algorithm (EDA) attracts more attention because of its simple principle and special mechanism. Unfortunately, it loses feasibility and effectiveness in multimodal problems because of premature convergence and the missing of multimodality-specific mechanism. So niching is adopted to help EDA to locate diverse promising solution regions in multimodal optimization. However, many of the niching techniques are either sensitive to parameters or cost a number of fitness evaluations. In this paper a new proposed fast cooperative clustering strategy is adopted to offer improved assistance for locating multiple optima based on both decision and target space information. Taking the advantage of EDA in preserving high diversity, this paper proposes a cooperative clustering based multimodal EDA (CMEDA). Integrated with new proposed cooperative clustering strategy, multimodal problems are divided into certain promising regions automatically.",
      "question_type": "evaluation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:41",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "In the described EDA framework, how does the initialization step, where N initial population Pop(0) is randomly generated, influence the subsequent clustering step, which divides the selected population into k clusters based on k-mean clustering, in terms of the diversity of solutions explored?",
      "topics": [
        "initialization",
        "clustering",
        "Estimation of Distribution Algorithms"
      ],
      "answer": "The random initialization of N initial population Pop(0) at generation g=0 directly feeds into the clustering step. If the initial population lacks diversity, the subsequent k-mean clustering, which divides the selected population into k clusters, may converge to suboptimal regions of the objective value space. This is because the clustering is performed according to objective value, and a poorly initialized population will not adequately represent the Pareto front. The binary tournament selection used to create the subpopulation $SUB_g^{Se}$ will then be operating on a limited set of solutions, potentially reducing the effectiveness of the RBM-based modeling in each of the k isolated networks and subsequent reproduction steps.",
      "chunk_id": 14866,
      "paper_id": "Restricted Boltzmann machine based algorithm for multi-objective optimization",
      "paper_title": "Restricted Boltzmann Machine Based Algorithm for Multi-objective Optimization",
      "paper_year": "2010",
      "chunk_content": "In this implementation, non-dominance ranking, crowding distance, and binary tournament selection, similar to the operators in NSGAII [19], are utilized. Moreover, clustering based on k -mean clustering is employed. The overall algorithm works as follows: Step (1) Initialization: At generation $\\mathrm{g}=0$; randomly generate N initial population $\\operatorname{Pop}(0)$. Step (2) Ranking: Perform non-dominated ranking and crowding distance over the population. Step (3) Selection: Select N subpopulation $\\left(S U B_{g}^{S e}\\right)$ based on binary tournament selection. Step (4) Clustering: Divide the selected population (according to objective value) into k cluster based on k -mean clustering. Step (5) Modeling: In each cluster, build an isolated network ( k network) based on RBM and perform CD training (Eq. 5). Step (6) Reproduction: Compute probability of the joint configuration $P(v)$ (Eq. 4). Generate a new set of N solutions $(P)$ from $\\left(P\\left(h_{j} \\mid v\\right)_{o}\\right)$. Step (7) Archiving: Select N individuals form $P \\cup$ $\\operatorname{Pop}(g)$ to form $\\operatorname{Pop}(g+1)$. Step (8) Stopping Criteria: If stopping condition is reached, terminate.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:43",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model, differing from Genetic Algorithms (GAs)?",
      "topics": [
        "genetic"
      ],
      "answer": "In Estimation of Distribution Algorithms (EDAs), unlike Genetic Algorithms (GAs), the algorithmic steps involve sampling a new population from a probability distribution estimated from selected individuals. There are neither crossover nor mutation operators as found in GAs.",
      "chunk_id": 12457,
      "paper_id": "Prototype Selection and Feature Subset Selection by Estimation of Distribution Algorithms. A Case Study in the Survival of Cirrhotic Patients Treated with TIPS",
      "paper_title": "Prototype selection and feature subset selection by estimation of distribution algorithms.: A case study in the survival of cirrhotic patients treated with TIPS",
      "paper_year": "2001",
      "chunk_content": "Once the objective is fixed, FSS can be viewed as a search problem, with each state in the search space specifying a subset of the possible features of the task. Exhaustive evaluation of possible feature subsets is usually infeasible in practice because of the large amount of computational effort required. Many search techniques have been proposed to solve FSS problem when there is no knowledge about the nature of the task, carrying out an intelligent search in the space of possible solutions. # 5 Estimation of Distribution Algorithms Genetic Algorithms (GAs, see Holland [7]) are one of the best known techniques for solving optimization problems. Their use has reported promising results in many areas but there are still some problems where GAs fail. These problems, known as deceptive problems, have attracted the attention of many researchers and as a consequence there has been growing interest in adapting the GAs in order to overcome their weaknesses. GAs are a population based method. A set of individuals (or candidate solutions) is generated, promising individuals are selected, and new individuals are generated using crossover and mutation operators. An interesting adaptation of this is the Estimation of Distribution Algorithm (EDA) [14] (see Figure 3). In EDAs, there are neither crossover nor mutation operators, the new population is sampled from a probability distribution which is estimated from the selected individuals.",
      "question_type": "definition",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:44",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the EDA's output be measured using the criteria of spectral shape when the reflection spectrum is distorted?",
      "topics": [
        "search"
      ],
      "answer": "The quality of the EDA's output, specifically its ability to accurately detect the Bragg wavelength, can be measured by minimizing the error between a theoretical distorted spectrum and the actual observed spectrum. This involves constructing a theoretical spectrum based on the error and searching for the Bragg wavelength by minimizing this error as an objective function. The distorted spectrum is characterized by parameters such as the attenuation factor ($\\varepsilon$), broadening parameter ($\\tau$), and asymmetric parameter ($\\alpha$), which quantify light intensity attenuation, main peak broadening, and waveform asymmetry, respectively. The asymmetric generalized Gaussian distribution function, $g(\\lambda, \\lambda_{B})$, is used to model the distorted spectrum, where these parameters influence the accuracy of Bragg wavelength detection.",
      "chunk_id": 5507,
      "paper_id": "Distortion Tolerant Method for Fiber Bragg Grating Sensor Network Using Estimation of Distribution Algorithm and Convolutional Neural Network",
      "paper_title": "Distortion Tolerant Method for Fiber Bragg Grating Sensor Network Using Estimation of Distribution Algorithm and Convolutional Neural Network",
      "paper_year": "2024",
      "chunk_content": "This means that the detection accuracy of Bragg wavelength determines the reliability of sensing information. The traditional maximum method realizes the tracking of Bragg wavelength by identifying the position of the reflection spectrum peak power. When the reflection spectrum is distorted, the spectral pattern is correspondingly deformed. The wavelength corresponding to the spectral peak power is not the Bragg wavelength. To make sure the localization accuracy of the CNN model not be affected by distorted spectra, the wavelength data should reflect the actual variation of the monitored strain object correctly. Thus, we construct a theoretical spectrum by the error between the mathematical model of distorted spectrum and actual spectrum. The Bragg wavelength can be searched by minimizing the error as the objective function. There are three types of distorted spectrum: light intensity attenuation, main peak broadening, and waveform asymmetry, as shown in Fig. 5. The asymmetric generalized Gaussian distribution function is chosen to characterize the [IMAGE_4] Fig. 5. Spectral shapes of normal and distorted spectra. distorted spectrum, as shown in the following formula: $g\\left(\\lambda, \\lambda_{B}\\right)=\\varepsilon I_{r}\\left\\{\\begin{array}{l}\\exp \\left[-2^{\\tau} \\ln 2\\left(\\frac{\\lambda-\\lambda_{B}}{\\alpha \\Delta \\lambda_{B}}\\right)^{\\tau}\\right], \\quad \\lambda \\geq \\lambda_{B} \\\\ \\exp \\left[-2^{\\tau} \\ln 2\\left(\\frac{\\lambda-\\lambda_{B}}{\\Delta \\lambda_{B}}\\right)^{\\tau}\\right], \\quad \\lambda<\\lambda_{B}\\end{array}\\right.$ where $\\varepsilon, \\tau$, and $\\alpha$ are the attenuation factor, broadening parameter, and asymmetric parameter, respectively. The attenuation factor reflects the attenuation degree of reflected spectral light intensity, which is a constant not greater than 1.",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:46",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs when evaluating candidate solutions for combinatorial optimization problems like the knight's tour or magic square?",
      "topics": [
        "mutation"
      ],
      "answer": "The algorithmic approach aims to evaluate the fitness of candidate solutions by quantifying their adherence to problem-specific constraints. For example, in [FORMULA103]-pieces chess problems, the objective is to minimize conflicts, while for the knight's tour, the goal is to maximize legal moves, aiming for 63 or 64 for open or closed tours, respectively. For magic square problems, the objective function maximizes the number of summands equal to the magic number across rows, columns, and diagonals. Algorithms like COIN and NB-COIN use a learning rate of 0.05 for the learning step [FORMULA99], and selection pressures differ, with EHBSA/NHBSA using 50% of the population and COIN/NB-COIN rewarding the top 25% and punishing the bottom [FORMULA102].",
      "chunk_id": 307,
      "paper_id": "Negative Correlation Learning in the Estimation of Distribution Algorithms for Combinatorial Optimization",
      "paper_title": "Negative Correlation Learning in the Estimation of Distribution Algorithms for Combinatorial Optimization",
      "paper_year": "2013",
      "chunk_content": "The learning step, [FORMULA99], of COIN and NB-COIN, was set to 0.05 . The selection pressures of EHBSA and NHBSA without templates were set to $50 \\%$ of the whole population, while COIN and NB-COIN used $25 \\%$ of the top ranks for rewards and [FORMULA102] of the bottom ranks for punishment. The encoding schemes of each candidate is a straight-forward permutation string, in which each of the items refers to the position on the chess board or table ranging from the top left to the right and then is repeated in the next row until reaching the bottom right. The evaluation function is the total number of legal moves or valid positions that are found in a solution. For all of the [FORMULA103]-pieces chess problems, the objective functions are to minimize the conflicts of each chess piece. Therefore, the minimum fitness indicates the global optimum. For the knight's tour problem, the objective function is to maximize the legal moves, which are 63 for an open tour and 64 for a closed tour. Therefore, a higher fitness indicates a better performing algorithm. For the magic square problems, the objective functions are to maximize the number of summands that equal the magic number for each row, column and diagonal axis.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:48",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of initializing a probability matrix in PBILA with S2opt influence the generation of solutions?",
      "topics": [
        "search"
      ],
      "answer": "The PBILA with S2opt algorithm initializes a probability matrix, denoted as [FORMULA122], which is an $N \\times N$ matrix. Each element in this matrix, represented as [FORMULA125], signifies the probability that the $j^{\\text {th }}$ department is located in the [FORMULA127] location. Initially, all values in this matrix are set to $1 / N$. For instance, the provided example shows a matrix where $N$ is 6, and all elements are 1/6. This probability matrix is then used to generate a population \\$\\Phi\\$ of [FORMULA130] solutions by sampling probabilities in [FORMULA131].",
      "chunk_id": 22017,
      "paper_id": "Hybrid estimation of distribution algorithms for solving a keyboard layout problem",
      "paper_title": "Hybrid estimation of distribution algorithms for solving a keyboard layout problem",
      "paper_year": "2018",
      "chunk_content": "(7) Repeat steps 2 to 6 until termination criteria are satisfied. The termination criteria selected is the number of iterations and its value is $10 \\times$ number of facilities. (8) Save the solution with the lowest cost. The solution for the aforementioned problem is (1 3256 4) with a total cost of 20,253. ### 5.2. Working of the proposed PBILA with S2opt (1) Initialize a probability matrix [FORMULA122] as $N \\times N$ matrix with all values set to $1 / N$. Here, each [FORMULA125] element in the matrix represents the probability that $j^{\\text {th }}$ department is located in [FORMULA127] location. $$\n\\Pi=\\left[\\begin{array}{lllll}\n1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 \\\\\n1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 \\\\\n1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 \\\\\n1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 & 1 / 6 & 1 / 6\n\\end{array}\\right]\n$$ (2) Generate population $\\Phi=\\left\\{\\begin{array}{lllll}5 & 2 & 1 & 4 & 6 & 3 \\\\ 2 & 3 & 5 & 6 & 1 & 4 \\\\ & & \\vdots \\\\ 3 & 5 & 6 & 2 & 1 & 4\\end{array}\\right\\}$ of [FORMULA130] solutions by sampling probabilities in [FORMULA131]. (3) Run S2opt local search on each of the [FORMULA132] solutions. $$\n\\Phi=\\left\\{\\begin{array}{llll}\n3 & 1 & 2 & 4 & 6 & 5 \\\\\n4 & 6 & 5 & 3 & 1 & 2 \\\\\n& & \\vdots \\\\\n6 & 5 & 2 & 4 & 1 & 3\n\\end{array}\\right\\}\n$$ (4) Calculate the objective function value. $$\n\\Phi=\\left\\{\\begin{array}{c}\n20253 \\\\\n20253 \\\\\n\\vdots \\\\\n20361\n\\end{array}\\right\\}\n$$ (5) Select set [FORMULA135] from [FORMULA136] consisting of [FORMULA137] promising solutions, where $n \\leq p$.",
      "question_type": "algorithmic mechanism",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:49",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "What specific implementation steps are required for the exact learning algorithm presented, which aims to find the best Bayesian network structure?",
      "topics": [
        "heuristic"
      ],
      "answer": "The exact learning algorithm involves five key steps. First, calculate the local scores for all n * 2^(n-1) (variable, variable set)-pairs. Second, using these local scores, find the best parents for all (variable, variable set)-pairs. Third, find the best sink for all 2^n variables. Fourth, using the results from step 3, find a best ordering of the variables. Fifth, find a best network using the results computed in steps 2 and 4. The concept of the 'best sink' of the Bayesian network plays an important role in determining the optimal network structure.",
      "chunk_id": 13139,
      "paper_id": "Exact Bayesian network learning in estimation of distribution algorithms",
      "paper_title": "Exact Bayesian network learning in estimation of distribution algorithms",
      "paper_year": "2007",
      "chunk_content": "Singh and Moore [19] present a more efficient method called OPTORD which is feasible for $n<22$ and is shown to work with $n=22$. The method is compared to local search heuristic to learn Bayesian networks in a number of datasets, obtaining better scoring solutions that the alternatives tested. In [18], a more efficient method is presented. The algorithm was shown to learn a best network for a data set of 29 variables. Algorithm 1 presents the main steps of the method. ## Algorithm 1: Exact learning algorithm 1. Calculate the local scores for all $n 2^{n-1}$ different (variable, variable set)-pairs. 2 Using the local scores, find best parents for all $$\nf_{\\text {dec }}^{3}(u)= \\begin{cases}0.9 & \\text { for } \\quad u=0 \\\\ 0.8 & \\text { for } \\quad u=1 \\\\ 0.0 & \\text { for } \\quad u=2 \\\\ 1.0 & \\text { for } \\quad u=3\\end{cases}\n$$ (variable, variable set)-pairs. 3 Find the best sink for all $2^{n}$ variables. 4 Using the results from Step 3, find a best ordering of the variables. 5 Find a best network using results computed in Steps 2 and 4. The total score of the Bayesian structure can be decomposed in the computation of local scores. Therefore, in the first step only local scores are computed. On the other hand, the concept of sink, and specifically the best sink of the Bayesian network, plays an important role in the algorithm.",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:51",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the computational complexity of the DE-eEDA (Differential Evolution algorithm with Estimation of Distribution Algorithm) analyzed in relation to other algorithms, specifically concerning CPU time for processing global information?",
      "topics": [
        "performance"
      ],
      "answer": "The DE-eEDA, when collecting and processing global information, requires a CPU time that is described as 'a little more' than that of the GA (Genetic Algorithm) and the PSO (Particle Swarm Optimization). However, this increased CPU time is stated to be 'still within a few seconds'. This suggests a practical, although not formally specified, bound on the additional computational cost associated with the EDA component of the DE-eEDA compared to the GA and PSO, specifically in the context of the reentrant hybrid flow shop scheduling (RHFS) problem. The description focuses on empirical observation of CPU time rather than providing a formal complexity analysis using Big O notation or similar asymptotic measures.",
      "chunk_id": 22286,
      "paper_id": "A hybrid differential evolution algorithm with estimation of distribution algorithm for reentrant hybrid flow shop scheduling problem",
      "paper_title": "A hybrid differential evolution algorithm with estimation of distribution algorithm for reentrant hybrid flow shop scheduling problem",
      "paper_year": "2018",
      "chunk_content": "It can also be known that in order to collect and process the global information, the CPU time of the DE-eEDA is a little more than that of the GA and the PSO, which is still within a few seconds though. Thus, DE-eEDA can be applied to solve the RHFS problem and is usually true for its outperformance. To further analyze the data, we tested the effect of number of jobs, number of layers, number of workstations and number of machines on the performance of the four algorithms, respectively. The results are shown in Figs. 3, 4,5 and 6 . The vertical axis represents the [FORMULA241] average. Figure 3 shows that the DE-eEDA works the best in most of the job sizes except for the problem of 10 jobs. Figures 4, 5 and 6 show that the DE-eEDA outperforms the others in all different problem sizes. In Fig. 3, for all the algorithms, when the other factors remain unchanged, [FORMULA242] average becomes large with the increase in the number of jobs for the possible reason that the competition for the machines between different jobs becomes more intense when the number of jobs increases. As what we describe in \"Appendix\", the lower bound of our problem is got by using LR algorithm relaxing the [IMAGE_3] Fig.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:53",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What is cited as a main advantage of Estimation of Distribution Algorithms (EDAs) compared to most other meta-heuristics?",
      "topics": [
        "optimization"
      ],
      "answer": "A main advantage of EDAs is their ability to adapt operators to the structure of the problem by sampling a new population of individuals from the probabilistic distribution of superior candidate solutions. However, the adaptation in EDA is usually limited by the initial choice of the probabilistic model.",
      "chunk_id": 2350,
      "paper_id": "Estimation of distribution algorithm for a class of nonlinear bilevel programming problems",
      "paper_title": "Estimation of distribution algorithm for a class of nonlinear bilevel programming problems",
      "paper_year": "2014",
      "chunk_content": "Then, we propose an EDA based on the statistical information of the superior candidate solutions to solve the transformed problem. We stress that the new population of individuals is sampled from the probabilistic distribution of those superior solutions. Thus, one of the main advantages of EDA over most other meta-heuristics is its ability to adapt the operators to the structure of the problem, although adaptation in EDA is usually limited by the initial choice of the probabilistic model. In addition, two specific rules are established in the initialization procedure to make use of the hierarchical structure of BLPPs and to handle the constraints. Moreover, without requiring the differentiability of the objective function, or the convexity of the search space of the equivalent problem, the proposed algorithm can address nonlinear BLPPs with non-differentiable or non-convex upper level objective function and upper level constraint functions. Finally, the proposed algorithm has been applied to 16 benchmark problem; in five of these problems, all of the upper level variables and lower level variables are 10-dimensional. The numerical results compared with those of other methods reveal the feasibility and effectiveness of the proposed algorithm. (c) 2013 Elsevier Inc. All rights reserved. ## 1. Introduction The bilevel programming problem (BLPP) addresses an optimization problem whose constraint region is determined implicitly by another (parametric) mathematical programming problem [15].",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-28 23:59:54",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What programming techniques are suggested to optimize the EDA's execution time compared to Genetic Algorithms (GAs), based on the description?",
      "topics": [
        "optimization",
        "Estimation of Distribution Algorithms",
        "Genetic Algorithms"
      ],
      "answer": "To optimize execution time, the description suggests leveraging the fact that Estimation of Distribution Algorithms (EDAs) do not perform selection and crossover operations, unlike Genetic Algorithms (GAs). This avoids the computational overhead associated with these operations. Additionally, the text implies that EDAs require less parameter tuning compared to GAs, which reduces the time spent on parameter optimization, further contributing to faster execution.",
      "chunk_id": 10531,
      "paper_id": "Performance Evaluation of an EDA-Based Large-Scale Plug-In Hybrid Electric Vehicle Charging Algorithm",
      "paper_title": "Performance Evaluation of an EDA-Based Large-Scale Plug-In Hybrid Electric Vehicle Charging Algorithm",
      "paper_year": "2012",
      "chunk_content": "Despite this potential for variation, the standard deviation was found to be less than $0.0079 \\%$ of the average, showing the accuracy and consistency of the final solutions. Furthermore, the simulation results show that the proposed method is capable of obtaining high quality solutions with stable convergence characteristics. Initially, the authors simulated the algorithm with 30 PHEVs. Then, the authors increased the number of PHEVs to 300 and 3000. Compared with the sampling time $\\Delta t, 5 \\mathrm{~s}$ is a reasonable decision making time on 3000 vehicles. As seen in Table II, the execution time does not exponentially grow with respect to the number of [FORMULA87], demonstrating that the EDA- [IMAGE_6] Fig. 7. Comparisons on fitness values over 30 runs. TABLE IV Optimal Solutions With 300 PHEVs/PEVs based method is suitable for energy management at a large-scale municipal parking deck. The computational speed of EDA and GA is compared in Table III. EDA is less time-consuming than GA because it does not perform selection and crossover operations. GA requires several parameters to be tuned by users. Accordingly, the optimization performance mainly relies on the parameter settings, which is a big challenge for a researcher who has less experience.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-28 23:59:56",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the effectiveness of the EDA model in the context of structure-control design for a mechatronic system with a parallelogram mechanism?",
      "topics": [
        "control"
      ],
      "answer": "The provided content does not explicitly list specific metrics used to evaluate the effectiveness of the EDA model. It only mentions the use of an estimation of distribution algorithm for structure-control design of a mechatronic system with a parallelogram mechanism.",
      "chunk_id": 24683,
      "paper_id": "Structure-control design of a mechatronic system with parallelogram mechanism using an estimation of distribution algorithm",
      "paper_title": "Structure-control design of a mechatronic system with parallelogram mechanism using an estimation of distribution algorithm",
      "paper_year": "2016",
      "chunk_content": "Mechanics Based Design of Structures and Machines An International Journal ## Structure-control design of a mechatronic system with parallelogram mechanism using an estimation of distribution algorithm S. Ivvan Valdez, E. Chavez-Conde, Eusebio E. Hernandez \\& M. Ceccarelli To cite this article: S. Ivvan Valdez, E. Chavez-Conde, Eusebio E. Hernandez \\& M. Ceccarelli (2016) Structure-control design of a mechatronic system with parallelogram mechanism using an estimation of distribution algorithm, Mechanics Based Design of Structures and Machines, 44:1-2, 58-71, DOI: 10.1080/15397734.2015.1035785 To link to this article: http://dx.doi.org/10.1080/15397734.2015.1035785 Published online: 06 Apr 2016. Submit your article to this journal Article views: 2 View related articles View Crossmark data Full Terms \\& Conditions of access and use can be found at http://www.tandfonline.com/action/journallnformation?journalCode=Imbd20 # Structure-control design of a mechatronic system with parallelogram mechanism using an estimation of distribution algorithm S.",
      "question_type": "evaluation",
      "complexity": "advanced",
      "generated_at": "2025-06-28 23:59:57",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of coupling RESEDA with brute force techniques, as described in the context of solving Sudoku puzzles?",
      "topics": [
        "constraints"
      ],
      "answer": "The primary objective is to reduce the CPU time required to solve Sudoku puzzles using RESEDA. This is achieved by considering fixed permutations of a given block, such as the upper left corner block, which adds clues to the puzzle and significantly increases the success rate while reducing the mean time of success.",
      "chunk_id": 9993,
      "paper_id": "A restarted estimation of distribution algorithm for solving sudoku puzzles",
      "paper_title": "A restarted estimation of distribution algorithm for solving sudoku puzzles",
      "paper_year": "2012",
      "chunk_content": "Moreover, even if a few zones are less hard to solve, we have no evidence that they are the [FORMULA68] blocks of the first diagonal. So for this kind of puzzle, we recommend to use exhaustively the 27 zones for the partial restarts. The execution time (with success or not) is roughly twice the time needed to solve a fiendish puzzle but the probability of success is only $\\frac{9}{1000}$. This leads to a mean time of success of about 778 seconds which is quite long. In order to reduce this CPU times, we propose to couple RESEDA with brutal force techniques. The idea is to run the algorithm considering as fixed each of the admissible permutations of a given [FORMULA70] block. If we do this for the upper left corner block, there are only 24 permutations meeting the basic constraints. One of these 24 permutations is the right permutation, and it adds 6 clues to the puzzle. When it is correctly guessed, the success rate grows to $\\frac{580}{1000}$ with a CPU time of 1.517 seconds, which leads to a mean time of success of $\\frac{1000 \\cdot 24 \\cdot 1.517}{580} \\approx 63$ seconds. Using this new idea, the CPU times are divided by ten.",
      "question_type": "methodology",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:01",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of bivariate Estimation of Distribution Algorithms, what mathematical principles govern the optimization goal of minimizing \\$\\sum_{j=1}^{n} I(X_{ij}, X_{in}) / H(X_{ij})\\$ and how does this relate to clustering accuracy?",
      "topics": [
        "optimization"
      ],
      "answer": "For bivariate EDAs, the algorithmic goal is to minimize \\$\\sum_{j=1}^{n} I(X_{ij}, X_{in}) / H(X_{ij})\\$. Here, \\$I(X_{ij}, X_{in})\\$ represents the mutual information between \\$X_{ij}\\$ and \\$X_{in}\\$, where \\$X_{i}\\$ is the cluster ensemble with \\$n\\$ ensemble committees. \\$H(X_{ij})\\$ represents the entropy of \\$X_{ij}\\$. This minimization is directly related to the clustering accuracy, defined as \\$\\operatorname{Accuracy}(X_{i})=\\frac{\\sum_{j=1}^{n} I(X_{ij}, X_{in}) / H(X_{ij})}{M}\\$, where \\$M\\$ is the number of correlations of the variables. A smaller value of \\$\\sum_{j=1}^{n} I(X_{ij}, X_{in}) / H(X_{ij})\\$ indicates better clustering accuracy, implying that minimizing the normalized mutual information leads to improved performance of the algorithm.",
      "chunk_id": 15336,
      "paper_id": "Hybrid sampling on mutual information entropy-based clustering ensembles for optimizations",
      "paper_title": "Hybrid sampling on mutual information entropy-based clustering ensembles for optimizations",
      "paper_year": "2010",
      "chunk_content": "Suppose the cluster ensemble $X_{i}$ has $n$ ensemble committees, and the clustering accuracy can be defined as $\\operatorname{Accuracy}\\left(X_{i}\\right)=\\frac{\\sum_{j=1}^{n} I\\left(X_{i j}, X_{i n}\\right) / H\\left(X_{i j}\\right)}{M}$ where $M$ is the number of correlations of the variables, and $r_{j}$ represents the father of node $j$ in mutual information tree. Here the accuracy is a normalized parameter for information measurement. For bivariate EDAs, the goal is to minimize $\\sum_{j=1}^{n} I\\left(X_{i j}, X_{i n}\\right) / H\\left(X_{i j}\\right)$. If the value of the above formula is smaller, the accuracy of the algorithm is better [8]. Since $\\alpha=0.1$ belongs to $[0,1]$, we need to do some normalization and make it divided by $(200,000)$. As mentioned above, $\\max \\left(\\sum_{i=1}^{n} x_{i}\\right) \\quad x_{i} \\in\\{0,1\\}$ is the number of correlations of the variables, here it can also be regarded as the number of edges in the clusters. The clustering diversity can also be set as follows: $\\operatorname{Diversity}\\left(X_{i}\\right)=\\sum_{i, j=1, j \\neq i}^{N} \\sum_{i=1}^{L} \\frac{f\\left(X_{i m}, X_{j m}\\right)}{N-1}$ $f\\left(X_{i m}, X_{j m}\\right)= \\begin{cases}1.0 & \\text { if } X_{i m}!=X_{j m} \\\\ 0 & \\text { else }\\end{cases}$ where $L$ is the number of the variables, and $f_{2}(0,0)=0.9$ is the length of the cluster committee. The diversity can be evaluated by the differences of the values in the same bit for different individuals. For example, $X$ and $Y$ are two individual in the population.",
      "question_type": "definition",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:03",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners determine the number of iterations required for the Univariate Marginal Distribution Algorithm (UMDA) to sample the optimum with high probability, given that position [FORMULA218] is critical and $\\mu \\geq 4\\left\\lceil\\frac{1-\\delta}{2^{2}} \\ln n\\right\\rceil$ for sufficiently large [FORMULA220]?",
      "topics": [
        "optimization",
        "Estimation of Distribution Algorithms",
        "Univariate Marginal Distribution Algorithm"
      ],
      "answer": "Practitioners should configure the UMDA to run for at most [FORMULA237] iterations to sample the optimum for the first time with a probability of at least [FORMULA238]. This is determined by applying a union bound over all failure probabilities. Specifically, all frequencies are at [FORMULA227] after the first $\\left\\lceil\\frac{n}{d+1}\\right\\rceil$ iterations and stay there for at least [FORMULA229] additional iterations with a probability of at least [FORMULA230]. Following this, the optimum is sampled in each iteration with a probability of $\\left(1-\\frac{1}{n}\\right)^{n} \\geq \\left(1-\\frac{1}{n}\\right) \\frac{1}{e}$, and after $\\left\\lceil\\frac{n}{n-1} e \\ln n\\right\\rceil$ additional iterations, the optimum is sampled with a probability of at least $1-(1-\\frac{n-1}{n} \\frac{1}{e})^{\\left\\lceil\\frac{n}{n-1} e \\ln n\\right\\rceil} \\geq 1-n^{-1}$.",
      "chunk_id": 17861,
      "paper_id": "A simplified run time analysis of the univariate marginal distribution algorithm on LeadingOnes",
      "paper_title": "A simplified run time analysis of the univariate marginal distribution algorithm on LeadingOnes",
      "paper_year": "2021",
      "chunk_content": "Consider an iteration $t \\leq n$ such that position [FORMULA218] is critical. Note that $\\mu \\geq 4\\left\\lceil\\frac{1-\\delta}{2^{2}} \\ln n\\right\\rceil$ for sufficiently large [FORMULA220]. By Lemma 2, with a probability of at least [FORMULA221], for each frequency at position in [FORMULA222], we have [FORMULA223]. That is, $d+1$ additional frequencies are set to [FORMULA225]. Applying a union bound for the first [FORMULA226] iterations of the UMDA shows that all frequencies are at [FORMULA227] after the first $\\left\\lceil\\frac{n}{d+1}\\right\\rceil$ iterations and stay there for at least [FORMULA229] additional iterations with a probability of at least [FORMULA230]. Consequently, after the first [FORMULA231] iterations, the optimum is sampled in each iteration with a probability of $\\left(1-\\frac{1}{n}\\right)^{n} \\geq$ $\\left(1-\\frac{1}{n}\\right) \\frac{1}{e} \\cdot$ Thus, after $\\left\\lceil\\frac{n}{n-1} e \\ln n\\right\\rceil$ additional iterations, the optimum is sampled with a probability of at least $1-(1-$ $\\left.\\frac{n-1}{n} \\frac{1}{e}\\right)^{\\left\\lceil\\frac{n}{n-1} e \\ln n\\right\\rceil} \\geq 1-n^{-1}$. Overall, by applying a union bound over all failure probabilities, the UMDA needs at most [FORMULA237] iterations to sample the optimum for the first time with a probability of at least [FORMULA238].",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:05",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Estimation of Distribution Algorithm (EDA) discussed, what statistical analysis tools are mentioned that are used for efficient computation?",
      "topics": [
        "analysis"
      ],
      "answer": "The provided text mentions efficient computational tools for statistical analysis without specifying details about the EDA itself. Francois Berenger is thanked for providing these unspecified tools.",
      "chunk_id": 449,
      "paper_id": "Efficient Sampling in Fragment-Based Protein Structure Prediction Using an Estimation of Distribution Algorithm",
      "paper_title": "Efficient Sampling in Fragment-Based Protein Structure Prediction Using an Estimation of Distribution Algorithm",
      "paper_year": "2013",
      "chunk_content": "(PDF) ## Acknowledgments We thank RIKEN, Japan, for an allocation of computing resources on the RIKEN Integrated Cluster of Clusters (RICC) system. We are grateful to the Rosetta Commons Member Institutions (http://www.rosettacommons. org/ ) for making Rosetta source code available. We thank Francois Berenger for providing efficient computational tools for statistical analysis. We thank Prof. Jeremy Tame for proofreading the manuscript. ## Author Contributions Conceived and designed the experiments: DS KYJZ. Performed the experiments: DS. Analyzed the data: DS KYJZ. Wrote the paper: DS KYJZ. 13. Jones DT, McGuffin LJ (2003) Assembling novel protein folds from supereccondary structural fragments. Proteins 53 Suppl 6: 480-5. 14. Hamdryck T, Kent JT, Krogh A (2006) Sampling realistic protein conformations using local structural bias. PLoS Comput Biol 2: e131. 15. Li SC, Bu D, Xu J, Li M (2008) Fragment-HMM: a new approach to protein structure prediction. Protein Sci 17: 1925-34.",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:06",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "According to the details in the chunk, what advantages does the ISEDPSO algorithm have over ISEDPSO-1 and ISEDPSO-2 in the context of water distribution network (WDN) optimization?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "Particle Swarm Optimization",
        "Hybrid Algorithms",
        "Water Distribution Network Optimization"
      ],
      "answer": "The ISEDPSO algorithm uses historical best positions to construct the EDA samples, whereas ISEDPSO-1 uses the better half of the personal best (Pbest) positions, and ISEDPSO-2 uses the better half of the global best (Gbest) positions. The advantage of using historical best positions in ISEDPSO is not explicitly stated in terms of performance improvement but is implied to be beneficial for reducing calculation demands in WDN optimization, which involves complex hydraulic calculations at each iteration, because these two parameters are critical to reduce calculation demands.",
      "chunk_id": 24474,
      "paper_id": "Estimation of distribution algorithm enhanced particle swarm optimization for water distribution network optimization",
      "paper_title": "Estimation of distribution algorithm enhanced particle swarm optimization for water distribution network optimization",
      "paper_year": "2016",
      "chunk_content": "For WDN optimization that involves complex hydraulic calculations at each iteration, these two parameters are critical to reduce calculation demands. The Pseudo-code of this improved sequential PSO-EDA (ISEDPSO) algorithm can be found in Appendix C. For the purpose of exploring the benefit of using historical best positions, two other variants of this algorithm, ISEDPSO-1 and ISEDPSO-2 are designed. While ISEDPSO algorithm uses historical best positions to construct the EDA samples, ISEDPSO-1 and ISEDPSO-2 use the better half of the personal best $\\left(\\mathrm{P}_{\\text {best }}\\right)$ and the better half of the global best $\\left(\\mathrm{G}_{\\text {best }}\\right)$, respectively. ### 2.7 Parallel hybridization of PSO and EDA (PEDPSO) In previous versions of hybridization of the PSO with EDA that are discussed in the literature review, although they have different specific implementation strategies, it is a common practice that the PSO and EDA update in sequence. They either update the particles using PSO and EDA alternately, or select a PSO and EDA update process for each particle. In the proposed ISEDPSO, the PSO and EDA update processes are implemented alternately for the whole population.",
      "question_type": "advantage",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:08",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing the EDA-LF algorithm, what considerations should be taken into account regarding the parameter settings to ensure effective performance compared to a Genetic Algorithm (GA), specifically concerning the balance between exploration and exploitation in the context of the MCEVRP-STW\\&MCT problem?",
      "topics": [
        "mutation"
      ],
      "answer": "To effectively implement the EDA-LF algorithm and outperform a GA on the MCEVRP-STW\\&MCT problem, one must carefully tune its parameters, as indicated in Table 5 (though the table itself is not provided in the text). The text suggests that EDA-LF performs better on medium and large-scale instances compared to small-scale instances, implying parameter tuning should prioritize exploration for larger search spaces. This means adjusting parameters to maintain diversity in the probabilistic model. In contrast, the GA uses crossover probability $P_{\\text {co }}=0.9$, swap mutation operator probability $P_{\\text {swap }}=0.05$, and inversion mutation operator probability $P_{\\text {inv }}=0.05$. The EDA-LF parameter settings should be calibrated to achieve a similar, or better, balance between exploration (similar to mutation in GA) and exploitation (similar to crossover in GA) to efficiently solve the MCEVRP-STW\\&MCT problem, especially for medium and large-scale instances.",
      "chunk_id": 19048,
      "paper_id": "An improved estimation of distribution algorithm for multi-compartment electric vehicle routing problem",
      "paper_title": "An improved estimation of distribution algorithm for multi-compartment electric vehicle routing problem",
      "paper_year": "2021",
      "chunk_content": "Moreover, to compare the performance of these two algorithms fairly, we all set the stopping criterion of 100 s , which is an acceptable short time. The parameters of the GA are set as follows: the crossover probability $P_{\\text {co }}=0.9$, the probability in the swap mutation operator $P_{\\text {swap }}=0.05$ and the probability in the inversion mutation operator $P_{\\text {inv }}=0.05$. The parameters of the EDA-LF are set as in Table 5. For each of these 27 instances, the EDA-LF and the GA are used to solve the MCEVRP-STW\\&MCT and run 10 times independently, then the mean values are listed and the difference values $\\Delta_{\\mathrm{E}-\\mathrm{G}}$ are calculated in Table 6. Table 6 Comparison between the EDA-LF and the GA From Table 6, it can be seen that the EDA-LF outperforms the GA in 18 instances, whose difference value is negative and bold. And we can also find that the EDA-LF is better than the GA for the medium and large-scale instances except for instance 50C103, while worse than the GA for the small-scale instances except for the instance 25R101. Therefore, in general, compared with the GA, the EDA-LF can often get better solutions in a relatively short time when solving medium and large-scale instances.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:11",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How is the cost function, C(Y), defined for evaluating the combination of solvents in the Estimation of Distribution Algorithm?",
      "topics": [
        "optimization"
      ],
      "answer": "The cost function, C(Y), is defined as the sum of the product of the percentage of each solvent in the mixture (y_i) and its cost (c_i), summed over all p solvents:  C(Y) = ∑(from i=1 to p) y_i * c_i, where ∑(from i=1 to p) y_i = 1.",
      "chunk_id": 4768,
      "paper_id": "Estimation of distribution algorithms using Gaussian Bayesian networks to solve industrial optimization problems constrained by environment variables",
      "paper_title": "Estimation of distribution algorithms using Gaussian Bayesian networks to solve industrial optimization problems constrained by environment variables",
      "paper_year": "2022",
      "chunk_content": "If the dependencies between and within both subsets of variables are not considered, only the $p$ variables involved in the cost function will be observed during the optimization process. Thus, the cheapest combination of solvents found by the algorithm will be to use $100 \\%$ of the cheapest solvent and not using the others, as they would increase the cost. Optimizing only the solvent variables could be solved using a simpler optimizer, such as a gradient descent. The aim of this paper is to find the cheapest solvent combination, but keeping the patterns identified in the initial data between and within both subsets of variables. These patterns constrain the cost, and, for this reason, the implemented algorithm must identify the patterns in the initial data, and generate valid solutions according to them. [IMAGE_0] Fig. 1 Sketch of the chemical process to optimize. The combination of solvents is influenced by the substance properties and the quality that the final product must attain (Color figure online) The cost function is then given by, $$\nC(\\boldsymbol{Y})=\\sum_{i=1}^{p} y_{i} c_{i}\n$$ where $y_{i}$ and $c_{i}$ are the percentage of each solvent in the mixture $\\left(\\sum_{i=1}^{p} y_{i}=1\\right)$ and its cost, respectively, and $$\nP\\left(X_{1}, X_{2}, \\ldots, X_{n}\\right)=\\prod_{i=1}^{n} P\\left(X_{i} \\mid \\mathbf{P a}\\left(X_{i}\\right)\\right)\n$$ is the total number of solvents.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:12",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs when handling the product and summation of quantities in logarithmic form, particularly addressing potential overflows?",
      "topics": [
        "algorithm"
      ],
      "answer": "The algorithmic approach aims to efficiently compute products and summations of quantities within Estimation of Distribution Algorithms by storing values in logarithmic form to avoid underflow. When computing equation (11), the algorithm adds stored logarithmic values for products. For summations, it converts values to exponential form, adds them, and converts the result back to logarithmic form. To prevent overflows during summation, a rescaling technique is applied using the variables $F$, $M$, $E$, and $D$ to ensure that the maximum value being added is $E$, calculated as $F-20$, where $e^{F}$ is the maximum representable floating-point number.",
      "chunk_id": 26638,
      "paper_id": "On the convergence of an estimation of distribution algorithm based on linkage discovery and factorization",
      "paper_title": "On the convergence of an estimation of distribution algorithm based on linkage discovery and factorization",
      "paper_year": "2005",
      "chunk_content": "Thus, we store these quantities in logarithmic form. In the computation of equation (11), the quantities $e_{i}\\left(x_{s_{i}}\\right)=$ $e^{u f_{i}\\left(\\pi_{s_{i}} x\\right)}$ are stored in logarithmic form as $u f_{i}\\left(\\pi_{s_{i}} x\\right)$. The computation of equation (11) involves both product and summation of these quantities. To compute a product, we add the stored values. To compute a summation, we must convert the stored values to exponential form, perform the addition and then convert the final result to logarithmic form. In the computation of a summation, additional precautions are taken to avoid overflows. For example, to compute $$\n\\sum_{j \\in \\mathcal{S}(i)} e^{u f_{j}\\left(\\pi_{s_{j}} x\\right)}\n$$ the stored value could be computed as $$\n\\log \\sum_{j \\in \\mathcal{S}(i)} e^{u f_{j}\\left(\\pi_{s_{j}} x\\right)}\n$$ However, this computation might overflow. So we make the following modifications. Define $F$ such that $e^{F}$ is the maximum representable floating point number in the language to be implemented. Let $M=\\max _{j \\in \\mathcal{S}(i)} u f_{j}\\left(\\pi_{s_{j}} x\\right), E=F-20$ and we rescale all the stored values to be added in such a way that the maximum value will be $E$. The computation of the stored value for the summation is done by $$\nD+\\log \\left(\\sum_{j \\in \\mathcal{S}(i)} e^{u f_{j}\\left(\\pi_{s_{j}} x\\right)-D}\\right)\n$$ where $D=M-E$. In other words, first $D$ is subtracted from each value to be added.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:14",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of the cellular compact Genetic Algorithm (CCGA), how does the algorithmic mechanism of adaptive weight, denoted as $\\beta$, influence the probability vector combination, and what problem, specifically associated with the cooperative compact genetic algorithm, does this mechanism aim to address?",
      "topics": [
        "genetic",
        "Estimation of Distribution Algorithms",
        "cellular compact Genetic Algorithm",
        "probability vector combination",
        "adaptive weight"
      ],
      "answer": "In the cellular compact Genetic Algorithm (CCGA), the adaptive weight, $\\beta$, is used in the probability vector combination equation: $\\mathcal{P}_{i}^{t+1}(x)=\\beta \\mathcal{P}_{i}^{t}(x)+(1-\\beta) \\mathcal{P}_{i}^{t}(x)$. Here, $\\mathcal{P}_{i}^{t+1}(x)$ represents the new inner probability vector of a CCGA cell, and $\\mathcal{P}_{i}^{t}(x)$ represents the best incoming probability vector from neighbors. The value of $\\beta$ is adaptively calculated from the best confident counter among neighbors. This adaptive weighting scheme is designed to mitigate the greedy search characteristic inherent in the cooperative compact genetic algorithm. The confident counter, which reflects the frequency of updating the probability vector of each CCGA cell, determines the updating rate, influencing the probability of reaching a better solution. The higher the confident counter values, the greater the chance of reaching a better solution.",
      "chunk_id": 12137,
      "paper_id": "FPGA Implementation of a Cellular Univariate Estimation of Distribution Algorithm and Block-Based Neural Network as an Evolvable Hardware",
      "paper_title": "FPGA Implementation of a Cellular Univariate Estimation of Distribution Algorithm and Block-Based Neural Network as an Evolvable Hardware",
      "paper_year": "2008",
      "chunk_content": "3 shows the pseudocode of the cellular compact GA. After probability vector of each cell in the cellular automata space is initialized to the mid-point range, two individuals are generated from the probability vector, then compete similar to a normal compact GA. The proposed algorithm is different from the normal compact GA and the cooperative compact GA [14] in four ways: (1) The cellular-like topology employs uniform cell type. This allows flexibility and ease of implementation. (2) The confidence toward the better probability vector is calculated as confident counters and passed directly to neighbor cells. In Fig. 3, the step 3, 4 and 5 are modified to the normal compact GA. (3) Improved probability vector combination by local search and adaptive combination in step 6 shown in Fig. 3. This combination scheme provides a solution to the greedy search characteristic of the cooperative compact genetic algorithm $[14,18]$. The local search is implemented through the use the confident counter that keeps the frequency of the updating to the probability vector of each cellular compact GA cell. The higher confident counter values contribute to higher chance to reach the better solution The probability vector combination use the following equation: $$\n\\mathcal{P}_{i}^{t+1}(x)=\\beta \\mathcal{P}_{i}^{t}(x)+(1-\\beta) \\mathcal{P}_{i}^{t}(x)\n$$ Where $\\beta$ is an adaptive weight calculated from the best confident counter among neighbors $\\mathcal{P}_{i}^{t+1}(x)$ is a new inner probability vector of a CCGA cell $\\mathcal{P}_{i}^{t}(x)$ is the best incoming probability vector from neighbors (4) Adaptive migration rate of the probability vector for each CCGA cell using confident counter. Since the updating rate of each CCGA cell depends on its confident counter which is different for each cell.",
      "question_type": "algorithmic mechanism",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:17",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "What specific implementation steps are required for the EDA with a path relinking technique (P-EDA) to avoid performing a blind search?",
      "topics": [
        "Estimation of Distribution Algorithm",
        "path relinking"
      ],
      "answer": "The implementation of P-EDA utilizes a path relinking technique to avoid blind searches. While the exact steps are not detailed, the core idea involves using path relinking to guide the search process, presumably by exploring paths between elite solutions instead of randomly sampling the search space.",
      "chunk_id": 4142,
      "paper_id": "A matrix-cube-based estimation of distribution algorithm for blocking flow-shop scheduling problem with sequence-dependent setup times",
      "paper_title": "A matrix-cube-based estimation of distribution algorithm for blocking flow-shop scheduling problem with sequence-dependent setup times",
      "paper_year": "2022",
      "chunk_content": "The test results demonstrated that DEABC was superior to the compared algorithms. Tasgetiren, et al. (2015) presented a populated local search with differential evolution (DEPLS). The test results showed that DEPLS performed better than some of the best performing algorithms from the literature. Han, et al. (2016) introduced a modified fruit fly optimization (MFFO) algorithm, in which a problem-specific heuristic, a neighborhood strategy, and a speedup insert-neighborhood based local search are employed. Shao, et al. (2018a) developed an EDA with a path relinking technique (P-EDA). The path relinking technique here is utilized to avoid performing the blind search. The results compared with various other high-performing algorithms verified the effectiveness of P-EDA. Shao, et al. (2019) proposed a discrete invasive weed optimization (DIWO), in which a random-insertion-based spatial dispersal, a shufflebased referenced local search, and an improved competitive exclusion are devised. The results demonstrated that DIWO outperformed the compared algorithms. Besides the makespan minimization, Ribas, et al. (2015) devised an effective discrete artificial bee colony algorithm (DABCRCT) for minimizing the total flowtime criterion.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:18",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the significance-based compact genetic algorithm (sig-cGA), which is a type of Estimation-of-Distribution Algorithm (EDA) used to optimize a function \\(f: {0,1}^n \\rightarrow \\mathbb{R}\\), what is the high-level goal of evolving a probabilistic model of the solution space [FORMULA949]?",
      "topics": [
        "genetic"
      ],
      "answer": "The high-level goal of evolving the probabilistic model [FORMULA949] in significance-based compact genetic algorithm (sig-cGA) is to direct the model in such a way that it becomes more likely to sample the optimum of the function \\(f: {0,1}^n \\rightarrow \\mathbb{R}\\).",
      "chunk_id": 5867,
      "paper_id": "Choosing the right algorithm with hints from complexity theory",
      "paper_title": "Choosing the right algorithm with hints from complexity theory",
      "paper_year": "2024",
      "chunk_content": "Hence, we conclude that in expected runtime $O(\\log n), y$ and [FORMULA942] are only different in bit position [FORMULA943]. As discussed above, [FORMULA944] and [FORMULA945] are set to the optimum after having run the for loop for [FORMULA946] times. Lemma 19 thus implies the following theorem on the binary unbiased black-box complexity of the DLB problem. Theorem 20. The binary unbiased black-box complexity of the DLB problem is [FORMULA947]. Algorithm 5 reveals that the ability to learn how good solutions look like plays an important role in solving the DLB problem. This inspires the study of EDAs, and more specifically, the significance-based EDA proposed by Doerr and Krejca [21] in the following. # 4.2. Significance-based compact genetic algorithm (sig-cGA) Estimation-of-distribution algorithms (EDAs) optimize a function $f:\\{0,1\\}^{\\mathrm{n}} \\rightarrow \\mathbb{R}$ by evolving a probabilistic model of the solution space [FORMULA949] in such a direction that this probabilistic model becomes more likely to sample the optimum. We now present the sig-cGA (Algorithm 6) proposed by Doerr and Krejca [21].",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:20",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "In the context of the Reversible Lanes Optimization Problem (RLOP), how does the Frank-Wolfe (FW) algorithm address the objective function of the User Equilibrium (UE) assignment problem, particularly when a roadway's capacity, denoted as $n^{\\mathrm{m}}$, equals 0?",
      "topics": [
        "objectives"
      ],
      "answer": "The Frank-Wolfe (FW) algorithm is adopted to solve the User Equilibrium (UE) assignment problem. Specifically, when the capacity $n^{\\mathrm{m}}$ of a roadway $t^{\\mathrm{m}}$ equals 0, the roadway is excluded from the set $_{n}$ and, consequently, is not considered in the objective function. This implies that the algorithm avoids evaluating or optimizing for roadways with no capacity, simplifying the computation by focusing only on viable routes.",
      "chunk_id": 16819,
      "paper_id": "A Histogram Estimation of Distribution Algorithm for Reversible Lanes Optimization Problems",
      "paper_title": "A Histogram Estimation of Distribution Algorithm for Reversible Lanes Optimization Problems",
      "paper_year": "2019",
      "chunk_content": "$$\nt_{a}\\left(x_{r}, n_{r}\\right)=t_{r}^{0}\\left(1+0.15\\left(\\frac{x_{r}}{n_{r} K_{a}}\\right)^{4}\\right), n_{r} \\neq 0\n$$ Apart from the network information given by the graph $G$, the RLOP also takes origin-destination (O-D) pairs as input variables. An O-D pair consists of an origin node, a destination node and a flow demand between these two nodes. Usually a RLOP includes several O-D pair demands. The number of O-D pair will affect the complexity of RLOP. ## B. Problem Objectives In the lower-level model, the objective function of UE assignment problem is as Eq. (4) shows. In our work, we adopt FW algorithm [14] to solve this problem. In particular, when $n^{\\mathrm{m}}$ equals to 0 , roadway $t^{\\mathrm{m}}$ will not exist in set $_{n}$. Thus, there is no need to consider that roadway in objective functions. The optimal solution of lower-level is $$\n\\begin{gathered}\nP_{n}^{i}(j)=\\frac{H_{n}^{i}(j)}{\\sum_{i=0}^{n^{\\max }} H_{n}^{i}(i)} \\\\\n\\sum_{i=0}^{n^{\\max }} H_{n}^{i}(i)=1\n\\end{gathered}\n$$, which denotes the equilibrium flow in roadway $t_{0}^{0}$. To be specific, $\\{2.5,5,7.5,10\\} ; n_{a}$ has to contain all the O-D pair demands that pass through roadway $\\{1,2,3,4\\}$. In fact, some nodes may be isolated in some solutions, which let the traffic demands toward these isolated nodes cannot be satisfied.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:22",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing Tree-EDA, what programming considerations should be taken into account regarding the size of the problem being addressed, specifically in the context of learning accurate interaction models?",
      "topics": [
        "EDAs"
      ],
      "answer": "When implementing Tree-EDA, the population size is a critical consideration. The implementation needs to ensure that the population size is sufficient to learn an accurate model of the interactions, especially in very large problems. Insufficient population sizes can hinder the algorithm's ability to effectively model dependencies within the problem space.",
      "chunk_id": 11535,
      "paper_id": "Adding Probabilistic Dependencies to the Search of Protein Side Chain Configurations Using EDAs",
      "paper_title": "Adding Probabilistic Dependencies to the Search of Protein Side Chain Configurations Using EDAs",
      "paper_year": "2008",
      "chunk_content": "This might be explained by the fact that the population sizes used by Tree-EDA and Tree-EDA [FORMULA63] are still insufficient to learn an accurate model of the interactions in very large problems. An exhaustive analysis of this question is beyond the scope and space limitations of this paper. Table 2. Results achieved by UMDA, Tree-EDA and Tree-EDA [FORMULA64] for the selected instances Additional experiments were conducted to investigate the structural differences between the best solutions found by UMDA and those obtained using TreeEDA and Tree-EDA [FORMULA65]. This type of experiments is illustrated using dimer protein pdb1tki. This protein has two symmetric chains and the number of residues before the application of the Goldstein criterion is 576 . The energies corresponding to best side chain structures found by UMDA, Tree-EDA and Tree-EDA [FORMULA66] were respectively $858.67,856.62$ and 855.56 . The structures found by Tree-EDA and Tree-EDA [FORMULA68] are respectively shown in Figure 1 (left) and Figure 1 (right). The protein structures found by UMDA and Tree-EDA are different in only 7 residues.",
      "question_type": "implementation focused",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:23",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What statistical measure is used to compare the performance of different feature selection methods, and what p-value threshold is used to determine significant differences?",
      "topics": [
        "selection"
      ],
      "answer": "A McNnemar statistical test is used to compare the performance of different feature selection methods. A p-value of 0.05 is used as the threshold to determine if there is a significant difference between the methods based on their q9 values.",
      "chunk_id": 26928,
      "paper_id": "Fast feature selection using a simple estimation of distribution algorithm  a case study on splice site prediction",
      "paper_title": "Fast feature selection using a simple estimation of distribution algorithm: a case study on splice site prediction",
      "paper_year": "2003",
      "chunk_content": "A McNnemar statistical test with $\\mathrm{p}=0.05$ did not reveal any significant difference to prefer one method over the other on the basis of their q9 values. Furthermore the results of the EDA-approach can be obtained in much less time, showing the advantage of a simple EDA method like the UMDA. To compare the running time (speed) of the greedy and heuristic algorithm, two aspects need to be considered : the number of model evaluations and the average number of features for a model evaluation. The number of model evaluations needed for both methods can be calculated using formulas 10 and 12. For the greedy algorithm the average number of features that has to be evaluated is calculated with formula 11, the heuristic algorithm with a constrained subset size has a fixed number of features. Table 4 shows the results of the time comparisons for constrained subsets. Clearly the EDA-approach needs considerably less model evaluations as the size of the feature subsets decreases, resulting in a faster feature selection algorithm. Furthermore, all models in the EDAapproach have a fixed number [FORMULA120] of features to be trained on, whereas the greedy approach starts with the full feature set and gradually decreases the number of features until a set of [FORMULA121] features remains.",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:25",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for RM-MEDA, particularly focusing on its modeling and sampling steps within the evolutionary loop?",
      "topics": [
        "objectives"
      ],
      "answer": "The RM-MEDA algorithm aims to efficiently explore the search space by employing a local PCA-based clustering of the selected individuals \\mathbf{U}_{i}^{j} into K disjoint clusters, denoted as S^{1}, S^{2}, \\ldots, S^{K}. For each cluster, a model is built using equations (2), (4), and (5). The subsequent sampling step generates new populations \\mathbf{O}(\\mathbf{t}) from this model, incorporating a perturbation term \\(v \\sim N\\left(0, \\delta_{j} \\mathbf{I}\\right)\\), where \\(\\delta_{j}\\) is calculated based on the eigenvalues \\(\\lambda_{m}^{j}\\) to \\(\\lambda_{n}^{j}\\) of the covariance matrix. This process, combined with non-dominated sorting and crowding distance assignment, facilitates the creation of the next generation \\mathbf{P}(\\mathbf{t}+\\mathbf{1}) by balancing exploration and exploitation around the estimated Pareto set.",
      "chunk_id": 914,
      "paper_id": "Multiobjective Memetic Estimation of Distribution Algorithm Based on an Incremental Tournament Local Searcher",
      "paper_title": "Multiobjective Memetic Estimation of Distribution Algorithm Based on an Incremental Tournament Local Searcher",
      "paper_year": "2014",
      "chunk_content": "Modeling: Perform local PCA to partition $\\mathbf{U}_{i}^{j}$ into $K$ disjoint clusters $S^{1}, S^{2}, \\ldots, S^{K}$. For each cluster $S^{i}$, build model (2) by (4) and (5). Step 4. Sampling: Sample new population $\\mathbf{O}(\\mathbf{t})$ from model (2) and evaluate $v \\sim N\\left(0, \\delta_{j} \\mathbf{I}\\right)$. Step 5. Non-dominated Sorting and Crowding Distance Assignment: Use the famous fast non-dominated sorting procedure and select solutions at first several low ranks from $$\n\\delta_{j}=\\frac{1}{n-m+1}\\left(\\lambda_{m}^{j}+\\lambda_{m+1}^{j}+\\cdots+\\lambda_{n}^{j}\\right)\n$$ and $\\mathbf{N}(\\mathbf{t})$. At the critical rank, the crowding distance computation is employed to select individuals with higher values of crowding distance. Then, $\\mathbf{P}(\\mathbf{t}+\\mathbf{1})$ is created. Step 6. set $t:=t+1$ and go to Step 2. Algorithm 1: The main loop of RM-MEDA. [IMAGE_0] Figure 1: Illustration of individual solutions scattered around PS in the decision space. ## 3. The Framework of RM-MEDA Under some smoothness assumptions, it could be induced from the Karush-Kuhn-Tucker condition that the PS of (1) defines a piecewise continuous ( $n_{t}$ )-dimensional manifold, where $N(t)$ is the number of objectives.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:27",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of modeling wind speed uncertainty, what mathematical principles govern the relationship between the mean wind speed, represented by $\\mu$, and the standard deviation, represented by $\\sigma$, as described by the equation $\\sigma=0.231+0.197 \\mu$?",
      "topics": [
        "algorithm"
      ],
      "answer": "The equation $\\sigma=0.231+0.197 \\mu$ describes a linear relationship where the standard deviation $\\sigma$, which represents the amount of uncertainty in wind speed, increases as the mean wind speed $\\mu$ increases. This principle suggests that when the average wind speed is high, the variability around that average is also high, and vice versa. This relationship implies that a fixed standard deviation should not be used, and more scenarios are needed when the mean value of wind is high due to the high uncertainty.",
      "chunk_id": 4028,
      "paper_id": "Reliability evaluation of a composite power system in the presence of renewable generations",
      "paper_title": "Reliability evaluation of a composite power system in the presence of renewable generations",
      "paper_year": "2022",
      "chunk_content": "The relationship between $\\mu$ and $\\mu$ can be attained based on the measurement and experimental studies as displayed in Fig. 3 for wind speed [30]. Generally, these experiments are performed by predicting the hourly average wind speed for a one-year period. It can be seen in Fig. 3 that increasing the mean value of wind speed also leads to an increase in standard deviation $$\n\\begin{aligned}\n& f(g)=\\omega\\left(k_{1} / c_{1}\\right)\\left(g / c_{1}\\right)^{(k_{1}-1)} e^{-\\left(g / c_{1}\\right)^{2} t}+(1-\\omega)\\left(k_{2} / c_{2}\\right)\\left(g / c_{2}\\right)^{(k_{2}-1)} e^{-\\left(g / c_{2}\\right)^{2} t}, 0<g<\\infty\n\\end{aligned}\n$$. Whenever, the mean value of wind is high, the amount of uncertainty is also high, and unlike most previous studies, fixed standard deviation values for wind generation should not be adopted. Therefore, in the MCS method to generate wind power scenarios, more scenarios are needed to be generated during the hours with high mean values for RES units due to the high uncertainty. On the other hand, for hours with the small mean values, the uncertainties of wind speed can be modeled with fewer scenarios. The linear relationship between $\\mathrm{S}_{\\mathrm{i}}$ and $\\mathrm{FP}_{\\mathrm{i}}$ for wind is considered as given in Eq. (5). Accurate modeling of wind power generations in the states of the composite power systems will lead to more precise reliability assessment results. $$\n\\sigma=0.231+0.197 \\mu\n$$ To clarify this issue, the relationship between the amount of uncer- $$\n\\begin{aligned}\n& f(g)=\\omega\\left(k_{1} / c_{1}\\right)\\left(g / c_{1}\\right)^{(k_{1}-1)} e^{-\\left(g / c_{1}\\right)^{2} t}+(1-\\omega)\\left(k_{2} / c_{2}\\right)\\left(g / c_{2}\\right)^{(k_{2}-1)} e^{-\\left(g / c_{2}\\right)^{2} t}, 0<g<\\infty\n\\end{aligned}\n$$ tainty and the average wind speed is shown in Fig.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:29",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm that uses an elitist population, how should practitioners calculate the coding similarity at each string position using the elitist probability schema (EPS), given the number of individuals with 0s ($m_{0i}$) and 1s ($m_{1i}$) at the $i$th string position?",
      "topics": [
        "fitness"
      ],
      "answer": "To calculate the coding similarity at each string position using the EPS, practitioners must first determine $p_{0i}$ and $p_{1i}$, which represent the probabilities of 0s and 1s at the $i$th string position, respectively. Then, for each position $i$, calculate $w_{ci}$ based on whether the coding $c_i$ equals $e_i$. If $c_i = e_i$, then $w_{ci} = \\frac{\\max \\{p_{0 i}, p_{1 i}\\}}{\\min \\{p_{0 i}, p_{1 i}\\}}$. Otherwise, if $c_i \\neq e_i$, then $w_{ci} = \\frac{\\min \\{p_{0 i}, p_{1 i}\\}}{\\max \\{p_{0 i}, p_{1 i}\\}}$. Here, $e_i$ refers to the elitist coding at position $i$. The values $m_{0i}$ and $m_{1i}$ (where $m_{0i} + m_{1i} = M$, with $M$ being the number of individuals in the elitist population) are used to derive $p_{0i}$ and $p_{1i}$, although the specific derivation is not provided.",
      "chunk_id": 1667,
      "paper_id": "An exploratory research of elitist probability schema and its applications in evolutionary algorithms",
      "paper_title": "An exploratory research of elitist probability schema and its applications in evolutionary algorithms",
      "paper_year": "2014",
      "chunk_content": "$M$ is the number of individuals in the elitist population. $m_{0 i}$ and $m_{1 i}$ are the number of individuals with 0 s at the $i$ th string position and 1 s at the $w_{c i}= \\begin{cases}\\frac{\\max \\left\\{p_{0 i}, p_{1 i}\\right\\}}{\\min \\left\\{p_{0 i}, p_{1 i}\\right\\}} & c_{i}=e_{i} \\\\ \\frac{\\min \\left\\{p_{0 i}, p_{1 i}\\right\\}}{\\max \\left\\{p_{0 i}, p_{1 i}\\right\\}} & c_{i} \\neq e_{i}\\end{cases}$ th string position, respectively $\\left(m_{0 i}+m_{1 i}=M\\right)$. We illustrate the rationales of the EPS in the evolution process using a simple example. As shown in Fig. 1, the coding similarity of the population increases gradually in EAs, and EPS can express the coding similarity of the population more flexibly and accurately than schemas and probability vectors. It is worth noticing that, for each generation, EPS can objectively quantify the coding similarity of a population. EPS is only based on the coding of populations, and is not related to the fitness function. However, EPS expresses the accumulative effect of fitness selection with respect to the coding similarity of the population. The elitist population determines the inherent property of the EPS. A prominent feature of the EPS is that it can continuously provide the search direction of the promising solutions. In essence, each pair of $p_{0 i}$ and $p_{1 i}$ expresses a judgment of the promising solutions at the $E_{I}$ th string position.",
      "question_type": "application",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:31",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed in the context of gene selection, how is the relevance of a gene, denoted as 'x', quantified before its inclusion in the evolutionary process, and what is the mathematical formulation used to rank its significance based on expression values?",
      "topics": [
        "analysis"
      ],
      "answer": "Prior to applying the Estimation of Distribution Algorithm, genes are ranked using the ratio of between-groups sum of squares (BSS) to within-groups sum of squares (WSS), represented as Rank(x) = BSS(x) / WSS(x). BSS(x) is calculated as \\$\\sum_{i=1}^{n} \\sum_{j=1}^{c} I(y_{j}=j)(\\bar{x}_{, j}-\\bar{x})^{2}\\$, where \\$x_{i}\\$ is the expression value of gene x on the ith sample, n is the number of samples, c is the number of sample types, \\$\\bar{x}\\$ is the average expression value across all n samples, and \\$\\bar{x}_{, j}\\$ is the average expression value in samples belonging to class j. WSS(x) is calculated as \\$\\sum_{i=1}^{n} \\sum_{j=1}^{c} I(y_{j}=j)(x_{i}-\\bar{x}_{, j})^{2}\\$. This ranking aims to narrow down the gene pool for subsequent optimization by the MOEA.",
      "chunk_id": 11446,
      "paper_id": "Optimal Genes Selection with a New Multi-objective Evolutional Algorithm Hybriding NSGA-II with EDA",
      "paper_title": "Optimal genes selection with a new multi-objective evolutional algorithm hybriding NSGA-II with EDA",
      "paper_year": "2008",
      "chunk_content": "A further advantage of EDAs is that the relationships between the random variables can be explicitly expressed through the joint probability distribution learnt from them, instead of being implicitly kept by the individuals of successive populations as building blocks. ### 3.2 Hybriding NSGA-II with EDA to Selection Genes 3.2.1 Initial gene pool. Because the number of genes in the dataset is always huge and some of them are not useful for classification, it is necessary to narrow down genes from many thousand down to the order of $10^{2}$ to speed up searching with MOEAs which may cost a lot of running time. There are many filter approaches can do this. Instead of using principle component analysis (PCA), which is most common way, we use the ratio of between-groups to within-groups sum of squares (BSS/WSS) given by Dudoit[9]. $$\n\\operatorname{Rank}(x)=\\frac{B S S(x)}{W S S(x)}=\\frac{\\sum_{j=1}^{n} \\sum_{j=1}^{c} I\\left(y_{j}=j\\right)\\left(\\bar{x}_{, j}-\\bar{x}\\right)^{2}}{\\sum_{i=1}^{n} \\sum_{j=1}^{c} I\\left(y_{j}=j\\right)\\left(x_{i}-\\bar{x}_{, j}\\right)^{2}}\n$$ Here, for gene $x, x_{i}$ is the expression value on the ith sample, n is the number of samples and c is the number of sample types, $\\bar{x}$ is the average expression value in all n samples, $\\bar{x}_{, j}$ is the average expression value in samples belonging to class j.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:34",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the General Mallows Model EDA (GM-EDA) compare to Algebraic Differential Evolution Algorithm (DEP) in the context of Permutation Flowshop Scheduling Problem (PFSP) regarding the optimization of the Total Flow Time (TFT)?",
      "topics": [
        "search"
      ],
      "answer": "The text mentions that both General Mallows Model EDA (GM-EDA) and Algebraic Differential Evolution Algorithm (DEP) have been proposed to solve the Permutation Flowshop Scheduling Problem (PFSP) with the objective of optimizing the Total Flow Time (TFT), which is calculated as the sum of completion times of all jobs. However, the text does not provide a direct comparison of their performance or specific advantages/disadvantages of one over the other.",
      "chunk_id": 14258,
      "paper_id": "Multiobjective decomposition-based Mallows Models estimation of distribution algorithm. A case of study for permutation flowshop scheduling problem",
      "paper_title": "Multiobjective decomposition-based Mallows Models estimation of distribution algorithm. A case of study for permutation flowshop scheduling problem",
      "paper_year": "2017",
      "chunk_content": "As the release time is always 0 , therefore, $F_{j}(\\sigma)=C_{i}(\\sigma)$. The total flow time is formulated as follows: $$\n\\operatorname{TFT}(\\sigma)=\\sum_{i=1}^{n} C_{i}(\\sigma), \\quad i=1, \\ldots, n\n$$ Different metaheuristics have been proposed to solve PFSP regarding the optimization of the TFT, such as, Variable Neighborhood Search (VNS) [14], Asynchronous Genetic Local Search (AGA) [49], General Mallows Model EDA (GM-EDA) [7], Algebraic Differential Evolution Algorithm (DEP) [42]. As in several real-world problems, PFSP can be defined as a multi-objective optimization problem. If no decision making preference is known a priori, all the objectives are considered equally important. A multi-objective PFSP can be stated as: $$\n\\begin{aligned}\n& \\operatorname{minimize} F(\\sigma)=\\left(C_{\\max }(\\sigma), \\operatorname{TFT}(\\sigma)\\right) \\\\\n& \\text { subject to } \\sigma \\in \\Omega\n\\end{aligned}\n$$ # 4.2. MoPFSP: related Work Different algorithms have been proposed for solving the MoPFSP. In the following, we discuss the related works. Minella et al. [36] reviewed and evaluated a total of 23 algorithms including both flowshop-specific and general multiobjective optimization approaches on 110 benchmark test instances. These PFSP instances are those of Taillard [45] with the addition of due dates (which are used for the total tardiness criterion).",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:36",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model within the MOEA/D framework, specifically focusing on how new solutions are generated?",
      "topics": [
        "search",
        "Estimation of Distribution Algorithms (EDAs)",
        "MOEA/D framework"
      ],
      "answer": "Within the MOEA/D framework, the EDA model replaces traditional crossover and mutation operators. The algorithmic steps involve first extracting population distribution information from neighboring solutions of subproblems. Next, a probability model is built based on this information. Finally, new solutions are sampled from this probability model to perform reproduction, differing from traditional EAs that rely on crossover and/or mutation.",
      "chunk_id": 1382,
      "paper_id": "A Probability Model based Evolutionary Algorithm with Priori and Posteriori Knowledge for Multiobjective Knapsack Problems",
      "paper_title": "A Probability Model based Evolutionary Algorithm with Priori and Posteriori Knowledge for Multiobjective Knapsack Problems",
      "paper_year": "2014",
      "chunk_content": "A variety of MOEAs have been proposed for tackling MOPs. Among them, decomposition based multiobjective evolutionary algorithm (MOEA/D) [2] has shown promising results. The basic idea of MOEA/D is to decompose an MOP into a set of single objective subproblems, so that these subproblems can be solved simultaneously to obtain the solutions of the MOP. Recently, many works have been done to solve MOKP within the framework of MOEA/D. Kafafy et al. [3] used MOKP as the benchmark to study the effect of the hybridization of different metaheuristics within MOEA/D framework. A new version of MOEA/D with uniform design for solving MOKP is proposed in [4]. Ishibuchi et al. [5] examined the relation between the neighborhood size and the performance of MOEA/D on MOKP instances and two different neighborhoods, one for local mating and the other for local replacement, are introduced. It advocates that the integration of machine learning techniques into heuristics and self-tunes algorithm behaviors based on online information collected during the search [6]. Estimation of distribution algorithms (EDAs) [7] are among the promising evolutionary computation paradigms which adopt this principle. Unlike traditional evolutionary algorithms (EAs), which use crossover and/or mutation operators to generate new solutions, EDAs explicitly extract the population distribution information by probabilistic models and then sample new solutions from the models. The EDA method is used as the reproduction operator in our previous work [8] which builds an probability model based on the neighboring solutions of subproblems to do the reproduction within the MOEA/D framework.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:37",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of feature selection using Estimation of Distribution Algorithms (EDAs), what specific approach is employed to reduce computational cost related to fitness evaluation, and how does it impact the performance assessment of newly generated individuals?",
      "topics": [
        "evolutionary"
      ],
      "answer": "A \"fast estimation of distribution algorithm\" (FEDA) is used which avoids evaluating all new individuals using the actual fitness function. Instead, an extended Bayesian network serves as an approximate model to the fitness function, and this model is used to assign fitness values to each new individual. This approximation is intended to reduce the computational cost associated with evaluating the fitness of all individuals, thereby improving overall performance.",
      "chunk_id": 26759,
      "paper_id": "Fitness approximation in estimation of distribution algorithms for feature selection",
      "paper_title": "Fitness approximation in estimation of distribution algorithms for feature selection",
      "paper_year": "2005",
      "chunk_content": "Fitness Approximation in Estimation of Distribution Algorithms for Feature Selection Haixia Chen ${ }^{1}$, Senmiao Yuan ${ }^{2}$, and Kai Jiang ${ }^{2}$$d$ College of Computer Science and Technology, Jilin University, Changchun 130025, Chinahxchen2004@sohu.com$X=\\left\\{X_{1}, X_{2}, \\cdots, X_{d}\\right\\}$ The 45th Research Institute of CETC, Beijing 101601, Chinakjiang2004@sohu.com #### Abstract Estimation of distribution algorithms (EDAs) are popular and robust algorithms that combine two technical disciplines of soft computing methodologies, probabilistic reasoning and evolutionary computing, for optimization problems. Several algorithms have already been proposed by different authors. However, these algorithms may require huge computation power, which is seldom considered in those applications. This paper introduces a \"fast estimation of distribution algorithm\" (FEDA) for feature selection that does not evaluate all new individuals by actual fitness function, thus reducing the computational cost and improve the performance. Bayesian networks are used to model the probabilistic distribution and generate new individuals in the optimization process. Moreover, fitness value is assigned to each new individual using the extended Bayesian network as an approximate model to fitness function.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:39",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of defining a Petri Net (PN) model within the context of scheduling for Estimation of Distribution Algorithms (EDAs)?",
      "topics": [
        "scheduling"
      ],
      "answer": "The overall purpose is to provide a formal mathematical representation using a 4-tuple $N=(P, T, F, W)$, where $P$ represents places, $T$ represents transitions, $F$ represents directed arcs, and $W$ represents the weight of each arc, to model the scheduling problem. This model allows for analysis and manipulation of the schedule using PN properties such as markings $M: P \tothe problem.",
      "chunk_id": 21441,
      "paper_id": "Deadlock-Free Scheduling of Flexible Assembly Systems Based on Petri Nets and Local Search",
      "paper_title": "Deadlock-Free Scheduling of Flexible Assembly Systems Based on Petri Nets and Local Search",
      "paper_year": "2018",
      "chunk_content": "Next, we describe the PN model of FAS for scheduling. ## A. Basic Definitions of PNs A PN is a 4-tuple $N=(P, T, F, W)$, where $P$ and $T$ are finite and disjoint sets. $T \\neq \\varnothing . F \\subseteq$ is a set of places, and $(P \\times T) \\cup(T \\times P)$ is a set of transitions with $P \\cap T=\\varnothing, P \\neq \\varnothing$, and $T \\neq \\varnothing . F \\subseteq$ $(P \\times T) \\cup(T \\times P)$ is a set of directed arcs. $W: F \\rightarrow \\mathbb{N}^{+}$is a mapping that assigns to each arc a positive integer or weight. ${ }^{*} x=\\{y \\in P \\cup T \\mid(y, x) \\in F\\}$ is ordinary if $\\forall(x, y) \\in F, W(x, y)=1$, and in this case, $W$ can be omitted. Given a vertex $x \\in P \\cup T$, its preset and postset are defined as ${ }^{*} x=\\{y \\in P \\cup T \\mid(y, x) \\in F\\}$ and $x^{*}=\\{y \\in P \\cup T \\mid(x, y)$ $\\in F\\}$, respectively. Given $X \\subseteq P \\cup T$, we define ${ }^{\\bullet} X=\\cup_{x \\in X}$ ${ }^{\\bullet} x$ and $X^{\\bullet}=\\cup_{x \\in X} x^{\\bullet}$. A path in $M: P \\rightarrow \\mathbb{N}$ is a sequence of vertices $\\alpha=x_{1} x_{2} \\ldots x_{k}$, where $x_{i} \\in P \\cup T$ and $\\left(x_{i}, x_{i+1}\\right) \\in F \\forall i \\in$ $\\mathbb{N}_{k-1}$. A marking of $M_{0}$ is a mapping $M: P \\rightarrow \\mathbb{N}$. Given a place $p$ $\\in P$ and a marking $M, M(p)$ denotes the number of tokens in $t$ at $M . N$ with an initial marking $M_{0}$ is called a marked PN, denoted as $\\left(N, M_{0}\\right)$.",
      "question_type": "definition",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:42",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of Population-Based Incremental Learning (PBIL) update its probability vector, and what mathematical formula governs this update process according to the provided text?",
      "topics": [
        "evolutionary",
        "Population-Based Incremental Learning (PBIL)",
        "probability model update"
      ],
      "answer": "In the PBIL algorithm, the probability vector, denoted as $\\boldsymbol{P}_{i}(X)=\\left(p\\{x_{1}\\}, p\\{x_{2}\\}, \\ldots, p\\{x_{n}\\}\\right)$, where $p(x_{i})$ represents the probability of element $i$ being in a normal state, is updated based on the distribution characteristics of excellent individuals from the last generation. The update process for the probability vector from generation $l$ to $l+1$ is defined by the formula $P_{l+1}(x)=\\boldsymbol{P}_{l}(x)+a\\left(\\frac{1}{K} \\sum_{k=1}^{K} X_{l}^{2}-\\boldsymbol{P}_{l}(x)\\right)$, where $X_{l}^{i}, X_{l}^{j}, \\ldots, X_{l}^{K}$ represent the state spaces of the best $K$ individuals sampled from the $l_{\\mathrm{th}}$ generation, and 'a' is a learning rate (not explicitly defined in the provided text but implied as a factor influencing the update step). This formula adjusts the probability vector $\\boldsymbol{P}_{l}(x)$ by adding a fraction 'a' of the difference between the average state space of the best K individuals and the current probability vector.",
      "chunk_id": 14429,
      "paper_id": "Fast reliability evaluation method for composite power system based on the improved EDA and double cross linked list",
      "paper_title": "Fast reliability evaluation method for composite power system based on the improved EDA and double cross linked list",
      "paper_year": "2017",
      "chunk_content": "An EDA typically work with the probability model and its update in order to describe the spatial distribution and overall evolution trend of population [23, 24]. The characteristic that most differentiates EDA from other evolutionary search strategies such as GA is that the evolution from a generation to the next one is conducted by estimating the probability distribution of the fittest individuals, and afterwards by sampling the induced probability model (Fig. 2). According the differences between probability models, EDA can be divided into population-based incremental learning algorithm (PBIL), univariate marginal distribution algorithm, mutual information maximisation for input clustering and so on. Among these algorithms, PBIL is a recently developed technique that has drawn great concern. In the PBIL algorithm, the probability vector is updated on the basis of the distribution characteristics of the excellent individuals in last generation of population. The probability vector of the $l_{0}$, generation is represented by $\\boldsymbol{P}_{i}(X)=\\left(p\\left(x_{1}\\right), p\\left(x_{2}\\right), \\ldots, p\\left(x_{n}\\right)\\right)$, where $p\\left(x_{i}\\right)$ denotes the probability of element $i$ in normal state. The update process is as follows [24]: [IMAGE_1] Fig. 2 Basic procedures of EDA $$\nP_{l+1}(x)=\\boldsymbol{P}_{l}(x)+a\\left(\\frac{1}{K} \\sum_{k=1}^{K} X_{l}^{2}-\\boldsymbol{P}_{l}(x)\\right)\n$$ where $X_{l}^{i}, X_{l}^{j}, \\ldots, X_{l}^{K}$ are the state spaces of the best $K$ individuals sampled from the $l_{\\mathrm{th}}$ generation.",
      "question_type": "algorithmic",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:44",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm within the context of fuel cell modeling, how should practitioners balance the computational cost associated with complex fuel cell geometries against the need for accurate parameter estimation when simpler surrogate models are employed?",
      "topics": [
        "optimization"
      ],
      "answer": "Practitioners should prioritize simpler mathematical models as surrogate models to reduce computational cost during the numerous objective function evaluations required for parameter estimation. While complex models with fine meshes increase computational time, simpler models can speed up numerical simulation without significantly sacrificing accuracy in the output of interest, such as the polarization curve. This approach can also reduce numerical instability, contributing to faster calibration and uncertainty analysis when testing different parameter values.",
      "chunk_id": 7802,
      "paper_id": "Parameter estimation for empirical and semi-empirical models in a direct ethanol fuel cell",
      "paper_title": "Parameter estimation for empirical and semi-empirical models in a direct ethanol fuel cell",
      "paper_year": "2023",
      "chunk_content": "The numerical simulation depends on the geometry of the different layers of the fuel cell, which often require fine meshes, hence increasing [^0]: * Corresponding author. E-mail address: sergio.valdez@conacyt.mx (S.I. Valdez). ## List of important variables for mathematical models the computational time (Blanco-Cocom et al., 2022b). Regarding optimization and parameter estimation, simpler mathematical models are better suited for the thousand of evaluations of the objective function (Blanco-Cocom et al., 2021), but some of the estimated parameters could be useful for a variety of models with different complexities. Mathematical models where the transport and diffusion of the species are considered through the different layers in alcohol fuel cells are found in Gomes and De Bortoli (2016), Pinto et al. (2018), Gomes et al. (2018), Sun et al. (2018), Gomes et al. (2019) and Sun et al. (2020b). Nevertheless, for the purpose of reducing computation cost, simple models are often used as surrogate models (Fan et al., 2022; Lan et al., 2020; Tirnovan et al., 2008); they have the potential to speed up the numerical simulation without significantly sacrificing accuracy in the output of interest, for instance, the polarization curve, and may reduce numerical instability, which contributes to calibration and uncertainty analysis (Cai et al., 2021; Christelis et al., 2019; Asher and Crokei, 2015; Viana et al., 2009), hence, speeding up the testing of different parameter values.",
      "question_type": "process",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:46",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, how does the update rule for the weights, denoted as \\(w^t\\), incorporate momentum to improve the stability and learning speed of the metamodel, and what is the recommended range for the momentum parameter \\(\\alpha\\)?",
      "topics": [
        "complexity"
      ],
      "answer": "The weights, denoted as \\(w^t\\), are updated using the formula \\(w^{t}=\\alpha w^{t-1}+\\Delta w\\), where \\(\\alpha\\) represents the momentum and \\(\\Delta w\\) is the weight change. The momentum \\(\\alpha\\) is suggested to be within the range of 0.7 to 0.9, which improves stability and increases the speed of learning the metamodel. This means the current weight update \\(w^t\\) is a combination of the previous weight \\(w^{t-1}\\) scaled by the momentum \\(\\alpha\\), and the current weight change \\(\\Delta w\\).",
      "chunk_id": 2002,
      "paper_id": "Evolution Strategies with an RBM-Based Meta-Model",
      "paper_title": "Evolution Strategies with an RBM-Based Meta-Model",
      "paper_year": "2014",
      "chunk_content": "(3) 10}\\mathrm{ calculate }v\\mathrm{ , eq. (5) 11 end 12}\\mathrm{ end 13}\\mathrm{ end 14}\\mathrm{ return individual with the best fitness ``` The RBM is trained to minimise joint probability of data and hidden units with CD method described earlier. We have found that updating weighs with momentum $\\alpha=0.7 \\ldots 0.9$ improves stability and increases the speed of the learning of the metamodel [16]: $$\nw^{t}=\\alpha w^{t-1}+\\Delta w\n$$ The variance $g=6$ in eq. (5) is set to 0.4 (see Discussion section for more information on the choice of this parameter). The noise decay value $\\varepsilon$ is set to 0.99 , but it might require to be adjusted to account for the complexity and dimensionality of the target function. The Gibbs sampling chain (lines 8-11 in Alg. 1) in theory should be repeated until the system reaches its equilibrium. However in practice, small number of iterations is usually sufficient. That is, we use $g=6$ iterations in all our experiments. The learning rate should be set to a small value; otherwise it might lead to the explosion of the RBM's weights.",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:48",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What is identified as a key difference in designing effective Estimation of Distribution Algorithms (EDAs) for continuous versus discrete domains, and what general approach is mentioned to address this difference?",
      "topics": [
        "design"
      ],
      "answer": "The key difference lies in the need to address additional issues in continuous EDAs to decrease the probability of failure, compared to discrete EDAs. A general remedy is presented to meet these additional issues, leading to improved optimization performance by estimating a probability distribution from selected solutions, specifically using maximum-likelihood estimates of the normal distribution.",
      "chunk_id": 11315,
      "paper_id": "Matching inductive search bias and problem structure in continuous Estimation-of-distribution Algorithms",
      "paper_title": "Matching inductive search bias and problem structure in continuous Estimation-of-distribution Algorithms",
      "paper_year": "2008",
      "chunk_content": "Generally speaking, in order to build efficient optimizers using the EDA principle, the induced bias in the form of the estimated probability distribution has to fit to the structure of the problem at hand. The central topic of this paper is to assess the discrepancies between the concept of problem structure in the discrete and continuous domain and to assess to which extent the probabilistic search bias can fit the problem structure in the continuous domain. We indicate that indeed compared to the discrete case there are additional issues that need to be addressed in the design of the continuous EDA to decrease the probability of failure. We also present a simple remedy to meet with the additional issues we identify and show on the basis of experimental results that consequently the optimization performance of the continuous EDA indeed improves substantially. The remainder of this paper is organized as follows. In Section 2 we outline the concept of EDAs and provide insight into the main lessons learned from discrete GAs and EDAs. In Section 3 we then show how continuous EDAs can be constructed by conforming to the main concept of EDAs and estimating a probability distribution from the selected solutions. In our case, we use maximum-likelihood estimates of the normal distribution. We also point out how the main lessons learned from the discrete domain can be transferred to the continuous domain.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:49",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the specific EDA mentioned for the stochastic resource-constrained project scheduling problem (SRCPSP), what data structures are required to represent the precedence constraints between activities, given that the activities are indexed from 0 to J+1?",
      "topics": [
        "scheduling",
        "data structures",
        "estimation of distribution algorithms"
      ],
      "answer": "To represent precedence constraints, an indicator function, denoted as I(j, j), is used. This function evaluates to 1 if activity E_t is a predecessor of activity j, and 0 otherwise. The set of precedence activities for activity j, denoted as P_j, can be represented as a list or array where each element corresponds to an activity index j' such that I(j, j') = 1. Specifically, activity 0 and J+1 are dummy activities with r_{0,k}=r_{J+1,k}=0.",
      "chunk_id": 19237,
      "paper_id": "Stochastic resource-constrained project scheduling problem with time varying weather conditions and an improved estimation of distribution algorithm",
      "paper_title": "Stochastic resource-constrained project scheduling problem with time varying weather conditions and an improved estimation of distribution algorithm",
      "paper_year": "2021",
      "chunk_content": "The improved EDA including an R\\&S procedure is developed in Section 4. Section 5 evaluates the performance of the improved EDA using numerical examples modified from benchmark instances in the J120 dataset of PSPLIB. Finally, a practical case study of windfarm construction is presented in Section 6. ## 2. Problem formulation and assumptions The SRCPSP can be regarded as scheduling a set of activities under precedence and resource constraints considering uncertainties. In this study, only the uncertainty of activity durations induced by weather conditions is considered, whereas other parameters (availability and amount of resources, precedence constraints, and execution mode of activities) are assumed to be determinate. There are $J+2$ activities in the project, i.e. $\\{0,1, \\cdots, J, J+1\\}$. Activities 0 and $J+1$ are dummy activities that represent the start and end of the project, respectively. The duration of the two dummy activities is zero. The precedence constraint between activities $r_{0, k}=r_{J+1, k}=0$ and $\\bar{j}$ is modelled by an indicator function $I(j, j)$. Activity $E_{t}$ is a predecessor of activity $\\lambda=\\left[a_{0}, a_{1}, \\cdots\\right.$ if $I(j, j)=1$. Then, $P_{j}=\\{j \\mid I(j, j)=1\\}$ is the set of precedence activities of activity $a_{J+1}=J+1$.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:51",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the speedup of the sampling step in the Estimation of Distribution Algorithm (EDA), and how does problem size influence this metric?",
      "topics": [
        "analysis"
      ],
      "answer": "The speedup of the sampling step is evaluated by observing its growth with the problem size. The speedup for sampling grows linearly with the problem size. For trap-5 problems, the speedups for sampling converge towards a maximum value as the problem size increases.",
      "chunk_id": 2242,
      "paper_id": "An implicitly parallel EDA based on restricted boltzmann machines",
      "paper_title": "An Implicitly Parallel EDA Based on Restricted Boltzmann Machines",
      "paper_year": "2014",
      "chunk_content": "If the input size doubles, we observe twice the speedup. In a second analysis we set the problem size to 100 ( [FORMULA179] ). Figure 4, right hand side, shows the speedups for various population sizes. The speedup for model building is unrelated to the population size for mini-batch learning. The underlying trend for sampling is linear. The vertical offset of the linear approximation is high and the familiar saw-tooth pattern is much stronger. This indicates that the CUBLAS implementation uses the GPU resources efficiently. The speedup for the model building and sampling steps grow linearly with the problem size. The previous results show that for solving trap-5 problems, the speedups for sampling already converge towards a maximum value (Figure 3, right hand side). As the problem size grows, the required population size also increases. For sampling, the utilization of the GTX TITAN is already close to the theoretical maximum. The clock speed of each GPU core is at 837 MHz , which is around $40 \\%$ of the CPU clock speed of $2,100 \\mathrm{MHz}$. Assuming a similar number of instructions per clock cycle, the $2,688 \\mathrm{GPU}$ cores should be able to process $\\approx 40 \\% \\times 2,688=1,070$ times as fast as a single CPU core not using its SIMD extensions.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:52",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs with respect to memory usage when constructing a Dependency Structure Matrix (DSM)?",
      "topics": [
        "complexity"
      ],
      "answer": "The primary objective is to reduce the memory footprint during DSM construction for EDAs. While pair-wise dependencies are calculated based on a population, the methodology aims to avoid storing all individuals in memory simultaneously by incrementally calculating values, effectively reducing the required population size to O(1). The overall memory requirement is bounded by O(n^2) due to the need for four n x n matrices to store pair-wise dependencies, where 'n' represents the problem size, and additional memory for initial and secondary clusters during the DSM clustering process.",
      "chunk_id": 23488,
      "paper_id": "Efficient model building in competent genetic algorithms using DSM clustering",
      "paper_title": "Efficient model building in competent genetic algorithms using DSM clustering",
      "paper_year": "2011",
      "chunk_content": "Based on this population, pair-wise dependencies are calculated. But keeping all the individuals in the memory is not necessary and calculating [FORMULA145] values may be done incrementally, reducing the required population size to $\\mathrm{O}(1)$. Therefore, the total number of individuals evaluated by the fitness function to construct the DSM is more plausible than the size of population. Besides the memory used for keeping the population of individuals, four $n \\times n$ matrix $\\left(\\mathrm{O}\\left(n^{2}\\right)\\right)$ are needed to store pair-wise dependencies, where [FORMULA149] is the problem size. The amount of memory required by the proposed DSM clustering is [FORMULA150]. In the first phase, at most [FORMULA151] initial clusters may be created, each of which may have (at most) [FORMULA152] members. Therefore, it requires an [FORMULA153] matrix to store the initial clusters. In the second phase, secondary clusters are constructed that require exactly an array with the length of [FORMULA154]. The third and the fourth phase do not require any extra memory. So overall, the proposed DSM clustering memory requirement is bounded by [FORMULA155]. ### 5.2. Time complexity There are two ways to report the time complexity of DSM construction.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:54",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the dynamic bandwidth scheduling strategy based on Nash equilibrium and network pricing, and how is the overall performance of the control system evaluated in this context?",
      "topics": [
        "search"
      ],
      "answer": "The dynamic bandwidth scheduling strategy is designed to find a flexible bandwidth allocation strategy based on Nash equilibrium and network pricing. The goal is to simultaneously satisfy the dynamic performance requirements of individuals and the whole. The network pricing mechanism is implemented in a centralized control mode in the network controller, and the overall performance of the control system is evaluated by the cumulative amount of the output error of each control loop.",
      "chunk_id": 20478,
      "paper_id": "Dynamic Bandwidth Scheduling of Software Defined Networked Collaborative Control System",
      "paper_title": "Dynamic Bandwidth Scheduling of Software Defined Networked Collaborative Control System",
      "paper_year": "2020",
      "chunk_content": "In [8] a collaborative design method combining adaptive controller and feedback scheduling strategy is proposed, which overcomes some limitations of application and execution platform. The other is to maximize the performance of the control system in collaborative design and see [9]-[11] [^0]: The associate editor coordinating the review of this manuscript and approving it for publication was Qiong Wu. for a partial list of references. For example, in [12] joint optimization of information transmission and state estimation developed to reduce the influence of network inducing factors on the convergence and accuracy of estimation. However, a key research issue in the NCS over SDN is the lack of dynamic scheduling mechanisms for network resources [13]. Motivated by the above analyses and under the SDN framework, from the perspective of collaborative design of control and scheduling, a software-defined networked collaborative control system is proposed. Furthermore, under the framework of SD-NCCS, the non-cooperative game model and network pricing mechanism are introduced, and a dynamic bandwidth scheduling strategy suitable for SD-NCCS is proposed. The goal is to find a flexible bandwidth allocation strategy based on Nash equilibrium and network pricing. Thus, the dynamic performance requirements of individuals and the whole are simultaneously satisfied. The network pricing mechanism is implemented in a centralized control mode in the network controller, and the overall performance of the control system is evaluated by the cumulative amount of the output error of each control loop.",
      "question_type": "conceptual deep",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:00:55",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm for gene selection using the SRBCTs dataset, how should practitioners preprocess the 2308 genes in the training data?",
      "topics": [
        "algorithm",
        "EDA",
        "gene selection",
        "data preprocessing"
      ],
      "answer": "Practitioners should preprocess the 2308 genes in the training data using BSS/WSS before implementing the Estimation of Distribution Algorithm. The number of top genes is denoted as S, and the number of final gene pool is usually less than S*2^(S-1), where S is set to 30.",
      "chunk_id": 11450,
      "paper_id": "Optimal Genes Selection with a New Multi-objective Evolutional Algorithm Hybriding NSGA-II with EDA",
      "paper_title": "Optimal genes selection with a new multi-objective evolutional algorithm hybriding NSGA-II with EDA",
      "paper_year": "2008",
      "chunk_content": "4. Experiment We code our method with VC++ 6.0 and Matlab and evaluate it on the dataset of SRBCTs (the small, round blue-cell tumors) published by Khan in [10]. The SRBCTs of childhood, which includes four sample types of neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), are so named because of their similar appearance on routine histology. The dataset is a collection of 2308 genes whose red intensity more than 20 in 88 samples. For the classifier module, we use KNN, which has such advantages that easy realization, high running speed and being suitable for datasets with multi-modal classes. The same with khan's work, use 63 samples as the training set, and 25 samples as the test set. 2308 genes in the training data are first preprocessed by BSS/WSS. For some top genes may repeatedly appear in some combination situations, the number of the final gene pool is usually less than $\\mathrm{S}^{*} 2^{\\left(\\mathrm{S}^{-}\\right)}$. Here we donate the top genes number S as 30 and Q as 50.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:00:57",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Estimation of Distribution Algorithm (EDA) mentioned in the context of wireless network planning, what specific hypothesis is used to formulate the unicost set covering problem (USCP) model, and what limitations does this hypothesis have in reflecting real-world profit considerations for telecommunication companies?",
      "topics": [
        "genetic"
      ],
      "answer": "The unicost set covering problem (USCP) model, as applied within the context of an Estimation of Distribution Algorithm (EDA) for wireless network planning, is based on the hypothesis that a company's profit is linearly proportional to the covered acreage. A limitation of this hypothesis is that it does not account for the fact that companies make different profits on the same acreage in different regions (e.g., urban vs. suburban areas). Furthermore, the hypothesis does not consider that an uncovered region can negatively affect the profit of another covered region, leading to a discrepancy between the model's assumptions and the real-world aim of maximizing profit with minimum cost.",
      "chunk_id": 26605,
      "paper_id": "Parallel island-based estimation of distribution algorithms for wireless network planning",
      "paper_title": "Parallel island-based estimation of distribution algorithms for wireless network planning",
      "paper_year": "2005",
      "chunk_content": "Through simulation tests, we find that our algorithms outperform other stochastic heuristic search algorithms. Index Terms-Estimation of Distribution Algorithm (EDA), PEDA, Genetic algorithms, Wireless Network Planning ## I. INTRODUCTION The selection of a good set of sites among the candidate base transceiver stations (BTSs) is a critical problem that the telecommunication companies must solve when building a wireless network. Many solutions were proposed based on the hypothesis that the problem comes down to serving a maximum surface of a geographical area with a minimum number of BTSs [1, 2, 4]. According to the hypothesis, the unicost set covering problem (USCP) was proposed in [2]. In the model, genetic algorithms and parallel island-based genetic algorithms were introduced [2, 4]. The USCP model was proposed by the assumption that company's profit is linear proportion to the covered acreage. However, the companies make different profits on the same acreage in different regions. For example, the benefits they get in the urban are more than that in the suburb given the same covered acreage. Furthermore, the uncovered region can negatively affect the profit of another covered region. That is to say, the assumption cannot reflect the companies' aims that making maximum profit with minimum cost.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:00:59",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How do MCEDA/B and MCEDA/RM compare in terms of decision space coverage, as illustrated by the solution sets generated on multi-objective DTLZ problems?",
      "topics": [
        "bounds"
      ],
      "answer": "MCEDA/B and MCEDA/RM are both variants of the MCEDA algorithm. The text suggests that MCEDA/B exhibits 'explosive' solution generation via its bits model, leading to a wider coverage of the decision space. This is in contrast to MCEDA/RM, where the decision space coverage might be different. The difference is visualized through plots of the solution sets obtained by each algorithm on multi-objective DTLZ problems, though specific metrics quantifying the difference are not provided beyond the visual representation in the figure.",
      "chunk_id": 16649,
      "paper_id": "MCEDA- A novel many-objective optimization approach based on model and clustering",
      "paper_title": "MCEDA: A novel many-objective optimization approach based on model and clustering",
      "paper_year": "2019",
      "chunk_content": "Then the probability distribution of each bit is used to generate the new variable. The reason why make this transform is in the hope of exploring more information from the solution sets as BOA has done. Experiments have shown that the new solutions generated by the bits model is explosive which can [IMAGE_0] Fig. 2. Plots for the solution set in the best run obtained by MCEDA/B (first row) and MCEDA/RM (second row) on 3 objectives DTLZ problems. cover enough decision space. In this section, a simple version of bits model is presented. The number of the bits of each variable is represented by $n b$. In a solution set $P$, the $i$ th solution $S_{i}$ can be represented by its decision variables, $\\left(x_{i}^{1}, x_{i}^{2}, \\ldots, x_{i}^{n}\\right)$. The formula transferring a double variable $x$ to the corresponding binary $x b$ is presented as follows. $x b=\\operatorname{toBinary}\\left(\\operatorname{round}\\left(\\frac{\\left(x-\\text { lowLimit }\\right)\\left(2^{n b}-1\\right)}{\\text { upperLimit }- \\text { lowLimit }}\\right)\\right)$ where lowLimit and upperLimit are the variable bounds of the variable $m\\left(C_{i}\\right)$, which depend on the specific problem, round $(d)$ obtains the closest integer of $d$ and toBinary $(t)$ transfers the integer $t$ to the corresponding binary.",
      "question_type": "comparison",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:00",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model described, specifically focusing on the iterative loop condition?",
      "topics": [
        "Estimation of Distribution Algorithms"
      ],
      "answer": "The EDA model executes iteratively until a stopping criterion is met. The algorithm proceeds as follows: while the stopping criterion is not met, the current generation `t` is incremented (`t=t+1`), and the algorithm returns to step 4 to continue the evolutionary process. The specific details of step 4 are not provided in this excerpt.",
      "chunk_id": 18964,
      "paper_id": "NMIEDA- Estimation of distribution algorithm based on normalized mutual information",
      "paper_title": "NMIEDA: Estimation of distribution algorithm based on normalized mutual information",
      "paper_year": "2021",
      "chunk_content": "Step7: If the stopping criterion is met, stop; otherwise, set the current generation $t=t+1$ and go to step 4. ## 3 | EXPERIMENTS This section collects all the information about the experiments that are carried out to validate our proposed algorithm. First, we describe the set of test functions that are used. Then, we test the influence of the new updating mechanism and the new sampling mechanism. Third, the comparisons of NMIEDA with four bivariate EDAs are presented. Finally, two different sizes of two-dimensional Ising Spin Glass (ISG) problems are tested for demonstrating the feasibility of NMIEDA in solving real-world problems. ## 3.1 | Benchmark functions A test suite with five additively decomposable functions and one overlapping function is used in our experimental study. The functions are defined as follows. ## 1. OneMax funciton $$\nf_{\\text {one }}(x)=\\max \\left(\\sum_{i=1}^{n} x_{i}\\right) \\quad x_{i} \\in\\{0,1\\}\n$$ where [FORMULA156] represents the $i$ th bit of the input string, which has length [FORMULA158]. The OneMax function is a decomposable function of order 1 and attains its optimum value on the string of all ones. 2. Quadratic function $$\nf_{\\text {Quad }}(x)=\\max \\left(\\sum_{i=1}^{n} F\\left(x_{2 i-1}, x_{2 i}\\right)\\right) \\quad x_{2 i-1}, x_{2 i} \\in\\{0,1\\}\n$$ where $F(s, t)=0.9-0.9(s+t)+1.9$ st.",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:03",
      "generation_style": "implementation_focused"
    },
    {
      "question": "Within the ULI framework for Estimation of Distribution Algorithms, what role does the 'linkage improver' component play in relation to the evolutionary optimizer, such as P3, LT-GOMEA, or DSMGA-2?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The ULI framework incorporates a 'linkage improver' as a separate component that provides additional linkage information to the evolutionary optimizer. The evolutionary optimizer can be P3, LT-GOMEA, or DSMGA-2.",
      "chunk_id": 21094,
      "paper_id": "On measuring and improving the quality of linkage learning in modern evolutionary algorithms applied to solve partially additively separable problems",
      "paper_title": "On measuring and improving the quality of linkage learning in modern evolutionary algorithms applied to solve partially additively separable problems",
      "paper_year": "2020",
      "chunk_content": "ULI overview is given in Pseudocode 1.  ULI is built from two parts: the evolutionary optimizer (eg., P3, LT-GOMEA, or DSMGA-2) and the separate part supposed to support additional linkage information (denoted as linkage improver).",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:05",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs when optimizing control loop bandwidth allocation, considering the interplay between average sensitivity, utility functions, and bandwidth constraints?",
      "topics": [
        "design"
      ],
      "answer": "The core purpose is to maximize the total utility, J, of all control loops within a Software Defined Networked Collaborative Control System (SD-NCCS) by dynamically allocating bandwidth, $x_i$. This allocation is achieved by optimizing a utility function, $U_i(x_i)$, that is an increasing function of the bandwidth $x$ and is affected by the average sensitivity, $\\bar{P}_{n, o}^{i}(t_{k})$, of the reference control signal change for control loop $S_i(\\cdot)$. The optimization is constrained by $\\sum_{i=1}^{M} x_{i} \\leq 1-\\omega g$, where $\\omega$ is a retention scale factor to reserve bandwidth for unexpected overload conditions, and $x_i \\geq 0$. The goal is to find an equilibrium where the bandwidth allocated to each control loop maximizes the overall system utility while accounting for the sensitivity of each loop to changes in the reference control signal.",
      "chunk_id": 20483,
      "paper_id": "Dynamic Bandwidth Scheduling of Software Defined Networked Collaborative Control System",
      "paper_title": "Dynamic Bandwidth Scheduling of Software Defined Networked Collaborative Control System",
      "paper_year": "2020",
      "chunk_content": "Based on the above analysis, the average sensitivity of the control output to the reference control signal due to network delay is defined as follows [14]: $$\n\\begin{aligned}\n\\bar{P}_{n, o}\\left(t_{k}\\right) & =\\frac{\\sum_{j=0}^{N-1} P_{n, o}\\left(t_{k}^{j}\\right)}{N} \\\\\nP_{n, o}\\left(t_{k}^{j}\\right) & =\\frac{\\left(y^{(o)}\\left(t_{k}^{j}\\right)-y^{(n)}\\left(t_{k}^{j}\\right)\\right) / y^{(n)}\\left(t_{k}^{j}\\right)}{\\left(r^{(o)}\\left(t_{k}^{j}\\right)-r^{(n)}\\left(t_{k}^{j}\\right)\\right) / r^{(n)}\\left(t_{k}^{j}\\right)}\n\\end{aligned}\n$$ In practical engineering applications, the average sensitivity of the reference control signal can be calculated in advance or determined by simulation experiments according to the control requirements and then obtained by table lookup. Finally, the designed utility function must be an increasing function of the bandwidth $x$, which is also affected by the average sensitivity of the reference control signal. In addition, to satisfy the existence of the Nash equilibrium, it must also be quasi-concave and continuously differentiable. Based on the above conditions, the utility function of the control loop $x_{i}^{*}$ designed in this paper during a data transmission period $t \\in\\left[t_{k}^{0}, t_{k+1}^{0}\\right)$ is as follows: $$\nU_{i}\\left(x_{i}\\right)=\\ln \\left(1+\\frac{\\bar{P}_{n, o}^{i}\\left(t_{k}\\right) x_{i}}{\\sum_{j \\neq i}^{M} \\bar{P}_{n, o}^{i}\\left(t_{k}\\right) x_{j}}\\right)\n$$ where $\\bar{P}_{n, o}^{i}\\left(t_{k}\\right)$ is the average sensitivity of the reference control signal change of the control loop $S_{i}(\\cdot)$. The optimization goal is to maximize the total utility of all control loops in the SD-NCCS as follows: $$\n\\begin{aligned}\nJ= & \\max \\sum_{i=1}^{M} U_{i}\\left(x_{i}\\right) \\\\\n& \\text { s.t. } \\sum_{i=1}^{M} x_{i} \\leq 1-\\omega g, \\quad x_{i} \\geq 0\n\\end{aligned}\n$$ where $\\omega_{R} \\in(0,1)$ is a retention scale factor, which was introduced to retain part of the bandwidth for unexpected overload conditions. However, simply using $J$ as the optimization objective does not consider that each control loop should pay a corresponding price for bandwidth resources.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:07",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the Parzen estimator, denoted as $\\hat{\\boldsymbol{p}}_{n}^{S}(z)$, leverage a kernel PDF $g_{S}(\\cdot)$ and a scale factor $h_{n}$ to smooth the empirical estimator $\\hat{\\boldsymbol{p}}_{n}^{E}(x)$ and achieve a finite variance?",
      "topics": [
        "optimization"
      ],
      "answer": "The Parzen estimator $\\hat{\\boldsymbol{p}}_{n}^{S}(z)$ is calculated as $\\hat{\\boldsymbol{p}}_{n}^{S}(z)=\\int_{-\\\\\\infty}^{\\\\infty} \\hat{\\boldsymbol{p}}_{n}^{E}(x) \\frac{1}{h_{n}} g_{S}(\\frac{z-x}{h_{n}}) d x=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h_{n}} g_{S}(\\frac{z-z_{i}}{h_{n}})$. This involves convolving the empirical estimator $\\hat{\\boldsymbol{p}}_{n}^{E}(x)$ with a squared integrable kernel PDF $g_{S}(\\cdot)$. The kernel acts as a low-pass filter, and its bandwidth is regulated by the scale factor $h_{n}$. This smoothing action reduces the sensitivity of $\\hat{\\boldsymbol{p}}_{n}^{S}(z)$, resulting in a finite variance, specifically $\\operatorname{Var}[\\hat{\\boldsymbol{p}}_{n}^{S}(z)]<\\infty$ for all $z$ in the real numbers, unlike the empirical estimator whose variance is infinite.",
      "chunk_id": 26872,
      "paper_id": "An hybrid neural:genetic approach to continuous multi-objective optimization problems",
      "paper_title": "An hybrid neural/genetic approach to continuous multi-objective optimization problems",
      "paper_year": "2003",
      "chunk_content": "Of course some restrictions are in order: here is a brief explanation. Let $z$ be a real-valued random variable. Let $p^{z}(\\cdot): \\mathfrak{R} \\rightarrow \\mathfrak{R}, \\cup\\{0\\}$ be the associated PDF. Let $\\boldsymbol{D}_{n}=\\left\\{z_{1}, \\ldots z_{n}\\right\\}$ be a collection of $\\operatorname{Var}\\left[\\hat{\\boldsymbol{p}}_{n}^{E}(z)\\right]=\\infty$ independent replicas of $\\hat{\\boldsymbol{p}}_{n}^{S}(\\cdot)$. The empirical estimator $\\hat{\\boldsymbol{p}}_{n}^{E}(\\cdot)$ of $p^{z}(\\cdot)$ based on $\\boldsymbol{D}_{n}$ is defined as follows: $$\n\\forall z \\in \\mathfrak{R} \\quad \\hat{\\boldsymbol{p}}_{n}^{E}(z)=\\frac{1}{n} \\sum_{i=1}^{n} \\delta\\left(z-z_{i}\\right)\n$$ The estimator just defined is unbiased everywhere but it converges nowhere to $\\operatorname{Var}\\left[\\hat{\\boldsymbol{p}}_{n}^{S}(z)\\right]<\\infty \\forall z \\in \\mathfrak{R}$ in the mean square sense because $\\operatorname{Var}\\left[\\hat{\\boldsymbol{p}}_{n}^{E}(z)\\right]=\\infty$ irrespective of both $$\n\\forall z \\in \\Re \\quad \\lim _{n \\rightarrow \\infty} h_{n}=0 \\Rightarrow \\lim _{n \\rightarrow \\infty} E\\left[\\hat{\\boldsymbol{p}}_{n}^{S}(z)\\right]=p^{z}(z)\n$$ and $\\mathrm{N}_{\\text {std }}$. This last result is not surprising, since the Dirac's delta is not squared integrable. The Parzen estimator $\\hat{\\boldsymbol{p}}_{n}^{S}(\\cdot)$ of $2-\\alpha$ based on $\\alpha$ is obtained by convolving the empirical estimator with some squared integrable kernel $\\operatorname{PDF} g_{S}(\\cdot)$ : $$\n\\forall z \\in \\mathfrak{R} \\quad \\hat{\\boldsymbol{p}}_{n}^{S}(z)=\\int_{-\\infty}^{\\infty} \\hat{\\boldsymbol{p}}_{n}^{E}(x) \\frac{1}{h_{n}} g_{S}\\left(\\frac{z-x}{h_{n}}\\right) d x=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h_{n}} g_{S}\\left(\\frac{z-z_{i}}{h_{n}}\\right)\n$$ The kernel acts as a low-pass filter whose \"bandwidth\" is regulated by the scale factor $h_{n} \\in \\Re_{n}$. It exerts a \"smoothing\" action that lowers the sensitivity of $\\hat{\\boldsymbol{p}}_{n}^{S}(z)$ w.r.t. $\\left(\\mathrm{N}_{\\text {ind }}=100\\right)$ so as to make $\\operatorname{Var}\\left[\\hat{\\boldsymbol{p}}_{n}^{S}(z)\\right]<\\infty \\forall z \\in \\mathfrak{R}$.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:10",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm within the provided framework, how should practitioners configure the `Neighbors` function, specifically regarding the `flipOnce` parameter, to balance exploration and exploitation?",
      "topics": [
        "Estimation of Distribution Algorithms"
      ],
      "answer": "When configuring the `Neighbors` function, if `flipOnce` is set to `true`, the algorithm randomly selects one dimension `i` from the set `{1, 2, ..., d}`, where `d` is the dimension of the solution `x`. For each dimension `j`, if `i` equals `j`, the value of `neighbor_j` is randomly mutated; otherwise, the value of `neighbor_j` is randomly mutated with probability `p`. If `flipOnce` is set to `false`, the algorithm randomly mutates each dimension `j` of the neighbor with probability `p`. The parameter `p` controls the mutation probability, and `n` controls the number of neighbors generated. The look-up table (LUT) stores the number of samples, the sum of the fitness of samples, and the sum of the square of the fitness of the samples for each N-Tuple.",
      "chunk_id": 22817,
      "paper_id": "The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation",
      "paper_title": "The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation",
      "paper_year": "2018",
      "chunk_content": "Require: $\\mathbb{S}$ : search space Require: fitness: noisy solution evaluator Require: $n \\in N^{+}$: number of neighbors Require: $p \\in(0,1)$ : mutation probability Require: flipOnce $\\in\\{t r u e, f a l s e\\}$ : flip at least once or not Require: $n b E v a l s$ : total number of evaluations allowed $1: t=0 \\quad \\triangleright$ Counter for fitness evaluations 2: Model $\\leftarrow \\varnothing \\quad \\triangleright$ Initialise the fitness landscape model 3: current $\\leftarrow$ a random point $\\in \\mathbb{S}$ 4: while $t<n b E v a l s$ do 5: value $\\{1,2, \\ldots, d\\}$ fitness(current) 6: add $<$ current, value $>$ to LModel $7:$ Population $R A N D<p$ Neighbors(Model, current, $n, p$, flipOnce) 8: current $\\leftarrow \\arg \\max _{x \\in P o p u l a t i o n} v_{U C B}(x)$ $9: \\quad t \\leftarrow t+1$ 10: return LModel 11: function Neighbors(model, $x, n, p$, flipOnce) $12:$ Population $L U T[00110]$ Initialise empty set $13: \\quad d \\leftarrow|x| \\quad \\triangleright$ Get the dimension 14: for $k \\in\\{1, \\ldots, n\\}$ do $15:$ neighbor $\\leftarrow x$ $16: \\quad i \\leftarrow 0$ $17:$ if flipOnce then $18: \\quad i \\leftarrow$ randomly selected from $\\{1,2, \\ldots, d\\}$ for $j \\in\\{1, \\ldots, d\\}$ do if $i==j$ then Randomly mutate value of neighbor ${ }_{j}$ else if $R A N D<p$ then Randomly mutate value of neighbor $/ d$ Add neighbor to Population return Population In our model, however, each N-Tuple has a look-up table (LUT) that stores statistical summaries of the values it encounters; the basic numbers stored are the number of samples, the sum of the fitness of samples, and the sum of the square of the fitness of the samples.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:12",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the computational complexity of the machine assignment step analyzed within the described algorithm for the Hybrid Flow-shop Scheduling Problem (HFSP), specifically considering the selection of the machine based on minimizing $P_{i,j,s} + \\max(r_s, C_{i,j-1})$?",
      "topics": [
        "optimization"
      ],
      "answer": "The machine assignment step involves choosing the machine $s$ that minimizes $P_{i,j,s} + \\max(r_s, C_{i,j-1})$, where $P_{i,j,s}$ represents the processing time of job $j$ on machine $s$ at stage $i$, $r_s$ is the earliest release time of machine $s$, and $C_{i,j-1}$ is the completion time of job $j-1$ at stage $i$. The algorithm iterates through all available machines to find the one that minimizes this expression. The complexity is given as $O(n^2)$, which suggests that for each job $j$, the algorithm considers on the order of $n$ machines at each of the $n$ stages, contributing to the $O(n^2)$ complexity. The indicator function $I_{i j}$ is defined as 1 if job $j$ appears before or in position $i$, and 0 otherwise, but it doesn't directly impact the $O(n^2)$ complexity of machine selection.",
      "chunk_id": 10377,
      "paper_id": "A compact estimation of distribution algorithm for solving hybrid flow-shop scheduling problem",
      "paper_title": "A Compact Estimation of Distribution Algorithm for Solving Hybrid Flow-shop Scheduling Problem",
      "paper_year": "2012",
      "chunk_content": "Job 4 is the last job to be processed. To decode a sequence is to divide the jobs to the machines at each stage so as to form a feasible schedule and calculate the makespan of the schedule. For the HFSP, the decoding can be divided into two parts: to decide the jobs order and the machines assignment. For the jobs order, the algorithm decides the processing order of the first stage referring to the sequence order given. From stage 2 on, the algorithm decides the processing order of the current stage according to the completion time of each job from the previous stage in a non-decreasing order. If some jobs have the same completion time, then the algorithm will randomly determine the processing orders of these jobs. As for the machines assignment, based on the first available machine rule [18], we proposed the machine assigning rule as follows: Step 1: $j=1$; Step 2: For each job $j, p_{i j}$, decide the earliest start time of each job $p_{i j}(0)=1 / n$ according to the completion time of each job at the previous stage $C_{i, j-1}$ and the earliest release time of each machine $r_{k}$, and compute $\\max \\left(r_{k}, C_{i, j-1}\\right)$. If $j=1, C_{i, j-1}=0$; Step 3: For each job $$\nI_{i j}=\\left\\{\\begin{array}{l}\n1, \\text { if job } j \\text { appears before or in position } i \\\\\n0, \\text { else }\n\\end{array}\\right.\n$$, choose the machine $O\\left(n^{2}\\right)$ if $P_{i, j, s}+$ $\\max \\left(r_{s}, C_{i, j-1}\\right)$ has the smallest value.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:14",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of GNP compare to PMBGNP, specifically regarding premature convergence and average fitness?",
      "topics": [
        "control"
      ],
      "answer": "GNP sometimes obtains a good track but achieves lower average fitness than the proposed algorithm, while PMBGNP cannot solve the problem due to premature convergence.",
      "chunk_id": 14691,
      "paper_id": "Probabilistic model building Genetic Network Programming using multiple probability vectors",
      "paper_title": "Probabilistic Model Building Genetic Network Programming Using Multiple Probability Vectors",
      "paper_year": "2010",
      "chunk_content": "[^0]: ${ }^{2} 0$ means that no boject is perceived while 1023 means that an object is very close to the sensor (almost touching the sensor). [IMAGE6] Fig. 7. Simulation results of GNP, PMBGNP and PMBGNP [FORMULA110]. PMBGNP [FORMULA111] inherits the advantages of probabilistic model building evolutionary algorithms. A probabilistic model is constructed by learning the promising individuals that guide the more effective evolution than GNP. The simulation results shows that the proposed algorithm [FORMULA112] can find a better trade-off between exploitation and exploration, comparing with GNP and PMBGNP. Fig. 8 shows a successful track of the best solution controlled by [FORMULA113]. The figure shows that the proposed algorithm can solve the wall following problem well. The robot can move straight along the wall and avoid the obstacles. In some trails, GNP can also obtain good track. But in others, good results cannot be obtained, thus GNP achieves lower average fitness than [FORMULA114]. On the other hand, PMBGNP cannot solve the problem due to the premature convergence. [IMAGE7] Fig.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:15",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What programming techniques are suggested to optimize the EDA's runtime if the values of $\\bar{\\mu}$ (parameter value) and $x^{*}=(1, \\ldots, 1)$ (optimum) were known in advance?",
      "topics": [
        "algorithm"
      ],
      "answer": "If $\\bar{\\mu}$ and $x^{*}$ were known, the text suggests restarting the EDA with the parameter value $\\mu = \\bar{\\mu}$ and allocating a time budget of $\\operatorname{Pr}[x=x^{*}] \\leq \\exp(-D)$ until the problem is solved. This approach is expected to yield an algorithm with an expected runtime of at most $T^{*}=\\frac{4}{3} \\bar{\\mu} T$, where $T$ represents a time-related variable.",
      "chunk_id": 18486,
      "paper_id": "The Runtime of the Compact Genetic Algorithm on Jump Functions",
      "paper_title": "The Runtime of the Compact Genetic Algorithm on Jump Functions",
      "paper_year": "2021",
      "chunk_content": "Consequently, the total budget spent in the first $L=\\left\\{i \\in[1 \\ldots n] \\mid f_{i t}=1-\\frac{1}{n}\\right\\}, \\ell=|L|$ rounds is less than $i 2^{i}$. Note that after round $i_{0}:=1+\\left\\lceil\\log _{2} \\bar{\\mu}\\right\\rceil+\\left\\lfloor\\log _{2} T\\right\\rfloor$, the process started with parameter value $\\mu=\\mu_{0}:=2^{\\left\\lceil\\log _{2} \\bar{\\mu}\\right\\rceil} \\geq \\bar{\\mu}$ has started and has used a time budget of $$\n\\sum_{j=0}^{i_{0}-1} 2^{j} \\geq \\sum_{j=\\left\\lceil\\log _{2} \\bar{\\mu}\\right\\rceil}^{i_{0}-1} 2^{j}=\\mu_{0} \\sum_{j=0}^{\\left\\lceil\\log _{2} T\\right\\rceil} 2^{j} \\geq \\mu_{0} T\n$$ Consequently, with probability $f_{i, t+1}^{\\prime}<\\frac{1}{n}$ this process has found the optimum at that time. With the same type of computation, we see that after round $i_{0}+j$, the process with parameter value $\\mu=2^{j} \\mu_{0}$ is finished with probability $x_{i}^{1} \\neq x_{i}^{2}$. Consequently, the round in which we find the solution is stochastically dominated (see Sect. 3.2) by $i_{0}-1$ plus a geometric distribution (on $1,2, \\ldots$ ) with success rate $f_{i, t+1}>f_{i, t+1}^{\\prime} \\quad$. The expected time taken by this strategy to solve $\\quad f_{i, t+1}=f_{i, t+1}^{\\prime}+\\frac{1}{\\mu}, \\quad$ thus is at most $$\n\\sum_{i=i_{0}}^{\\infty}\\left(\\frac{1}{4}\\right)^{i-i_{0}}\\left(\\frac{3}{4}\\right) i 2^{i}=\\frac{3}{4} 2^{i_{0}} \\sum_{j=0}^{\\infty} 2^{-j}\\left(j+i_{0}\\right)=3 \\cdot 2^{i_{0}-1}\\left(i_{0}+1\\right)\n$$ using the well-known equality $\\sum_{j=0}^{\\infty} j 2^{-j}=2$. We continue estimating the expected runtime of our parallel-run strategy by $$\n3 \\cdot 2^{i_{0}-1}\\left(i_{0}+1\\right) \\leq 6 \\bar{\\mu} T\\left(\\log _{2}(\\bar{\\mu} T)+3\\right)=: T_{\\mathrm{par}}\n$$ We note that if the values of $\\bar{\\mu}$ and $x^{*}=(1, \\ldots, 1)$ were known in advance, then restarting the EDA with $\\mu=\\bar{\\mu}$ and with a budget of $\\operatorname{Pr}\\left[x=x^{*}\\right] \\leq \\exp (-D)$ until the problem is solved would immediately give an algorithm with expected runtime at most $T^{*}=\\frac{4}{3} \\bar{\\mu} T$.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:16",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What statistical measures are proposed to quantify the performance of the EDA when considering the effect of genetic drift, specifically focusing on parameter settings that mitigate this effect?",
      "topics": [
        "fitness"
      ],
      "answer": "The effect of genetic drift for \\(n / \\ln (n)\\)-valued EDAs is quantified using a martingale concentration result due to McDiarmid. This approach allows for determining parameter values that avoid the unwanted effect of genetic drift by profiting from small variances.",
      "chunk_id": 5552,
      "paper_id": "Estimation-of-distribution algorithms for multi-valued decision variables",
      "paper_title": "Estimation-of-distribution algorithms for multi-valued decision variables",
      "paper_year": "2024",
      "chunk_content": "Each frequency vector of the $O\\left(n \\lambda \\ln \\left(\\frac{2}{\\mu}\\right)\\right)$-cGA sums to 1 after the update (before the restriction) because exactly one entry is increased by $O(n \\ln (n))$ and exactly one value is decreased by this amount (noting that this can be the same frequency, in which case no change is made overall). # 5. Genetic drift We prove an upper bound on the effect of genetic drift for $n / \\ln (n)$-valued EDAs (Theorem 3) in a similar fashion as Doerr and Zheng [18] for binary decision variables. This allows us to determine parameter values for EDAs that avoid the usually unwanted effect of genetic drift. The main novelty of our result over that by Doerr and Zheng [18] is that we use a slightly technical martingale concentration result due to McDiarmid [19] that allows one to profit from small variances. Such an approach is necessary. If one directly applies ``` \\(t \\leftarrow 0 ;\\) \\(p^{(0)} \\leftarrow\\left\\{\\frac{1}{2} \\lambda{i, j} \\alpha{i j}\\left(\\alpha{i j}\\left(t{i}-t\\right)\\right\\}\\right.\\) repeat // iteration \\(t\\) \\(x^{(t, 1)}, x^{(t, 2)} \\leftarrow\\) two individuals, independently sampled from \\(p^{(t)}\\); \\(y^{(t, 1)} \\leftarrow\\) individual with the higher fitness from \\(\\left\\{x^{(t, 1)}, x^{(t, 2)}\\right\\}\\) (breaking ties uniformly at random); \\(p^{(t, 2)} \\leftarrow\\) individual from \\(\\left\\{x^{(t, 1)}, x^{(t, 2)}\\right\\} \\backslash\\left\\{y^{(t, 1)}\\right\\}\\); for \\((i, j) \\in[n] \\times[0 .",
      "question_type": "definition",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:18",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of the AVS eigenvalue scaling strategy in the context of EDAs when dealing with small population sizes?",
      "topics": [
        "performance"
      ],
      "answer": "AVS, when used with EDAs and small population sizes relative to the problem's dimensionality, can lead to a misdirection where the covariance matrix is excessively scaled up. This occurs because the algorithm interprets the lack of improvement as a need for greater scaling, even when the current distribution is already adequate, potentially causing the algorithm to break down due to exceeding computational limits, as observed with ED-EDA + ECMR AVS.",
      "chunk_id": 11701,
      "paper_id": "Unified eigen analysis on multivariate Gaussian based estimation of distribution algorithms",
      "paper_title": "Unified eigen analysis on multivariate Gaussian based estimation of distribution algorithms",
      "paper_year": "2008",
      "chunk_content": "The importance of CMR methods is also demonstrated simultaneously. Only for a large enough population does the classical maximum likelihood estimated Gaussian model have good performance. If the merits of eigenvalue tuning can be exerted in combination with classical Gaussian, we can make EDA solve problems with even smaller population sizes to achieve higher efficiency. # 6.2.3. Additional discussions on AVS There is another remarkable convergence issue of AVS observed in our tests. We have stated the lack of convergence proof of existing eigenvalue scaling strategies in Section 2.1. Compared with EEDA, AVS scales the covariance matrix faster and more intensively. The mechanism of AVS determines that if no improvement is achieved in the current generation, then the covariance matrix should be scaled up. When the population size is too small for the problem size, the attempts of new individuals are likely to be too sparse to find better solutions even if the current distribution is already adequate and accurate. In this case, AVS may be misguided and keep on scaling up the covariance matrix. We did find such an example of AVS on [FORMULA234] in the small population-high dimension test. When we recorded the average fitness of population and the standard error in ED-EDA + ECMR [FORMULA235] AVS, 78 runs out of 100 broke down because the sum of fitness and the sum of fitness square used for calculating standard error exceeded the maximum real value that the computer could represent. Therefore the average best fitness of ED-EDA + ECMR [FORMULA236] AVS in Table 3 was not presented.",
      "question_type": "synthesis summary",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:22",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of estimating a probabilistic model from selected solutions, as implemented in the real-coded Bayesian optimization algorithm (rBOA), contribute to generating new solutions that overcome the limitations of traditional EAs?",
      "topics": [
        "genetic"
      ],
      "answer": "The algorithmic mechanism in EDAs, specifically within the real-coded Bayesian optimization algorithm (rBOA), involves estimating a probabilistic model based on statistics collected from a set of selected, 'good' solutions. This probabilistic model encodes problem regularities. The rBOA then samples this estimated distribution to generate new solutions. This contrasts with traditional EAs that use genetic operators. By sampling from the learned probabilistic model, rBOA leverages the encoded problem regularities to guide the generation of new solutions, thus overcoming limitations of traditional EAs that may not explicitly capture and utilize these regularities.",
      "chunk_id": 2641,
      "paper_id": "A Hybrid Competent Multi-swarm Approach for Many-Objective Problems",
      "paper_title": "A hybrid competent multi-swarm approach for many-objective problems",
      "paper_year": "2014",
      "chunk_content": "Probabilistic modeling offers a systematic way of acquiring problem regularities, and therefore can help in problem solving by replacing the traditional genetic operators in the process of generating new solutions by estimating a probabilistic model based on the statistic collected from the set of solutions and then sampling this probabilistic model. Therefore problem regularities encoded in the probabilistic model are used to generate new solutions to overcome the limitations of traditional EAs. This incorporation of probabilistic modeling in EAs has led to a new paradigm known as Estimation of Distribution Algorithms (EDAs) or competent EAs [10]. General EDAs iterate three steps until some termination criterion is satisfied: select good solutions from a population, estimate the probability distribution from the selected individuals (learn a model) and generate new solutions (sample) from the estimated distributions (model). In spite of similar behavior patterns, EDAs can be characterized by its methods of learning a probabilistic model, which consist of two tasks: learning the structure of a probabilistic model, which defines conditional dependencies and independencies (model selection), and estimating the conditional probability distributions with regard to the found structure (model fitting) [5]. The EDA used in this work is the real-coded Bayesian optimization algorithm (rBOA) [5], which was chosen due to its good results and extensive documentation available in [5]. rBOA is a continuous, real-coded version of the Bayesian optimization algorithm (BOA) proposed to bring the power of the discrete BOA to the area of real-valued optimization.",
      "question_type": "process",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:24",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to solve an Enhanced Single Row Facility Layout Problem (SRFLP) incorporating safety constraints and fixed costs, how should practitioners modify the objective function to minimize the total distance between facilities while adhering to these constraints?",
      "topics": [
        "search"
      ],
      "answer": "To enhance the SRFLP objective function, practitioners should consider both fixed costs for facility installation and safety constraints preventing adjacency between specified facilities. The original objective is to minimize Z, the sum of distances between facility pairs, given facility lengths $l_i$ and traffic loads $T_{ij}$ between facilities $i$ and $j$. The enhanced objective function should incorporate additional terms that penalize layouts violating safety constraints and include fixed costs associated with facility placement at specific locations. This would involve introducing penalty terms for constraint violations and adding fixed cost values to the overall cost calculation, effectively making the optimization problem more reflective of real-world scenarios.",
      "chunk_id": 575,
      "paper_id": "Hybrid Estimation of Distribution Algorithm for solving Single Row Facility Layout Problem",
      "paper_title": "Hybrid Estimation of Distribution Algorithm for solving Single Row Facility Layout Problem",
      "paper_year": "2013",
      "chunk_content": "Heragu (2008) presents case study about real world problem in manufacturing company that consider technological constraint in a layout positioning. The technological constraint here means two facilities cannot be adjacent by each other. This kind of constraint, which cannot put the specified two facilities by each other, in construction site layout problem is called safety constraint (Mawdesley \\& Al-Jibouri, 2003). In another book, Sule (2009) presents a fixed cost concept that already described earlier about the cost for installing a facility that might differ in each location. Combining these two ideas, we can enhance the SRFLP objective function to be more comprehensive and more applicable in real case by considering fixed cost and safety constraints. This problem is labeled as Enhanced SRFLP. ## 3. Problem statement ### 3.1. Single Row Facility Layout Problem This research considers Single Row Facility Layout Problem (SRFLP) with different sizes of facilities. The objective is to minimize $Z$ which stands for sum of the distances between all facility pairs. The length $l_{i}$ of each facility $i$ and a non matrix $T=\\left[T_{i j}\\right]$ are given; $T_{i j}$ refers to the traffic loads between facilities $j$ and $j$.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:26",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, what population sizes ($NP$) are used for different dimensional problems, and how are these sizes related to the algorithm's ability to exploit structural information?",
      "topics": [
        "search"
      ],
      "answer": "The EDA uses a population size ($NP$) of 50 for 100-dimensional problems, $NP=200$ for 1000-dimensional problems. It is suggested that a smaller $NP$ is sufficient for easy problems, while a larger $NP$ is recommended for difficult problems to avoid stagnation at a local optimum. The text indicates that a much smaller population size is sufficient to exploit structural information of high dimensional search space efficiently compared to classical EDAs due to the use of strategies such as SGLD, STDC and restart strategy.",
      "chunk_id": 12188,
      "paper_id": "A Restart Univariate Estimation of Distribution Algorithm Sampling under Mixed Gaussian and Levy probability Distribution",
      "paper_title": "A Restart Univariate Estimation of Distribution Algorithm: Sampling under Mixed Gaussian and Levy probability Distribution",
      "paper_year": "2008",
      "chunk_content": "However, the structure of practical science and engineering problems are usually unknown beforehand. In order to make our algorithm general for most problems, we choose parameters as consistently as possible. It is obvious that for an easy problem (e.g., unimodel problems and low dimensional problems), a small value of [FORMULA115] is sufficient, but for difficult problems, a large value of [FORMULA116] is recommended in order to avoid stagnation to a local optimum. Benefit from these efficient strategies SGLD, STDC and restart strategy, a much smaller population size is sufficient to exploit structural information of high dimensional search space efficiently compared to classical EDAs. For conducting the experimental results, we set $N P=50$, $N=10$ for 100 dimension problems, set [FORMULA119], $N=20$ for 500 dimension problems and set $N P=200$, $N=30$ for 1000 dimension problems. ## C. Results and Discussion Experimental results conducted by LSEDA-gl are presented in the Appendix. Tables II-V show the 1st best, the 7th best, the 13th (median), the 19th best, the 25th (worst), mean, and standard deviation values of 25 independent runs under fixed max FES.",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:28",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does Principal Component Analysis (PCA) reduce the dimensionality of a dataset, and what mathematical representation is used to define the principal axes?",
      "topics": [
        "analysis"
      ],
      "answer": "PCA reduces dimensionality by identifying directions in data-space with the highest variance and projecting the data onto them. The \\(q\\) principal axes \\(\\mathbf{w}_{j}\\), where \\(j\\) ranges from 1 to \\(q\\), are orthonormal axes that maximize retained variance under projection. These axes are eigenvectors of the sample covariance matrix \\(\\mathbf{S}\\), represented as \\(\\mathbf{S w}_{j}=\\lambda_{j} \\mathbf{w}_{j}\\), where \\(\\lambda_{j}\\) is the eigenvalue corresponding to eigenvector \\(\\mathbf{w}_{j}\\). The principal components of the observed data \\(\\mathbf{x}_{i}\\) are then given by \\(\\mathbf{z}_{i}=\\mathbf{W}^{\\mathrm{T}}\\left(\\mathbf{x}_{i}-\\boldsymbol{\\mu}\\right)\\), where \\(\\boldsymbol{\\mu}\\) is the data sample mean.",
      "chunk_id": 12467,
      "paper_id": "Continuous Estimation of Distribution Algorithms with Probabilistic Principal",
      "paper_title": "Continuous estimation of distribution algorithms with probabilistic principal component analysis",
      "paper_year": "2001",
      "chunk_content": "Section 4 reports the results of experiments for some benchmark functions and Section 5 summarizes our findings in this study. ## 2 Basic Concept of Probabilistic Principal Component Analysis ### 2.1 Principal Component Analysis Principal component analysis (PCA) is a powerful technique in data analysis. The central idea of PCA is to reduce the dimensionality of a data set which consists of many interrelated variables. It is achieved by searching for the direction in data-space which have the highest variance, and subsequently projecting the data onto it [8]. For a set of observed $d$ dimensional data vector $\\left\\{\\mathbf{x}_{i}\\right\\}$, $i \\in\\{1,2, \\ldots, N\\}$, we define the $q$ principal axes $\\left\\{\\mathbf{w}_{j}\\right\\}$, $j \\in\\{1,2, \\ldots, q\\}$ as those orthonormal axes onto which the retained variance under projection is maximal. Then it can be shown that the vector $$\n\\mathbf{S}=\\frac{1}{N} \\sum_{i}^{N}\\left(\\mathbf{x}_{i}-\\boldsymbol{\\mu}\\right)\\left(\\mathbf{x}_{i}-\\boldsymbol{\\mu}\\right)^{\\mathrm{T}}\n$$ are given by the $\\boldsymbol{\\mu}$ dominant eigenvectors which correspond to the largest eigenvalues of the sample covariance matrix $$\n\\mathbf{S}=\\frac{1}{N} \\sum_{i}^{N}\\left(\\mathbf{x}_{i}-\\boldsymbol{\\mu}\\right)\\left(\\mathbf{x}_{i}-\\boldsymbol{\\mu}\\right)^{\\mathrm{T}}\n$$ where $\\boldsymbol{\\mu}$ is the data sample mean, $$\n\\boldsymbol{\\mu}=\\frac{1}{N} \\sum_{i}^{N} \\mathbf{x}_{i}\n$$ such that $\\mathbf{S w}_{j}=\\lambda_{j} \\mathbf{w}_{j}$. The $\\mathbf{W}=\\left(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{q}\\right)$ principal components of the observed data $\\mathbf{x}_{i}$ are given by the vector $\\mathbf{z}_{i}=\\mathbf{W}^{\\mathrm{T}}\\left(\\mathbf{x}_{i}-\\boldsymbol{\\mu}\\right)$, where $\\mathbf{W}=\\left(\\mathbf{w}_{1}, \\mathbf{w}_{2}, \\ldots, \\mathbf{w}_{q}\\right)$.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:29",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the specific EDA mentioned involving neighboring relations between nodes, what data structures are required to represent the network topology and feature values for each node, given that each node can communicate with others in a fully connected model?",
      "topics": [
        "search"
      ],
      "answer": "Given the fully connected model, an adjacency matrix could represent the network topology, where each entry (i, j) indicates the presence or absence of a direct communication link between node i and node j. Since the description states that each node can communicate with others, this matrix would primarily consist of 'true' values, except for the diagonal elements (i, i), which might be 'false' depending on whether self-loops are allowed. To store the feature values for each node, a suitable data structure would be an array of structs or objects. Each element in the array corresponds to a node, and the struct/object contains fields for the 10 selected features (rows 1 to 6, 10 to 11, 17, and 18 from Table 3). Each feature would need to be represented by an appropriate data type (e.g., integer, float, boolean) based on its nature. This allows for efficient access to a node's feature set, which is used to improve the inference of the node's state.",
      "chunk_id": 15876,
      "paper_id": "A probability distribution function for investigating node infection and removal times",
      "paper_title": "A probability distribution function for investigating node infection and removal times",
      "paper_year": "2019",
      "chunk_content": "As defined earlier, neighboring relations describe interactions between two nodes at the application layer. In the network topology presented in Section 4.1, each node can communicate with others, forming a fully connected model of relations. Node [FORMULA140] is a neighbor of node [FORMULA141], the feature values of which are used to improve the inference of the [FORMULA142] th node state. - We selected 10 features that had been previously pointed out. [FORMULA143] They have been listed in Table 3 in rows 1 to 6,10 to 11, 17, and 18 of Table 3. We gather the features for the ith node. Other features introduced in this reference, ie, number of ICMP packets, number of RST flag and differential packet size are not selected as they have no influence on preference scan worms. - We selected 10 features that had been previously pointed out. [FORMULA144] They have been listed in Table 3 in rows 1 to 6,10 to 11,17 , and 18 of Table 3 . The features were gathered for the ith node. Other features introduced in previous research, [FORMULA145] ie, the number of ICMP packets, number of RST flags, and differential packet sizes were not selected, as they have no impact on the preference scan worms.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:32",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the effectiveness of the HEDA model on the Two-peak function and Rosenbrock function?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The effectiveness of HEDA is evaluated using the mean number of fitness evaluations required to find the optimal solution. For the Two-peak function, the number of fitness evaluations is approximated by $O(n^{1.256})$. For the Rosenbrock function, the success of finding the optimal solution is assessed across multiple runs.",
      "chunk_id": 12720,
      "paper_id": "Optimizing Continuous Problems Using Estimation of Distribution Algorithm Based on Histogram Model",
      "paper_title": "Optimizing continuous problems using estimation of distribution algorithm based on histogram model",
      "paper_year": "2006",
      "chunk_content": "For each dimension, 20 independent runs are executed. Fig. 1 shows the mean number of fitness evaluations until HEDA finds the optimal solution. The number of fitness evaluations can be approximated by $O\\left(n^{1.256}\\right)$. Therefore, the results indicate that HEDA can solve Two-peak function in sub-quadratic number of evaluations. [IMAGE_1] Fig. 1. Scalability of HEDA for Two-peak problem ### 3.4 Drawback of HEDA In the previous subsections, experiments demonstrate that HEDA can efficiently solve the test functions and performs much better than some of well-known continuous evolutionary algorithms. However, in the procedure of HEDA, the relationship among variables is not taken into consideration. This will limit the ability of HEDA in solving complicated problems with strong linkage information. Here, 20-variable Rosenbrock function with optimum $[1,1, \\ldots, 1]$ is used to test the limited performance of HEDA: $$\nf(x)=\\sum_{i=2}^{n} 100\\left(x_{i}-x_{i-1}^{2}\\right)^{2}+\\left(1-x_{i-1}\\right)^{2}\n$$ There are strongly correlated variables in Rosenbrock function. In the experiment, 10 runs are executed for each population setting. The experimental results in Table 3 demonstrate it is hard to find optimal solution using HEDA. The drawback of HEDA is due to the fact that HEDA does not encode the linkage information.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:33",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs using Particle Swarm Optimization and Chaotic Evolutionary PSO?",
      "topics": [
        "optimization"
      ],
      "answer": "The algorithmic approach aims to optimize solutions by iteratively improving a population of candidate solutions. Particle Swarm Optimization (PSO) adjusts particle positions based on individual and global best positions, while Chaotic Evolutionary PSO (CEPSO) enhances PSO by incorporating chaotic agents and a Hopfield neuronal network-like structure to explore promising areas and avoid premature convergence during the search for the global optimum.",
      "chunk_id": 19194,
      "paper_id": "Cellular estimation of distribution algorithm designed to solve the energy resource management problem under uncertainty",
      "paper_title": "Cellular estimation of distribution algorithm designed to solve the energy resource management problem under uncertainty",
      "paper_year": "2021",
      "chunk_content": "The DE algorithm is described as follows: a P population of individuals is obtained randomly. Then, a candidate individual is created from each parent individual of the P population. DE creates new candidate solutions by combining the parent individual and several other individuals of the same population. Then, the candidate is evaluated, and if it is better than a parent, then it replaces to parent from the P population. Particle Swarm Optimization (PSO) (Kennedy and Eberhart, 1995; Kohler et al., 2019) simulates the elegant but unpredictable movement of a flock of birds that can be seen as a social organism, whose individuals are looking for the optima into a multidimensional search space. For each iteration, the fitness of the individuals (particles) is computed. Then from the best position of each particle and the best global position achieved by the algorithm, the speed of each particle is adjusted, and a new position is obtained. Below is the description of the state-of-the-art algorithms developed or tuned for the ERM problem under uncertain environments: - Chaotic Evolutionary PSO (CEPSO): It is a methodology that uses chaotic agents to search in promising areas that are explored by Particle Swarm Optimization (PSO). This algorithm uses a structure similar to Hopfield neuronal networks with transient chaos to avoid premature convergence of the algorithm in the search for the global optimum (Sun et al., 2009).",
      "question_type": "synthesis summary",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:35",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of Kalman filtering, specifically its prediction, observation, and revision steps, as implemented within the EMNA framework, relate to the updating of the revised status $x^{\\text {rev }}$ and the revision improvement matrix $\\tau$?",
      "topics": [
        "selection"
      ],
      "answer": "Within the EMNA framework, Kalman filtering's prediction, observation, and revision operations are integrated. The 'Perform observation' step obtains the observed status $x^{\\text {obs }}$. The 'Perform revision' step calculates the gain vector $g$ and updates the revised status $x^{\\text {rev }}$ along with updating the revision improvement matrix $\\tau$. This process incorporates the difference in function fitness into the gain coefficient to simulate a non-linear relationship. The observed value is represented as $y(k)$, and $v(k)$ represents the noise component during observation. The state transition matrix $A$ describes the transition from the previous to the current state, while $B$ is the observation coefficient matrix, and $H$ is the coefficient matrix of input variables.",
      "chunk_id": 5637,
      "paper_id": "An enhanced Kalman filtering and historical learning mechanism driven estimation of distribution algorithm",
      "paper_title": "An enhanced Kalman filtering and historical learning mechanism driven estimation of distribution algorithm",
      "paper_year": "2024",
      "chunk_content": "Perform observation: Obtain the observed status $x^{\\text {obs }}$. Perform revision: Calculate the gain vector $g$. Update the revised status $x^{\\text {rev }}$. Update revision improvement matrix $\\tau$. $n f e s=m f e s+1$ End while ## Algorithm 2 EMNA with the historical learning mechanism where NEW_POP represents the generated new population, $\\tau$ is the selection rate, and $N P$ is the population size. $w(k)$ represents the prediction error matrix, $y(k)$ represents the observed value at the current time, $v(k)$ represents the noise component during observation, $A$ is the state transition matrix from the previous to the current state, $B$ is the observation coefficient matrix, and $H$ is the coefficient matrix of input variables. Kalman filtering is well-suited for linear Gaussian systems and is applied to certain dynamic systems with uncertain information, including continuous systems. It can also be employed in complex nonlinear systems in virtue of non-linear mapping techniques [69]. In this paper, the difference in the function fitness is introduced in the gain coefficient and the non-linear relationship is simulated. Kalman filtering with the operation of prediction, observation, and revision is introduced into the EMNA.",
      "question_type": "relationship",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:36",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners use probabilistic models to represent dependencies between decision variables?",
      "topics": [
        "evolutionary"
      ],
      "answer": "In Estimation of Distribution Algorithms, probabilistic models are used to explicitly represent dependencies between decision variables and their structure. The algorithm populates the next generation by simulating individuals from the probabilistic model, transferring both the data dependencies and the structure found in the best individuals into the next population.",
      "chunk_id": 14017,
      "paper_id": "Using Copulas in Estimation of Distribution Algorithms",
      "paper_title": "Using Copulas in Estimation of Distribution Algorithms",
      "paper_year": "2009",
      "chunk_content": "Using Copulas in Estimation of Distribution Algorithms Rogelio Salinas-Gutierrez, Arturo Hernandez-Aguirre, and Enrique R. Villa-DiharceCentro de Investigacion en Matematicas, Guanajuato, Mexico\\{rsalinas,artha, villadi\\}@cimat.mx #### Abstract A new way of modeling probabilistic dependencies in Estimation of Distribution Algorithm (EDAs) is presented. By means of copulas it is possible to separate the structure of dependence from marginal distributions in a joint distribution. The use of copulas as a mechanism for modeling joint distributions and its application to EDAs is illustrated on several benchmark examples. ## 1 Introduction Estimation of Distribution Algorithms (EDAs) 17] are recognized as a new paradigm in Evolutionary Computation to deal with optimization problems 15. EDAs are a class of Evolutionary Algorithms (EAs) based on probabilistic models instead of genetic operators such as crossover and mutation. The use of probabilistic models allow us to explicitly represent: 1) dependencies between the decision variables; and 2) their structure. EDAs populate the next generation by simulating individuals from the probabilistic model, therefore, the goal is to transfer both the data dependencies and the structure found in the best individuals into the next population.",
      "question_type": "definition",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:38",
      "generation_style": "practical_application"
    },
    {
      "question": "In the described PSO-EDA algorithm, after a maximum of Gno continuous generations without improvement of the best solution x, a mutation operation is applied to each \\(\\bar{x}^{i}\\) particle, followed by a local search (LS). What is the purpose of this conditional mutation and local search in the context of the PSO-EDA's search strategy, and how might it theoretically influence the algorithm's convergence properties?",
      "topics": [
        "search"
      ],
      "answer": "The conditional mutation and local search (LS) applied to each particle \\(\\bar{x}^{i}\\) after Gno continuous generations without improvement of the best solution x is designed to introduce diversity into the population and escape local optima. The mutation operation aims to perturb the particle's position, while the local search (LS) attempts to refine the mutated particle's position within its neighborhood. This combination is intended to help the algorithm explore new regions of the search space and potentially find better solutions than it would if it continued with the standard PSO update rules. While the provided content does not explicitly state a convergence theorem, the strategy suggests an attempt to balance exploration (through mutation) and exploitation (through local search), which are crucial elements for convergence in evolutionary algorithms. Without the explicit mutation and local search, the PSO component alone might stagnate in a local optimum, preventing the algorithm from reaching a global optimum.",
      "chunk_id": 24588,
      "paper_id": "An Integrated Method Based on PSO and EDA for the Max-Cut Problem",
      "paper_title": "An Integrated Method Based on PSO and EDA for the Max-Cut Problem",
      "paper_year": "2016",
      "chunk_content": "(12) end if (13) Let \\(\\operatorname{Pop}(g+1)=\\operatorname{Pop}^{t}(g)\\). (14) if \\(x^{}\\) is not improved after \\(G{\\mathrm{no}}\\) continuous generations then (15) for each \\(\\bar{x}^{i}(i=1, \\ldots, s)\\) do (16) \\(\\bar{x}^{i}=\\) mutation \\(\\left(\\bar{x}^{i}\\right)\\). (17) \\(\\bar{x}^{i}=L S\\left(\\bar{x}^{i}\\right)\\). (18) end for (19) end if (20) Let \\(g=g+1\\). (21) end while (22) return \\(x^{\\prime \\prime}\\) ``` Algorithm 1: General structure of PSO-EDA. Particle swarm optimization (PSO) [14] is one of the most popular population-based algorithms. In this technique, all particles search for food in the search space based on their positions and velocities. Every particle can adjust its flying direction by learning from its own experience and the performance of its peers [15].",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:40",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the Estimation of Distribution Algorithm for topology optimization (EDATOP) utilize the parameters \\$\\phi\\$ and \\$n\n_{\\text{var}}\\$ to represent candidate solutions within its population-based search?",
      "topics": [
        "search"
      ],
      "answer": "EDATOP, an Estimation of Distribution Algorithm, represents candidate solutions as real-valued vectors \\$\\Phi_{\\text{Best}}^t\\$. The parameter \\$\\phi\\$ represents the optimization variables in \\$\\mathbb{R}^{n_{\\text{var}}}\\$, where \\$n_{\\text{var}} = n_{\\text{nodes}} - n_{\\text{cond}}\\$. Here, \\$n_{\\text{nodes}}\\$ is the number of nodes in the coarse mesh, and \\$n_{\\text{cond}}\\$ represents the number of nodes with force or fixed-displacement conditions. Thus, \\$\\phi\\$ defines the values at the nodes of the coarse mesh, which are then translated into a structure.",
      "chunk_id": 22381,
      "paper_id": "A meta-heuristic for topology optimization using probabilistic learning",
      "paper_title": "A meta-heuristic for topology optimization using probabilistic learning",
      "paper_year": "2018",
      "chunk_content": "$$\n\\begin{aligned}\nV M(\\varphi) & =\\sum_{i=1}^{n_{\\text {elements }}} \\varphi_{k} \\gamma_{k}\\left(\\sigma_{k}\\right) \\\\\n\\gamma_{\\text {stress }}\\left(\\sigma_{k}\\right) & =\\left\\{\\begin{array}{l}\n1 \\text { if } \\sigma_{k}>\\sigma_{\\max } \\\\\n0 \\text { otherwise }\n\\end{array}\\right.\n\\end{aligned}\n$$ Equation (5) below is used to compute the sum of the displacements exceeding the maximum displacement constraint $t, S^{t}$ at the loaded nodes. $n_{\\text {loaded }}$ is the number of loaded nodes, $\\left|\\delta_{l}\\right|$ is the displacement norm at the $l-t h$ node. $\\gamma_{\\text {disp }}$ indicates whether there exists a node that exceeds the maximum allowed displacement. The maximum allowed displacement $\\delta_{\\max }$ is a user-given parameter. $$\nU^{e x}(\\varphi)=\\gamma_{\\text {disp }}(\\delta) \\cdot \\sum_{l=1}^{n_{\\text {loaded }}}\\left|\\delta_{l}\\right|\n$$ $\\gamma_{\\text {disp }}(\\delta)=\\left\\{\\begin{array}{l}1 \\text { if any } \\delta_{l}>\\delta_{\\max }, l=1 . . n_{\\text {loaded }} \\\\ 0 \\text { otherwise }\\end{array}\\right.$ ## 3 Optimization algorithm We address the problem via an Estimation of Distribution Algorithm for global optimization (EDA), which we call EDA for topology optimization (EDATOP). EDAs are stochastic optimization methods which perform global search by building and sampling from probabilistic models. The optimization variables are denoted as $\\phi \\in \\mathbb{R}^{n_{\\text {var }}}, n_{\\text {var }}=$ $n_{\\text {nodes }}-n_{\\text {cond }}$, where $n_{\\text {nodes }}$ are the number of nodes in the coarse mesh, while $n_{\\text {cond }}$ are the nodes with forces or fixed-displacement conditions. EDAs are population based algorithms; thus, they sample a population of candidate solutions often called individuals. In this case, an individual is a real-valued vector $\\Phi_{\\text {Best }}^{t} . S^{t}$ of values at the nodes of the coarse mesh, which are translated into a structure by applying the process described in Section 2 and Fig.",
      "question_type": "COMPARATIVE EVALUATION",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:43",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the fire spreading simulation, what data structures are required to represent the graph and node states, and how are these states updated during each time step?",
      "topics": [
        "mutation"
      ],
      "answer": "The implementation requires a graph data structure to represent the connections between nodes. Each node needs a state attribute that can be 'U' (untouched), 'B' (burning), or 'D' (defended). During each time step, the algorithm updates the node states: 'U' nodes adjacent to 'B' nodes become 'B', and a predefined number $N_f$ of 'U' nodes are changed to 'D' based on a permutation $\\pi$.",
      "chunk_id": 14397,
      "paper_id": "Estimation of Distribution Algorithms for the Firefighter Problem",
      "paper_title": "Estimation of Distribution Algorithms for the Firefighter Problem",
      "paper_year": "2017",
      "chunk_content": "Spreading of fire is simulated in discrete time steps $t=0,1, \\ldots$ At $t=0$, the graph is in the initial state $S_{0}$. Most commonly, in the initial state vertices from a non-empty set $\\emptyset \\neq S \\subset V$ are burning ( $\\left.{ }^{\\prime} \\mathrm{B}^{\\prime}\\right)$ and the remaining ones are untouched (' U ') (no vertices are initially defended). In each time instant $t>0$ we are allowed to assign firefighters to a predefined number $N_{f}$ of still untouched ('U') nodes of the graph $\\pi$. These nodes become defended ('D') and are immune to fire for the rest of the simulation. Next, fire spreads along the edges of the graph $m$ from the nodes labelled ' B ' to all the adjacent nodes labelled ' U '. The simulation stops, when fire can no longer spread. It can happen either when all nodes adjacent to fire are defended ('D') or when all undefended nodes are burning ('B'). Solutions to the firefighter problem can be represented as permutations of the numbers $1, \\ldots, N_{v}$. During the simulation, in each time step, the first $v$ yet untouched nodes (' U ') are taken from the permutation $\\pi$ and become defended ('D'). In every time step exactly $c_{2}(v)$ nodes become defended, except for the final time step in which the number of the untouched nodes may be less than $c_{i}(v)$.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:44",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How does the Levy flight search enhance the exploitation capability of the Estimation of Distribution Algorithm (EDA)?",
      "topics": [
        "optimization"
      ],
      "answer": "The Levy flight search introduces a random search path, obeying a Levy distribution, to enhance local exploitation around the best solution of the current population. The Levy flight location update formula is \\(\\boldsymbol{x}_{i}^{s+1}=\\boldsymbol{x}_{i}^{s}+\\phi \\otimes \\operatorname{Lévy}(\\lambda), i=1,2, \\cdots, n\\), where \\(\\boldsymbol{x}_{i}^{s}\\) is the position of the \\(i\\)th individual in the \\(s\\)th generation population, \\(\\otimes\\) represents point-to-point multiplication, \\(\\phi\\) is the step control amount (set to 0.3), and \\(\\operatorname{Lévy}(\\lambda)\\) is a random search path obeying the Levy distribution \\(\\text { Lévy } \\sim u=t^{-\\lambda}, 1<\\lambda \\leqslant 3\\). The step size \\(s\\) is calculated as \\(s=\\frac{\\mu}{|v|^{1 / \\beta}}\\), where \\(\\mu \\sim N\\left(0, \\sigma_{\\mu}^{2}\\right)\\) and \\(v \\sim N\\left(0, \\sigma_{v}^{2}\\right)\\), with \\(\\sigma_{\\mu}=\\left{\\frac{\\Gamma(1+\\beta) \\sin (\\pi \\beta / 2)}{\\Gamma[(1+\\beta) / 2] 2^{\\left(\\beta-1 / 2\\right.} \\beta}\\right}\\), \\(\\sigma_{v}=1\\), and \\(\\beta\\) typically set to 1.5. This approach broadens the search scope, facilitating escape from local optima.",
      "chunk_id": 19038,
      "paper_id": "An improved estimation of distribution algorithm for multi-compartment electric vehicle routing problem",
      "paper_title": "An improved estimation of distribution algorithm for multi-compartment electric vehicle routing problem",
      "paper_year": "2021",
      "chunk_content": "To ensure certain learning efficiency, we set the learning rate no less than 0.01 for the above equations. ### 4.4 Local search strategy based on Levy flight The EDA pays more attention to global exploration while its exploitation capability is relatively limited, which makes it easy to premature convergence and fall into local optimum. Thus, an effective EDA should balance the exploration and the exploitation abilities. Therefore, in this paper, a Levy flight search is introduced into EDA to enhance its local exploitation around the best solution of the current population several times at every generation. The Levy flight search is an alternate way of walking with a short distance search combined with occasional longer distance walking [35], which is a random search path and it obeys a Levy distribution [36], that is a probability distribution proposed by the French mathematician Levy in the 1930s. Levy flight can increase the diversity of the population and broaden the scope of the search, which makes it easier to jump out of local optimal points in the intelligent optimization algorithm such as firefly algorithms [37] and cuckoo search [38]. The Levy flight location update formula is as follows: $$\n\\boldsymbol{x}_{i}^{s+1}=\\boldsymbol{x}_{i}^{s}+\\phi \\otimes \\operatorname{Lévy}(\\lambda), i=1,2, \\cdots, n\n$$ where $\\boldsymbol{x}_{i}^{s}$ is the position of the [FORMULA156] th individual in the [FORMULA157]th generation population, $\\otimes$ represents point-to-point multiplication, $\\phi$ is the step control amount which is set as 0.3 , and $\\operatorname{Lévy}(\\lambda)$ is a random search path which is essentially a random step and obeys the following Levy distribution: $$\n\\text { Lévy } \\sim u=t^{-\\lambda}, \\quad 1<\\lambda \\leqslant 3\n$$ Due to the complexity of Levy distribution, it is usually simulated by the Mantegna algorithm and the step size $s$ is calculated as follows: $$\ns=\\frac{\\mu}{|v|^{1 / \\beta}}\n$$ where $\\mu$ and $v$ obey the following normal distribution: $$\n\\begin{gathered}\n\\mu \\sim N\\left(0, \\sigma_{\\mu}^{2}\\right) \\\\\nv \\sim N\\left(0, \\sigma_{v}^{2}\\right)\n\\end{gathered}\n$$ And $\\sigma_{\\mu}=\\left\\{\\frac{\\Gamma(1+\\beta) \\sin (\\pi \\beta / 2)}{\\Gamma[(1+\\beta) / 2] 2^{\\left(\\beta-1 / 2\\right.} \\beta}\\right\\}, \\sigma_{v}=1$, where $\\beta$ is usually 1.5 . It should be noted that the new solution $\\boldsymbol{x}_{i}^{s+1}$ obtained by updating the current optimal solution through the Levy flight search is a vector with continuous values, while the natural number encoding is used as solution representation in this paper and it is discrete.",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:47",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs when addressing multiobjective optimization problems with constraints, focusing on the concept of Pareto optimality?",
      "topics": [
        "optimization"
      ],
      "answer": "The overall purpose is to find a set of trade-off optimal solutions, denoted as \\(\\mathcal{D}^*\\), for a multiobjective optimization problem subject to constraints, where no solution in \\(\\mathcal{D}^*\\) is dominated by any other solution in the decision space \\(\\mathcal{D}\\). This involves identifying solutions where improving one objective function \\(f_i(x)\\) would necessarily worsen at least one other objective function \\(f_j(x)\\), according to the Pareto dominance relation.",
      "chunk_id": 11710,
      "paper_id": "Scalable continuous multiobjective optimization with a neural network-based estimation of distribution algorithm",
      "paper_title": "Scalable continuous multiobjective optimization with a neural network-based estimation of distribution algorithm",
      "paper_year": "2008",
      "chunk_content": "$$\n\\left.\\begin{array}{rl}\n\\operatorname{minimize} & \\boldsymbol{F}(\\boldsymbol{x})=\\left\\langle f_{1}(\\boldsymbol{x}), \\ldots, f_{M}(\\boldsymbol{x})\\right\\rangle \\\\\n\\text { subject to } & c_{1}(\\boldsymbol{x}), \\ldots, c_{R}(\\boldsymbol{x}) \\leq 0 \\\\\n& \\text { with } \\boldsymbol{x} \\in \\mathcal{D}\n\\end{array}\\right\\}\n$$ where $\\mathcal{D}$ is known as the decision space. The functions $f_{1}(\\boldsymbol{x}), \\ldots, f_{M}(\\boldsymbol{x})$ are the objective functions. Their corresponding image set, $\\mathcal{O}$, of $(\\boldsymbol{F}: \\mathcal{D} \\rightarrow \\mathcal{O})$ is named objective space $(\\boldsymbol{F}: \\mathcal{D} \\rightarrow \\mathcal{O})$. Finally, inequalities $c_{1}(\\boldsymbol{x}), \\ldots, c_{R}(\\boldsymbol{x}) \\leq 0$ express the restrictions imposed to the values of $\\boldsymbol{x}$. In general terms, this class of problems does not have a unique optimal solution. Instead an algorithm solving (1) should produce a set containing equally good trade-off optimal solutions. The optimality of a set of solutions can be defined relying on the so-called Pareto dominance relation [6]: Definition 2 (Dominance Relation). Under optimization problem (1) and having $\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2} \\in \\mathcal{D}, \\boldsymbol{x}_{1}$ is said to dominate $\\boldsymbol{x}_{2}$ (expressed as $\\boldsymbol{x}_{1} \\prec \\boldsymbol{x}_{2}$ ) iff $\\forall f_{j}$, $f_{j}\\left(\\boldsymbol{x}_{1}\\right) \\leq f_{j}\\left(\\boldsymbol{x}_{2}\\right)$ and $\\exists f_{i}$ such that $f_{i}\\left(\\boldsymbol{x}_{1}\\right)<f_{i}\\left(\\boldsymbol{x}_{2}\\right)$. The solution of (1) is a subset of $\\mathcal{D}^{*} \\subseteq \\mathcal{D}$ that contains elements are not dominated by other elements of $\\forall \\boldsymbol{x}_{1} \\in \\mathcal{D}^{*} \\nexists \\boldsymbol{x}_{2}$, or, in more formal terms, Definition 3 (Pareto-optimal Set). The solution of problem (1) is the set $\\mathcal{D}^{*}$ such that $\\mathcal{D}^{*} \\subseteq \\mathcal{D}$ and $\\forall \\boldsymbol{x}_{1} \\in \\mathcal{D}^{*} \\nexists \\boldsymbol{x}_{2}$ that $\\boldsymbol{x}_{2} \\prec \\boldsymbol{x}_{1}$.",
      "question_type": "application",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:49",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of Tree-EDA, as described here, use a probabilistic model to optimize a function?",
      "topics": [
        "search"
      ],
      "answer": "Tree-EDA uses a tree probabilistic model where each variable depends on at most one other variable, referred to as the parent. The algorithm optimizes the function $f(\\mathbf{x})$, which represents the number of PNGs found after finishing the simulation. The algorithm uses truncation selection with truncation parameter $T=0.5$ to select individuals. The Tree-EDA runs for 85 generations with a population size of 100.",
      "chunk_id": 10144,
      "paper_id": "Maximizing the Number of Polychronous Groups in Spiking Networks",
      "paper_title": "Maximizing the Number of Polychronous Groups in Spiking Networks",
      "paper_year": "2012",
      "chunk_content": "The objective or fitness function $f(\\mathbf{x})$ assigned to a solution $\\mathbf{x}$ is the number of PNGs found after finishing the simulation. We would optimize this function using and EDA. Tree-EDA [4] uses a tree probabilistic model in which each variable may depend on at most another variable, which is called the parent. The algorithm uses a population size of 100 individuals and 85 generations. The population size and number of generations was considerably constrained by the cost of the optimization function that is very expensive. Truncation selection is used with truncation parameter $T=$ 0.5 . For the optimization experiments, 25 runs of the EDAs are executed. We apply the optimization algorithm to modify the number of homogeneous delays. Figure 2 shows the average fitness of the new generated solutions at different generations of Tree-EDA. The average fitness steadily increases with generations. Figure 3 shows the relationship between the number of neurons with homogeneous delay values and the number of PNGs for random and optimized networks. It can be seen in figures 2 and 3 that the EDA consistently improves the number of PNGs. ## 3. CONCLUSIONS Although it has been recognized that polychronization can be used as a tool for exploiting the computational power of synaptic delays and for monitoring the topology and activity of a spiking neuron network [3], more research is needed to determine the influence of the delays values in the number of PNGs and other features of the networks.",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:50",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "In which practical scenarios does the EDA model EDA-MCC outperform UMDA and EEDA, considering the impact of shifting the global optima?",
      "topics": [
        "performance"
      ],
      "answer": "EDA-MCC tends to outperform UMDA and EEDA, especially when the global optima of the optimization problem are shifted. UMDA and EEDA perform significantly worse than EDA-MCC on shifted versions of functions like $F_{4}, F_{6}$, and [FORMULA331], which are shifted versions of $F_{3}, F_{5}$, and [FORMULA333], respectively. EMNA global also performs poorly in such scenarios.",
      "chunk_id": 165,
      "paper_id": "Scaling Up Estimation of Distribution Algorithms for Continuous Optimization",
      "paper_title": "Scaling Up Estimation of Distribution Algorithms for Continuous Optimization",
      "paper_year": "2013",
      "chunk_content": "EMNA global performs the worst and EEDA performs generally between UMDA [FORMULA325] and TABLE VI COMPARISON BETWEEN EEDA AND EDA-MCC ON 50-D-200-D [FORMULA326] Population sizes used are shown in brackets. In each row, the significantly better result (determined by nonparametric Mann-Whitney [FORMULA327] test) is shown in bold. For all results of EEDA, the value of asymp. sig. (two-tailed) $<$ 0.001 when compared with the results of EDA-MCC. [IMAGE_5] Fig. 9. Average CPU time of EEDA and EDA-MCC on [FORMULA329]. EDA-MCC. Note that $F_{4}, F_{6}$, and [FORMULA331] are shifted versions of $F_{3}, F_{5}$, and [FORMULA333], respectively. On the unshifted versions, although UMDA [FORMULA334] and EEDA perform significantly worse than EDA-MCC, the solutions they found are not too bad. However, once the global optima are shifted away, their performance become much worse. EMNA global has a similar issue and it always performs the worst.",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:51",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, how are node histogram models (NHM) and edge histogram models (EHM) employed in probability modeling, and what mechanism adjusts the selection ratio between these two models?",
      "topics": [
        "estimation of distribution algorithm",
        "probability modeling",
        "node histogram models",
        "edge histogram models"
      ],
      "answer": "The multi-model estimation of distribution algorithm (EDA) utilizes node histogram models (NHM) and edge histogram models (EHM) within its probability modeling component for solving the ARP-MPDT problem. The algorithm adaptively adjusts the selection ratio between the NHM and EHM probability models, though the specific mathematical formulation or algorithmic steps for this adaptive adjustment are not detailed.",
      "chunk_id": 23019,
      "paper_id": "A Multi-Model Estimation of Distribution Algorithm for Agent Routing Problem in Multi-Point Dynamic Task",
      "paper_title": "A Multi-model Estimation of Distribution Algorithm for Agent Routing Problem in Multi-point Dynamic Task",
      "paper_year": "2018",
      "chunk_content": "ARP-MPDT is a typical NP-hard optimization problem. In this paper, we establish the nonlinear ARP-MPDT model. A multi-model estimation of distribution algorithm (EDA) employing node histogram models (NHM) and edge histogram models (EHM) in probability modeling is used to solve the ARP-MPDT. The selection ratio of NHM and EHM probability models is adjusted adaptively. Finally, performance of the algorithm for solving the ARP-MPDT problem is verified by the computational experiments. Key Words: Estimation of distribution algorithm, Multi-model, Routing, Multi-point dynamic task ## 1 Introduction The common routing problems are often limited to the shortest routing optimization, such as the traveling salesman problem (TSP) ${ }^{[1]}$ and the vehicle routing problem (VRP) ${ }^{[2]}$. Currently, there are many studies on the route problems with time windows. In recent research, a multi-agent routing problem called multi-point dynamic aggregation (MPDA) ${ }^{[3]}$ is proposed. In the MPDA task, a number of task points are located in different places and their states change over time ${ }^{[4]}$. Multiple unnamed vehicles aggregate to these task points and execute the tasks cooperatively to make the states of all the task points change to an expected level.",
      "question_type": "algorithm mechanics",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:53",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the probability model sampling in Algorithm 2 determine which sub-population to sample from, and what factor influences this selection?",
      "topics": [
        "optimization"
      ],
      "answer": "In Algorithm 2, Step 1 generates an integer 'q' randomly to select a sub-population. The probability of selecting a specific sub-population 'i' is determined by the ratio of the (m-1)-dimensional volume of \\$\\Psi_{i}\\$ (denoted as vol(\\$ \\Psi_{i}\\$)) to the sum of volumes of all sub-populations, represented as \\$\\sum_{j=1}^{m} \\alpha_{j}\\$. Thus, sub-populations with larger volumes have a higher probability of being selected for sampling.",
      "chunk_id": 8962,
      "paper_id": "A hybrid multi-objective PSO-EDA algorithm for reservoir flood control operation",
      "paper_title": "A hybrid multi-objective PSO-EDA algorithm for reservoir flood control operation",
      "paper_year": "2015",
      "chunk_content": "(3) is extended by $50 \\%$ along each of the directions $U_{i}^{1}, U_{i}^{2}, \\ldots, U_{i}^{m-1}$ to form the new probability model $Q_{i}^{i}(t)$, as is described in Eqs. (5)-(8), which is used to generate offspring individuals. $$\n\\begin{aligned}\n& \\tilde{\\Psi}_{i}=\\tilde{C}_{i}+\\varepsilon_{i} \\\\\n& \\tilde{C}_{i}=\\left\\{\\begin{array}{l}\nx \\in R^{n} \\mid x=\\tilde{x}_{i}+\\sum_{j=1}^{m-1} \\alpha^{j} U_{i}^{j}, \\quad \\alpha_{\\min }-0.25\\left(\\alpha_{\\max }-\\alpha_{\\min }\\right) \\\\\n\\quad \\leq \\alpha^{j} \\leq \\alpha_{\\max }+0.25\\left(\\alpha_{\\max }-\\alpha_{\\min }\\right)\n\\end{array}\\right\\} \\\\\n& \\alpha_{\\min }=\\min _{x \\in S_{i}}\\left(x-\\tilde{x}_{i}\\right)^{T} U_{i}^{j} \\\\\n& \\alpha_{\\max }=\\max _{x \\in S_{i}}\\left(x-\\tilde{x}_{i}\\right)^{T} U_{i}^{j}\n\\end{aligned}\n$$ Note $\\left|S_{i}\\right|$ as the particle number of the sub-population $R$, generate $p$ offspring particles by sampling the probability model $\\tilde{\\Psi}_{i}$ uniformly and randomly, giving rise to the offspring set $\\mathbf{H V}=\\Lambda\\left(\\bigcup_{p \\in \\mathrm{PF}_{m}}\\{x \\mid p<x<R\\}\\right)$. The details of how to sample the probability model can be summarized as following Algorithm 2. It returns one sample vector from the probability model at each call. ## Algorithm 2 (Probability model sampling.). Step 1: Generate an integer $q \\in(1,2, \\ldots, K)$ at random with the following probability, in which $\\operatorname{vol}\\left(\\Psi_{i}\\right)$ denotes the $(m-1)$-dimensional volume of $\\Psi_{i}$. probability $(q=i)=\\frac{\\operatorname{vol}\\left(\\Psi_{i}\\right)}{\\sum_{j=1}^{m} \\alpha_{j}}$. Step 2: Uniformly randomly generate a point $x^{\\prime}$ from $\\tilde{C}_{q}$ as defined in Eq. (6). Step 3: Generate a $12,200 \\mathrm{~m}^{3} \\mathrm{~s}^{-1}$-dimensional zero mean noise vector $\\varepsilon^{\\prime}$ from $N\\left(0, \\sigma_{q} t\\right)$, in which $\\sigma_{q}$ can be calculated by the following expression.",
      "question_type": "relationship",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:01:55",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the described optimization problem for the chain topology, what considerations must be taken into account regarding the loss functions of the subsystems?",
      "topics": [
        "optimization"
      ],
      "answer": "When implementing the optimization for the chain topology, the multimodal nature of loss functions $f_{1}$, $f_{2}$, and $f_{4}$ must be considered. Specifically, $f_{4}$ is nonderivable, meaning gradient-based optimization techniques may not be suitable. The overall loss function $F(\\\\mathbf{x})=\\\\sum_{i=1}^{4} f_{i}(\\\\mathbf{x})$ inherits both the multimodal and nonderivable properties, which can impact the choice of optimization algorithm. Also, $z_{i}=M x_{i}-o_{i}$ where $o_{i}$ is a random offset and $M$ is a random rotation matrix.",
      "chunk_id": 17471,
      "paper_id": "A Decentralized Continuous Estimation of Distribution Algorithm for Networked Systems",
      "paper_title": "A Decentralized Continuous Estimation of Distribution Algorithm for Networked Systems",
      "paper_year": "2019",
      "chunk_content": "Therefore, we apply our algorithm to different systems. ### 4.1 Example 1: topology of chain The topology of chain is shown as follows: [IMAGE_0] Fig. 1: 4 subsystems with a topology of chain In the problem statements below we use $o_{i}$ to refer to a random offset and we use $$\n\\min f_{7}(\\mathbf{x})=x_{71}^{4}+2 x_{72}^{4}+3 x_{4}^{4}+4 x_{8}^{4}\n$$ to refer to an random rotation matrix. Therefore, the optimization problems are more complex and representative.The loss functions are as follows: Subsystem 1: the Rastrigin function; $$\n\\begin{aligned}\n\\min f_{1}(\\mathbf{x})= & 10 \\times 4+z_{11}^{2}+z_{12}^{2}+z_{13}^{2}+z_{23}^{2} \\\\\n& -10 \\times\\left(\\cos \\left(2 \\pi z_{11}\\right)+\\cos \\left(2 \\pi z_{12}\\right)\\right. \\\\\n& \\left.+\\cos \\left(2 \\pi z_{13}\\right)+\\cos \\left(2 \\pi z_{23}\\right)\\right)\n\\end{aligned}\n$$ where $z_{i}=M x_{i}-o_{i}$. Subsystem 2: the quartic function; $$\n\\min f_{2}(\\mathbf{x})=z_{21}^{4}+2 z_{22}^{4}+3 z_{23}^{4}+4 z_{13}^{4}+5 z_{33}^{4}\n$$ where $[-5,5]$. Subsystem 3: the Griewank function; $$\n\\begin{aligned}\n\\min f_{3}(\\mathbf{x})= & 1+\\left(z_{31}^{2}+z_{32}^{2}+z_{33}^{2}+z_{23}^{2}+z_{43}^{2}\\right) / 4000 \\\\\n& -\\left[\\cos \\left(z_{31}\\right) \\cos \\left(z_{32} / \\sqrt{2}\\right) \\cos \\left(z_{33} / \\sqrt{3}\\right)\\right. \\\\\n& \\left.\\cos \\left(z_{23} / \\sqrt{4}\\right) \\cos \\left(z_{43} / \\sqrt{5}\\right)\\right]\n\\end{aligned}\n$$ where $N=1000, E=2, M=0.1 N$. Subsystem 4: the Schwefel absolute function; $$\n\\begin{aligned}\n\\min f_{4}(\\mathbf{x})= & \\left|z_{41}\\right|+\\left|z_{42}\\right|+\\left|z_{43}\\right|+\\left|z_{33}\\right| \\\\\n& +\\left|z_{41} z_{42} z_{43} z_{33}\\right|\n\\end{aligned}\n$$ where [FORMULA62]. The loss functions of subsystem 1, subsystem 2 and subsystem 4 are multimodal. The loss function $f_{4}$ is nonderivable. The overall loss of this problem $F(\\mathbf{x})=\\sum_{i=1}^{4} f_{i}(\\mathbf{x})$ is a multimodal function and nonderivable. Since there are random offset and random rotation matrix, we don't know the optimal solution here.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:01:57",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What statistical measures are proposed to quantify the performance of the EMSM-EDA algorithm in comparison to other algorithms on real-world optimization problems?",
      "topics": [
        "performance"
      ],
      "answer": "Friedman's test is conducted to compare the performance of EMSM-EDA against AMaLGaM, SaDE, JADE, and $\\mathrm{EMNA}_{\\mathrm{g}}$ on seven real-world engineering optimization problems. The ranking of each algorithm, based on the Friedman's test, is used to assess the relative performance, with EMSM-EDA achieving the best ranking in this instance.",
      "chunk_id": 3133,
      "paper_id": "An efficient mixture sampling model for gaussian estimation of distribution algorithm",
      "paper_title": "An efficient mixture sampling model for gaussian estimation of distribution algorithm",
      "paper_year": "2022",
      "chunk_content": "As shown in Table 13, EMSM-EDA surpasses [FORMULA299], and SaDE on 7, 5, 6, and 6 problems, respectively. As a contrast, EMSM-EDA is defeated by [FORMULA300], and SaDE on 0, 1, 0, and 0 problems, respectively. This indicates that EMSM-EDA can achieve the excellent performance on these real-world optimization problems. Furthermore, this paper conducts Friedman's test on five comparison algorithms based on seven real-world engineering optimization problems. Table 14 shows that EMSM-EDA has the best ranking, sequentially followed by AMaLGaM, SaDE, JADE, and $\\mathrm{EMNA}_{\\mathrm{g}}$. Therefore, the above results demonstrate the potential of EMSM-EDA on the real-world optimization problems. Table 12 Seven real-world optimization problems. Table 13 The means and the standard deviations of FEVs obtained by the five algorithms over 25 independent runs on seven real-world optimization problems. \"+\", \"\", and \" [FORMULA302] \" indicate that the performance of the corresponding algorithm is better than, worse than, and similar to that of EMSM-EDA, respectively. Table 14 Rankings of EMSM-EDA, AMaLGaM, SaDE, JADE, and EMNA $_{\\mathrm{R}}$ on seven real-world optimization problems based on Friedman's test.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:01:58",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "Based on the text, what is the main conclusion regarding the performance of the Tri-EDA model on Problem B?",
      "topics": [
        "performance"
      ],
      "answer": "The performance analysis of the Tri-EDA model on Problem B, specifically concerning the standard deviations of the primary model, indicates that more systematic studies are necessary to thoroughly assess the advantages and weaknesses of Tri-EDA, particularly in relation to two-peak problems. Further development should consider the interactions among various factors to design effective dynamic optimization techniques.",
      "chunk_id": 11921,
      "paper_id": "Extending a class of continuous estimation of distribution algorithms to dynamic problems",
      "paper_title": "Extending a class of continuous estimation of distribution algorithms to dynamic problems",
      "paper_year": "2008",
      "chunk_content": "7 The standard deviations of the primary model in Tri-EDA [FORMULA86] on Problem B [IMAGE_7] Fig. 8 The performance of Tri-EDA [FORMULA87] on Problem B without the optional model conducted on a class of two-peak problems and more systematic studies are needed to fully investigate the advantages and weaknesses of Tri-EDA [FORMULA88]. It is expected that good dynamic optimization techniques should be designed by taking into account all these factors as well as having a good understanding towards the interactions among them.",
      "question_type": "synthesis summary",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:00",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of scheduling problems, how does increasing the generality of a flow-shop scheduling problem by allowing different job permutations on each machine, as opposed to a Permutation Flow-Shop Problem (PFSP), affect the size of the solution space and what are the implications for algorithmic complexity?",
      "topics": [
        "complexity"
      ],
      "answer": "The more general flow-shop scheduling problem, where job permutations can differ on each machine, increases the size of the solution space from n! to (n!)^m, where 'n' represents the number of jobs and 'm' represents the number of machines. This substantial increase in the solution space implies a higher computational complexity when searching for optimal solutions, even though it may only lead to small improvements in solution quality compared to the PFSP.",
      "chunk_id": 2380,
      "paper_id": "A block based estimation of distribution algorithm using bivariate model for scheduling problems",
      "paper_title": "A block based estimation of distribution algorithm using bivariate model for scheduling problems",
      "paper_year": "2014",
      "chunk_content": "- Setup and removal times are independent from process sequence and are included in the processing times. There are many other criteria that can be considered for the purpose of optimization. We refer the reader to Bagchi (1999) for a detailed discussion of scheduling using GA. For details of the flow-shop as well as other scheduling and sequencing problems, we refer the reader to Baker (1975). As proposed by Reeves (1995), a more general flow-shop scheduling problem can be defined by allowing the permutation of jobs to be different on each machine. However, the more general flowshop scheduling problem tends to show only small improvement in solution quality over the PFSP, while it substantially increases the complexity of the problem. The size of the solution space increases from n ! to ( $\\mathrm{n}!) \\mathrm{m}$. Other objective functions for PFSP have also received a lot of attention, such as the mean flow-time (the time a job spends in the process), or minimizing the mean tardiness (assuming some deadline for each job). Some of the PFSP problems in manufacturing industries like their jobs may have non-identical release dates, and there may be sequence-dependent setup times, and limited buffer storage between machines and so on. The problem in real world will make the problem more complex. However, genetic algorithm provides a more realistic view to solve the problem.",
      "question_type": "advantage",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:01",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm for service composition planning, how should practitioners integrate QoS attributes like time, cost, and availability, while also incorporating a reputation attribute derived from user experiences?",
      "topics": [
        "metrics"
      ],
      "answer": "Practitioners should integrate time, cost, and availability within the EDA's fitness function to guide the search for optimal service composition plans. The reputation attribute, derived from user experiences, can be incorporated as a self-rating metric that influences the probability model used by the EDA to sample new solutions. This self-rating metric should be weighted appropriately within the fitness function to balance objective QoS attributes with subjective user feedback, allowing the EDA to adapt to evolving user preferences.",
      "chunk_id": 11292,
      "paper_id": "QoS-based service provision schemes and plan durability in service composition",
      "paper_title": "QoS-based service provision schemes and plan durability in service composition",
      "paper_year": "2008",
      "chunk_content": "The paper [12] proposes a broker that supports planning and execution of any composite services with multiple QoS classes. Since a particular plan can be reused and executed repetitively as a flow, the QoS can be guaranteed on a per-flow rather than a per-request basis, and different QoS levels can be negotiated with respect to the volume of execution requests. Time, cost, and availability are included in the QoS model, and linear programming is used as the planning algorithm. The work in [13] presents a semantics-based planning approach in which data semantics, functional semantics, QoS (i.e. time, cost, reliability, availability, domain-specific QoS metrics), and constraints of service instances are considered. Ontology-based service dependencies such as business/technological constraints and partnership between services are addressed, and integer linear programming is used as the planning algorithm. Regarding the works above, we see that EDA is only an alternative planning algorithm with a means to utilise prior knowledge when finding a solution plan and thus we do not aim to compare its performance with the algorithms in other approaches. Nevertheless, we share with them the common QoS attributes, but the reputation attribute is captured by a self-rating metric which is derived from a user's own experiences in the delivered QoS rather than from other users' subjective opinions.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:03",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the computational burden addressed in the described control framework for cooperative power source management?",
      "topics": [
        "analysis"
      ],
      "answer": "The two-layer control framework moves the computation task from the on-board controller to the partial cloud-based controller with several asynchronous units, thereby relieving heavy computation intensity originally imposed on the on-board hardware. The comparative analysis performed investigates the performance in general computation, communication speed and communication delay.",
      "chunk_id": 20935,
      "paper_id": "A novel strategy for power sources management in connected plug-in hybrid electric vehicles based on mobile edge computation framework",
      "paper_title": "A novel strategy for power sources management in connected plug-in hybrid electric vehicles based on mobile edge computation framework",
      "paper_year": "2020",
      "chunk_content": "The combination of the long-term and short-term optimization prompts the performance of instant PSMSs. Despite the laudable merits, some work can be further performed to gain better performance: 1) The predictive framework can be rationally refined to enable each local controller to share computation burden evenly with zero-lag communication. 2) The calculation speed of planning reference trajectories can be accelerated while keeping the effect that is tightly close, or equal, to global optimization. ### 1.4. Contribution In this context, we hereby presented a novel cooperative PSMS for cPHEVs based on the evolutional control framework, and the main contributions of this study can be attributed to the following two aspects: 1) A brand-new hierarchical control framework is designed. The twolayer control framework moves the computation task from the original on-board controller to the partial cloud based controller with several asynchronous units, thereby relieving heavy computation intensity originally imposed on the on-board hardware. The comparative analysis is performed to elaborately investigate the performance in general computation, communication speed and communication delay. 2) A cooperative control strategy is developed to conduct active energy consumption plan and adaptive energy management successively. The active plan module programs the energy consumption globally, while the adaptive management module achieves the immediate control by tracking the optimally planned results.",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:04",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance evaluation of algorithms tested in the CEC 2014 test suite compare to those tested in the CEC 2017 test suite, given that the termination criterion is max_nfes = 10000 * D, where D represents the dimensionality (10, 30, 50, or 100)?",
      "topics": [
        "comparison"
      ],
      "answer": "The algorithms selected from the CEC 2014 test suite are compared against those tested in the CEC 2017 test suite to illustrate their performance. The experimental environment is kept identical, and parameters are set according to the original literature to ensure fairness. The termination criterion is max_nfes = 10000 * D, where D = 10, 30, 50, 100. The algorithms are executed independently 51 times to reduce random errors, and the data is obtained using Matlab2016b on a PC with a 3.4 GHz Intel (R), Core (TM) i7-6700 CPU, 8 GB of RAM, and a 64-bit Operating System.",
      "chunk_id": 8424,
      "paper_id": "A knowledge-driven co-evolutionary algorithm assisted by cross-regional interactive learning",
      "paper_title": "A knowledge-driven co-evolutionary algorithm assisted by cross-regional interactive learning",
      "paper_year": "2023",
      "chunk_content": "These benchmark functions with minimization problems (Liang et al., 2014) are widely adopted by the researchers to test the performance of the algorithms. Among 30 functions, $f_{1}-f_{3}$ are 3 unimodal functions, $f_{4}-f_{16}$ is 13 simple multimodal functions, $f_{17}-f_{22}$ are 6 hybrid functions, and $f_{23}-f_{30}$ are 8 composition functions. As specified in the test suite, the termination criterion is max_nfes $=10000 \\times D$, where $\\mathrm{D}=10,30,50,100$. All the algorithms are carried out in the same experimental environment to ensure the fairness of the experiment and executed independently 51 times to reduce random errors. All experimental data are obtained by Matlab2016b on a PC with a 3.4 GHz Intel (R), Core (TM) i7-6700 CPU, 8 GB of RAM, and a 64-bit Operating System. ### 4.2.2. Comparison with the state-of-the-art algorithms The comparison algorithms selected in the CEC 2014 test suite are from those tested in the CEC 2017 test suite to fully illustrate their performance. The experimental environment of all the algorithms is identical to guarantee fairness, and the parameters are set as the original literature.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:06",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the on-demand evaluation strategy (ODES) described, what programming considerations are necessary to manage the selection of individuals for fitness function evaluation based on entropy computations?",
      "topics": [
        "search"
      ],
      "answer": "Implementing ODES requires a mechanism to compute and track the entropy value of the population with respect to some model. The code must differentiate between cases where the entropy value monotonically decreases (indicating optimization progress) and cases where it fluctuates (signaling individuals with promising or unpromising building blocks). Based on these entropy computations, the implementation must include conditional logic to decide whether an individual requires evaluation by the accurate fitness function or can be selected without it. This on-demand evaluation should be implemented to dynamically adjust the evaluation frequency based on the population's entropy characteristics.",
      "chunk_id": 10040,
      "paper_id": "Entropy-based efficiency enhancement techniques for evolutionary algorithms",
      "paper_title": "Entropy-based efficiency enhancement techniques for evolutionary algorithms",
      "paper_year": "2012",
      "chunk_content": "Evaluation relaxation methods [13,42,44] try to partially replace accurate, but expensive, fitness functions with estimation/approximation models that are more error-prone, but also more economical. Notably, Jin [18] conducted a comprehensive survey of fitness approximations in EAs; however, a more detailed discussion of the aforementioned methods is beyond the scope of this paper. In this paper, we analyze the inherent randomness of the EA population. More specifically, we discuss the variation tendency of the entropy value of a population, calculated with respect to some model. The entropy observation, combined with the characteristics of the building block structure of the problem at hand, can be used to develop several strategies that enhance the performance of the evolutionary optimization process. This paper demonstrates that there are different types of entropy variations, depending on the search landscape. In some cases, where the entropy value of the population has been monotonically reduced, it can be used to indicate the optimization progress (e.g., initialization, stabilization, or convergence). For other cases, in which the randomness of a population has fluctuated, the entropy value can still be used to signal the existence of an individual with promising or unpromising building blocks. Based on these entropy computations, we can design an on-demand evaluation strategy (ODES) to recognize which individuals should be evaluated by the fitness function and which individuals can be selected without requiring exact evaluations.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:08",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What statistical measures are used to quantify the performance of the EDAMA model when evaluating the sampling mechanism?",
      "topics": [
        "optimization"
      ],
      "answer": "The performance of the EDAMA model's sampling mechanism is quantified using the best, average, and worst RPD values, along with the standard deviation. ANOVA is also carried out at a 95% confidence level, and the p-value is assessed to determine if the selective-enhancing sampling mechanism is significantly better than the uniform sampling mechanism.",
      "chunk_id": 25866,
      "paper_id": "An Estimation of Distribution Algorithm-Based Memetic Algorithm for the Distributed Assembly Permutation Flow-Shop Scheduling Problem",
      "paper_title": "An Estimation of Distribution Algorithm-Based Memetic Algorithm for the Distributed Assembly Permutation Flow-Shop Scheduling Problem",
      "paper_year": "2016",
      "chunk_content": "The EDAMA with each different value of [FORMULA159] is TABLE VIII EFFECTS OF THE SAMPLING MECHANISM [IMAGE_7] Fig. 11. Interval plots by different values of [FORMULA160]. TABLE IX ANOVA ReSULTS OF THE SAMPLING MECHANISM run 20 times independently. The statistical results are listed in Table VIII, including the best, average, and worst RPD values as well as the standard deviation. Besides, the interval plots by different values of [FORMULA161] are illustrated in Fig. 11. Note that, the case of $\\mu=1$ means that traditional uniform sampling mechanism [23] is employed. In addition, ANOVA is carried out at $95 \\%$ confidence level. The results of ANOVA are listed in Table IX. From Table VIII and Fig. 11, it can be seen that EDAMA performs better when [FORMULA164] is larger than 1. From Table IX, it also can be seen that $p$-value is $0<0.05$. Therefore, selective-enhancing sampling mechanism is considered to be significantly better than uniform sampling mechanism [23].",
      "question_type": "evaluation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:09",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for Evolution Strategies (ES) such as the $(\\mu, \\lambda)$-ES, considering the need to specify parameters for optimization problems?",
      "topics": [
        "bounds"
      ],
      "answer": "The core purpose is to define a complete, parameterized algorithm instance ready for execution on a given optimization problem. For a $(\\mu, \\lambda)$-ES, this involves setting the parent population size $(\\mu)$, offspring population size $(\\lambda)$, and the standard deviation $\\sigma$ of the Gaussian mutation distribution $\\mathcal{N}(0, \\sigma)$. These parameter settings, along with the algorithm's structure, fully specify how the metaheuristic will operate, ultimately influencing the best objective function value found during an experimental trial.",
      "chunk_id": 12673,
      "paper_id": "A general-purpose tunable landscape generator",
      "paper_title": "A general-purpose tunable landscape generator",
      "paper_year": "2006",
      "chunk_content": "For example, the simplest possible form of an evolution strategy (ES) consists only of a mutation operator, where each component or variable of an individual $x_{i}$ is mutated by the addition of a single sample drawn from a Gaussian distribution $\\mathcal{N}(0, \\sigma)$ (see, e.g., [45]). To specify an algorithm of this class (i.e., a $(\\mu, \\lambda)$-ES), settings are required for the size of the parent $(\\mu)$ and offspring $(\\lambda)$ populations, as well as the value of the standard deviation of the mutation distribution $\\sigma$. Thus, a specified algorithm includes all the parameterized information needed to run the metaheuristic on a given optimization problem. ${ }^{1}$ Different algorithm classes will, of course, have different numbers of parameters that need to be specified. As mentioned earlier, in this context the literature has proposed a large number (at least hundreds) of algorithm classes. Assume that a single optimization problem is to be solved. A performance measurement of an experimental trial is typically a scalar value such as the best objective function value found by an algorithm. The parameters of a specified algorithm are numerical, typically continuous or integer-valued, and may have defined bounds on the range of sensible values that they can take.",
      "question_type": "synthesis summary",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:10",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the improved estimation of distribution algorithm, as applied to truck scheduling in an underground mine, determine the optimal truck transportation task sequence based on minimizing the total truck waiting time?",
      "topics": [
        "scheduling"
      ],
      "answer": "The improved estimation of distribution algorithm determines the optimal truck transportation task sequence by minimizing the total truck waiting time, which serves as the objective function. This is done in accordance with the optimal ore blending scheme. The algorithm first determines the optimal ore blending scheme based on minimizing the shift's total transportation distance. Then, using the optimal ore blending scheme, it obtains the optimal scheduling scheme (i.e., the optimal truck transportation task sequence) by minimizing the total truck waiting time.",
      "chunk_id": 6377,
      "paper_id": "Scheduling optimization of underground mine trackless transportation based on improved estimation of distribution algorithm",
      "paper_title": "Scheduling optimization of underground mine trackless transportation based on improved estimation of distribution algorithm",
      "paper_year": "2024",
      "chunk_content": "12. (continued). and truck travel time constraints, a mathematical model of the transport truck scheduling in an underground mine is developed. (2) The improved estimation of distribution algorithm is used to solve the truck scheduling model step by step. Firstly, the optimal ore blending scheme is determined based on the objective function of minimizing the shift's total transportation distance. Secondly, in accordance with the optimal ore blending scheme, the optimal scheduling scheme, i.e., the optimal truck transportation task sequence, is obtained by minimizing the total truck waiting time as the objective function. In comparison to other algorithms, the improved estimation of distribution algorithm yields the best result, with a waiting time of 7.5 min corresponding to the scheduling scheme it generates. It demonstrates that the implementation of this scheduling scheme can prevent traffic meetings on the ramp segment and significantly improve the efficiency of truck transportation. (3) The travel time diagram for each transport truck can be calculated based on the optimal scheduling scheme obtained by the improved estimation of distribution algorithm. According to the diagram, the position of each transport truck at any given time can be determined, which has significant guiding significance for the mine's actual truck transportation. # CRediT authorship contribution statement Ning Li: Funding acquisition, Conceptualization, Methodology, Software, Data curation, Visualization, Writing - original draft.",
      "question_type": "definition",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:12",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners generate new solutions, given the estimated mean $\\mu_{i}^{i}$ and standard deviation $\\sigma_{i}^{i}$?",
      "topics": [
        "Estimation of Distribution Algorithms"
      ],
      "answer": "Practitioners should generate new solutions with gene values $x_{i}$ with a probability $p_{f}(x_{i})$ based on the estimated mean $\\mu_{i}^{i}$ and standard deviation $\\sigma_{i}^{i}$. If the constraints are not met, the individual value should be reset to the boundary.",
      "chunk_id": 11196,
      "paper_id": "Computational intelligence-based energy management for a large-scale PHEV PEV enabled municipal parking deck",
      "paper_title": "Computational intelligence-based energy management for a large-scale PHEV/PEV enabled municipal parking deck",
      "paper_year": "2012",
      "chunk_content": "The two parameters mean $\\mu_{i}^{i}$ and standard deviation $\\sigma_{i}^{i}$ can be estimated. (5) Generate new solutions which have gene values $x_{i}$ with the probability $p_{f}\\left(x_{i}\\right)$ from step 4. Energy scheduling at a PHEV/ PEV parking deck is also subject to different constraints that limit the search space to a certain feasible region. Reset the individual value (i.e., the amount of power allocated to a specific vehicle) to the boundary if the constraints do not hold true. (6) Replace some members from the current population with offspring generated in previous step. (7) Repeat steps 2-6 until the maximum number of iterations or the minimum error criteria is met. Save the near-optimal solution (i.e., the amount of power allocated to each PEHVs/ PEVs under a set of system constraints). ### 3.2. Particle Swarm Optimization (PSO) PSO is an iterative stochastic optimization method. It simulates the behavior of flocks of birds or schools of fish. In 1995, Kennedy and Eberhart presented a new, evolutionary computation algorithm called Particle Swarm Optimization (PSO) [27,28]. In PSO, each solution is a \"bird\" (or, more generally, a \"particle\") in the search space.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:13",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the EDA discussed, how does the model complexity (univariate, bivariate, and multivariate) impact the convergence speed in NK landscapes, considering the trade-off between expressiveness and computational cost?",
      "topics": [
        "complexity"
      ],
      "answer": "The EDA study examines univariate, bivariate, and multivariate models in NK landscapes, noting that complex models can express variable relationships more accurately but may be impractical due to high computational cost and large population size requirements. The bivariate model outperformed the univariate and multivariate models in both solution quality and convergence speed on the NK landscape problem. Kruskal-Wallis non-parametric analysis of variance with multiple comparison post-hoc analysis is used to analyze the results with a significance level denoted as [FORMULA181].",
      "chunk_id": 683,
      "paper_id": "Effect of model complexity for estimation of distribution algorithm in NK landscapes",
      "paper_title": "Effect of Model Complexity for Estimation of Distribution Algorithm in NK Landscapes",
      "paper_year": "2013",
      "chunk_content": "In general, it shows that a simple model is not necessarily worse than a complex one for EDAs: The bivariate model outperforms the univariate and multivariate models in both solution quality and convergence speed on the NK landscape problem. ## V. CONCLUSIONS The model complexity is a key factor in the performance of EDA. Complex models can express the relations among variables more accurately than simple models. However, for some problems with strong interaction among variables, building a complex model for all the relations becomes unrealistic and impractical due to its high computational cost and requirement for large population size. In this study, we take the EDAs based on three different models, namely, univariate, bivariate and multivariate models, to observe their behaviors in terms of solution quality and convergence speed in NK landscapes. This study further tracks the variation in the logarithmic probability of generating the best known solution to examine Table II: Kruskal-Wallis non-parametric analysis of variance with multiple comparison post-hoc analysis for the NK landscape problems with [FORMULA179] and [FORMULA180]. All methods are tested under 30 trials and the significance level [FORMULA181]. (a) [FORMULA182] (a) [FORMULA183] (b) [FORMULA184] Table III: Kruskal-Wallis non-parametric analysis of variance with multiple comparison post-hoc analysis for NK landscapes problems with [FORMULA185] and [FORMULA186].",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:15",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "In the context of Estimation of Distribution Algorithms (EDAs), what is a key advantage of modeling the distribution of solutions using a Gaussian distribution, specifically concerning the calculation of the probability distribution P(x₁, x₂ ... xₘ) of m variables?",
      "topics": [
        "search"
      ],
      "answer": "A key advantage lies in simplifying the calculation of the probability distribution P(x₁, x₂ ... xₘ) by expressing it as a product of the distributions of individual variables, P(xᵢ). This simplification allows for estimating the mean (μᵢ(k)) and covariance (σᵢ²(k)) parameters of the normal probability density function (pdf) independently for each variable using maximum likelihood estimation. The mean is computed as the average of the variable xᵢ across N selected individuals, and the covariance is calculated based on the squared difference between each individual's value and the mean, summed across BN selected individuals.",
      "chunk_id": 17429,
      "paper_id": "EDA based Deep Neural Network Parameter Optimization",
      "paper_title": "EDA based Deep Neural Network Parameter Optimization",
      "paper_year": "2019",
      "chunk_content": "Meanwhile, the probability model is updated in each generation according to the new population. therefore, the population evolves, and finally search the optimal solutions. The diagram of the EDA is shown in Figure 3. The most important step of EDAs is the construction of probabilistic model, and the Gaussian distribution of individuals is assumed to model and estimate the distribution of solutions[9, 10]. Therefore, mean and variance of promising individuals are computed according to the maximum likelihood. There the probability distribution $P\\left(x_{1}, x_{2} \\cdots x_{m}\\right)$ of the vector $\\left(x_{1}, x_{2} \\cdots x_{m}\\right)$ of m variables is a product of the distributions of individual variables: $$\nP\\left(x_{1}, x_{2} \\cdots x_{m}\\right)=\\prod_{i=1}^{m} P\\left(x_{i}\\right)\n$$ The mean and covariance parameters of the normal pdf can be estimated according to the promising individuals[11]. $$\n\\widehat{\\mu}_{i}(k)=\\frac{1}{N} \\sum_{s=1}^{N} x_{i}^{s}(k)\n$$ $$\n\\sigma_{i}^{2}(k)=\\frac{1}{N} \\sum_{n=1}^{B N}\\left(x_{i}^{n}(k)-\\bar{\\mu}_{i}(k)\\right)\\left(x_{i}^{n}(k)-\\bar{\\mu}_{i}(k)\\right)^{T}\n$$ $\\bar{\\mu}_{i}(k)$ is the mean of $i$ th variable in kth iteration, BN is the selected individuals size. $\\sigma_{i}^{2}(k)$ is the covariance of [FORMULA34] th variable in $k$ th iteration. [IMAGE_2] Figure 3: Flowchart of Traditional EDA. The normal pdf $N\\left(\\mu_{i}, \\sigma_{i}\\right)$ is defined as equation (8). $$\nN\\left(x_{i}, \\mu_{i}, \\sigma_{i}\\right)=\\frac{1}{\\sigma_{i} \\sqrt{2 \\pi}} e^{-\\frac{\\left(x_{i}-\\mu_{i}\\right)^{2}}{2 \\sigma_{i}^{2}}}\n$$ The probability distribution [FORMULA38] can be described as equation (9).",
      "question_type": "advantage",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:17",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the Tri-EDA${_G}$ model, as described in the provided procedure?",
      "topics": [
        "optimization"
      ],
      "answer": "The Tri-EDA${_G}$ model involves these steps: 1) Initialize primary, secondary and optional models $P_{0}^{1}(X), P_{0}^{2}(X), P_{0}^{3}(X)$. 2) Sample $O_1$ from $P_{t}^{1}(X)$. 3) Sample $O_2$ from a specified formula. 4) Sample $O_3$ from another formula if a certain condition is active. 5) Combine the samples into a set $O = {O_1, O_2, O_3}$. 6) Evaluate individuals. 7) Select a subset of the population. 8) Update a specific formula. 9) Determine whether to activate another formula. 10) Update yet another formula if a condition is active. 11) Increment a counter. 12) Repeat from Step 2 until the stopping criteria are met. The best individual from the current population is always carried over to the next population.",
      "chunk_id": 11916,
      "paper_id": "Extending a class of continuous estimation of distribution algorithms to dynamic problems",
      "paper_title": "Extending a class of continuous estimation of distribution algorithms to dynamic problems",
      "paper_year": "2008",
      "chunk_content": "The full procedure of the dynamic version of [FORMULA44] (referred to as $\\mathrm{Tri}-\\mathrm{EDA}_{\\mathrm{G}}$ ) is shown in Table 2. Note that the best individual from the current population is always carried to the next population to make sure that the best solution found so far will not be lost due to random factors. Table 2 The framework of Tri-EDA ${ }_{\\mathrm{G}} P_{t}^{1}(X)$ : primary model; $P_{t}^{2}(X)$ : secondary model; $P_{t}^{3}(X)$ : optional model Step 1: Initialize $P_{0}^{1}(X), P_{0}^{2}(X), P_{0}^{3}(X), t=0$ Step 2: Sample $O_{1}$ from $P_{t}^{1}(X)$ Step 3: Sample $O_{2}$ from [FORMULA53] Step 4: Sample $O_{3}$ from [FORMULA55] if [FORMULA56] is active Step 5: $O=\\left\\{O_{1}, O_{2}, O_{3}\\right\\}$ Step 6: Evaluate individuals in [FORMULA58] Step 7: Select a subset of the population [FORMULA59] Step 8: Update [FORMULA60] Step 9: Determine whether to activate [FORMULA61] Step 10: Update [FORMULA62] if [FORMULA63] is active Step 11: [FORMULA64] Step 12: Go to Step 2 until stopping criteria are met # 5 Experiments ### 5.1 Test problems For any experimental studies of EAs, a key decision that needs to be made in the first place is what kind of test problems should be included in the experiments.",
      "question_type": "implementation focused",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:19",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What statistical measure is used to compare the performance of the proposed algorithm against IM-MEDA, RM-MEDA, and CSEA, and what information is provided to summarize the statistical results?",
      "topics": [
        "fitness"
      ],
      "answer": "The Wilcoxon rank sum test is used at an importance level of 0.05 to compare the results. The mean values and standard deviations (SDs) are presented to summarize the statistical results.",
      "chunk_id": 20741,
      "paper_id": "An inverse model-based multiobjective estimation of distribution algorithm using Random-Forest variable importance methods",
      "paper_title": "An inverse model-based multiobjective estimation of distribution algorithm using Random-Forest variable importance methods",
      "paper_year": "2020",
      "chunk_content": "The population size is adjusted to 100 for proposed algorithm and IM-MEDA, RM-MEDA, and CSEA. We carried out 20 independent runs for each compared algorithm on each test instance. The final condition for each algorithm is adjusted to a maximum of 100000 fitness evaluations for all the test instances. ## 5.5 | Evaluation and comparison of algorithms In this section, the performance of three proposed methods are compared with the IM-MOEA, [FORMULA405] that is to construct Gaussian process-based inverse model using the random grouping technique that map all found nondominated solutions from the objective space to the decision space, RM-MEDA, plan a reproduction operator by explicitly modeling the regularity in the distribution of Pareto optimal solutions and CSEA, use a artificial neural network in order to predict the dominance relationship between candidate solutions and reference solutions instead of approximating the objective values separately. TABLE 1 Parameter settings of the four algorithms in comparison The Wilcoxon rank sum test is adapted to compare the results obtained by our proposed algorithm and the other four algorithms at an importance level of 0.05 . In the tables that summarize the statistical results, the first and second lines illustrate the mean values and the SDs, respectively.",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:20",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of adaptively changing the inertia weight and acceleration coefficients within the BPSO and ABPSO algorithms detailed in the context of Estimation of Distribution Algorithms?",
      "topics": [
        "search"
      ],
      "answer": "The adaptive adjustment of inertia weight and acceleration coefficients, specifically c1 and c2, in Binary Particle Swarm Optimization (BPSO) and Adaptive BPSO (ABPSO) aims to enhance computational efficiency, improve search capabilities, and facilitate convergence towards a global optimal solution by controlling particle movement within the search space. The time-varying acceleration coefficients restrict particle movement early on, and later encourage particles to follow social trends, reducing premature convergence. Different 'S' type transfer functions (Sx) impact the performance of both BPSO and ABPSO.",
      "chunk_id": 21540,
      "paper_id": "An insight to the performance of estimation of distribution algorithm for multiple line outage identification",
      "paper_title": "An insight to the performance of estimation of distribution algorithm for multiple line outage identification",
      "paper_year": "2018",
      "chunk_content": "The time varying acceleration coefficients efficiently controls the search process and the convergence to the global solution. The time varying acceleration coefficients restrict the movement of particles around the search space during the early generations, whereas in the later generations they force the particles to follow the trend of social behavior, which eventually reduces the affect of pre-mature convergence in BPSO. The time varying acceleration coefficients are shown as, $c_{1}=\\left(c_{1 i}-c_{1 j}\\right)\\left(\\frac{i}{G}\\right)+c_{1 i}$, $c_{2}=\\left(c_{2 i}-c_{2 j}\\right)\\left(\\frac{i}{G}\\right)+c_{2 i}$ where $c_{1 f}<c_{1 i}$ and $c_{2 f}<c_{2 j}$ are constants. From (21) it is observed that the value of acceleration coefficients vary from $c_{i}$ to $\\left(2 c_{i}-c_{f}\\right) . G$ is the maximum algorithm iterations, and [FORMULA142] is current iteration. In (20) and (21), the inertia weight and acceleration coefficients are adaptively changing with a view of enhancing the computational efficiency, improving the search capabilities and obtaining the global optimal solution. Transformation function is an important factor, which eventually affects the performance of a BPSO algorithm. The simulations are carried out using different ' S ' type transfer functions for BPSO and ABPSO. In the detection performance and time consumption results, the BPSO(Sx) term means the BPSO result with ' Sx ' transfer function. Similarly, ABPSO(Sx) means the ABPSO result with ' Sx ' transfer function.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:22",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what algorithmic mechanism allows for the definition of new univariate EDAs that might outperform existing ones, as suggested for the OneMax and LeadingOnes benchmarks?",
      "topics": [
        "benchmarks"
      ],
      "answer": "The general formulation allows defining new univariate EDAs. The text suggests that these new EDAs, defined with 'intuitive arguments', can 'beat existing EDAs' for the OneMax and LeadingOnes benchmarks. This implies an algorithmic mechanism that allows for the creation of novel EDAs with the potential for improved performance on specific optimization problems.",
      "chunk_id": 2696,
      "paper_id": "General Univariate Estimation-of-Distribution Algorithms",
      "paper_title": "General Univariate Estimation-of-Distribution Algorithms",
      "paper_year": "2022",
      "chunk_content": "Our general formulation also allows to define new univariate EDAs, which might turn out to be superior to the existing ones. With intuitive arguments, we define such EDAs and show experimentally that they beat existing EDAs for the OneMax and LeadingOnes benchmarks. We are optimistic that this approach can be profitable also for other optimization problems. Acknowledgment. This work was supported by a public grant as part of the Investissements d'avenir project, reference ANR-11-LABX-0056-LMH, LabEx LMH.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:23",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to estimate parameters and noise distribution in a linear non-Gaussian system, what specific steps are involved in adopting an improved EDA algorithm to calculate the entropy of the feedback invariant?",
      "topics": [
        "search"
      ],
      "answer": "The improved EDA algorithm is adopted to estimate the parameters and noise distribution of the system, and then calculate the entropy of the feedback invariant. The effectiveness of the CPA procedures is tested by simulations, but the efficiency of the improved EDA algorithm is still an open problem, suggesting that further research is needed to optimize algorithms or consider other strategies for parameter identification and PDF estimation.",
      "chunk_id": 21227,
      "paper_id": "Non-Gaussian Systems Control Performance Assessment Based on Rational Entropy",
      "paper_title": "Non-Gaussian Systems Control Performance Assessment Based on Rational Entropy",
      "paper_year": "2018",
      "chunk_content": "The above simulation results show that the proposed scheme is more suitable for non-Gaussian systems CPA. # 5. Conclusions In this paper, by analyzing the limitation of MVC-based CPA method, a new MEC-based CPA method is developed for linear non-Gaussian system and the entropy index is obtained by PDF estimation. Furthermore, the improved EDA algorithm is adopted to estimate the parameters and noise distribution of the system and then calculate the entropy of the feedback invariant. The effectiveness of the CPA procedures is tested by many simulations. It can be concluded that the new MEC-based CPA index can be used to assess the control loop performance of linear systems with non-Gaussian stochastic disturbance. However, the efficiency of the improved EDA algorithm is still an open problem. Further research will focus on optimization algorithms or consider other strategies for parameter identification and PDF estimation. Author Contributions: J.Z. and H.J. conceived and designed the experiments; Y.J. and S.F. performed the experiments; Y.J. and S.F. analyzed the data; J.Z. and H.J. wrote the paper. Funding: This work was supported in part by NSFC (Grant No. 61473025, 61573050), the open-project grant funded by the State Key Laboratory of Synthetical Automation for Process Industry at the Northeastern University (PAL-N201702).",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:25",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the described hybrid EDA-IPM algorithm used for multi-objective meter placement, how does the number of Monte Carlo (MC) trials considered during the simulation study relate to the trade-off between computational cost and result accuracy?",
      "topics": [
        "complexity"
      ],
      "answer": "The description indicates that a lower number of Monte Carlo (MC) trials reduces the computational cost of the hybrid EDA-IPM algorithm. However, a higher MC value is expected to yield more accurate results. Therefore, the selection of the number of MC trials represents a trade-off between computational cost and the accuracy of the state estimation error minimization, where the computational complexity of the technique is deemed to not have a significant impact on the planning study of the distribution system.",
      "chunk_id": 22565,
      "paper_id": "Trade-offs in PMU and IED Deployment for Active Distribution State Estimation Using Multi-objective Evolutionary Algorithm",
      "paper_title": "Trade-offs in PMU and IED Deployment for Active Distribution State Estimation Using Multi-objective Evolutionary Algorithm",
      "paper_year": "2018",
      "chunk_content": "In these cases also, the performance of the proposed hybrid EDA-IPM algorithm is found to be better than those of all other algorithms used in this paper. The main advantage of using this multi-objective meter placement technique is that the operator can obtain a best compromised or a trade-off solution between the objectives to minimize the cost as well as the state estimation error. Basically, the selection of optimal solution depends on the decision maker. But in this paper, fuzzy theory has been used to find the best compromised solution between the objectives. Generally, meter placement techniques are used for the study of distribution systems. Therefore, the computational cost and complexity of the proposed technique do not have a significant impact on the planning study of the distribution system. The computational cost can be reduced if less number of MC trials is considered in the simulation study. However, if the MC value is high then more accurate results can be expected. ## B. Practical Indian 85 Bus System The effectiveness of the proposed algorithm has also been tested on a large scale practical Indian 85 -bus, $11-\\mathrm{kV}$ radial distribution network. The system includes 85 buses and 84 lines along with two DG at bus numbers 45 and 61, respectively. This system carries a total load of 2.574 MW and 2.622 MVAr.",
      "question_type": "relationship",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:26",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the Estimation of Distribution Algorithm (EDA) differ from Genetic Algorithms (GAs) in terms of evolutionary operators?",
      "topics": [
        "genetic"
      ],
      "answer": "The Estimation of Distribution Algorithm (EDA) does not use crossover or mutation operators, whereas Genetic Algorithms (GAs) utilize both crossover and mutation operators for population evolution.",
      "chunk_id": 12350,
      "paper_id": "Feature selection for high-dimensional remote sensing data by Maximum Entropy Principle based optimization",
      "paper_title": "Feature selection for high-dimensional remote sensing data by Maximum Entropy Principle based optimization",
      "paper_year": "2001",
      "chunk_content": "For such high dimensionality, pattern recognition techniques suffer from the well-known curse-of-dimensionality phenomenon. This problem is resulting from the fact that the required number of labeled samples for supervised classification increases dramatically as a function of dimensionality [1]. In this paper, an approximate application of the Maximum Entropy Principle [3] is applied to feature selection problem for high dimentional remote sensing data. This method based on the EDA (Estimation of Distribution Algorithm) [4] paradigm, avoids the use of crossover and mutation operators to evolve the populations, in contrast to Genetic Algorithms. It is combined with an approximate application of the Maximum Entropy Principle as the models for representing the probability distribution of a set of candidate solution in the feature selection problem, using the application of automatic learning methods to induce the right distribution model in each generation. This paper is organized as follows: Section 2 introduces the EDA paradigm, Bayesian networks and the EBNA search algorithm. Section 3 presents a new algorithm base on an approximate application of the maximum entropy principle. Section 4 summaries the primary results of comparasion. ## 2 EDA Paradigm and EBNA Algorithm Genetic Algorithms(GAs) [2] are one of the well known optimization tools, based on population search with two operators: crossover and mutation. In contrast to GAs, the Estimation of Distribution Algorithm (EDA) [4] has neither crossover operator nor mutation operator.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:27",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the specific EDA mentioned, what data structures are required to represent the population and subpopulation at each generation, and how are these structures used in the algorithm?",
      "topics": [
        "optimization",
        "data structures",
        "algorithm implementation"
      ],
      "answer": "The EDA implementation requires data structures to represent the population at each generation, denoted as $\\boldsymbol{Pop}(\\boldsymbol{gen})$, and the high-quality subpopulation, denoted as $\\boldsymbol{S Pop}(\\boldsymbol{gen})$. $\\boldsymbol{Pop}(\\boldsymbol{gen})$ has a size of popsize, while $\\boldsymbol{S Pop}(\\boldsymbol{gen})$ has a size of spsize. The individuals in $\\boldsymbol{S Pop}(\\boldsymbol{gen})$ are represented as $\\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}$, where each individual is a sequence of job positions $\\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}=\\left[\\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}(1), \\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}(2), \\ldots, \\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}(n)\\right]$. Additionally, a matrix cube $\\boldsymbol{M C}_{\\boldsymbol{n} \\times \\boldsymbol{n} \\times \\boldsymbol{n}}^{\\text {gen }}$ is used, where each element $\\boldsymbol{M C}_{\\boldsymbol{n} \\times \\boldsymbol{n} \\times \\boldsymbol{n}}^{\\text {gen }}(x, y, z)$ stores a value $r$ in the range [0,1]. The ternary subscript $(x, y, z)$ represents the job block $[y, z]$ at position $x$ in the solution sequence. These data structures are used to model the search space and guide the evolution of solutions across generations.",
      "chunk_id": 4163,
      "paper_id": "A matrix-cube-based estimation of distribution algorithm for blocking flow-shop scheduling problem with sequence-dependent setup times",
      "paper_title": "A matrix-cube-based estimation of distribution algorithm for blocking flow-shop scheduling problem with sequence-dependent setup times",
      "paper_year": "2022",
      "chunk_content": "Let $\\boldsymbol{P o p}(\\boldsymbol{g e n})$ be the population at generation gen, and $\\boldsymbol{S P o p}(\\boldsymbol{g e n})$ be the high-quality subpopulation or superior solutions derived from $\\boldsymbol{M} \\boldsymbol{C}_{4 \\times 4 \\times 4}^{1}$, i.e., $\\boldsymbol{S P o p}(\\boldsymbol{g e n})=$ $\\left\\{\\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}, \\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, 2}, \\ldots, \\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }} \\text {, }_{2}\\right\\}$, where popsize and spsize respectively represent the size of $\\operatorname{Pop}(\\boldsymbol{g e n})$ and $\\boldsymbol{P M}_{\\boldsymbol{n}=\\boldsymbol{n}=\\boldsymbol{n}}^{\\text {ave }}$, gen $=1.2 \\ldots$ maxgen. maxgen is the maximum number of runs of the algorithm. Let $\\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}$ be the $k$ th individual in $\\boldsymbol{S}_{\\boldsymbol{P M}}^{\\text {ave }}(\\boldsymbol{x})$, i.e., $\\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}=\\left[\\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}(1), \\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}(2), \\ldots, \\boldsymbol{\\pi}_{\\text {Shoor }}^{\\text {gen }, k}(n)\\right], k=1$, ..., spsize. Without loss of generality, $\\boldsymbol{M C}_{\\boldsymbol{n} \\times \\boldsymbol{n} \\times \\boldsymbol{n}}^{\\text {gen }}$ is defined as the matrix cube at generation gen. $M C_{\\boldsymbol{n} \\times \\boldsymbol{n} \\times \\boldsymbol{n}}^{\\text {gen }}(x, y, z), x, y, z \\in\\{1,2, \\ldots, n\\}$ is the element in $r \\in[0,1]$ with the ternary subscript $(x, y, z)$, where $r$ corresponds to the $\\boldsymbol{S}_{\\boldsymbol{M C}}^{\\text {ave }}(\\boldsymbol{x})=1$ th position of the solution sequence, and $(y, z)$ is used to represent the job block $[y, z]$ at that position. Firstly, for the $\\boldsymbol{P M}_{\\boldsymbol{n}=\\boldsymbol{n}=\\boldsymbol{n}}^{\\boldsymbol{a}}$ th individual $\\boldsymbol{P M}_{\\boldsymbol{n}=\\boldsymbol{n}=\\boldsymbol{n}}^{\\text {0 }}(\\boldsymbol{x}, \\boldsymbol{y}, \\boldsymbol{z})=$ in $\\left\\{\\begin{array}{c}0, x=1 ; y, z=1, \\ldots, n \\\\ 1 / n^{2}, x=2,3, \\ldots, n-1 ; y, z=1, \\ldots, n .^{\\prime}\\end{array}\\right.$, the sequential relationship of the job # Algorithm 3:PFZ_RZ $\\boldsymbol{M C}_{\\boldsymbol{n}=\\boldsymbol{n}=\\boldsymbol{n}}^{\\mathbf{1}}$ Input: $\\pi, \\overline{\\mathbf{n}}, x, \\lambda$.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:30",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the effectiveness of the EDA model in identifying parameters for non-Gaussian systems, and how are these metrics quantified?",
      "topics": [
        "convergence"
      ],
      "answer": "The effectiveness of the EDA model is evaluated by comparing the estimated variance of the disturbance with the real variance. Specifically, the real variance is compared to the estimated values produced by the EDA. For example, a real variance of 0.0128 is compared with estimated values of 0.0139 and 0.0578 from two different methods. A closer estimated variance to the real variance indicates a more effective EDA model.",
      "chunk_id": 14499,
      "paper_id": "Non-Gaussian system identification based on improved estimation of distribution algorithm",
      "paper_title": "Non-Gaussian System Identification Based on Improved Estimation of Distribution Algorithm",
      "paper_year": "2017",
      "chunk_content": "But it is clear that the result of the improved EDA method is less deviate from the actual value than the previous method. The Fig. 8 shows the PDF of actual and estimated disturbance. Green line depicts that although the EDA algorithm terminates, the effect is not optimal. And through the improved EDA algorithm, the estimation effect shown in Fig. 8 has been significantly improved. As shown in Table 2, the real variance of the disturbance is 0.0128 , and the estimated values by the two methods are 0.0139 and 0.0578 , respectively. From the above description, it can be seen that the improved EDA method enhances the effectiveness of parameter identification for non-Gaussian systems. The Fig. 5 and Fig. 9 show that the parameters obtained by this method are optimal and have good fitting effect. ## 5 Conclusions This paper briefly introduces an improved method based on the traditional identification method with the estimation of distribution algorithm. The simulation results show that the proposed algorithm can accelerate the convergence speed and enhance the recognition effect through parameter preliminary estimation and data filtering. For the system with non-Gaussian noise, the parameter estimation could achieve a relatively high level of data accuracy and at the same time, the convergence process becomes more stable and rapid.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:32",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of Adaptive Resonance Theory (ART) systems in the context of online learning for large and evolving databases?",
      "topics": [
        "representation"
      ],
      "answer": "Adaptive Resonance Theory (ART) systems utilize a match-based learning process where memories are updated only when external inputs closely align with internal expectations or when novel information is encountered, making them particularly suitable for online learning scenarios with large and evolving databases. This contrasts with error-based learning, which adjusts memories to reduce the difference between target and actual outputs.",
      "chunk_id": 545,
      "paper_id": "Multi-objective optimization with an adaptive resonance theory-based estimation of distribution algorithm",
      "paper_title": "Multi-objective optimization with an adaptive resonance theory-based estimation of distribution algorithm",
      "paper_year": "2013",
      "chunk_content": "In other words, how to switch between stable and its plastic modes to achieve stability without rigidity and plasticity without chaos? Furthermore, how can it preserve its previously learned knowledge while continuing to incorporate new information, while preventing this new learning from \"overwriting\" the codes previously stored? The cornerstone property of ART systems is a pattern matching process that compares an external input with a pattern stored in the internal memory. As matching takes place, it leads either to a resonant or stable state, which persists long enough to permit learning, or to a parallel memory search. If the search ends with a positive match with stored code, the memory representation may either remain the same or incorporate new information from matched portions of the current input. If the search indicates that a new code is needed, then memory representation learns the current input. This match-based learning process is the foundation of ART code stability. Match-based learning allows memories to change only when input from the external world is close enough to internal expectations, or when something completely new occurs. This feature makes ART systems well suited to problems that require online learning of large and evolving databases. This class of learning is complementary to error-based learning, which responds to a mismatch by changing memories so as to reduce the difference between a target output and an actual output, rather than by searching for a better match.",
      "question_type": "advantage",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:33",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms employing a consensus approach, what mathematical principles govern the construction of a hierarchy of consensus solutions by decreasing the value of $k_{M}=\\max \\{k_{A}, k_{B}\\}$, and how does this relate to the inclusion property described by $x_{i, T}^{C}=1 \\Longrightarrow x_{i, T_{0}}^{C}=1$ for all $i=1, \\ldots, n$?",
      "topics": [
        "search"
      ],
      "answer": "The hierarchy of consensus solutions in this EDA-based approach is built by iteratively decreasing $k_{M}$, which represents the maximum of $k_{A}$ and $k_{B}$. Here, $k_{A}$ and $k_{B}$ seem to represent features flagged as selected. Decreasing $k_{M}$ allows for the creation of a hierarchy of consensus solutions. The inclusion property, expressed as $x_{i, T}^{C}=1 \\Longrightarrow x_{i, T_{0}}^{C}=1$, where $T_{0} \\leq T$, implies that if a feature $i$ is selected at a consensus level $T$, it will also be selected at any lower consensus level $T_{0}$. This means that as the value of $\\mathbf{S}=\\{S_{1}, S_{2}, \\ldots, S_{m}\\}$ decreases, the number of selected features increases, helping the search procedure avoid local optima and explore other parts of the search space. The overall consistency of the subsets $S_i$ and $S_j$ is measured by $\\Sigma(\\mathbf{S})=\\frac{2}{m(m-1)} \\sum_{i=1}^{m-1} \\sum_{j=i+1}^{m} I(S_{i}, S_{j})$, where $I(S_{i}, S_{j})$ denotes the consistency between subsets $S_i$ and $S_j$. The algorithm outputs consensus solutions within a user-defined range of $\\Sigma(\\mathbf{S})$.",
      "chunk_id": 24127,
      "paper_id": "Peakbin Selection in Mass Spectrometry Data Using a Consensus Approach with Estimation of Distribution Algorithms",
      "paper_title": "Peakbin Selection in Mass Spectrometry Data Using a Consensus Approach with Estimation of Distribution Algorithms",
      "paper_year": "2011",
      "chunk_content": "This corresponds to the features that have always been flagged as selected in $k_{B}$. By decreasing the value of $k_{M}=\\max \\left\\{k_{A}, k_{B}\\right\\}$, a hierarchy of consensus solutions can be built. This hierarchy fulfills the inclusion property, stating in this case that given $$\nI_{K}(A, B)=\\frac{r n-k_{M}^{2}}{k_{M}\\left(n-k_{M}\\right)}\n$$ and any $T_{0} \\leq T$, the following implication $x_{i, T}^{C}=1 \\Longrightarrow x_{i, T_{0}}^{C}=1$ holds for all $i=1, \\ldots, n$. As the value of $\\mathbf{S}=\\left\\{S_{1}, S_{2}, \\ldots, S_{m}\\right\\}$ decreases, the number of features selected in the consensus solution should increase. In fact, the addition of new features helps the search procedure to not get trapped in a local optimum and other parts of the search space are also considered. Thanks to this flexibility, a whole range of subsets can be evaluated instead of just a single one. Therefore, the user can set up a maximum and a minimum value for $$\n\\Sigma(\\mathbf{S})=\\frac{2}{m(m-1)} \\sum_{i=1}^{m-1} \\sum_{j=i+1}^{m} I\\left(S_{i}, S_{j}\\right)\n$$, and the procedure will output all the consensus solutions within that range. Thus, the features returned by this consensus approach are expected to be the most reliable and, at the same time, best suited to the classifier used by the wrapper evaluation. Section 4 introduces two different consistency metrics to properly analyze the consistency of the population consensus method against the classical procedure. ## 4 Consistency Measures and Stability Index Stability analysis is a recent topic in the feature selection domain [40], [41]. The main aim of stability analysis is to provide a means to state whether the features selected by a given selection approach are robust to changes in the data. In domains where knowledge discovery is a key objective, the stability of the selected features is a highly desirable property.",
      "question_type": "conceptual deep",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:36",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm with layered learning, how should practitioners estimate the covariance of each layer to maintain high-quality probability models for sampling diversified offspring?",
      "topics": [
        "optimization"
      ],
      "answer": "Practitioners should estimate the covariance of each layer based on the generated new mean position and the individuals within that specific layer. This approach maintains multiple probability models, which are then used to sample promising and diversified offspring separately. The LLEDA algorithm uses this to build the probability distribution.",
      "chunk_id": 3260,
      "paper_id": "A Layered Learning Estimation of Distribution Algorithm",
      "paper_title": "A Layered Learning Estimation of Distribution Algorithm",
      "paper_year": "2022",
      "chunk_content": "At last, the covariance of each layer is estimated based on the generated new mean position and the individuals in this layer. By this means, multiple probability models with high quality are maintained and then are used to sample promising and diversified offspring separately. Comparative experiments conducted on a widely used benchmark problem set demonstrate that the proposed LLEDA achieves competitive or even much better performance than several state-of-the-art and representative EDAs. ## CCS CONCEPTS - Mathematics of computing $\\rightarrow$ Evolutionary algorithm; Bioinspired optimization; ## KEYWORDS Estimation of Distribution Algorithms, Multivariate Gaussian Distribution, Layered Learning, Multiple Distribution Models, Global Numerical Optimization ## ACM Reference format: Yong Li, Qiang Yang, Xu-Dong Gao, Zhen-Yu Lu and Jun Zhang. 2022. A Layered Learning Estimation of Distribution Algorithm. In Proceedings of Genetic and Evolutionary Computation Conference Companion (GECCO '22 Companion), July 9-13, 2022, Boston, MA, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3520304.3528904 [^0] ## 1 INTRODUCTION Estimation of distribution algorithms (EDAs) [1-4] have been regarded as a special branch of evolutionary algorithms (EAs) since it was proposed in 2001 [5]. Different from traditional EAs, like particle swarm optimization (PSO) [6-8], which generate offspring based on individuals, EDAs generate new solutions via a specific probability distribution model, which is estimated based on highquality solutions in the current population.",
      "question_type": "application",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:38",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Estimation of Distribution Algorithm (EDA) mentioned in comparison to the DABC and hDDE algorithms for the permutation flow shop problem (PFSP), what performance characteristics are described in relation to the Taillard's benchmark suite?",
      "topics": [
        "search"
      ],
      "answer": "The Estimation of Distribution Algorithm (EDA) is noted to have provided 44 out of the 90 best-known solutions recently, which were further improved by the DABC and hDDE algorithms with short-term searches on Taillard's benchmark suite. The DABC and hDDE algorithms are described as highly competitive with, if not better than, the best performing estimation distribution algorithms that have appeared recently in the existing literature.",
      "chunk_id": 24254,
      "paper_id": "A discrete artificial bee colony algorithm for the total flowtime minimization in permutation flow shops",
      "paper_title": "A discrete artificial bee colony algorithm for the total flowtime minimization in permutation flow shops",
      "paper_year": "2011",
      "chunk_content": "6. Conclusions In this paper, we considered the applications of the DABC and hDDE algorithms to the PFSP under the TFT criterion. The DABC algorithm is hybridized with a variant of iterated greedy algorithms employing a local search procedure based on insertion and swap neighborhood structures. In addition, we also presented a hybrid version of our previous discrete differential evolution algorithm employing the same local search procedure. To the best of our knowledge, our proposal is the first application of DABC to the PFSP with the TFT criterion. The performances of the proposed algorithms were tested by using Taillard's benchmark suite that is commonly used in the scheduling literature. The proposed algorithms were superior to the traditional IG_RS algorithm, and it has been shown that the performances of the DABC and hDDE algorithms are highly competitive with (if not better than) the best performing estimation distribution and genetic local search algorithms that have appeared recently in the existing literature. Ultimately, 44 out of the 90 best-known solutions provided recently by the EDA, tsGLS, and hGLS algorithms are further improved by the DABC and hDDE algorithms with short-term searches. With longterm searches, the new best-known solutions are reported for all problems in the benchmark suite of Taillard.",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:39",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "In the described Particle Swarm Optimization (PSO) algorithm, how are the current positions of particles, denoted as $\\boldsymbol{X}_{i}^{\\prime}$, updated in relation to their previous positions, $\\boldsymbol{X}_{i}$, and what other factors influence this update?",
      "topics": [
        "search"
      ],
      "answer": "The current position, $\\boldsymbol{X}_{i}^{\\prime}$, of particle $i$ in the PSO algorithm is updated by adding the current velocity, $\\boldsymbol{V}_{i}^{\\prime}$, to its previous position, $\\boldsymbol{X}_{i}$, according to the equation $\\boldsymbol{X}_{i}^{\\prime}=\\boldsymbol{X}_{i}+\\boldsymbol{V}_{i}^{\\prime}$. The current velocity, $\\boldsymbol{V}_{i}^{\\prime}$, is influenced by the previous velocity, $\\boldsymbol{V}_{i}$, an inertia weight $\\omega$, acceleration constants $\\eta_{1}$ and $\\eta_{2}$, random numbers $r_{1}$ and $r_{2}$, the best-so-far position of particle $i$, $\\boldsymbol{P}_{i}$, and the best position found in the whole swarm so far, $\\boldsymbol{P}_{g}$.",
      "chunk_id": 11605,
      "paper_id": "An Island Based Hybrid Evolutionary Algorithm for Optimization",
      "paper_title": "An Island Based Hybrid Evolutionary Algorithm for Optimization",
      "paper_year": "2008",
      "chunk_content": "PSO is motivated from the social behavior of organisms, such as bird flocking and fish schooling. Particles \"fly\" through the search space by following the previous best positions of their neighbors and their own previous best positions. There are several main versions of the PSO algorithms. The following version modified by Shi and Eberhart [10] is used in this paper. Each particle is represented by a position and a velocity, which are updated as follows: $$\n\\begin{gathered}\n\\boldsymbol{V}_{i}^{\\prime}=\\omega \\boldsymbol{V}_{i}+\\eta_{1} r_{1}\\left(\\boldsymbol{P}_{i}-\\boldsymbol{X}_{i}\\right)+\\eta_{2} r_{2}\\left(\\boldsymbol{P}_{g}-\\boldsymbol{X}_{i}\\right) \\\\\n\\boldsymbol{X}_{i}^{\\prime}=\\boldsymbol{X}_{i}+\\boldsymbol{V}_{i}^{\\prime}\n\\end{gathered}\n$$ where $\\boldsymbol{X}_{i}^{\\prime}$ and $\\boldsymbol{X}$ represent the current and previous positions of particle $i, \\boldsymbol{V}_{i}$ and $\\boldsymbol{V}_{i}^{\\prime}$ are the previous and current velocity of particle $i, \\boldsymbol{P}_{i}$ and $\\boldsymbol{P}_{g}$ are the best-so-far position of particle $i$ and the best position found in the whole swarm so far respectively. $\\omega \\in(0,1]$ is an inertia weight which determines how much the previous velocity is preserved, $\\eta_{1}$ and $\\eta_{2}$ are acceleration constants, and $r_{1}$ and $r_{2}$ are random numbers generated from the interval $[0.0,1.0]$. The framework of the PSO algorithm is given as follows: Step 1: Generate the initial particles by randomly generating the position and velocity for each particle.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:41",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the UMDA and Tree-EDA as described, what specific programming considerations are necessary when utilizing the Matlab Bayes Net (BNT) toolbox for the learning and sampling steps?",
      "topics": [
        "optimization"
      ],
      "answer": "When implementing the UMDA and Tree-EDA using the Matlab Bayes Net (BNT) toolbox, the developer must configure the Bayesian network's scoring metric, specifically using the Bayesian metric with uniform priors. Furthermore, the implementation requires setting a constraint on the maximum number of parents for each node in the network, limiting it to 5. The truncation parameter, denoted as T, should be set to 0.5. Finally, the implementation should incorporate best elitism, ensuring the selected population is directly passed to the subsequent generation.",
      "chunk_id": 15264,
      "paper_id": "Using Probabilistic Dependencies Improves the Search of Conductance-Based Compartmental Neuron Models",
      "paper_title": "Using Probabilistic Dependencies Improves the Search of Conductance-Based Compartmental Neuron Models",
      "paper_year": "2010",
      "chunk_content": "The algorithm was implemented in Matlab using the MATEDA-2.0 software [15. The implementations of the UMDA and Tree-EDA follow the same scheme but the learning and sampling steps are modified accordingly. These were implemented using the Matlab Bayes Net (BNT) toolbox 11. The scoring metric used for the Bayesian network was the Bayesian metric with uniform priors, and each node was allowed to have a maximum number of 5 parents. The truncation parameter was $T=0.5$. Best elitism, in which the selected population is passed to the next population, was used.  # 6 Related Work There is considerable work on the application of optimization methods to neuron model parameter optimization.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:43",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What benchmarking approach is outlined to assess the EDA when classifying SRBCTs?",
      "topics": [
        "algorithm"
      ],
      "answer": "The benchmarking approach involves using the SRBCTs dataset, dividing it into a training set of 63 samples and a test set of 25 samples. The 2308 genes in the training data are preprocessed using BSS/WSS before being used by the KNN classifier module.",
      "chunk_id": 11450,
      "paper_id": "Optimal Genes Selection with a New Multi-objective Evolutional Algorithm Hybriding NSGA-II with EDA",
      "paper_title": "Optimal genes selection with a new multi-objective evolutional algorithm hybriding NSGA-II with EDA",
      "paper_year": "2008",
      "chunk_content": "4. Experiment We code our method with VC++ 6.0 and Matlab and evaluate it on the dataset of SRBCTs (the small, round blue-cell tumors) published by Khan in [10]. The SRBCTs of childhood, which includes four sample types of neuroblastoma (NB), rhabdomyosarcoma (RMS), non-Hodgkin lymphoma (NHL) and the Ewing family of tumors (EWS), are so named because of their similar appearance on routine histology. The dataset is a collection of 2308 genes whose red intensity more than 20 in 88 samples. For the classifier module, we use KNN, which has such advantages that easy realization, high running speed and being suitable for datasets with multi-modal classes. The same with khan's work, use 63 samples as the training set, and 25 samples as the test set. 2308 genes in the training data are first preprocessed by BSS/WSS. For some top genes may repeatedly appear in some combination situations, the number of the final gene pool is usually less than $\\mathrm{S}^{*} 2^{\\left(\\mathrm{S}^{-}\\right)}$. Here we donate the top genes number S as 30 and Q as 50.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:45",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of utilizing Rational Entropy (RE) instead of Shannon Entropy (SE) as a performance assessment criterion within Estimation of Distribution Algorithms (EDAs)?",
      "topics": [
        "performance"
      ],
      "answer": "The primary objective of using Rational Entropy (RE) is to overcome the limitations of Shannon Entropy (SE), which can produce negative or negative infinite values, thus failing to satisfy the 'consistency' property. RE, defined as \\(H_{R E}=-\\int \\gamma(x) \\log \\frac{\\gamma(x)}{1+\\gamma(x)} d x\\), maintains most properties of SE while ensuring consistency, making it a more reliable measure of uncertainty in the process output for performance index calculation within EDAs. It reflects the uncertainty of random variables, unlike relative entropy, which measures the distance between two probability distributions.",
      "chunk_id": 21215,
      "paper_id": "Non-Gaussian Systems Control Performance Assessment Based on Rational Entropy",
      "paper_title": "Non-Gaussian Systems Control Performance Assessment Based on Rational Entropy",
      "paper_year": "2018",
      "chunk_content": "Therefore, the identification of distribution function is an indispensable step and it will be illustrated in subsequent sections. In the previous studies, Shannon Entropy is one of performance assessment criteria based on minimum entropy control [28]. It is defined as, $$\nH=-\\int \\gamma(x) \\ln \\gamma(x) d x, x \\in R\n$$ It seems to be a new benchmark to describe. But the Shannon entropy of the continuous random variable may be negative or even negative infinite, this means that the SE does not satisfy the \"consistency\" property. As a result, its uncertainty determines that it cannot be used as a new standard. Fortunately, a rational entropy (RE) instead of the SE is proposed by Zhou [24]. This type of entropy exhibits most properties of the Shannon's entropy and, at the same time, satisfies the \"consistency\" property. In this paper, we use the rational entropy of the process output with MEC and the actual output to calculate performance index. Let $x$ be a random variable in $R$ and $\\gamma(x)$ be its PDF, the rational entropy (RE) is given as [24] $$\nH_{R E}=-\\int \\gamma(x) \\log \\frac{\\gamma(x)}{1+\\gamma(x)} d x\n$$ Although the expression of RE is similar to that of the relative entropy, RE and relative entropy have different meanings: RE reflects the uncertainty of random variables and relative entropy reflects the distance between two probability distributions.",
      "question_type": "definition",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:46",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of constructing a dominating tree using the MDT heuristic differ from the H_DT heuristic of Sundar and Singh (2013), specifically in terms of the cost of the resulting tree?",
      "topics": [
        "heuristic"
      ],
      "answer": "The MDT heuristic constructs a dominating tree, represented by [FORMULA249], and the provided text indicates that MDT can produce a dominating tree with a lower cost compared to the H_DT heuristic of Sundar and Singh (2013). Specifically, the example shows that MDT returns a dominating tree with a cost of 24, while H_DT returns one with a cost of 36 on the same graph. After computing a minimum spanning tree (MST) on the set of nodes in [FORMULA250] to reconnect these nodes, and applying a pruning operator (similar to that of Sundar and Singh (2013)) to remove redundant nodes and edges, the final dominating tree [FORMULA254] has a cost of 22. The pruning operator removes node 7 and edge $e_{11,7}$.",
      "chunk_id": 24828,
      "paper_id": "A hybrid heuristic for dominating tree problem",
      "paper_title": "A hybrid heuristic for dominating tree problem",
      "paper_year": "2016",
      "chunk_content": "At this point MDT stops as no WHITE node remained, and, the construction of the dominating tree [FORMULA249] is complete. This dominating tree has cost 24 (Fig. 3e). Comparing Fig. 2e with Fig. 3e, we can see that the cost of the dominating tree returned by the heuristic HDT is 36 , whereas the MDT returns a dominating tree with cost 24 on the same graph. Thus, we can say that our heuristic MDT can perform better than the heuristic H_DT of Sundar and Singh (2013). Hereafter, we compute a minimum spanning tree (MST) on the set of nodes in [FORMULA250] to reconnect these nodes via this MST in a bid to reduce the cost of DT further. As the nodes of [FORMULA251] are already connected via a MST so cost of [FORMULA252] remains the same. Next the pruning operator is applied to remove the redundant nodes as well as redundant edges which removes the node 7 and the edge $e_{11,7}$ leading to the final [FORMULA254] with cost 22 as shown in Fig. 3f. ### 3.2 Pruning operator Our Pruning operator is similar to the pruning procedure of Sundar and Singh (2013).",
      "question_type": "comparison",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:48",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners approach probabilistic modelling when considering the relationships between variables?",
      "topics": [
        "optimization"
      ],
      "answer": "When implementing an EDA, probabilistic modelling techniques can be classified into univariate, bivariate, and multivariate methods. Univariate modelling methods are simple and easy to implement but do not fully utilize the linkage information of the decision variables. Therefore, practitioners should consider bivariate or multivariate methods to better guide the evolutionary process if the linkage information is deemed important for solving the problem.",
      "chunk_id": 22297,
      "paper_id": "Restricted Boltzmann Machine-Assisted Estimation of Distribution Algorithm for Complex Problems",
      "paper_title": "Restricted Boltzmann Machine-Assisted Estimation of Distribution Algorithm for Complex Problems",
      "paper_year": "2018",
      "chunk_content": "2. Related Work 2.1. EDA. EDA [29-31] is a stochastic optimization algorithm based on statistical theory, which establishes the probability model from the macro perspective and describes the distributed information of the candidate solutions in the search space. Then, it predicts the promising region by statistical learning and produces new individuals by random sampling of the probability model. Meanwhile, the probability model is gradually updated with the increasing information on the better solutions. This process continues to realize the evolutionary progress and achieve excellent solutions until the termination conditions are met. Compared with GA methods, EDA makes full use of the global information of the solution space and the historical information of the promising region to explore and exploit better solutions in the evolutionary process, which effectively and efficiently improves the searching ability to solve the nonlinear optimization and the variable coupling problems. It has become a hot topic and has been successfully applied in many engineering fields. EDA forms an effective parallel search framework based on building and sampling the probability model. According to the structure of the probability model and the relationship between the variables, probabilistic modelling techniques can be classified into univariate, bivariate, and multivariate methods. Univariate modelling methods are simple and easy to implement but do not fully utilize the linkage information of the decision variables to guide the evolutionary process.",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:50",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the MARLEDA, what mechanism within the fitness function encourages the reduction of 'ActiveRules' and 'ActiveVars', and what are the observed effects on these ratios as the algorithm progresses?",
      "topics": [
        "fitness"
      ],
      "answer": "The MARLEDA's fitness function incorporates a weight factor of 0.01 assigned to the 'ActiveRules' term, and a weighting factor applied to the compatibility score of each rule in the classification phase. These factors influence the reduction of classifier complexity. As the MARLEDA progresses from the 25th to the 4000th generation, while increasing average training Accuracy by approximately 18 percentage points, it simultaneously reduced 'ActiveRules' by 20 percentage points and 'ActiveVars' by approximately two percentage points.",
      "chunk_id": 10498,
      "paper_id": "Detection of Financial Statement Fraud Using Evolutionary Algorithms",
      "paper_title": "Detection of Financial Statement Fraud Using Evolutionary Algorithms",
      "paper_year": "2012",
      "chunk_content": "Insert Table 3 Here Figure 7 shows the progression of Accuracy while Figure 8 reveals the progression of the ActiveRules and ActiveVars ratios, as the GA and MARLEDA both attempt to extract as much accuracy as possible from a small set of active rules and variables. While increasing the average training Accuracy (over the ten most-accurate training FRBCs, one from each validation run) by approximately 18 percentage points from the $25^{\\text {th }}$ generation to the $4000^{\\text {th }}$ generation, MARLEDA also simultaneously reduced ActiveRules by 20 percentage points and ActiveVars by approximately two percentage point during the same time period. The GA exhibited a similar phenomenon, but was able to reduce the average ActiveRules ratio even further to 63.5 percent, or a decrease of 29 percentage points. These reductions in classifier complexity were largely influenced by both the 0.01 weight factor that was assigned to the ActiveRules term in the fitness function and the weighting factor that was applied to the compatibility score of each rule in the classification phase. Since the weight of the Accuracy term is set at a disproportionately high value of 0.99 in the fitness function, the evolutionary processes naturally favor more-accurate classifiers and, thus, a point is reached at which fewer rules can be deactivated without sacrificing significant classification accuracy.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:51",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "According to the details in the chunk, what advantages does the Firefly Algorithm have over Differential Evolution, Particle Swarm Optimization, and Variable Neighborhood Search in the context of the 'Competition on Evolutionary Computation in the Energy Domain: Smart Grid Applications'?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The chunk indicates that algorithms developed in the 'Competition on Evolutionary Computation in the Energy Domain: Smart Grid Applications' are based on the Firefly algorithm, Differential Evolution, Particle Swarm Optimization, and Variable Neighborhood Search, but it does not specify any advantages or disadvantages of the Firefly Algorithm compared to the other algorithms. It only mentions that the Firefly Algorithm is inspired by the flashing light of fireflies.",
      "chunk_id": 19192,
      "paper_id": "Cellular estimation of distribution algorithm designed to solve the energy resource management problem under uncertainty",
      "paper_title": "Cellular estimation of distribution algorithm designed to solve the energy resource management problem under uncertainty",
      "paper_year": "2021",
      "chunk_content": "Besides, an introduction to the EDAs, the class to which the proposed algorithm belongs, is provided. ### 4.1. EAs used to solve the ERM problem under uncertainty Despite the excellent performance of some of the EAs used to solve the ERM problem under uncertainty, not all of them and their versions have been published, which limits the available information and details on their implementation. Nevertheless, the implementation of the algorithms are publicly available as part of the series \"Competition on Evolutionary Computation in the Energy Domain: Smart Grid Applications\" (Lezama et al., 2018c, 2019a). In general, the developed algorithms in this competition are based on Firefly algorithm (Wang and Song, 2019; Yang, 2009), Variable Neighborhood Search (Mladenovic and Hansen, 1997; Qiu et al., 2018), Differential Evolution (Ali et al., 2019; Storn and Price, 1997), Particle Swarm Optimization (Kennedy and Eberhart, 1995; Kohler et al., 2019), and hybrid versions of them. Firefly algorithm (Wang and Song, 2019; Yang, 2009) is inspirited by the flashing light of fireflies.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:53",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "In the described algorithm, what role does the spread parameter $\\theta$ of Mallow's model play, and how should a developer consider this parameter when implementing the Estimation of Distribution Algorithm (EDA)?",
      "topics": [
        "control"
      ],
      "answer": "The spread parameter $\\theta$ in Mallow's model is crucial for controlling the search performance of the EDA. As the value of $\\theta$ increases, the probability concentrates on the central permutation. Therefore, a developer should carefully tune $\\theta$ to balance exploration and exploitation during the search process within the EDA.",
      "chunk_id": 1284,
      "paper_id": "An online method for the real-time aircraft arrival sequencing and scheduling problem",
      "paper_title": "An Online Method for the Real-time Aircraft Arrival Sequencing and Scheduling Problem",
      "paper_year": "2014",
      "chunk_content": "Then return to Step 1). Otherwise, skip this step and return to Step 3); 6) If the simulation ends, output the best solution in pop. ## IV. SimULATION ReSults In this part, the online algorithm is applied to solve the ASS problem. In addition, it is also compared with the results obtained by the static case in which all the information of the landing aircrafts is known in advance. Four kinds of aircrafts are considered as shown in Table 1. All the aircrafts will arrive at the airport at their PLTs and an optimal landing sequence is needed to be achieved. An experiment result which solves the ASS problem of 30 aircrafts in both static case and real-time scenario is represented in Table 2. Where, \"No.\" and \"Cat.\" stand for the flight number and category of the aircraft separately. The simulation is implemented in MATLABAT 7.1 on a PC running an Intel Core i7 CPU at 1.60 GHz with $4.0-\\mathrm{GB}$ random access memory. In our algorithm, the spread parameter $\\theta$ of Mallow's model is definitely the key to control the search performance. As the value of [FORMULA39] increases, the probability tends to concentrate on the central permutation [FORMULA40].",
      "question_type": "implementation focused",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:02:54",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of subspace Estimation of Distribution Algorithms (EDAs), what is the Random Picking (RP) method, and how does it determine which variables are selected and perturbed?",
      "topics": [
        "representation"
      ],
      "answer": "The Random Picking (RP) method is a stochastic scheme used to determine which variables will be perturbed at each generation in subspace EDAs. In RP, a float number is randomly generated from the interval $[0,1]$. If this generated value is greater than 0.5, the variable is selected and perturbed; otherwise, the variable is neglected. The initial population is obtained by sampling a Bernoulli distribution with a parameter value of 0.5.",
      "chunk_id": 23161,
      "paper_id": "Subspace estimation of distribution algorithms- To perturb part of all variables in estimation of distribution algorithms",
      "paper_title": "Subspace estimation of distribution algorithms: To perturb part of all variables in estimation of distribution algorithms",
      "paper_year": "2011",
      "chunk_content": "4.1. To determine which variables will be perturbed at each generation As reviewed in Section 3, parts of the variables have been developed for EDAs. To find the determined variables is the key theme in the process of subEDAs. Therefore, one major step of subEDAs is to determine which variables will be perturbed at each generation. In this paper, the following three simple schemes are described into details: the random picking method (RP), the majority voting based on the similarity between high quality individuals (MVSH) and the majority voting based on the difference between high quality and low quality individuals (MVDHL). In this paper, we first investigate a explicit stochastic scheme, named as RP. Since the initial population is usually obtained at random by sampling a Bernouilli distribution with a parameter value equal to 0.5 in EDAs, the evolved variables can be taken as the natural representation of the current information. The full steps of RP are given in Algorithm 2, where a float number is randomly generated from $[0,1]$ and its value is compared with 0.5 ; if its value is bigger than 0.5 , then the variable is selected and perturbed, otherwise the variable is neglected.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:02:56",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of Pareto front approximation in the context of PEDAI, PEDAA, PEDAN, PEDA_S, and PEDA, as evaluated by the unary epsilon indicator?",
      "topics": [
        "optimization"
      ],
      "answer": "The key takeaway is that the unary epsilon indicator, calculated as $f_{j}^{\\prime}=\\left(f_{j}-f_{j}^{\\min }\\right) /\\left(f_{i}^{\\max }-f_{i}^{\\min }\\right)+1$, assesses the proximity of the approximated Pareto front (A) generated by PEDAI, PEDAA, PEDAN, PEDA_S, and PEDA to a reference Pareto front (B), where values closer to 1 indicate higher proximity and values closer to 2 suggest greater distance, thereby quantifying the algorithms' performance in approximating the true Pareto front.",
      "chunk_id": 17517,
      "paper_id": "A Pareto-Based Estimation of Distribution Algorithm for Solving Multiobjective Distributed No-Wait Flow-Shop Scheduling Problem With Sequence-Dependent Setup Time",
      "paper_title": "A Pareto-Based Estimation of Distribution Algorithm for Solving Multiobjective Distributed No-Wait Flow-Shop Scheduling Problem With Sequence-Dependent Setup Time",
      "paper_year": "2019",
      "chunk_content": "[IMAGE9] Fig. 11. Mean interaction plot of hypervolume and unary epsilon between algorithms and running time. [IMAGE10] Fig. 12. Mean interaction plot of hypervolume and unary epsilon between algorithms and jobs. [IMAGE11] Fig. 13. Mean interaction plot of hypervolume and unary epsilon between algorithms and factories. obtained Pareto front and the referenced Pareto front. As [49], because the optimal Pareto front for each instance is not known, so we collect the whole of nondominated solutions generated by all considered algorithms as a reference set. Each objective of solutions should be also normalized to one unit by the following equation: $f_{j}^{\\prime}=\\left(f_{j}-f_{j}^{\\min }\\right) /\\left(f_{i}^{\\max }-f_{i}^{\\min }\\right)+1$. TABLE IV Computational Result of PEDAI, PEDAA, PEDAN, PEDA_S, AND PEDA The values of [FORMULA337] are between 1 and 2 . The value close to 1 means the considered frontier is close to the reference set, and the value being close to 2 means that the set of solution is distant. Let $\\mathbf{B}$ be the reference set and $\\mathbf{A}$ be an approximation to the Pareto front.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:02:58",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of multi-pass turning operations optimization, what cost elements contribute to the total Unit Cost (UC), as adopted from the mathematical model proposed by Chen and Tsai?",
      "topics": [
        "optimization"
      ],
      "answer": "The Unit Cost (UC) for multi-pass turning operations, based on the model adopted from Chen and Tsai, is divided into four basic cost elements: Cutting cost by actual time in cut (\\(C_{M}\\)), Machine idle cost for loading and unloading operations and idling tool motion (\\(C_{I}\\)), Tool replacement cost (\\(C_{R}\\)), and Tool cost (\\(C_{T}\\)). The objective of the optimization is to determine the optimal machining parameters, including cutting speed, feed rate, and depth of cut for both rough and finish machining (\\(V_{r}, V_{s}, f_{r}, f_{s}, d_{r}, d_{s}\\)), to minimize UC.",
      "chunk_id": 13726,
      "paper_id": "Optimization of machining parameters using estimation of distribution algorithms",
      "paper_title": "Optimization of Machining Parameters Using Estimation of Distribution Algorithms",
      "paper_year": "2009",
      "chunk_content": "Simulation results and discussions are given in section 4. In the last section, we conclude this work. [^0] ## II. MATHEMATICAL MODEL OF MULTI-PASS TURNING OPERATIONS In this paper, the multi-pass turning operations include multiple rough cuts and a single finish cut. The mathematical model proposed by Chen and Tsai $[6,9]$ was adopted for optimizing machining parameters. The objective of the optimization was to determine the optimal machining parameters, including cutting speed, feed rate and depth of cut for both rough and finish machining (namely $V_{r}, V_{s}, f_{r}, f_{s}, d_{r}, d_{s}$ ), in order to minimize UC. The UC for multi-pass turning operations can be divided into four basic cost elements. (1) Cutting cost by actual time in cut $\\left(C_{M}\\right)$. (2) Machine idle cost for loading and unloading operations and idling tool motion $\\left(C_{I}\\right)$. (3) Tool replacement cost $\\left(C_{R}\\right)$. (4) Tool $\\operatorname{cost}\\left(C_{T}\\right)$.",
      "question_type": "conceptual deep",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:00",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm that utilizes a mask vector, how should practitioners update the probabilistic model P(x) based on the sets M and \\(\\bar{M}\\)?",
      "topics": [
        "algorithm"
      ],
      "answer": "When updating the probabilistic model P(x) in an Estimation of Distribution Algorithm using a mask vector, the update strategy depends on whether a data point \\(x_i\\) belongs to the set \\(\\bar{M}\\). Specifically, the probabilistic model P is updated as follows:  \\(P=\\left\\{\\begin{array}{c} P{i}(k) \\\\ P{i}(k-1) \\\\ \\boldsymbol{x}{i} \\in \\overline{\\mathbf{M}}\\end{array}\\right.\\), where \\(P_i(k)\\) represents the updated probabilistic model at iteration k, and \\(P_i(k-1)\\) represents the model at the previous iteration. If \\(x_i\\) is in \\(\\bar{M}\\), the probabilistic model is updated based on \\(P_i(k-1)\\).",
      "chunk_id": 19590,
      "paper_id": "Random mask-based estimation of the distribution algorithm for stacked auto-encoder one-step pre-training",
      "paper_title": "Random mask-based estimation of the distribution algorithm for stacked auto-encoder one-step pre-training",
      "paper_year": "2021",
      "chunk_content": "Mask vector: Generate random vector randomly, and define \\(M\\) and \\(\\bar{M}\\) set 3. Probabilistic model updating: Update probabilistic model \\(P(x)\\) by different strategies according to \\(M\\) and \\(\\bar{M}\\) set \\(P=\\left\\{\\begin{array}{c} P{i}(k) \\\\ P{i}(k-1) \\\\ \\boldsymbol{x}{i} \\in \\overline{\\mathbf{M}} \\end{array}\\right.\\) 4. New population generation: Generate new population by different strategies  Algorithm 3: Improved estimation of distribution algorithm \\(z{i}^{\\alpha}(\\boldsymbol{k})=\\left\\{\\begin{array}{c}\\operatorname{Sum}\\left(\\boldsymbol{P}{i}^{\\alpha}(\\boldsymbol{k})\\right) \\\\ \\boldsymbol{p}{i} \\in \\mathbf{M} \\\\ \\boldsymbol{x}{i} \\in \\overline{\\mathbf{M}}\\end{array}\\right.\\) 5.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:02",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding individual-based metaheuristics applied to solve the Graph Coloring Problems (GCPs), how do evaluation functions, as incorporated by Porumbel et al., improve the performance of Tabu Search (TS)?",
      "topics": [
        "search"
      ],
      "answer": "Porumbel et al. improved the performance of Tabu Search (TS) by incorporating evaluation functions that use structural or dynamic information in addition to the number of conflicting edges. This enhancement aims to provide a more informed search process within the TS framework for solving Graph Coloring Problems (GCPs).",
      "chunk_id": 6626,
      "paper_id": "A distribution evolutionary algorithm for the graph coloring problem",
      "paper_title": "A distribution evolutionary algorithm for the graph coloring problem",
      "paper_year": "2023",
      "chunk_content": "Rest of this paper is organized as follows. Section 2 presents a brief review on related works. The proposed distribution model is presented in Section 3, and Section 4 elaborates details of DEA-PPM. Section 5 investigates the influence of parameter and the distribution evolution strategies, and the competitiveness of DEA-PPM is verified by numerical experiments. Finally, we summarize the work in Section 6. ## 2. Literature review ### 2.1. Individual-based metaheuristics for GCPs Besides the simulated annealing [9,10] and the variable neighborhood search [11], the tabu search (TS) is one of the most popular individual-based metaheuristics applied to solve the GCPs [12]. Porumbel et al. [13] improved the performance of TS by evaluation functions that incorporates the structural or dynamic information in addition to the number of conflicting edges. Blochliger and Zufferey [14] proposed a TS-based constructive strategy, which constructs feasible but partial solutions and gradually increases its size to get the optimal color assignment of a GCP. Hypothesizing that high quality solutions of GCPs could be grouped in clusters within spheres of a specific diameter, Porumbel et al.",
      "question_type": "definition",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:03",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "Based on the information provided, is there any comparison of different Estimation of Distribution Algorithms (EDAs) or their performance characteristics?",
      "topics": [
        "optimization"
      ],
      "answer": "The provided text does not contain any information about different Estimation of Distribution Algorithms (EDAs) or a comparison of their performance characteristics. The text focuses on OFDM systems and pilot tone selection.",
      "chunk_id": 8912,
      "paper_id": "Pilot Pattern Optimization for Sparse Channel Estimation in OFDM Systems",
      "paper_title": "Pilot Pattern Optimization for Sparse Channel Estimation in OFDM Systems",
      "paper_year": "2015",
      "chunk_content": "R. China. The associate editor coordinating the review of this paper and approving it for publication was K. J. Kim. H. Wang is with the School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin 150080, China, and also with the Institute of Communications Engineering, PLA University of Science and Technology, Nanjing 210007, China (e-mail: wanghanemail@163.com). Q. Guo is with the School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin 150080, China (e-mail: qguo@hit. edu.cn). G. Zhang and G. Li are with the Institute of Communications Engineering, PLA University of Science and Technology, Nanjing 210007, China (e-mail: gengxinzhang@126.com; guangxia@tom.com). W. Xiang is with the School of Mechanical and Electrical Engineering, University of Southern Queensland, Toowoomba, QLD 4350, Australia (e-mail: wei.xiang@usq.edu.au). ## II. System Model and Problem Formulation We assume that an OFDM system has $N$ subcarriers, among which $P(P \\leq N)$ subcarriers indexed by $\\mathcal{K}=\\left\\{k_{1}, \\ldots, k_{P}\\right\\}(0 \\leq$ $k_{1}<\\cdots<k_{P} \\leq N-1$ ) are selected as pilot tones.",
      "question_type": "comparison",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:04",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the feature subset selection approach described, what data structures are required to construct and process the difference matrix (DM)?",
      "topics": [
        "selection",
        "data structures"
      ],
      "answer": "The implementation requires a data structure to represent the difference matrix (DM). This DM is constructed by comparing pairs of features from different classes. The matrix entries are binary, with '1' indicating a difference exists between objects from different classes based on the feature pair, and '0' indicating no difference. Therefore, a two-dimensional array or matrix data structure, such as a list of lists in Python or a NumPy array, is necessary to store the DM. The size of the DM depends on the number of features and classes, potentially becoming very large when many differences between members of different classes are identified.",
      "chunk_id": 14174,
      "paper_id": "Hybridization of Evolutionary Mechanisms for Feature Subset Selection in Unsupervised Learning",
      "paper_title": "Hybridization of Evolutionary Mechanisms for Feature Subset Selection in Unsupervised Learning",
      "paper_year": "2009",
      "chunk_content": "Then it was extended to variables selection on geological problems [1]. The pioneer work in the use of typical testors for feature subset selection was from Dmitriev Zhuravlev et al. [6]. This research presents two mechanisms that were applied to both: supervised and unsupervised learning. A testor is a subset of features that distinguishes objects from different classes. Satiesteban and Pons say that a typical testor is the testor which we cannot eliminate any of its features without changing its condition of testor [26]. We could say it in other words: a typical testor is a testor whose redundancy has been eliminated. Let's suppose that $U$ is a collection of objects, and these objects are described by a set of $n$ features; also let's suppose that the objects are grouped into $k$ classes. By comparing each pair of features that belong to different classes, using any criterion, we can obtain the difference matrix DM that is made capturing all the differences between objects from different classes; this difference is coded with 1 if difference exists and 0 if difference does not exist. This DM can be very large when we identify many differences between members of a class with regard to others.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:06",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the performance of the HPPCM algorithm compared to other algorithms on the ZJZ test suite (F5-F9)?",
      "topics": [
        "objectives"
      ],
      "answer": "The MIGD (Mean Inverted Generational Distance) and MHVD (Mean Hypervolume Difference) metrics are used to assess the performance of the HPPCM algorithm on the ZJZ test suite (F5-F9). HPPCM demonstrates competitive performance by achieving better MIGD and MHVD values compared to other algorithms, irrespective of the value of [FORMULA340].",
      "chunk_id": 3471,
      "paper_id": "Combining a hybrid prediction strategy and a mutation strategy for dynamic multiobjective optimization",
      "paper_title": "Combining a hybrid prediction strategy and a mutation strategy for dynamic multiobjective optimization",
      "paper_year": "2022",
      "chunk_content": "In addition, HPPCM is better at tracking the POS (or POF) with major changes. On the FDA4 issue of the three objectives, the statistics of each algorithm running 20 times are relatively poor. ### 5.2. Results on ZJZs It is shown by Tables 4 and 5 that HPPCM is the most competitive one on the ZJZ test suite (F5-F9) compared to other well/known algorithms because it almost always obtains the best MIGD and MHVD no matter what the value of [FORMULA340] is large or small. It indicates that the POF obtained [IMAGE4] Fig. 5. Evolution curve of MIGD value of six algorithms on representative test cases for time period [FORMULA341] from 0 to 30 , with $\\left(n_{t}, \\tau_{t}\\right)=(10,30)$. [IMAGE5] Fig. 6. Scatter plots of the population obtained by HPPCM and the other peer algorithms (mean MIGD value) on dMOP2 for time period [FORMULA343] from 0 to 30 . The green line is POF in different environments.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:07",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "Based on the text, what is the main conclusion regarding the performance of the HEDA model compared to the GA model in optimizing network instances?",
      "topics": [
        "fitness"
      ],
      "answer": "The Histogram Estimation of Distribution Algorithm (HEDA) consistently finds better optima and more stable solutions than the Genetic Algorithm (GA) across various network instances. Specifically, HEDA achieves smaller objective function values, even as network size and O-D pair demands increase, indicating superior performance in travel time cost minimization compared to GA.",
      "chunk_id": 16830,
      "paper_id": "A Histogram Estimation of Distribution Algorithm for Reversible Lanes Optimization Problems",
      "paper_title": "A Histogram Estimation of Distribution Algorithm for Reversible Lanes Optimization Problems",
      "paper_year": "2019",
      "chunk_content": "It should be noticed that all the test network instances are generated manually, thus we have no idea about the optimal solutions to these problems. TABLE V Optimal Fitness and Wilcoxon Rank-Out Results in 10 Instances of HEDA and GA First, observing Table V, we can find that in all instances the HEDA could find better optima than GA. The results shown in Rank Sum entry indicate these two different algorithms have significant difference in all instances. Besides, the solutions gained by HEDA are more stable than use GA, since from these tables we could find that all the optima intervals of HEDA are smaller compare to GA. Second, from these tables, we can also observe that when the O-D pair demands are same, with the increase of the network size, the travel time cost of both HEDA and GA are getting larger. This is because in a complex road network, the path from origin to destination is long and tortuous. When the traffic capacity of each section is exact same, the travel time is proportional to the length of path. Even under this condition, HEDA could obtain a smaller objective function value. Third, in the same network, with the increase of O-D pair demands, the fitness value of HEDA and GA are getting worse.",
      "question_type": "synthesis summary",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:09",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of RM-MEDA (Regularity Model Based Multi-objective Estimation of Distribution Algorithm) utilize the probability distribution of individuals to improve convergence performance, and what specific modification is made to the traditional RM-MEDA to achieve this?",
      "topics": [
        "optimization",
        "Estimation of Distribution Algorithms",
        "RM-MEDA"
      ],
      "answer": "RM-MEDA, a representative Estimation of Distribution Algorithm, learns and samples the probability distribution of the best individuals. To enhance convergence, a modification is introduced: instead of using the entire parent population, only a subset with better performance is used to establish a more accurate manifold model. This 'RM-MEDA based on elitist strategy' aims to improve convergence and algorithm runtime compared to the original RM-MEDA.",
      "chunk_id": 15658,
      "paper_id": "The RM-MEDA Based on Elitist Strategy",
      "paper_title": "The RM-MEDA Based on Elitist Strategy",
      "paper_year": "2010",
      "chunk_content": "The RM-MEDA Based on Elitist Strategy Li Mo, Guangming Dai, and JianKai ZhuScholl of Computer, China University of Geosciences, Wuhan 430074, Chinamoliwh@gmail.com #### Abstract The Estimation of Distribution Algorithms(EDAs) is a new paradigm for Evolutionary Computation. This new class of algorithms generalizes Genetic Algorithms(GAs) by replacing the crossover and mutation operators by learning and sampling the probability distribution of the best individuals of the population at each iteration of the algorithm. In this paper, we review the EDAs for the solution of combinatorial optimization problems and optimization in continuous domains. The paper gives a brief overview of the multiobjective problems(MOP) and estimation of distribution algorithms(EDAs). We introduce a representative algorithm called RMMEDA (Regularity Model Based Multi-objective Estimation of Distribution Algorithm). In order to improve the convergence performance of the algorithm, we improve the traditional RM-MEDA. The improvement we make is using part of the parent population with better performance instead of the entire parent population to establish a more accurate manifold model, and the RM-MEDA based on elitist strategy theory is proposed. Experimental results show that the improved RMMEDA performs better on the convergence metric and the algorithm runtime than the original one.",
      "question_type": "conceptual deep",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:10",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to minimize the function f, as defined in equation (10), what configurations are required for the hybrid EDA regarding the hidden Markov model (HM) topology and learning factor?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "parameter selection",
        "hidden Markov model"
      ],
      "answer": "When implementing the hybrid EDA for the given optimization problem, the learning factor \\(\\epsilon\\) should be set to 0.05. The HM topology requires three layers: an input layer with 30 neurons, corresponding to the 10-bit binary coding of each of the three design variables \\(r_2\\), \\(h_2\\), and \\(d_2\\); a hidden layer with 50 neurons; and a second hidden layer with 20 neurons. Note that this topology was arbitrarily chosen.",
      "chunk_id": 13800,
      "paper_id": "Hybrid Estimation of Distribution Algorithm Using Local Function Approximations",
      "paper_title": "Hybrid Estimation of Distribution Algorithm Using Local Function Approximations",
      "paper_year": "2009",
      "chunk_content": "Fig. 2(a) shows the upper half model of this device. Table I gives the variable limits for the three design parameters $r_{2}, h_{2}$, and $d_{2}$, as well as some other relevant information to the problem. Mathematically, the optimization task is defined as in (10) $$\n\\begin{aligned}\n\\operatorname{minimize}: f & =\\sqrt{\\frac{\\sum_{i=1}^{21}\\left|B_{S}^{(i)}\\right|^{2}}{21}} \\\\\n\\text { subject to: } g & =B_{\\max }-4.92 \\leq 0 \\\\\nh & =\\frac{\\text { Energy }}{E_{\\text {ref }}}-1=0\n\\end{aligned}\n$$ where $E_{\\text {ref }}=180 \\mathrm{MJ}$ is the reference value for the stored energy, and $B_{\\max }$ is the maximum value of the magnetic field on the coils, which is involved in the maintenance of the superconducting state. The current density flowing through the coils is kept constant $\\left|J_{\\text {inner }}\\right|=\\left|J_{\\text {outer }}\\right|=22.5 \\mathrm{~A} / \\mathrm{mm}^{2}$, as are the dimensions and position of the inner coil. More detailed information about this problem can be found in the literature [11]. The hybrid EDA was set to solve this problem with the learning factor set to $\\epsilon=0.05$ and a three-layer topology for TABLE I PROBLEM SPECIFICATIONS the HM, similar to the one from Fig. 1. The input layer was composed of 30 neurons (corresponding to the coding of each variable by a 10-bit binary vector), and the hidden layers were set to 50 and 20 neurons, respectively. It is important to notice that this topology was arbitrarily chosen, and very likely does not represent the best compromise between representation capability and computational cost.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:12",
      "generation_style": "practical_application"
    },
    {
      "question": "How does the SMS-EDA-MEC algorithm, which combines the $\\boldsymbol{F}(\\cdot), \\mathcal{O} \\subseteq \\mathbb{R}^{M}$-metric selection mechanism with an augmented version of the EDA-MEC, address the issue of speeding up multi-objective optimization computation time?",
      "topics": [
        "heuristic"
      ],
      "answer": "The SMS-EDA-MEC algorithm speeds up multi-objective optimization computation time by relying on Clayton's copula within the EDA-MEC component. The EDA-MEC is based on Multivariate Extension of Copulas and the use of Clayton's copula helps to avoid loss of population diversity and promote search by priming precursor individuals. Furthermore, the earlier mentioned algorithms mentioned in the chunk, [11] and [12], also focus on Archimedean copulas for their ability to model dependence in high dimensions with only one parameter, which has the positive effect of speeding up multi-objective optimization computation time.",
      "chunk_id": 24504,
      "paper_id": "SMS-EDA-MEC Extending Copula-based EDAs to Multi-Objective Optimization",
      "paper_title": "SMS-EDA-MEC: Extending Copula-based EDAs to Multi-Objective Optimization",
      "paper_year": "2016",
      "chunk_content": "Similarly, [10] focused on objective space by proposing a first estimation of the Pareto front relying on Archimedean copulas. [11], [12] also focused on Archimedean copulas for their ability to model dependence in high dimensions with only one parameter, which has the positive effect of speeding up multi-objective optimization computation time. In this paper we propose a novel MOEDA algorithm: the $\\mathbb{R}^{n}$ Metric Selection Estimation of Distribution Algorithm based on Multivariate Extension of Copulas (SMS-EDA-MEC). SMS-EDA-MEC deals with the above issues by combining the successful $\\boldsymbol{F}(\\cdot), \\mathcal{O} \\subseteq \\mathbb{R}^{M}$-metric selection mechanism [13] with an augmented version of the (single-objective) Estimation of Distribution Algorithm based on Multivariate Extension of Copulas (EDA-MEC) [14] which relies on Clayton's copula [15] and is complemented with heuristics meant to avoid loss of population diversity and to promote search by priming precursor individuals. As part of the proposal we present a discussion on current MOEDA issues and drawbacks and how to overcome them. We formally described SMS-EDA-MEC and its components and a series of experiments on community-accepted benchmarks. These experiments are devised to study the influence of each of the improvements that are put forward as part of the proposal and to compare our approach with comparable state-of-the-art and 'classical' algorithms.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:14",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the global search capability of the Island based Hybrid Evolutionary Algorithm (IHEA) compare to the three original algorithms it is based on, according to the benchmark problem results?",
      "topics": [
        "search"
      ],
      "answer": "Based on the benchmark problems tested, the provided text indicates that the Island based Hybrid Evolutionary Algorithm (IHEA) exhibits a better global search capability than the three original algorithms. The IHEA reduces the probability of becoming trapped at a local optimum compared to these original algorithms.",
      "chunk_id": 11616,
      "paper_id": "An Island Based Hybrid Evolutionary Algorithm for Optimization",
      "paper_title": "An Island Based Hybrid Evolutionary Algorithm for Optimization",
      "paper_year": "2008",
      "chunk_content": "An information sharing mechanism is developed among the three islands. A set of benchmark problems were tested. From the result, it can be seen that IHEA greatly reduces the probability of becoming trapped at a local optimum. The global search capability of IHEA is better than the three original algorithms. In the future, we will focus on the further study of IHEA in deep level. There are some issues that should be considered, such as, the resources distribution of different islands, the migration topology and algorithms used on different islands. ## Acknowledgement This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) of UK under Grant EP/E060722/1.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:15",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the fEDA for the Gaussian Elimination application described, how should a developer structure the code to generate the benchmark set, specifically concerning the number of DAG scales and combinations of CCR and Loose parameters?",
      "topics": [
        "algorithm"
      ],
      "answer": "For the Gaussian Elimination application, the code should first generate 10 different scales of DAGs, where b ranges from 10 to 19. Then, for each of these 10 scales, the developer must implement the CCR and Loose combinations as defined in Table 5 (though the specifics of Table 5 are not detailed here). Finally, the code should generate $10 \\times 4 \\times 3 = 120$ instances as the benchmark set. The same approach applies to FFT application, but with 5 DAG scales (k=3-7) and $5 \\times 4 \\times 3 = 60$ instances produced.",
      "chunk_id": 19310,
      "paper_id": "An evolutionary fuzzy scheduler for multi-objective resource allocation in fog computing",
      "paper_title": "An evolutionary fuzzy scheduler for multi-objective resource allocation in fog computing",
      "paper_year": "2021",
      "chunk_content": "It is referred to [27] for more details. For Gauss Elimination application, 10 different scales of DAGs are produced ( $b=10-19$ ). For FFT application, 5 different scales of DAGs are produced ( $k=3-7$ ). For each scale of DAG, CCR and Loose combinations in Table 5 are used to generate different cases. In this way, $10 \\times 4 \\times 3=120$ and $5 \\times 4 \\times 3=60$ instances are produced as a benchmark set respectively. Here, [FORMULA228] values are calculated and presented as follows. The plots show how the average IGD value varies in accordance [FORMULA229], CCR and Loose. For Gaussian Elimination Application results presented in Fig. 11, it can be seen that fEDA performs much better on IGD [IMAGE_9] (b) 'IGD-Loose' histogram Fig. 11. Variable dependent IGD value plots of Gaussian Elimination Application. Table 8 IGD values compared with multi-objective algorithms. values compared with MOEA/D and FDHEFT. As the task number increases from 50 to 200, IGD values of fEDA equals to 0.0 under most situations, which means that the solutions produced by fEDA accounts for a large proportion of the general Pareto front.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:17",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How is the joint probabilistic distribution defined in the described uni-variate Estimation of Distribution Algorithm?",
      "topics": [
        "analysis"
      ],
      "answer": "The joint probabilistic distribution over a set of random variables \\(x=\\{x_{i}\\}\\) is defined as the product of the individual probability distributions for each variable, represented as \\(P(x) = \\prod_{i=1}^{D} P(x_{i})\\), where \\(D\\) is the dimensionality of the space and each variable \\(P(x_{i})\\) is modeled by a mixed Gaussian and Cauchy distribution.",
      "chunk_id": 13819,
      "paper_id": "A Self-adaptive Mixed Distribution Based Uni-variate Estimation of Distribution Algorithm for Large Scale Global Optimization",
      "paper_title": "A Self-adaptive Mixed Distribution Based Uni-variate Estimation of Distribution Algorithm for Large Scale Global Optimization",
      "paper_year": "2009",
      "chunk_content": "Therefore, it seems to be a promising way to develop more robust EDA by using mixed sampling operator. # 3.2 Algorithm Compared with the mutation operators of GA, ES and EP that mutate the individuals, the sampling operator of EDA mutates the distribution of the whole population in each generation. To strengthen the global search ability, a mixed Gaussian and Levy distribution is adopted here, which is similar to [33]. Based on the above analysis, a new self-adaptive uni-variate EDA is proposed here. In order to reduce the complexity of conducting the probabilistic model, the uni-variate EDA whose variables are considered independently is adopted in our algorithm. Similar to UMDAc [12], the joint probabilistic distribution over a set of random variables $x=\\left\\{x_{i}\\right\\}$ where $\\mathrm{i}=1,2 \\ldots D$ for $$\nX_{j i}=\\bar{X}_{i}+\\sqrt{\\delta_{i}} \\cdot N_{L}\n$$ dimensional space is defined as follows: $$\nP(\\mathbf{x})=\\prod_{i=1}^{D} P\\left(x_{i}\\right)\n$$ The probability distribution used to model each variable $P\\left(x_{i}\\right)$ is a single mixed Gaussian and Cauchy distribution. In contrast to iterated density estimation algorithm (IDEA) [1] developed by Bosman which requires computing all elements of covariance matrix to adapt an arbitrary Gaussian, MUEDA abandons adapting the non-diagonal elements in covariance matrix, which remarkably reduces the computational cost for LSGO problems.",
      "question_type": "definition",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:19",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "Based on the text, what is the main conclusion regarding the performance of the AffEDA and ECGA algorithms when applied to the HP protein folding problem, considering metrics such as best fitness and average fitness?",
      "topics": [
        "fitness"
      ],
      "answer": "The main conclusion is that, for the HP protein folding problem instances tested, there are few performance differences observed between the AffEDA and ECGA algorithms, as measured by the best fitness value found and the average fitness across independent experiments. Both algorithms were run for a maximum of 200 generations, and the results were analyzed using the Wilcoxon rank sum test.",
      "chunk_id": 15403,
      "paper_id": "Learning Factorizations in Estimation of Distribution Algorithms Using Affinity Propagation",
      "paper_title": "Learning Factorizations in Estimation of Distribution Algorithms Using Affinity Propagation",
      "paper_year": "2010",
      "chunk_content": "Although the MPMs seem not to be the most appropriate models for representing the type or dependencies arising in the HP problem, we research to what extent AffEDA can cope with this class of difficult problems. The results for ECGA are included for comparison. Table 4 shows the results achieved by AffEDA and ECGA for different protein instances of the HP problem. In the table, Inst indicates the instance from Table 1, [FORMULA340] refers to the [FORMULA341]-regular lattice in which instances were folded, psize is the population size, and $T$ is the tournament size used. The variables best and $\\mu(e)$ are respectively the best fitness and average fitness found from the set of independent experiments conducted. Both algorithms were run for a maximum of 200 generations. The number of experiments was 50 . Due to the computational cost, they were executed in a cluster of computers of different architectures. Therefore, computing times are not available from these experiments. However, regarding the differences in the time spent by the algorithms, the same trend appreciated in previous experiments was manifested. Regarding the results, there are few differences between both algorithms. We applied the Wilcoxon rank sum test for equal medians on the average results of the two Table 5: Results of AffEDA and ECGA for different protein instances of the HP problem.",
      "question_type": "comparison",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:20",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of sampling an offspring from a constructed probabilistic model differ from the reproduction methods employed when either a Genetic Algorithm (GA) or Differential Evolution (DE) is activated?",
      "topics": [
        "mutation",
        "Estimation of Distribution Algorithms",
        "Genetic Algorithms",
        "Differential Evolution"
      ],
      "answer": "When EDA is activated, an offspring $P v_{g}^{\\prime}(x)$ is sampled from a constructed probabilistic model, differing from the GA and DE approaches. If GA is activated, the SBX (Simulated Binary Crossover) and polynomial mutation are used to create offspring. If DE is activated, the DE operator and polynomial mutation are used to generate offspring. The EDA approach relies on learning and sampling from a probability distribution, whereas GA and DE utilize crossover and mutation operators directly on the population.",
      "chunk_id": 10903,
      "paper_id": "A hybrid adaptive evolutionary algorithm in the domination-based and decomposition-based frameworks of multi-objective optimization",
      "paper_title": "A Hybrid Adaptive Evolutionary Algorithm in the Domination-based and Decomposition-based Frameworks of Multi-objective Optimization",
      "paper_year": "2012",
      "chunk_content": "In the reproduction stage, if GA is activated, the SBX and polynomial mutation are used to create an offspring. If DE is activated, the DE operator and polynomial mutation are used to generate an offspring. Similarly, if EDA is activated, then an offspring is sampled from the constructed probabilistic model. After producing $P v_{g}^{\\prime}(x)$ children solutions, evaluation is performed to calculate their objective values. All of the parent and children solutions are stored in an archive. Elitism is performed to select $j=1$ solutions with the lowest Pareto rank or highest crowding distance from the archive to form the new population. Next, perform the EGS if it is activated. All of the solutions generated from the EGS will also undergo archiving and elitism before forming the new population. A generation is terminated here. The evolutionary processes are continued until the maximum number of fitness evaluations is reached. The second proposed algorithm is the hybrid MOEA/D (hMOEA/D). hMOEA/D adapts the decomposition algorithm suggested by Li and Zhang [6] and its process flow is illustrated in Fig. 4. This study also uses Tchebycheff approach, as suggested by Li and Zhang [6], to transform an MOP into $\\mathbf{n}$ scalar optimization sub-problems.",
      "question_type": "process",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:22",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm with a Tabu Search component, how should practitioners update the potential values, denoted as p_j, after flipping a variable x_i to its complement (1-x_i)?",
      "topics": [
        "complexity"
      ],
      "answer": "After flipping a variable x_i, the potential p_j is updated based on the following conditions: If x_i equals 0 and i is not equal to j, then p_j is updated as p_j + d_ij. If x_i equals 1 and i is not equal to j, then p_j is updated as p_j - d_ij. If i equals j, then p_j remains unchanged. This update rule is used to maintain the potential values during the iterative process, and this method reduces the computational complexity to O(n) per iteration.",
      "chunk_id": 17542,
      "paper_id": "A Hybrid Metaheuristic of Integrating Estimation of Distribution Algorithm with Tabu Search for the Max-Mean Dispersion Problem",
      "paper_title": "A Hybrid Metaheuristic of Integrating Estimation of Distribution Algorithm with Tabu Search for the Max-Mean Dispersion Problem",
      "paper_year": "2019",
      "chunk_content": "Specifically, we use an additional vector to record the potential [FORMULA148] of each variable [FORMULA149], defined as $$\np_{i}=\\sum_{i=1, i \\neq j}^{n} d_{i j} x_{j}\n$$ Then, the move gain of flipping the variable [FORMULA151] can be represented as $$\n\\Delta_{i}= \\begin{cases}\\frac{m f(x)+p_{i}}{m+1}-f(x), & x_{i}=0 \\\\ \\frac{m f(x)-p_{i}}{m-1}-f(x), & x_{i}=1\\end{cases}\n$$ where [FORMULA153] denotes the objective value of the solution [FORMULA154] and $m$ denotes the number of the selected elements in the current solution, i.e., $m=\\sum_{i=1}^{n} x_{i}$. It can also be written as $$\n\\Delta_{i}= \\begin{cases}\\frac{-f(x)}{m+1}+\\frac{p_{i}}{m+1}, & x_{i}=0 \\\\ \\frac{f(x)}{m-1}-\\frac{p_{i}}{m-1}, & x_{i}=1\\end{cases}\n$$ After flipping a variable [FORMULA158] to be its complement $1-x_{i}$, the potential $p_{j}$ is updated as $$\np_{j}= \\begin{cases}p_{j}+d_{i j}, & x_{i}=0, i \\neq j \\\\ p_{j}-d_{i j}, & x_{i}=1, i \\neq j \\\\ p_{j}, & i=j\\end{cases}\n$$ Using the abovementioned method, the computational complexity of each iteration is reduced to $O(n)$. Tabu rule requires that each time a move is performed, its reverse move is prohibited from being performed during the following specified number of iterations (called tabu tenure). A dynamic tabu tenure management strategy that divides the iterations into different intervals and uses a specified order of different tabu tenures for these intervals is employed. This strategy was first proposed in [24] for solving the graph partitioning problem and demonstrated to be effective in [9] for the Max-Mean DP.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:24",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the UMDA and Tree-EDA implementations, how does the scoring metric used for the Bayesian network impact the algorithm, and what is the significance of allowing each node to have a maximum of 5 parents?",
      "topics": [
        "optimization"
      ],
      "answer": "The UMDA and Tree-EDA implementations utilize a Bayesian network, where the Bayesian metric with uniform priors is used for scoring. This metric guides the learning step by evaluating the quality of different network structures representing probabilistic dependencies between variables. Allowing each node to have a maximum of 5 parents restricts the complexity of the Bayesian network, limiting the number of dependencies considered. The truncation parameter, denoted as $T=0.5$, is used. Best elitism, in which the selected population is passed to the next population, was used.",
      "chunk_id": 15264,
      "paper_id": "Using Probabilistic Dependencies Improves the Search of Conductance-Based Compartmental Neuron Models",
      "paper_title": "Using Probabilistic Dependencies Improves the Search of Conductance-Based Compartmental Neuron Models",
      "paper_year": "2010",
      "chunk_content": "The algorithm was implemented in Matlab using the MATEDA-2.0 software [15. The implementations of the UMDA and Tree-EDA follow the same scheme but the learning and sampling steps are modified accordingly. These were implemented using the Matlab Bayes Net (BNT) toolbox 11. The scoring metric used for the Bayesian network was the Bayesian metric with uniform priors, and each node was allowed to have a maximum number of 5 parents. The truncation parameter was $T=0.5$. Best elitism, in which the selected population is passed to the next population, was used.  # 6 Related Work There is considerable work on the application of optimization methods to neuron model parameter optimization.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:25",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does using an Extreme Learning Machine (ELM) to approximate the reward function in this control design compare to directly using the evaluation score, considering the implications for optimization and physical meaning?",
      "topics": [
        "control"
      ],
      "answer": "Using an ELM to represent the evaluation score results in an approximate reward function, denoted as R(a, k). While this allows for performance evaluation, it lacks direct physical meaning. Consequently, optimizing torque commands with powertrain dynamics using this ELM-based reward function might not equate to achieving the highest evaluation score. Furthermore, there is no guarantee of convexity for extremum seeking or optimal control problems when using the ELM-approximated reward function, unlike directly using the evaluation score.",
      "chunk_id": 17841,
      "paper_id": "Acceleration Control Design of HEVs with Comfortability Evaluation based on IRL",
      "paper_title": "Acceleration Control Design of HEVs with Comfortability Evaluation based on IRL",
      "paper_year": "2021",
      "chunk_content": "7 ), where the corresponding scores are $79.9,66.8$ and 58.0 , respectively. To evaluate the ELM-based cost function, the cost function was tested using a set of 90 data points. The root mean square error (RMSE) for this set was found to be 3.71 . [IMAGE6] Fig. 7. Three samples of the training data [IMAGE7] Fig. 8. The obtained reward function by IRL The result of IRL is shown in Fig. 8, where the value of $R(a, k)$ is sketched. Observe that the performance during Table 2. Physical parameter of powertrain the initial raising period is mostly focused on evaluating the score. However, it should be noted that this reward function is nothing but an approximate representation with ELM of the evaluation score, hence, has no physical meaning. This implies that searching for the most comfortable acceleration profile $a^{*}(t)$ using this reward function or directly using this reward function as a cost function in optimizing the torque commands with the powertrain dynamics might not equate to the highest score. Moreover, there is no guarantee of the convexity of the extremum seeking or optimal control problem.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:27",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing the NMBDE algorithm for numerical optimization, what is the suggested default value for balancing performance on various problems, based on the provided information?",
      "topics": [
        "convergence"
      ],
      "answer": "The suggested default value is 20, which, according to the text, aims to achieve a balance in performance across different problems when using the NMBDE algorithm.",
      "chunk_id": 10107,
      "paper_id": "A novel modified binary differential evolution algorithm and its applications",
      "paper_title": "A novel modified binary differential evolution algorithm and its applications",
      "paper_year": "2012",
      "chunk_content": "Based on an overall analysis of experimental results, we suggest that $\\pm 0.7126) \\cdot f_{12}^{*} \\approx-1.03162845$ is set between 6 and 100, and 20 was selected as the default value for $$\n\\min f_{13}=-\\left[\\prod_{i=1}^{n} \\sin \\left(x_{i}\\right)\\right] \\cdot \\sqrt{\\prod_{i=1}^{n}\\left(x_{i}\\right)}, \\quad \\text { subject to } 0 \\leq x_{i} \\leq 10\n$$ in our work to achieve a balance performance on various problems. [IMAGE7] Fig. 5. Convergence curves of NMBDE, DBDE, DBPSO, BAS and MBDE on the 30-dimensional functions. [IMAGE8] Fig. 5. (continued) Table 7 Experimental results of NMBDE, DBDE, DBPSO, BAS and MBDE on 10.100 MKP instances. Table 7 (continued) # 5. Applications of NMBDE ### 5.1. Parameter settings In the following section, NMBDE are used to solve the numerical optimization problems and multidimensional knapsack problems to test its optimization ability. In order to compare the solution quality and performance of NMBDE, four binary-coded optimization algorithms, i.e., DBDE [26], DBPSO [27], BAS [28], and MBDE [23] with the recommended parameter values were applied to the both applications as well.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:28",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What benchmark functions from CEC2013 are used to assess the performance of EDA-PSO and PSEDA?",
      "topics": [
        "performance"
      ],
      "answer": "The 28 benchmark functions defined in CEC2013 are used for experimental tests to evaluate the performance of EDA-PSO and PSEDA, among other algorithms. These benchmark functions are widely used for real-parameter optimization.",
      "chunk_id": 8710,
      "paper_id": "Composite Particle Swarm Optimizer With Historical Memory for Function Optimization",
      "paper_title": "Composite Particle Swarm Optimizer With Historical Memory for Function Optimization",
      "paper_year": "2015",
      "chunk_content": "11: end 12) It defines a new particle generation mechanism. The idea behind it is to generate a new particle through the competition among those generated based on historical memory, particles' current pbests, and the swarm's gbest. ## IV. EXPERIMENTAL EVALUATION Extensive simulations are carried out in order to compare HMPSO with CPSO-H6 [3], CLPSO [6], ALCPSO [7], FIPS [4], HPSO-TVAC [5], EDA-PSO [14], and PSEDA [15]. Further experimental evaluations with two HMPSO variants are then carried out to analyze and compare the performance of proposed HMPSO when it uses two different ways to build historical memory. All the mentioned algorithms are coded in MATLAB-R2010a and simulations are executed on a 2.4 GHz Xeon E5-2665 processor with 32 GB main memory running under Windows server 2008 environment. ## A. Benchmark Functions and Algorithm Configuration In order to study their performance deeply, 28 benchmark functions in CEC2013 [42] are used for the experimental tests here, which have been widely used for real-parameter optimization, as shown in Table I.",
      "question_type": "evaluation metrics",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:29",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs utilizing the Population-Based Local Search (PBLS) and frequency matrix, and how does it ensure consistency in the probabilistic model?",
      "topics": [
        "algorithm"
      ],
      "answer": "The primary objective of the presented approach is to refine the elite set of solutions within an Estimation of Distribution Algorithm by employing a Population-Based Local Search (PBLS) to improve individual solutions. Subsequently, the elite set updates the frequency matrix \\$\\varphi_{g+1}\\$ based on a weighted average of the frequency matrices \\$I_{g}(q)\\$ of the elite individuals, modulated by the learning rate \\$\\beta\\$. The frequency function \\$\\delta_{i j g}(q)\\$ determines the relative order of activities, and the updating mechanism maintains the characteristic \\$p(X_{i j}=1)+p(X_{j i}=1)=1\\$, ensuring that the probabilistic model remains consistent by preserving the complementary probabilities between activity pairs.",
      "chunk_id": 9560,
      "paper_id": "An estimation of distribution algorithm and new computational results for the stochastic resource-co",
      "paper_title": "An estimation of distribution algorithm and new computational results for the stochastic resource-constrained project scheduling problem",
      "paper_year": "2015",
      "chunk_content": "Secondly, the PBLS is employed to improve each individual of the elite set. Then, the elite set is chosen to update [FORMULA186] according to equation  Fig. 2 Procedure of PBLS $$\n\\varphi_{g+1}=(1-\\beta) \\cdot \\varphi_{g}+\\beta \\cdot \\frac{1}{Q} \\sum_{q \\in \\Omega_{g}^{\\text {Elite }}} I_{g}(q)\n$$ where $\\beta$ is the learning speed and $I_{g}(q)$ is the frequency matrix of the $q^{\\text {th }}$ individual of $\\Omega_{g}^{\\text {Elite }}$ in generation [FORMULA192] which is defined as $$\nI_{g}(q)=\\left(\\begin{array}{ccc}\n\\delta_{11 g}(q) & \\cdots & \\delta_{1 J g}(q) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\delta_{J 1 g}(q) & \\cdots & \\delta_{J J g}(q)\n\\end{array}\\right)\n$$ employing the frequency function $\\delta_{i j g}(q)$ according to: $\\delta_{i j g}(q)=\\left\\{\\begin{array}{l}1, \\text { if activity } i \\text { is placed before activity } j \\text { in the } q^{\\text {th }} \\text { individual of } \\Omega_{g}^{\\text {Elite }} \\text { in generation } g ; \\\\ 0, \\text { else. }\\end{array}\\right.$ Note that the updating mechanism always maintains the characteristic $p\\left(X_{i j}=1\\right)+p\\left(X_{j i}=1\\right)=1$ of [FORMULA197].",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:31",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of maximum likelihood estimation of parameters relate to ensuring the algorithm finds the optimum?",
      "topics": [
        "algorithm"
      ],
      "answer": "Within the framework of Estimation of Distribution Algorithms, the maximum likelihood estimation of parameters is modified to ensure that the algorithm goes through the optimum.",
      "chunk_id": 26466,
      "paper_id": "Average time complexity of estimation of distribution algorithms",
      "paper_title": "Average time complexity of estimation of distribution algorithms",
      "paper_year": "2005",
      "chunk_content": "To ensure that the algorithm goes through the optimum, the maximum likelihood estimation of parameters is modified via  Fig.",
      "question_type": "relationship",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:32",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners use Variable Neighborhood Search (VNS) to address overfitting in the probability model, specifically detailing the neighborhood search strategies?",
      "topics": [
        "complexity"
      ],
      "answer": "When overfitting occurs, VNS can be employed to search for a better probability model. This involves defining neighborhoods either in the space of model parameters (keeping the graphical model structure fixed) or in the space of model structures (modifying the structure and re-estimating parameters). A simple approach is to change the complexity of the probabilistic models within these neighborhoods. When searching within the parameter space, credal networks, which use interval estimation for conditional probabilities instead of point estimation, can be used instead of Bayesian networks.",
      "chunk_id": 11403,
      "paper_id": "Combining variable neighborhood search and estimation of distribution algorithms in the protein side chain placement problem",
      "paper_title": "Combining variable neighborhood search and estimation of distribution algorithms in the protein side chain placement problem",
      "paper_year": "2008",
      "chunk_content": "However, sometimes the probability model cannot fulfill this goal. This may be due, for instance, to the problem of overfitting, which refers to an overly accurate approximation of data which does not reflect more general features of the search space. In this case, and starting from the initial model learned, VNS can be used to search for a better model, able to generate solutions with a better fitness average. The quality of the searched models can be evaluated by calculating the fitness average of a set of solutions sampled from it. The neighborhoods can be defined in the space of the parameters (in this case, the structure of the graphical model will be fixed) or in the space of the structures (the structure is modified and new parameters are estimated from the data). Neighborhood structures for problems defined on graphs have been proposed (Brimberg et al. 2005; Kochetov and Velikanova 2005). A simple idea to move between neighborhoods is to change the complexity of the probabilistic models. When the neighborhood is defined exclusively in the space of parameters, credal networks (Zaffalon 2002) could be used instead of Bayesian networks. In credal networks, the estimation of the conditional probabilities is done by means of interval estimation instead of by means of usual point estimation.",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:34",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the process of generating test cases by optimizing an objective function related to program coverage, how is the selection of the objective entity, particularly in the context of branch coverage, mathematically represented using a control flow graph, and what properties of this graph are leveraged?",
      "topics": [
        "selection"
      ],
      "answer": "The selection of the objective entity for test case generation is often determined using a graph reflecting structural characteristics of the program. In the case of branch coverage, a control flow graph is typically employed, where each vertex represents a basic block in the code (a maximal sequence of code statements such that if one is executed, then all of them are). An arc $(x, y)$ exists if control can be transferred from block $x$ to $y$ without crossing any other block. A program branch is defined by every vertex $h_{2}(\\boldsymbol{x})$ with outdegree $(x) > 1$. The function value of an input is proportional to how close the input is to achieving objective coverage, with an optimum value when the objective is exercised. This formulation allows the test case generation to be tackled as the resolution of a number of function optimization problems, one for each objective entity, such as the vertices with outdegree greater than 1 in the control flow graph representing branches.",
      "chunk_id": 12293,
      "paper_id": "DYNAMIC SEARCH SPACE TRANSFORMATIONS FOR SOFTWARE TEST DATA GENERATION",
      "paper_title": "Dynamic search space transformations for software test data generation",
      "paper_year": "2008",
      "chunk_content": "In the second step, the objective entity is assigned a function dependent on the program input, and its optimization is sought. This objective function is formulated in such a way that, if an executed input exercises the objective, the value is optimum. Otherwise, the value is proportional to how close the input is to the objective coverage. Consequently, to obtain the function value of an input it must be previously executed on an instrumented version of the program which will provide the information necessary. This way, the test case generation is tackled as the resolution of a number of function optimization problems, one for each objective entity. Despite the different selection options, the most common practice is to determine the objective entity with the help of a graph that reflects the structural characteristics of the program. In the case of branch coverage, a control flow graph (Fenton 1985) is typically employed. In such a graph, each vertex represents a basic block in the code, i.e., a maximal sequence of code statements such that if one is executed, then all of them are. There is an arc $(x, y)$ if the control of the program can be transferred from block $x$ to $y$ without crossing any other block. Hence, in this kind of graph, a program branch is defined by every vertex $h_{2}(\\boldsymbol{x})$ with outdegree $(x)>1$.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:36",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of Tri-EDA with the optional model compare to Tri-EDA without the optional model in handling changing environments, based on the information provided?",
      "topics": [
        "fitness"
      ],
      "answer": "Tri-EDA with the optional model was able to recapture the new global optimum (fitness value $=1.0$) after each change of problem structure. By contrast, Tri-EDA without the optional model only found the new global optimum on one occasion, indicating a significant decrease in its capability to handle changing environments.",
      "chunk_id": 11919,
      "paper_id": "Extending a class of continuous estimation of distribution algorithms to dynamic problems",
      "paper_title": "Extending a class of continuous estimation of distribution algorithms to dynamic problems",
      "paper_year": "2008",
      "chunk_content": "Fifty random problem sequences were tested each of which contained six different stationary test problems. Fig. 3 shows the typical performance of Tri-EDA [FORMULA77] on a certain problem sequence (Problem A). It is clear that Tri-EDA [FORMULA78] was always able to recapture the new global optimum (fitness value $=1.0$ ) after each change of problem structure. The standard deviations of the primary model are plotted in Fig. 4, which shows that the diversity of the model increased dramatically (the transition from exploitation to exploration) a few generations after the problem was modified. By contrast, without the optional model, the capability of Tri-EDA [FORMULA80] in handling changing environments suffered significantly and the new global optimum was only found in one occasion. Similar conclusions can be also drawn from the results on another problem sequence (Problem B), as shown in Figs. 6, 7, and 8, respectively. # 6 Conclusions The major contribution of this paper is a novel triple-model strategy as the first attempt to extend the applicability of a class of continuous Gaussian EDAs to the domain of [IMAGE2] Fig. 3 The performance of [FORMULA81] on Problem A [IMAGE3] Fig.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:38",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model to build a fuzzy histogram for a one-dimensional problem, according to the provided procedure?",
      "topics": [
        "search"
      ],
      "answer": "The steps to build a fuzzy histogram are as follows: Step 1, set the estimated range of the histogram model [Lbound, Ubound] based on the minimum value ($p_{min}$) and maximum value ($p_{max}$) of the individuals $\\boldsymbol{P}$, extending this range by $\\varepsilon \\cdot \\Delta x$ but ensuring it stays within predefined LBOUND and UBOUND. Step 2, construct a fuzzy partition of the search space by generating $N$ fixed points $\\boldsymbol{S}$ on the interval $\\Omega=[Lbound, Ubound]$, where the distance $\\Delta x$ between each point is calculated as $(\\text{Ubound} - \\text{Lbound}) / (N - 1)$. Finally, define $N$ fuzzy subsets $A_{1}, A_{2}, \\ldots, A_{N}$ using these fixed points to create membership functions, calculating $l_k$ and $u_k$ based on Lbound, Ubound, and $\\Delta x$.",
      "chunk_id": 13862,
      "paper_id": "A Novel Fuzzy Histogram Based Estimation of Distribution Algorithm for Global Numerical Optimization",
      "paper_title": "A Novel Fuzzy Histogram based Estimation of Distribution Algorithm for Global Numerical Optimization",
      "paper_year": "2009",
      "chunk_content": "Take a one-dimension problem for example, let $\\boldsymbol{P}=\\left\\{p_{1}\\right.$, $\\left.p_{2}, \\ldots, p_{S e}\\right\\}$ be $\\mu_{A_{i}}$ better individuals and the procedures of building a fuzzy histogram are as follows. Step 1: Set the estimated range of the histogram model. In this study, the estimated range of the histogram model [Lbound, Ubound] corresponding to $\\boldsymbol{P}$ is compute by Lbound $=\\left\\{\\begin{array}{ll}p_{\\min }-\\varepsilon \\cdot \\Delta x & \\text { if } p_{\\min }-\\varepsilon \\cdot \\Delta x \\geq L B O U N D \\\\ L B O U N D & \\text { otherwise }\\end{array}\\right.$ and $$\n\\text { Ubound }=\\left\\{\\begin{array}{ll}\np_{\\max }+\\varepsilon \\cdot \\Delta x & \\text { if } p_{\\max }+\\varepsilon \\cdot \\Delta x \\leq U B O U N D \\\\\nU B O U N D & \\text { otherwise }\n\\end{array}\\right.\n$$ where $p_{\\min }$ and $p_{\\max }$ are the minimum and maximum value in [IMAGE_1] Figure 2. Member functions of $t$ fuzzy subsets $$\n\\begin{aligned}\n& l_{k}= \\begin{cases}\\text { Lbound } & , \\text { if } k=1 \\\\\n\\text { Ubound }-\\Delta x / 2 & , \\text { if } k=N \\\\\n\\text { Lbound }+(k-1) \\cdot \\Delta x-\\Delta x / 2 & , \\text { otherwise }\\end{cases} \\\\\n& u_{k}= \\begin{cases}\\text { Lbound }+\\Delta x / 2 & , \\text { if } k=1 \\\\\n\\text { Ubound } & , \\text { if } k=N \\\\\n\\text { Lbound }+(k-1) \\cdot \\Delta x+\\Delta x / 2 & , \\text { otherwise }\\end{cases}\n\\end{aligned}\n$$ respectively, LBOUND and UBOUND are the defined minimum and maximum value of $\\boldsymbol{P}, \\varepsilon$ is the extending rate and $P(t)$ is the distance value defined as (9). Step 2: Construct fuzzy partition of the search space. With a predefined value $F_{\\text {sphere }}$, we can gain $F_{\\text {sumean }}$ fixed points $\\boldsymbol{S}=$ $\\left\\{s_{1}, s_{2}, \\ldots, s_{N}\\right\\}$ on the interval $\\Omega=[L b o u n d, U b o u n d]$ with $F_{\\text {rantrigin }}$ The distance between each point is the same, which is $$\n\\Delta x=(\\text { Ubound }- \\text { Lbound }) /(N-1)\n$$ With these $4 \\times 10^{5}$ fixed points, we can define $F_{\\text {schwefel }}$ fuzzy subsets $A_{1}$, $A_{2}, \\ldots, A_{N}$ on $F_{\\text {threepeaks. }}$. In this paper, the membership functions of these $C$ fuzzy subsets are shown as Fig.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:40",
      "generation_style": "implementation_focused"
    },
    {
      "question": "When comparing a Preventive Maintenance (PM) strategy based on the PSO-EDA algorithm to a general PM strategy based on the traditional PSO algorithm, what specific performance indicators and percentage improvements are used to demonstrate the effectiveness of the PSO-EDA based approach?",
      "topics": [
        "performance"
      ],
      "answer": "The effectiveness of the PM strategy based on the PSO-EDA algorithm is evaluated by comparing the total profit and production rate against a general PM strategy based on the traditional PSO algorithm. Under conditions of 20 machines and 60 pallets, the PM strategy based on the PSO-EDA algorithm demonstrates a 20.9% improvement in total profit and a 6.77% improvement in production rate compared to the general PM strategy.",
      "chunk_id": 7461,
      "paper_id": "Performance evaluation and optimization model for closed loop production lines considering preventive",
      "paper_title": "Performance evaluation and optimization model for closed-loop production lines considering preventive maintenance and rework process",
      "paper_year": "2023",
      "chunk_content": "are studied in order to explore the effect of decision variables and the proposed PSO-EDA algorithm. Based on the findings of the numerical example, the following remarks can be concluded. (1) The numerical example demonstrates that the proposed model can improve product quality, optimize the production processes, and enable the enterprises to obtain more profit than ever before. (2) It shows that the number of machines, the number of pallets, and PM policy have a comprehensive influence on the performance of the system. Consequently, it is necessary and sensible to optimize these variables simultaneously by trading off their effects. (3) The system's output under the PM strategy based on the PSO-EDA algorithm is higher than the general PM strategy based on traditional PSO algorithm. When comparing the total profit and production rate, the PM strategy based on the PSO-EDA algorithm shows better performance, outperforming the general PM strategy by $20.9 \\%$ and $6.77 \\%$, respectively, under 20 machines and 60 pallets. As future work, some assumptions can be relaxed. A straightforward relaxation consists to consider the economic efficiency of rework. It is assumed that the only solution for the unqualified products is to process them again. However, in practical production systems, due to the high cost of rework, unqualified products can be discarded unless necessary or economic, which complicates the model.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:41",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs within the context of the Steiner tree problem?",
      "topics": [
        "optimization"
      ],
      "answer": "The algorithmic approach aims to evolve trees using Estimation of Distribution Algorithm (EDA) principles to find optimized solutions for the Steiner tree problem. This evolution targets better performance, reduced time complexity, and optimized solutions by improving searching and converging capabilities.",
      "chunk_id": 487,
      "paper_id": "An Estimation of Distribution Algorithm for Steiner Tree Problem",
      "paper_title": "An Estimation of Distribution Algorithm for Steiner Tree Problem",
      "paper_year": "2013",
      "chunk_content": "The process is repeated until some termination criteria are met. The algorithm constantly evolves trees to obtain a better solution tree with EDA ideas. This method leads to better performance, reduced time complexity, and optimized solution. Simulation results also show that the algorithm has better performance in searching and converging. Keywords-Steiner Tree; estimation distribution algorithm; approximation algorithm; intelligence optimization; probabilistic model ## I. INTRODUCTION The Steiner tree problem, which is one of the most well known combinatorial optimization problems, has long been studied[1,2]. Lots of research papers have been contributed to the Steiner tree problem. It has been found that Steiner tree problem can be applied to solve many critical problems in computer science, biological science etc. Researchers have proposed many approaches to solving very large scale integrated circuits design, optical and wireless communication networks[3,4,5,6].Multi-constraint multicast application optimization, in particular, belongs to a certain Steiner tree problem[7,8,9,10]. Those applications usually require some modifications on the classical Steiner tree problem and hence demand new techniques for solving them. As a result, studying various variations of Steiner tree problems forms a very hot topic in the past twenty years. [^0]The study of Steiner tree problem in graph theory can be traced back to 1961[10]. In 1977, Garey proved that the minimal Steiner tree problem is NP Complete[11].",
      "question_type": "application",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:43",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of the BBIC algorithm, what algorithmic mechanism explains the inverse relationship between the average size of Building Blocks (BBs) and the number of BBs, specifically how does the competition between common and different BBs manifest during convergence?",
      "topics": [
        "convergence"
      ],
      "answer": "The BBIC algorithm exhibits a competition between common and different Building Blocks (BBs). As the average common BB size (CB) increases, the number of common BBs (\\#CB) decreases, indicating convergence. Conversely, if the average different BB size (DB) were to increase, the number of different BBs (\\#DB) would decrease. In successful cases within the BBIC algorithm, common BBs gradually develop while different BBs remain relatively constant, suggesting that the algorithm prioritizes the growth and consolidation of common BBs over different ones during the search process.",
      "chunk_id": 24574,
      "paper_id": "The use of explicit building blocks in evolutionary computation",
      "paper_title": "The use of explicit building blocks in evolutionary computation",
      "paper_year": "2016",
      "chunk_content": "Competition among BBs The data from the experiments on the BBIC algorithm are used to illustrate the competition among BBs. The average size of the BBs and the number of BBs are good indicators of the competition between common and different BBs. Figure 13(a)-(d) illustrates the competition behaviour in the Royal Road problem. Figure 14(a)-(d) illustrates the Trap-5 problem. In both figures, CB denotes the average common BB size, DB denotes the average different BB size, \\#CB denotes the number of common BBs, and \\#DB denotes the number of different BBs. The average sizes of BBs in Figure 13(a) and 13(b) and the number of BBs in Figure 13(c) and 13(d) indicate the convergence. These two values are opposite. If the size of BBs increases, the number of BBs decreases. In the success case (Figure 13(a)), only the common BBs are gradually developed while the different BBs remain constant. In the [IMAGE_6] Figure 12. The BBIC algorithm results for the Trap-5 60-bit problem: (a) and (b) the number of BBs in each generation, and (c) and (d) the number of ideal BBs (pure BBs) in each generation.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:44",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "What specific implementation steps are required for the hREDA algorithm, using the Tchebycheff approach, to determine the fitness value of the \\(j^{th}\\) subproblem, given a set of evenly distributed weight vectors \\(\\lambda^{1}, \\ldots, \\lambda^{S}\\) and reference point \\(z^{*}\\)?",
      "topics": [
        "fitness"
      ],
      "answer": "To implement the hREDA algorithm with the Tchebycheff approach, the fitness value \\(gt\\) for the \\(j^{th}\\) subproblem is calculated using the formula \\(gt(x | \\lambda^{j}, z^{*}) = \\max_{1 \\leq i \\leq P}\\{\\lambda_{i}^{j} | F_{i}(x)-z_{i}^{*}\\}\\), where \\(\\lambda^{j}\\) represents the weight vector for the \\(j^{th}\\) subproblem, \\(z^{*}\\) is the reference point, and \\(F_{i}(x)\\) represents the \\(i^{th}\\) objective value of the solution \\(x\\). The algorithm also requires determining the \\(Q\\) nearest neighbors \\(B(i) = \\{i_{1}, \\ldots, i_{Q}\\}\\) to each weight vector using Euclidean distance. Furthermore, the reference point \\(z^{*}\\) is set to the minimum objective value of the initial population, which is calculated based on the formulation in section III. Finally, a Restricted Boltzmann Machine (RBM) is trained using Contrastive Divergence (CD) to obtain the trained weights, biases, and hidden states of the network.",
      "chunk_id": 11055,
      "paper_id": "A hybrid estimation of distribution algorithm for solving the multi-objective multiple traveling salesman problem",
      "paper_title": "A Hybrid Estimation of Distribution Algorithm for Solving the Multi-objective Multiple Traveling Salesman Problem",
      "paper_year": "2012",
      "chunk_content": "In the decomposition framework, the fitness assigned to each solution can be based on any classical aggregation approach. In this paper, the Tchebycheff approach [11], [34] is used and hREDA is described according to this approach. A set of evenly distributed weight vectors $\\lambda^{1}, \\ldots, \\lambda^{S}$ and the reference point $z^{*}$ are generated, where $\\left.\\mathrm{m}, \\mathrm{n}-1\\right]$ is the number of subproblems. The algorithm decomposes the population into $F V^{i}=F\\left(x^{j}\\right)$ scalar optimization subproblems according to the Tchebycheff formulation and the fitness value of the $j^{\\text {th }}$ subproblem is defined as: $$\ng t\\left(x \\mid \\lambda^{j}, z^{*}\\right)=\\max _{1 \\leq i \\leq P}\\left\\{\\lambda_{i}^{j} \\mid F_{i}(x)-z_{i}^{*}\\right\\}\n$$ The pseudo code of hREDA is presented in Fig. 4. In step 1 , the $Q$ neighbors (denoted as $B(i)=\\left\\{i_{1}, \\ldots, i_{Q}\\right\\}$.) that are nearest, in terms of Euclidean distance, to each weight vector are determined. Then, initial chromosomes in the form as indicated in Fig. 3 are randomly generated. Objective values $F_{1}(x), \\ldots, F_{P}(x)$ are calculated based on the formulation in section III, and the reference point $N=\\mathrm{n}+m-1$ is set to the minimum objective value of the initial population. In Step 2, RBM is trained using the CD training mechanism [29] in order to obtain the trained weights, biases and hidden states of the network.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:47",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the selection function \\(D(p_j)\\) used in this Estimation of Distribution Algorithm, which combines coverage and separation, how is the value of \\(D(p_j)\\) determined for a probe \\(p_j\\), and what does this value represent in the context of dominated targets?",
      "topics": [
        "selection"
      ],
      "answer": "The selection function \\(D(p_j)\\) is defined as \\(D(p_j) = \\max\\{C(p_j), S(p_j)\\}\\), where \\(C(p_j)\\) favors the selection of probes that \\(c_{\\text{min}}\\)-cover dominated targets, and \\(S(p_j)\\) favors the selection of probes that \\(s_{\\text{min}}\\)-separate dominated target pairs. Therefore, \\(D(p_j)\\) represents the maximum value between the coverage score and the separation score for probe \\(p_j\\), indicating its degree of contribution to covering dominated targets and separating dominated target pairs.",
      "chunk_id": 14213,
      "paper_id": "Bayesian Optimization Algorithm for the Non-unique Oligonucleotide Probe Selection Problem",
      "paper_title": "Bayesian Optimization Algorithm for the Non-unique Oligonucleotide Probe Selection Problem",
      "paper_year": "2009",
      "chunk_content": "Function $C$ favors the selection of probes that $c_{\\text {min }}$-cover dominated targets. Target $t_{i j}$ dominates target $t_{j}$, if $P_{t_{j}} \\subseteq P_{t_{i}}$. Function $S$ favors the selection of the probes that $s_{\\text {min }}$-separate dominated target pairs. Target pair $t_{i j}$ dominates target pair $t_{k l}$, if $P_{t_{i j}} \\subseteq P_{t_{k l}}$. The functions $S D R C$ and $D$ have been defined as the maximum between the values of the function cov and sep, respectively. The selection function $D\\left(p_{j}\\right)$ which has been defined as follows will indicate the degree of contribution of $S^{\\prime}$. $$\nD\\left(p_{j}\\right)=\\max \\left\\{C\\left(p_{j}\\right), S\\left(p_{j}\\right)\\right\\} \\quad \\mid 1 \\leq j \\leq n\\}\n$$ The probes of highest value of $\\mathrm{C}++$ will be the candidate probes for the solution probe set. Calculation of the coverage and separation functions are given in Tables 2 and 3 based on DRC definitions in rows $(n)$ and $(k)$, respectively [12]. We see, by definition of DRC functions, these four probes have the same score for the coverage of the dominated targets and the same score for the separation of the dominated target pairs, and $D\\left(p_{1}\\right)=D\\left(p_{3}\\right)=D\\left(p_{4}\\right)=D\\left(p_{5}\\right)=\\frac{c_{\\min }}{3}$.",
      "question_type": "evaluation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:49",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "In the context of quay crane scheduling, how do the precedence relationships defined by the set $\\Phi$ differ from the constraints defined by the set $\\Psi$, and what implications does this have on the scheduling search space?",
      "topics": [
        "search"
      ],
      "answer": "The set $\\Phi$ represents task pairs (i, j) where task i must be completed before task j can start, indicating a strict order of execution. The set $\\Psi$ represents task pairs (i, j) that cannot be processed simultaneously, defining broader exclusivity constraints. The text specifies that $\\Phi$ is a subset of $\\Psi$ ($\\Phi \\subseteq \\Psi$), meaning all precedence relationships are also simultaneity constraints, but not all simultaneity constraints are precedence relationships. This distinction limits the search space to unidirectional schedules, where quay cranes move in the same direction without changing after initial positioning, influencing the determination of completion times $c_{i}$ to minimize the makespan $T(c_{T})$.",
      "chunk_id": 23902,
      "paper_id": "Estimation of Distribution Algorithm for the Quay Crane Scheduling Problem",
      "paper_title": "Estimation of Distribution Algorithm for the Quay Crane Scheduling Problem",
      "paper_year": "2011",
      "chunk_content": "Let $\\Phi$ be the set of task pairs within the same container bay for which exists a precedence relationship. On the other hand, $\\Psi$ is the set of task pairs that cannot be processed simultaneously. That is, $$\n\\begin{aligned}\n& \\Phi=\\{(i, j): i \\text { has to be completed before the starting of } j\\} \\\\\n& \\Psi=\\{(i, j): i \\text { and } j \\text { cannot be done simultaneously }\\}\n\\end{aligned}\n$$ Note that $\\Phi \\subseteq \\Psi$. The objective of the QCSP is to determine the completition times $c_{i}$ of all tasks $i \\in \\bar{\\Omega}$ so that the completition time of the last task $T\\left(c_{T}\\right)$ is minimized; that is, minimizing the makespan. Figure 13.1 shows an instance of the QCSP with $n=8$ tasks, $m=2$ quay cranes, $l_{0}^{1}=2, l_{0}^{2}=5, r^{1}=r^{2}=0, \\hat{t}=1$ and $\\delta=1$. Processing times and the location of the tasks on the bays are shown in Table 13.1. Additionally, $$\n\\begin{aligned}\n\\Phi & =\\{(1,2),(5,6),(5,7),(6,7)\\} \\\\\n\\Psi & =\\{(1,2),(3,4),(5,6),(5,7),(6,7)\\}\n\\end{aligned}\n$$ [IMAGE_0] Fig. 13.1 Example of an instance of the Quay Crane Scheduling Problem Table 13.1 Input data of the example instance In this work, the search space is limited to unidirectional schedules. A scheduling is denominated unidirectional if the quay cranes have the same sense of moving and this is not changed after the initial positioning (see [1]).",
      "question_type": "relationship",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:51",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model during the reproduction phase, specifically detailing the selection and application of mutation operators?",
      "topics": [
        "mutation",
        "Estimation of Distribution Algorithms"
      ],
      "answer": "During the reproduction phase, the algorithm first builds a probabilistic model $P v_{g}(x)$ representing the solution distribution. Then, for each individual from $i=1$ to $N$, a random value 'u' between 0 and 1 is generated. If $u < Ar_{0}^{2 / k}$, an offspring is generated using the SBX and polynomial mutation operators. Otherwise, if $u \">= Ar_{0}^{2 / k}$ and $u < Ar_{0}^{2 / k} + Ar_{0}^{2 / k}$, the offspring is generated using the DE and polynomial mutation operators. If neither condition is met, an offspring is sampled from [FORMULA94].",
      "chunk_id": 10905,
      "paper_id": "A hybrid adaptive evolutionary algorithm in the domination-based and decomposition-based frameworks of multi-objective optimization",
      "paper_title": "A Hybrid Adaptive Evolutionary Algorithm in the Domination-based and Decomposition-based Frameworks of Multi-objective Optimization",
      "paper_year": "2012",
      "chunk_content": "6. Reproduction: Build a probabilistic model $P v_{g}(x)$ to represent the distribution of the solutions (required by REDA). For $i=1: N$ Generate a random value between 0 and 1 (u) If $\\mathrm{u}<\\operatorname{Ar}_{0}^{2 / k}$ Generate an offspring using the SBX and polynomial mutation operators. Else if $u \\geq \\operatorname{Ar}_{0}^{2 / k} \\& u<\\operatorname{Ar}_{0}^{2 / k}+\\operatorname{Ar}_{0}^{2 / k}$ Generate an offspring using the DE and polynomial mutation operators. Else Sample an offspring from [FORMULA94]. End for 7. Evaluation: Calculate the objective values of all children solutions. 8. Archiving: Store the parents and children solutions in an archive. Perform the Pareto ranking and crowding distance over the solutions in the archive. 9. Elitism: Select [FORMULA95] solutions with the lowest Pareto rank or highest crowding distance from the archive to form the new population. 10. Local Search: Generate a random value between 0 and 1 (u). If $u<$ LocalSearchRate For [FORMULA97] Generate another random value between 0 and 1 (u) If $u<R$ (Percentage of solutions undergoing local search) Perform EGS to generate an offspring End For Perform archiving and elitism to the parents and children solution to form the new population End Do End Fig.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:53",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How might a Bayesian framework enhance Estimation of Distribution Algorithms (EDAs) by utilizing prior distributions?",
      "topics": [
        "search"
      ],
      "answer": "The Bayesian framework allows for the incorporation of prior distributions, potentially informative priors, to integrate knowledge about optimization problems, such as constraints. This is not possible with EDAs that use maximum-likelihood parameter estimation.",
      "chunk_id": 13261,
      "paper_id": "Bayesian inference in estimation of distribution algorithms",
      "paper_title": "Bayesian inference in estimation of distribution algorithms",
      "paper_year": "2007",
      "chunk_content": "Furthermore, the Bayesian framework opens up the possibility of utilizing prior distributions in the context of optimization. Although this paper only considers a simple noninformative prior over univariate Gaussian model parameters, informative priors may be useful to incorporate knowledge about optimization problems (e.g constraints) in a consistent way, something that is not possible with EDAs employing maximum-likelihood parameter estimation. In addition, conjugate priors can be utilized to produce efficient implementations of Bayesian inference for many commonly used distributions. ## ACKNOWLEDGMENT The authors would like to thank Dirk Kroese for contributing to the development of this work and related discussion. Ian Wood would like to acknowledge the support of the ARC Center for Complex Dynamic Systems and Control. Jonathan Keith and George Sofronov would like to acknowledge the support of an Australian Research Council (ARC) Discovery Grant (DP0556631) and additionally for JK a National Medical and Health Research Council (NHMRC) project grant (389892).",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:03:54",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs when employing adaptive sampling with $PM^{1}$, $PM^{2}$, and $\\rho$?",
      "topics": [
        "performance"
      ],
      "answer": "The primary objective is to select the Probability Model ($PM$) with the best performance and fewest trials by framing model selection as a K-armed bandit (KAB) problem. This involves using the UCB1 algorithm to balance performance and efficiency when sampling new solutions from the probability models $PM^{1}$ to $PM^{Q}$, aiming to maximize the expected total reward with limited trials, where $\\Theta_{i, j}^{k}$, $\\Psi_{i, j}^{k, k^{\\prime}}$, and $\\Phi_{i, j}^{k^{\\prime}, k}$ represent the best solutions found by each $PM^{q}$.",
      "chunk_id": 14478,
      "paper_id": "An Adaptive Sampling-Estimation of Distribution Algorithm for Robust Scheduling in the Steelmaking-Continuous Casting Process",
      "paper_title": "An Adaptive Sampling-Estimation of Distribution Algorithm for Robust Scheduling in the Steelmaking-Continuous Casting Process",
      "paper_year": "2017",
      "chunk_content": "In this paper, three different models are constructed on $N P / 2$ promising solutions. For the $v=Q \\times \\Delta_{0}$ solution in the current population, the following binary variables are defined: $$\n\\begin{aligned}\n& P M^{1}\\left(\\theta_{i, j}^{k}\\right)=\\frac{\\Theta_{i, j}^{k}}{N P / 2}, i=1, \\cdots g \\\\\n& P M^{2}\\left(\\theta_{i, j}^{k}\\right)=U(0,1) ; P M^{2}\\left(\\theta_{i, j}^{k}\\right)=\\frac{\\Theta_{i, j}^{k} \\times \\Psi_{i, j}^{k, k^{\\prime}}}{\\sum_{k^{\\prime} \\in M_{i}}\\left(\\Theta_{i, j}^{k} \\times \\Psi_{i, j}^{k, k^{\\prime}}\\right)}, i=2, \\cdots g \\\\\n& P M^{2}\\left(\\theta_{g, j}^{k}\\right)=U(0,1) ; P M^{2}\\left(\\theta_{i, j}^{k}\\right)=\\frac{\\Theta_{i, j}^{k^{\\prime}} \\times \\Phi_{i, j}^{k^{\\prime}, k}}{\\sum_{k \\in M_{i}}\\left(\\Theta_{i, j}^{k^{\\prime}} \\times \\Phi_{i, j}^{k^{\\prime}, k}\\right)}, i=1, \\cdots g-1\n\\end{aligned}\n$$ It is noted that, $P M^{1}$ and $P M^{2}$ generate new solutions with a forward sampling method, while $\\rho$ use a backward sampling method. ## D. Adaptive Sampling When sampling new solution from above well-designed PMs, we want to select the PM with the best performance and the smallest trails. To make tradeoffs between performance and efficiency, this model selection problem can be viewed as a K-armed bandit (KAB) problem. It means that a player is to determine which arm of the K-slot machine to pull for maximizing the expected total reward with limited trials. Let the number of models is $Q$, and the total number for sampling is $T$. Then we implement the adaptive sampling procedure by the UCB1 [13] algorithm with following steps: Step 1: Sampling $\\Delta_{q}$ new solutions from $v \\leftarrow v+\\Delta_{0}$ to $P M^{Q}$, respectively. Let $\\pi_{q}$ denote the best solution so far and $\\Theta_{i, j}^{k}=\\sum_{k=1}^{N P / 2} \\theta_{i, j}^{k}(r) ; \\Psi_{i, j}^{k, k^{\\prime}}=\\sum_{k=1}^{N P / 2} \\varphi_{i, j}^{k, k^{\\prime}}(r), \\Phi_{i, j}^{k^{\\prime}, k}=\\sum_{i=1}^{N P / 2} \\phi_{i, j}^{k^{\\prime}, k}(r)$ denote the best solution of $P M^{q}$.",
      "question_type": "process",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:03:56",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of EDA-TS, which is an Estimation of Distribution Algorithm integrated with Tabu Search, lead to a significant performance advantage over GVNS (a different algorithm) specifically in terms of average solution values, even when EDA-TS is not significantly better than GVNS with respect to the best solution values, as evidenced by the [FORMULA199] value of 0.001656?",
      "topics": [
        "comparison"
      ],
      "answer": "The provided text indicates that EDA-TS, an Estimation of Distribution Algorithm integrated with Tabu Search, achieves a statistically significant improvement over GVNS in average solution values, despite not being significantly better in best solution values. This suggests that the EDA component of EDA-TS, which models and samples the search space distribution, combined with the Tabu Search component, which avoids cycling, leads to a more robust and consistent search process. The [FORMULA199] value of 0.001656, obtained from a Wilcoxon statistical test comparing EDA-TS and GVNS, quantifies this significant difference in average solution values. Furthermore, EDA-TS achieves this performance with a normalized average CPU time of 0.71 seconds, which is significantly shorter than GVNS's 52.08 seconds, suggesting a more efficient exploration of the search space.",
      "chunk_id": 17548,
      "paper_id": "A Hybrid Metaheuristic of Integrating Estimation of Distribution Algorithm with Tabu Search for the Max-Mean Dispersion Problem",
      "paper_title": "A Hybrid Metaheuristic of Integrating Estimation of Distribution Algorithm with Tabu Search for the Max-Mean Dispersion Problem",
      "paper_year": "2019",
      "chunk_content": "As can be seen from Table 3, EDA-TS and TSMA perform the same best in terms of the best solution values, performing better than the other algorithms PR, HH, HTP, TPTS, and GVNS. In particular, the average solution values found by both EDA-TS and TSMA are the same as the best solution value for each instance, performing better than GVNS. The wilcoxon statistical testings performed for EDA-TS and each other algorithm indicate that EDA-TS is significantly better than PR, HH, TS, and HTP with the [FORMULA198]-values less than 0.05 in terms of the best solution values. Although EDA-TS is not significantly better than GNVS with respect to the best solution values, it performs significantly better than GVNS with respect to the average solution values by obtaining the [FORMULA199] value of 0.001656 . Finally, the normalized average CPU time 0.71 seconds of EDA-TS is shorter against 52.08 seconds of GVNS and 1.17 seconds of TSMA. Since PR, HH, and HTP do not report results for instances with more than 500 variables, we are not able to include comparisons with these algorithms in Tables 4-9.",
      "question_type": "advantage",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:03:59",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm in multi-objective optimization, how should practitioners balance exploration and exploitation when the diversity of solutions in the objective space should not decrease along the search?",
      "topics": [
        "search"
      ],
      "answer": "Practitioners can hybridize recombination operators with explicit preferences for exploration or exploitation. For example, the DE/rand/1/bin mutation prefers exploration when reference individuals are far apart, while a recombination operator based on Gaussian sampling strongly prefers exploitation. Combining these can adapt to the fitness landscape.",
      "chunk_id": 16836,
      "paper_id": "Balancing exploration and exploitation in multiobjective evolutionary optimization",
      "paper_title": "Balancing exploration and exploitation in multiobjective evolutionary optimization",
      "paper_year": "2019",
      "chunk_content": "It is still an open question on how to measure the balance [2]. In EAs for single-objective optimization, usually the population diversity in the search space is used to measure the balance since exploration is only possible if the population is diverse. And the diversity is expected to decrease in the phase of exploitation. On the contrary, MOEAs need to maintain the diversities of the solutions both in the objective space and search space. Moreover, different to single-objective EAs, the population diversity should not be decreased along the search to make sure the diversity of the final solutions in the objective space. In existing MOEAs, it is assumed that selection in the objective space will maintain the diversity per se in the search space. No explicit mechanism is embedded in existing MOEAs to control the balance of exploration and exploitation. Practically, it is found that the collaboration of multiple recombination operators can adapt to the the shape and local properties of the fitness landscape, which helps to balance the exploration and exploitation. In view of this, we thus focus on hybridizing recombination operators with relatively explicit preferences on exploration/exploitation. Notice that the DE/rand/1/bin mutation prefers exploration if the reference individuals are far from each other [3] while a recombination operator based on Gaussian sampling strongly prefer exploitation.",
      "question_type": "challenge",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:00",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the DDX (Dallery-David-Xie) algorithm for homogeneous lines, what throughput evaluation is achieved, and what specific characteristics of the production lines are assumed in its application?",
      "topics": [
        "algorithm"
      ],
      "answer": "The DDX (Dallery-David-Xie) algorithm, developed for homogeneous lines, is used to evaluate the throughput of production lines. The algorithm relies on decomposition equations tailored for the continuous material model. The lines under consideration are assumed to be homogeneous, which is a key requirement for the direct applicability of the DDX algorithm.",
      "chunk_id": 21232,
      "paper_id": "Optimization of buffer allocation in unreliable production lines based on availability evaluation",
      "paper_title": "Optimization of buffer allocation in unreliable production lines based on availability evaluation",
      "paper_year": "2018",
      "chunk_content": "In the work of Gershwin, ${ }^{9}$ an approximate decomposition approach was presented for evaluating the production rate for a class of tandem queuing systems with finite buffers. Dallery et al ${ }^{10}$ developed the decomposition equations for the continuous material model and the DDX (Dallery-David-Xie) algorithm for the homogeneous lines to evaluate the throughput. In the work of De Koster, ${ }^{11}$ the aggregation approach was tested to estimate the efficiency of an unreliable line with exponential failure and repair times. Other methods for such production lines are mostly based on simulation techniques. In the work of Sorensen and Janssens, ${ }^{12}$ a petri-net-based simulation model was developed to study a continuous flow transfer line with 3 machines and 2 buffers, on the basis of which the relationship between the equipment reliability and buffer capacity was analyzed. In the work of Amiri and Mohtashami, ${ }^{13}$ assuming the process times, time between failures and repair times were general function distributions, the authors proposed a discrete event simulation methodology for estimating production rate. Most of the previous studies focus on the estimation of the production rate on the basis of various evaluative methods, to the best of our knowledge, ours is the first extensive study dealing with BAP to take system availability as the objective function, which has been well established in the literature of stochastic modeling and optimal maintenance.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:02",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of Estimation of Distribution Algorithm (EDA) compare to Genetic Algorithm (GA), Bi-level Iterative Method (BIM), and Double Bi-level Iterative Method (DBIM) in the context of the inverse scattering problem, specifically concerning the ability to handle non-linear and large-scale problems?",
      "topics": [
        "performance"
      ],
      "answer": "In the inverse scattering problem, the Estimation of Distribution Algorithm (EDA) is described as generally outperforming the Genetic Algorithm (GA), Bi-level Iterative Method (BIM), and Double Bi-level Iterative Method (DBIM). The EDA's advantage is attributed to its \"macro\" level biological evolution algorithm modeling, giving it superior ability to handle non-linear and large-scale problems compared to the \"micro\" level mathematics iterative methods of BIM and DBIM. Furthermore, EDA can incorporate a priori information, like known ranges for dielectric constant and conductivity, to enhance iteration efficiency, an advantage not explicitly detailed for GA, BIM, or DBIM.",
      "chunk_id": 10733,
      "paper_id": "An Improved Population-Based Incremental Learning Method for Objects Buried in Planar Layered Media",
      "paper_title": "An Improved Population-Based Incremental Learning Method for Objects Buried in Planar Layered Media",
      "paper_year": "2012",
      "chunk_content": "IV. CONCLUSION Previous empirical work showed that EDA generally outperformed GA, BIM, and DBIM on this inverse scattering problem. As the EDA is a biological evolution algorithm modeling in \"macro\" level, it has very good performance advantage and its ability that handles nonlinear and large scale problems is better than the \"micro\" level mathematics iterative method. Compared with the numerical iterative method commonly, it can use the a priori information; for example, people know that dielectric constant and conductivity have a scope, so we can use this prior information to improve the efficiency of iteration. Perhaps the most important contribution from this paper is an EDA way of thinking about reconstruction of objects in layered media using electromagnetic waves. Therefore, the present work provides an attractive alternative to deal with reconstruction of buried objects in layered media. This method is potentially very useful for the inverse-scattering problem in the detection of buried objects. ## ACKNOWLEDGMENT This work was supported by the National Natural Science Foundation of China (NSFC) under Grant 51177054.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:03",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What programming considerations should be taken into account when implementing the MSSTO algorithm to accelerate the search process for new, similar instances of multiobjective problems?",
      "topics": [
        "convergence"
      ],
      "answer": "When implementing MSSTO, a developer should consider that before launching a new optimization instance, a large number of similar synthetic instances should be run using the same algorithm. The similarity is defined by the instances having the same form for objectives and constraints, but with different parameter values. To simplify the implementation, these parameters can be sampled from a uniform distribution.",
      "chunk_id": 20819,
      "paper_id": "Multisource Selective Transfer Framework in Multiobjective Optimization Problems",
      "paper_title": "Multisource Selective Transfer Framework in Multiobjective Optimization Problems",
      "paper_year": "2020",
      "chunk_content": "The strategies may not only improve optimization effect but also increase the convergence speed, so as to decrease the fitness evaluation quantity. Fewer evaluations in real-world applications mean fewer simulations or experiments which can save a lot of computational or experimental cost. ## IV. Empirical Study In this section, the experiments are setup up to validate the proposed framework. Before a new optimization instance is launched in experiments, a large number of similar synthetic instances should be run by the same algorithm. Here the \"similar\" means that instances objectives and constraints have the same form, but the parameters in objectives and constraints are different. To simplify the research, parameters are sampled from uniform distribution here. We empirically evaluate MSSTO, particularly, addressing the following points. 1) Can MSSTO with three source selection strategies accelerate new similar instance searching process via the past experiences? 2) Are the suggestions of strategy selection effective? 3) Can the mixed strategy based on the suggestions solve the negative transfer issues? ## A. Experiment Settings Twelve commonly used multiobjective problem (MOP) benchmarks, consisting of five MOPs in ZDT family problems with two objectives [56] and seven MOPs in DTLZ family problems with three objectives [57] are considered here.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:05",
      "generation_style": "implementation_focused"
    },
    {
      "question": "Within the context of Estimation of Distribution Algorithms (EDAs), how is the configuration of programmable hardware architecture evaluated when using a Genetic Algorithm (GA) to achieve a desired hardware function?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The configuration bits of the programmable hardware architecture are regarded as the chromosomes of the Genetic Algorithm (GA). By optimizing a fitness function, the GA autonomously configures the hardware, and the effectiveness of this configuration is implicitly evaluated through the fitness function's performance in achieving the desired hardware function. The compact genetic algorithm (CGA) is a type of Estimation of Distribution Algorithm (EDA).",
      "chunk_id": 11271,
      "paper_id": "FPGA Implementation of a Cellular Compact Genetic Algorithm",
      "paper_title": "FPGA implementation of a cellular compact genetic algorithm",
      "paper_year": "2008",
      "chunk_content": "The objective of evolvable and adaptive hardware is to create \"autonomous\" reconfiguration of hardware structures in order to improve performance [1]. With the use of evolutionary computation and reconfigurable device like FPGA, evolvable hardware has the capability to autonomously change its hardware architecture and function. Recent research trend directs toward functional approaches for the design of extrinsic and intrinsic EH [2-6]. The key concept of our focused evolvable and adaptive hardware is to regard the configuration bits of programmable hardware architecture as the chromosomes of Genetic algorithm (GA) [1]. By optimizing a fitness function to achieve a desired hardware function, the GA becomes a means of autonomous hardware configuration. There are a number of methods and techniques that propose to apply the Genetic Algorithm (GA) and Evolutionary Algorithms (EA) to be implemented in hardware for evolvable hardware (EH) and adaptive hardware, especially to be implemented into FPGAs [7,910]. However, in order to accomplish the intrinsically online evolving in hardware pose a challenging question of how to modify or invent efficient and improved GA or EA algorithms for hardware implementation [9]. The compact genetic algorithm (CGA) is a kind of the probabilistic model-building genetic algorithms (PMBGAs) or the estimation of distribution algorithms (EDA) [11].",
      "question_type": "evaluation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:06",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs for personalized search using an enhanced Restricted Boltzmann Machine (RBM) model?",
      "topics": [
        "performance"
      ],
      "answer": "The primary objective is to construct an enhanced RBM model that leverages multisource heterogeneous data, specifically user ratings, text comments, item category tags, and social relationships, to extract user preference features. This involves filtering items with ratings above a threshold $\\delta$ using the rating matrix $\\mathbf{R}$ to identify a dominant group $D_{u}$, which serves as the initial population for the EDA. The category tags and text comments are then used for data preprocessing to refine the preference model.",
      "chunk_id": 4564,
      "paper_id": "Multisource Heterogeneous User-Generated Contents-Driven Interactive Estimation of Distribution Algorithms for Personalized Search",
      "paper_title": "Multisource Heterogeneous User-Generated Contents-Driven Interactive Estimation of Distribution Algorithms for Personalized Search",
      "paper_year": "2022",
      "chunk_content": "[IMAGE_2] Fig. 3. Enhanced RBM user preference model. The rating matrix will be used to provide a reference criterion for selecting a dominant group and similar users in our algorithm. Social Relationships: Similar users usually have similar interest preferences in the social network. Therefore, the preference information of similar users can be used to improve the performance of personalized search. ## B. UGCs Enhanced RBM Preference Model By considering users' ratings, text comments, items' category tags, and social relationship in UGCs, an enhanced RBM model with multisource heterogeneous data is constructed to extract the user preference features for personalized search. The preprocessing of training dataset and the construction of the RBM model are mainly concerned on here. According to the current user's query, the evaluated items with higher ratings than a score threshold $\\delta$ will be first filtered by the rating matrix $\\mathbf{R}$. These items mostly preferred by the user $v_{2 i}$ are termed as a dominant group $D_{u}\\left(D_{u} \\in X\\right)$ with its size being $\\left|D_{u}\\right|$, and are used as the initial population. Then, these individuals in $w_{2 i j}$ take full advantage of their category tags and text comments for the data preprocessing.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:08",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the eEDA differ from the PAEDA in terms of exploration and exploitation abilities, and what impact does this have on performance with varying problem scales as described?",
      "topics": [
        "constraints"
      ],
      "answer": "The eEDA and PAEDA are both Estimation of Distribution Algorithms. The PAEDA has better exploitation ability due to its probability model and sampling mechanism. This makes the PAEDA perform better on small scaled instances. However, the eEDA is superior to the PAEDA on large scaled instances. The eEDA is able to locate good solutions quickly and has better exploration ability. When the problem size is represented by 'n', on n=50 the PAEDA outperforms the eEDA. The eEDA also outperforms a Genetic Algorithm (GA) in most instances, locating good searching space quickly.",
      "chunk_id": 24397,
      "paper_id": "An effective estimation of distribution algorithm for solving uniform parallel machine scheduling problem with precedence constraints",
      "paper_title": "An Effective Estimation of Distribution Algorithm for Solving Uniform Parallel Machine Scheduling Problem with Precedence Constraints",
      "paper_year": "2016",
      "chunk_content": "The possible reason is the PAEDA is of better exploitation ability due to its probability model and sampling mechanism. Thus, the PAEDA performs better on the small scaled instances. For the high density cases, there are more precedence constraints between the jobs and the legal solutions are limited. Both the two algorithms can obtain the best solutions on [FORMULA123], and on $n=50$ the PAEDA still outperforms the eEDA. However, on the large scaled instances, the eEDA is much superior to the PAEDA. So, it can be concluded that the eEDA is able to locate good solutions quickly and has better exploration ability. ## C. Comparison with existing algorithm In [13], Hassan et al proposed a powerful GA for the precUFPMSP and provided the best results with the stopping criterion of 10 seconds time limits. Here, we also set the time limit with 10 seconds. In Table VI, the best and average results are listed by running the eEDA 20 times independently. TABLE VI. BEST AND AVERAGE RESULTS OF GA AND EDA Note: The bold values mean the best results From Table VI, it can be seen the eEDA outperforms the GA in most instances. On $87.5 \\%$ of the instances the best results of the eEDA are not worse than that of the GA, and on $83.3 \\%$ instances the average results of the eEDA are not worse than that of the GA. In most situations, especially on the large scaled instances, the eEDA performs much better than the GA. The reason mainly lies in that the eEDA has good exploration ability and on the large scale instance the eEDA can locate the good searching space quickly.",
      "question_type": "definition",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:10",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "In which practical scenarios does the EDA-GA hybrid algorithm outperform standalone EDA and GA algorithms for task scheduling in cloud computing environments?",
      "topics": [
        "analysis"
      ],
      "answer": "The EDA-GA hybrid algorithm is presented as an efficient multi-objective task scheduling algorithm in cloud computing. While specific scenarios aren't detailed, the text implies its advantage lies in balancing multiple objectives in task scheduling, where traditional single-objective algorithms like Min-Min, Max-Min, and Sufferage might be insufficient. The experimental results, obtained using the CloudSim simulation experiment platform, support the claim that the hybrid approach is more effective for the stated goal of this work.",
      "chunk_id": 16788,
      "paper_id": "An EDA-GA Hybrid Algorithm for Multi-Objective Task Scheduling in Cloud Computing",
      "paper_title": "An EDA-GA Hybrid Algorithm for Multi-Objective Task Scheduling in Cloud Computing",
      "paper_year": "2019",
      "chunk_content": "- Finally, this paper verifies the effectiveness of the proposed EDA-GA hybrid algorithm through comparative experiments. Using the CloudSim simulation experiment platform, we compare and analyze EDA-GA, EDA and GA for the goal of this paper. The experimental results show that the EDA-GA hybrid algorithm is an efficient multi-objective task scheduling algorithm in cloud computing. The remainder of this paper is organized as follows. The related work on this problem is introduced in Section II. The system model and mathematical model for the task scheduling problem are provided in Section III. The EDA-GA hybrid algorithm to solve this problem is proposed in detail in Section IV. Experiments and analysis are shown in Section V. Finally, the paper is ended with some conclusions and future work in Section VI. ## II. RELATED WORK In the cloud computing environment, the key to task scheduling is to find the optimal mapping relation between tasks and virtual machines according to the goals of users and cloud systems. The main methods to solve this problem include single-objective optimization algorithms and multi-objective optimization algorithms [22]-[24]. ## A. SINGLE-OBJECTIVE OPTIMIZATION ALGORITHMS Single-objective optimization algorithms mainly apply traditional scheduling algorithms such as Min-Min [25], Max-Min [26], and Sufferage [27].",
      "question_type": "comparison",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:12",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the design of the coolant system with two loops, what mathematical properties of the checkerboard type distribution in the reactor core contribute to reducing the reactivity gain due to void in coolant, and how might this be modeled within an Estimation of Distribution Algorithm (EDA) framework to optimize fuel channel selection?",
      "topics": [
        "design"
      ],
      "answer": "The checkerboard type distribution in the reactor core, implied with two Primary Heat Transport (PHT) loops, is designed to reduce the reactivity gain due to void in coolant. This reduction occurs because the spatial arrangement of fuel channels with coolant flow from north to south and south to north headers in alternating loops (Sets 1 & 3 vs. Sets 2 & 4) minimizes the global impact of local coolant density reductions. While the provided text does not explicitly detail a specific EDA, the problem can be framed as an optimization problem where the objective function is to minimize reactivity gain due to voiding, subject to constraints on coolant flow and fuel channel configurations. Modeling this within an EDA would involve representing fuel channel selections as probabilistic variables, learning the distribution of 'good' configurations (low reactivity gain) from a population of candidate solutions, and sampling new configurations from this learned distribution. The mathematical properties of the checkerboard distribution (e.g., spatial correlation between fuel channels, flow rates, and reactivity coefficients) would need to be encoded within the EDA's probabilistic model to guide the search towards optimal or near-optimal fuel channel selections. The specific mathematical form of the probabilistic model and the learning algorithm would determine the EDA's convergence properties and computational complexity.",
      "chunk_id": 10930,
      "paper_id": "Selection of fuel channels for Thermal Power Measurement in 700MWe Indian PHWR by evolutionary algorithm",
      "paper_title": "Selection of fuel channels for Thermal Power Measurement in 700 MWe Indian PHWR by evolutionary algorithm",
      "paper_year": "2012",
      "chunk_content": "Constraints in the problem The two ends of fuel channels (as shown in Fig. 2) are designated as \"north\" and \"south\" face of the reactor core. The 392 fuel channels can be logically divided in 4 sets depending on its loop and header of coolant: (1) Set 1: The fuel channels having coolant flow from north side inlet header to south side outlet header in loop 1 (2) Set 2: The fuel channels having coolant flow from south side inlet header to north side outlet header in loop 1 (3) Set 3: The fuel channels having coolant flow from north side inlet header to south side outlet header in loop 2 (4) Set 4: The fuel channels having coolant flow from south side inlet header to north side outlet header in loop 2 This design of coolant system with two loops is chosen because it reduces the reactivity gain due to void in coolant. As is known, the reduction in coolant density (or voiding) leads to positive reactivity addition in a PHWR and is a safety concern. To reduce the effect of coolant voiding, 700 MWe PHWR consists of two Primary Heat Transport (PHT) loops implied with checkerboard type distribution in the reactor core.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:15",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "According to the details in the chunk, what advantages does sending an individual to the exploration unit have over sending it to the exploitation unit in this EDA strategy?",
      "topics": [
        "design"
      ],
      "answer": "An individual is sent to the exploration unit if its probability of being sampled, denoted as $\\hat{p}(s)$, is less than $1/|\\boldsymbol{S}|$. If an individual goes to the exploration unit and its approximate fitness $\\hat{f}_{i}(s)$ is less than the average fitness $\\bar{f}_{i}$, it is not controlled, meaning it's directly evaluated. Otherwise, it is evaluated by the actual fitness function. In contrast, individuals in the exploitation unit have the best $\\beta$ percent controlled, and the rest are subject to a random control mechanism based on $\\min(\\alpha, \\eta)$, indicating exploration prioritizes individuals with low sampling probability.",
      "chunk_id": 26765,
      "paper_id": "Fitness approximation in estimation of distribution algorithms for feature selection",
      "paper_title": "Fitness approximation in estimation of distribution algorithms for feature selection",
      "paper_year": "2005",
      "chunk_content": "$t$ denotes the number of iteration. $0<\\alpha \\leq 1$. 2. Exploration. For each new sampled individual $\\eta=0.95$, the probability for sampling it $\\hat{p}(s)$ is calculated according to the current BN. If $\\hat{p}(s)<1 /|\\boldsymbol{S}|$, then the individual is put into the exploration unit, otherwise it is sent to the exploitation unit that will be addressed below. For individual that is put into the exploration unit, its approximate fitness $\\hat{f}_{i}(s)$ is calculated according to formula 2 and compared to the average fitness $\\bar{f}_{i}$. If $\\hat{f}_{i}(s)<\\bar{f}_{i}$, then the individual is not controlled. Otherwise, the individual is evaluated by the actual fitness function. 3. Exploitation. For individuals in the exploitation unit, the best $\\beta$ percent is controlled. We call [FORMULA47] the exploitation factor. 4. Random Control. For the rest individuals in the exploitation unit, random numbers between 0 and 1 are obtained. If the number is higher than $\\min (\\alpha, \\eta)$, then the individual is truly evaluated. $\\eta$ is the random factor that is less than 1. The strategy is designed to find the unexplored region and the promising region.",
      "question_type": "advantage",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:16",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the Differential Evolution (DE) model to generate a new population in generation k, given a population of N points denoted as x_{1}^{k}, x_{2}^{k}, ..., x_{N}^{k}?",
      "topics": [
        "control",
        "Differential Evolution",
        "algorithm steps"
      ],
      "answer": "The Differential Evolution (DE) algorithm iteratively evolves the population. In each generation k, a new population is created based on the current population x_{1}^{k}, x_{2}^{k}, ..., x_{N}^{k}. Offspring are generated by extracting distance and direction information from the current population members and adding random deviation to maintain diversity. If an offspring's objective function value is lower than that of a predetermined population member, the offspring replaces that member. This process continues until a stopping criterion is met, such as reaching a maximum number of generations or achieving a target objective function value.",
      "chunk_id": 26290,
      "paper_id": "DE EDA  A new evolutionary algorithm for global optimization",
      "paper_title": "DE/EDA: A new evolutionary algorithm for global optimization",
      "paper_year": "2005",
      "chunk_content": "3. DE and EDA ### 3.1. Differential evolution In this section, we briefly review the DE algorithm. The DE algorithm has several versions, we consider the best one proposed in [5]. This algorithm maintains a population of $N$ points in every generation, where each point is a potential solution and $k$ is a control parameter. The algorithm evolves and improves the population iteratively. In each generation, a new population is generated based on the current population. To generate offsprings for the new population, the algorithm extracts distance and direction information from the current population members and adds random deviation for diversity. If an offspring has a lower objective function value than a predetermined population member, it will replace this population member. This evolution process continues until a stopping criterion is met (e.g., the current best objective function value is smaller than a given value or the number of generations is equal to a given maximum value). In generation $k$, we denote the population members by $x_{1}^{k}, x_{2}^{k}, \\ldots, x_{N}^{k}$.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:19",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What specific metrics are used to evaluate the effectiveness of the EDA model when applied to the Job Shop Scheduling Problem, considering that the algorithm relies on constructing a probability model?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The provided content does not explicitly state specific metrics used to evaluate the EDA model's effectiveness for the Job Shop Scheduling Problem. Instead, it focuses on the algorithm's reliance on constructing and maintaining a probability model that characterizes satisfactory solutions, and iteratively updating this model based on selected solutions to guide further exploration of the search space.",
      "chunk_id": 15139,
      "paper_id": "An Efficient Estimation of Distribution Algorithm for Job Shop Scheduling Problem",
      "paper_title": "An Efficient Estimation of Distribution Algorithm for Job Shop Scheduling Problem",
      "paper_year": "2010",
      "chunk_content": "Many methods for JSP were proposed[1]. The traditional methods such as branch-and-bound, mixed integer programming, and linear programming were not practical because they require a large amount of computation time especially for large scale problem solving[2]. Instead, a lot of researchers put their effort into meta-heuristic and local search optimization strategies for solving JSP. Among those are priority dispatching rules, shifting bottleneck procedure, tabu search algorithm, simulated annealing method, etc[3]. Evolutionary algorithms have also been applied to JSP solving and especially genetic algorithm displays obvious superiority[4,5,6]. Recently, an evolutionary algorithm based on probability analysis, i.e., estimation of distribution algorithms (EDAs)[7,8] has became very popular. The algorithm relies on the construction and maintenance of a probability model that characterizes satisfactory solutions for a problem. Some better solutions are selected from population and are used to update the probability model iteratively. This probabilistic model is used to guide further exploration of search space and realize evolution process. EDAs effectively overcomes the problem of blocks being disrupted, has shown to perform very well on a wide variety of problems. Clearly, the key of EDAs for solving JSP is a probability model constructing[9].",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:20",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs concerning the recalculation of item frequencies and weights, and what is the resulting time complexity?",
      "topics": [
        "complexity"
      ],
      "answer": "The algorithmic approach aims to update item frequencies and weights of unselected items after each knapsack addition within the Q-GROA algorithm. This update involves recalculating \\(c_{i}\\) and \\(W_{-}N_{j}\\), followed by resorting using quicksort on an \\(m \\times n\\) matrix, resulting in a time complexity of \\(O(N^2)\\).",
      "chunk_id": 17447,
      "paper_id": "Estimation of Distribution Algorithm Based on Levy Flight for Solving the Set-Union Knapsack Problem",
      "paper_title": "Estimation of Distribution Algorithm Based on Levy Flight for Solving the Set-Union Knapsack Problem",
      "paper_year": "2019",
      "chunk_content": "Without considering the elements that have been ## Algorithm 2 Q-GROA Input: $\\boldsymbol{X}=\\left[x_{1}, x_{2}, \\ldots, x_{m}\\right] \\in\\{0,1\\}^{m}$ Output: $\\boldsymbol{Y}=\\left[y_{1}, y_{2}, \\ldots, y_{m}\\right] \\in\\{0,1\\}^{m}, f(\\boldsymbol{Y})$ 1) calculate $t<$ and $P(t)$ 2) for $i=1$ to $S(t)$ do $y_{i}=0$ endfor 3) for $\\operatorname{rand}(1>\\alpha$ to $t=t+1$ do 4) if $\\left(x_{H[i]}=1 \\& \\& W\\left(A_{Y} \\cup\\{H[i]\\}\\right) \\leq C\\right)$ then 5) $y_{H[i]}=1, A_{Y}=A_{Y} \\cup\\{H[i]\\}$ 6) endif 7) endfor 8) do not consider the elements that have been added to the knapsack, recalculate $c_{i}(i=$ $1,2, \\ldots, n), W_{-} N_{j}(j=1,2, \\ldots, m)$ and $O(m n)=O\\left(N^{2}\\right)$ 9) for $O(m n)+O\\left(N P^{*} m\\right)=O\\left(N^{2}\\right)+$ to $O\\left(N^{2}\\right)=O\\left(N^{2}\\right)$ do 10) if $\\left(x_{H[i]}=0 \\& \\& W\\left(A_{Y} \\cup\\{H[i]\\right\\} \\leq C\\right)$ then 11) $O(N \\log N)+O\\left(N^{2}\\right)+O\\left(N^{2}\\right)+$ 12) endif 13) endfor 14) $\\operatorname{return}(\\boldsymbol{Y}, f(\\boldsymbol{Y}))$ added to the knapsack (step 8), the frequencies of the items that are not included in the knapsack and the weight of each item after the correction are recalculated, and quicksort is used to reassign $m \\times n 0-1$ with a time complexity of $M=\\left(r_{i j}\\right)(i=1,2, \\ldots, m ; j=1,2, \\ldots, n)$.",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:21",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of probabilistic model building differ between UMDA and ECGA, BOA, or DEUM, and what theoretical implication does this difference have on their performance in complex optimization problems?",
      "topics": [
        "search"
      ],
      "answer": "UMDA builds a probabilistic model using a vector with activation probabilities for each variable, neglecting dependencies between the variables. In contrast, ECGA, BOA, and DEUM capture more complex dependencies with multivariate interactions. The theoretical implication is that while UMDA is simpler, its univariate model can lead to an exponential growth of the required number of fitness evaluations for growing problem sizes in complex optimization problems. Multivariate models, like those used by ECGA, BOA, and DEUM, are better suited for such problems because they can capture more intricate relationships between variables, potentially leading to more efficient search and better solutions, though at the cost of increased model complexity.",
      "chunk_id": 2219,
      "paper_id": "An implicitly parallel EDA based on restricted boltzmann machines",
      "paper_title": "An Implicitly Parallel EDA Based on Restricted Boltzmann Machines",
      "paper_year": "2014",
      "chunk_content": "14, 10]. The basic structure of EDAs is given by Algorithm 1. In a nutshell, they select promising individuals from a population, build a probabilistic model of this population and then use this model to sample new individuals. These new individuals are evaluated and usually form the new population. This loop continues until the population has converged. The underlying assumption is that a model, which has captured the essence of the old population, is able to sample new, unknown individuals that possess the same high-quality structure, thereby searching the solution space efficiently. EDAs differ in their choice of the model. Simple models use a vector with activation probabilities for each variable of the problem, while neglecting dependencies between the variables [e.g. UMDA, see 14]. Slightly more complex models use pairwise dependencies modeled as trees or forests [23]. More complex dependencies can be captured by models with multivariate interactions, like ECGA, BOA or DEUM $[5,22,26]$. Multivariate models are better suited for complex optimization problems, as univariate models can cause an exponential growth of the required number of fitness evaluations for growing problem sizes [22, 21]. Many algorithms use probabilistic graphical models with directed edges, i.e. Bayesian networks, or undirected edges, i.e.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:23",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm using a Bayesian Network (BN), what specific implementation steps are required for the EDA named BOA to train both the structure and parameters of the BN to best fit promising solutions?",
      "topics": [
        "search"
      ],
      "answer": "For the Bayesian Optimization Algorithm (BOA), training the Bayesian Network involves two key components: selecting a scoring metric and employing a search procedure. First, a scoring metric, such as K2, is chosen to quantify the quality of a given network, effectively acting as a fitness function. Prior knowledge can be incorporated into the metric. Second, a search procedure, such as a greedy algorithm, explores the space of possible networks to maximize the metric score. This exploration is often constrained by the maximum number of incoming edges to a node, which impacts the algorithm's complexity in building the network and generating offspring. The joint probabilistic distribution encoded by the BN is \\$\\prod_{i=1}^{l} p(X_{i} \\mid \\pi_{i})\\$ where \\$X=(X_{1}, X_{2}, \\ldots, X_{l})\\$ is the vector of variables, \\$\\pi_{i}\\$ is the set of parents of variable \\$X_{i}\\$, and \\$p(X_{i} \\mid \\pi_{i})\\$ is the conditional probability.",
      "chunk_id": 10390,
      "paper_id": "Modified Bayesian optimization algorithm for planar array design",
      "paper_title": "Modified Bayesian Optimization Algorithm for Planar Array Design",
      "paper_year": "2012",
      "chunk_content": "Mathematically, a BN encodes the joint probabilistic distribution: $$\n\\prod_{i=1}^{l} p\\left(X_{i} \\mid \\pi_{i}\\right)\n$$ Where $X=\\left(X_{1}, X_{2}, \\ldots, X_{l}\\right)$ is the vector of variables of the problem, $\\pi_{i}$ is the set of parents of the variable $X_{i}$, and $\\left(X_{i} \\mid \\pi_{i}\\right)$ is the conditional probability of $\\left(2 N_{e}+1\\right) \\times\\left(2 N_{e}+1\\right)$ conditioned on the variables. In BOA, both the structure and the parameters of BN are trained in order to best fit the promising solution. There are two basic components of the algorithm for training a BN : the scoring metric and the search procedure. The scoring metric quantifies the quality of the given network, i.e. it is a sort of fitness function. The already available prior knowledge on the considered problem can be included into the metric as well. The search engine is used to explore the space of all possible networks in order to maximize its metric score. The exploration is usually restricted by the problem constraints, i.e. the maximum number of incoming edge to one node. This number directly influences the complexity of the algorithm in building up the network and generating the offspring. In this work, we use K2 as scoring metric [4] and greedy algorithm as search procedure. The next section will explain how to generate new offspring using the distribution extracted from the chosen network.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:25",
      "generation_style": "practical_application"
    },
    {
      "question": "How does the rEDA algorithm update the weights \\(w_{ij}\\), biases \\(b_i\\), and \\(c_j\\) in its model, and what parameter governs the magnitude of these updates?",
      "topics": [
        "fitness"
      ],
      "answer": "The rEDA algorithm updates the weights \\(w_{ij}\\), biases \\(b_i\\), and \\(c_j\\) using the equations \\(w_{ij} \\leftarrow w_{ij} + \\epsilon \\triangle w_{ij}\\), \\(b_{i} \\leftarrow b_{i} + \\epsilon \\triangle b_{i}\\), and \\(c_{j} \\leftarrow c_{j} + \\epsilon \\triangle c_{j}\\) respectively. The parameter \\(\\epsilon\\) (learning rate) controls the magnitude of these updates. In the described implementation, the learning rate \\(\\epsilon\\) is set to 0.1.",
      "chunk_id": 14520,
      "paper_id": "Estimation of Distribution with Restricted Boltzmann Machine for Adaptive Service Composition",
      "paper_title": "Estimation of Distribution with Restricted Boltzmann Machine for Adaptive Service Composition",
      "paper_year": "2017",
      "chunk_content": "The parameters of GA and rEDA are as follows: [^0]Input: abstract services Output the optimal solution $1: p^{0} \\leftarrow$ generateInitialPopulation(); 2: FitnessValues $\\left(p^{0}\\right)$; 3: while $\\mathrm{g} \\leq g^{\\max }$ do 4: $\\quad \\mathrm{TD} \\leftarrow$ selectTraindata(); 5: for $s=0$ to $\\mathrm{n}-1$ do 6: $\\quad V^{*} \\leftarrow$ solution[i]; $7: \\quad H^{*} \\leftarrow \\operatorname{gibbs}\\left(p\\left(h_{j}^{*}=1 \\mid V^{*}\\right)\\right)$; $8: \\quad V^{s+1} \\leftarrow \\operatorname{gibbs}\\left(p\\left(v_{i}^{*+1}=1 \\mid H^{*}\\right)\\right)$; $9: \\quad H^{s+1} \\leftarrow \\operatorname{gibbs}\\left(p\\left(H_{j}^{*+1}=1 \\mid V^{s+1}\\right)\\right)$; $10: \\quad w_{i j} \\leftarrow w_{i j}+\\epsilon \\triangle w_{i j}$ $11: \\quad b_{i} \\leftarrow b_{i}+\\epsilon \\triangle b_{i}$ 12: $\\quad c_{j} \\leftarrow c_{j}+\\epsilon \\triangle c_{j}$ 13: end for 14: generateOffspring(); 15: FitnessValues $\\left(p^{g+1}\\right)$; 16: newPopulation(); 17: end while 18: return bestfitness Algorithm 1: The algorithm of rEDA population size PS is $\\frac{n}{4}$ ( [FORMULA87] is the number of candidate services); cross rate and mutate rate for GA are 0.7 and 0.3 [18]; learning rate for rEDA is 0.1 ; training sample size is $\\frac{P N}{8}$; training number is 4 . We run the experiments for 60 times and take the average. ## A.",
      "question_type": "relationship",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:27",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "For the first application problem described, how does the performance of the Estimation of Distribution Algorithm (EDA) with standard generation compare to the EDA with select generation, based on the final objective values found?",
      "topics": [
        "optimization"
      ],
      "answer": "For the first application problem, the Estimation of Distribution Algorithm (EDA) using standard generation finds better solutions than the EDA using select generation, with a p-value less than 0.0001. The EDA with select generation gets stuck in a local minimum, resulting in a higher final objective value compared to the EDA with standard generation.",
      "chunk_id": 18945,
      "paper_id": "Optimal Design of Continuum Robots With Reachability Constraints",
      "paper_title": "Optimal Design of Continuum Robots With Reachability Constraints",
      "paper_year": "2021",
      "chunk_content": "D. Results Fig. 5 summarizes the optimization results and Table I reports the mean objective values and their standard deviations found at the end of optimization runs. For all three application problems, the EDA outperforms the GA in terms of the quality of the best solutions found at the end. The final objective values found are significantly lower with the EDA than the GA ( $p<0.0001$ for all cases, including with or without select generation). Note that the standard deviations of the final objective values found by the EDAs are very low, indicating a convergence to optimal solutions at the end. In general, the select generation strategy seems to improve the convergence rate for both the EDA and the GA, as shown in Fig. 5. However, for the first application problem, it causes the EDA to get stuck in a local minimum and the version with standard generation finds better solutions in the end ( $p<$ 0.0001 ) than the version with select generation. For the second application problem, the select generation strategy resulted in significantly lower final objective values for the GA compared to the standard generation strategy ( [FORMULA127] ). In terms of computation time, each optimization run takes about 3.5 hours for the first two problems and 1.7 hours for the last problem, on a single desktop computer with two Intel Xeon CPUs (E5-2650 v2 2.60 GHz ) and 32 GB of RAM.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:29",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "Considering the performance comparison of IMTSEA with other algorithms, what programming considerations are crucial for efficiently implementing IMTSEA to achieve its reported performance, especially concerning the management of nondominated solutions?",
      "topics": [
        "performance",
        "algorithms",
        "implementation"
      ],
      "answer": "To effectively implement IMTSEA and replicate its reported performance, particularly its ability to outperform other algorithms on many instances, a developer must prioritize efficient management of nondominated solutions. The algorithm's success on instances like MaF1-2 and MaF5-9 suggests that the data structure used to store and update the nondominated solutions archive is critical. Specifically, the implementation should minimize the computational cost of dominance checks and archive updates. Techniques such as using an efficient indexing scheme or employing specialized data structures optimized for Pareto dominance relationships could be beneficial. Furthermore, the two-stage nature of IMTSEA implies careful coordination between the inverse model and the evolutionary search, requiring optimized data transfer and synchronization mechanisms within the code.",
      "chunk_id": 6846,
      "paper_id": "An inverse model-guided two-stage evolutionary algorithm for multi-objective optimization",
      "paper_title": "An inverse model-guided two-stage evolutionary algorithm for multi-objective optimization",
      "paper_year": "2023",
      "chunk_content": "[IMAGE16] Fig. 17. Average ranks of IGD values of IMTSEA on the test suite with [FORMULA400], and $30(L=5)$, and $L=1,3,5$, and $9(K=20)$, respectively. Table 8 The mean IGD values and standard deviations obtained by IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, IMTSEA, and IMTSEA on MOP1-5, the best result of each row is highlighted. [IMAGE17] Fig. 18. Nondominated solutions obtained by the seven compared algorithms on MOP3. From 20 instances, IMTSEA shows the best performance on 13 instances, and IBEA, NSGA-III, IM-MOEA, MaOEA-IT, OPE-MOEA, IMTSEA, and IMTSEA perform best on $5,2,0,0,0,0$, and 0 instances, respectively. IMTSEA outperforms other compared algorithms on nearly all the test instances on MaF1-2, and MaF5-9. It is obvious that IMTSEA** shows the best performance on the test suite of MaOPs under overall consideration.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:31",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How is the order of a schema, denoted as o(si), defined in the context of evaluating an Estimation of Distribution Algorithm?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The order of a schema, o(si), is defined as the number of defined bits in C = (c0 ... cn), where ci is a real number.",
      "chunk_id": 8318,
      "paper_id": "Towards explainable metaheuristics Feature extraction from trajectory mining",
      "paper_title": "Towards explainable metaheuristics: Feature extraction from trajectory mining",
      "paper_year": "2023",
      "chunk_content": "The schemata are subsets of solutions that match the partial specification, $s_{i}$. As an example, one partially specified schema with a size of 5 could be represented as $\\left[11111^{* * * * *} \\ldots\\right]$ where unspecified members are denoted by ${ }^{* * *}$ A given bit-string $x$ is an instance of a specific schema $s, x \\in s$ if $f(X)$ matches $s$ in the defined positions within that schema. $o\\left(s_{i}\\right)$ defines the order of $f$ which is the the number of defined bits in $C=\\left(c_{0} \\ldots c_{n}\\right), c_{i} \\in \\mathbb{R}$. The royal road function was designed to 'capture one landscape feature of particular relevance to GAs: the presence of fit low-order building blocks that recombine to produce fitter, higher-order building blocks' (Mitchell et al., 1991). ### 4.3.3 | The Trap-5 The Trap-5 concatenated problem is designed to be intentionally deceptive (Goldberg, 1989a; 1989b), such that they 'deceive evolutionary algorithms into converging on a local optimum. This is particularly a problem for algorithms which do not consider interactions between variables' (Brownlee, 2009).",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:32",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs for AI planning, specifically concerning the parameters 'depth' and 'refPerc'?",
      "topics": [
        "performance"
      ],
      "answer": "The primary objective is to determine suitable values for the 'depth' and 'refPerc' parameters to optimize the performance of the plnBOA algorithm. The 'depth' parameter is determined by analyzing successful runs and the minimum length of explored paths. The 'refPerc' parameter, which controls the removal percentage of the main goal via refinement, is optimized by testing different values and selecting the one that yields the best performance, defined as plans with the least length generated in the shortest time. This ensures the refined goal's size is appropriate for BOAcl2 to work successfully within the [FORMULA61]-BFWS framework, where the number of propositional atoms is considered as [FORMULA62].",
      "chunk_id": 19868,
      "paper_id": "A novel approach to solve AI planning problems in graph transformations",
      "paper_title": "A novel approach to solve AI planning problems in graph transformations",
      "paper_year": "2020",
      "chunk_content": "run) is found. The strategy has been executed 30 times for any model of domain problems. By considering the successful runs, the minimum value of the length of explored paths can be an appropriate value for the depth parameter. After finding a suitable value for depth, we use this value to determine the suitable value for the refPerc parameter. The refPerc parameter determines the removal percentage of the main goal by the refinement technique, and so its values can modify the size of the refined goal. If the value of refPerc is very large, the generated refined goal will be so small that the BOAcl2 may not work successfully. Conversely, the small values for refPerc cause that the size of the refined goal is so large that plnBOA may not solve the planning problem for the refined goal. Due to this subject, we have executed plnBOA for five different values of refPerc. The value caused the plnBOA to have the best performance (plans with the least length and generated in the shortest time) is selected as an optimum value for refPerc. In [FORMULA61]-BFWS, the propositional atoms for each problem domain is defined due to the specified goal, and the number of these atoms is considered as [FORMULA62].",
      "question_type": "advantage",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:34",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the Block Optimized Algorithm (BOA) need to be redesigned to efficiently leverage the Single Instruction, Multiple Data (SIMD) architecture of modern Graphics Processing Units (GPUs), as exemplified by the proposed gBOA, and what specific optimization strategies, such as coalesced global memory access and shared memory utilization, are employed to reduce overall execution time?",
      "topics": [
        "design"
      ],
      "answer": "To efficiently utilize the SIMD architecture of GPUs, the Block Optimized Algorithm (BOA) requires a redesign, as traditional implementations are not compatible. The proposed gBOA achieves this by maximizing the advantages of the GPU's SIMD architecture. Key optimization strategies involve coalescing global memory access and utilizing shared memory wherever possible. These techniques aim to minimize the overall execution time of the algorithm. Future improvements could involve parallelizing other parts of BOA, such as selection and Bayesian Network (BN) sampling, on the GPU. Additionally, using CPU and GPU resources in parallel is suggested for potentially achieving even greater speedup.",
      "chunk_id": 13441,
      "paper_id": "Theoretical and Empirical Analysis of a GPU Based Parallel Bayesian Optimization Algorithm",
      "paper_title": "Theoretical and Empirical Analysis of a GPU based Parallel Bayesian Optimization Algorithm",
      "paper_year": "2009",
      "chunk_content": "Even though different implementations of parallel BOA exists in literature, none of them are compatible with the architecture of modern GPUs. For an efficient implementation, the algorithm needs to be redesigned with a considerable care. In this paper, we have proposed a GPU compatible approach to parallel BOA. We call it BOA on GPU or gBOA. gBOA takes maximum advantage of the SIMD architecture of the GPU. Moreover, the global memory access is coalesced and shared memory is used wherever possible in an attempt to reduce the overall execution time. In the results section, we show that significant speedups can be achieved by implementing gBOA over a modern commodity GPU. In future we would like to parallelize other parts of BOA including selection and BN sampling over the GPU. This will further improve the overall speedup. Testing other variants of BOA over GPU will be another interesting area of research. In this paper, we have used a GPU only implementation for gBOA. Although it is not in the scope of this paper but a better approach would be to use the CPU and GPU resources in parallel to achieve an even better speedup. ## ACKNOWLEDGMENT We would like to thank Jiri Ocenasek for his help regarding the parallel Bayesian Optimization Algorithms.",
      "question_type": "challenge",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:36",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to determine the influence of gene joint expression in synapse predictability, what is the optimization goal?",
      "topics": [
        "EDAs"
      ],
      "answer": "The optimization goal is to minimize the conditional entropy of gene subsets with respect to the synaptic connectivity phenotype.",
      "chunk_id": 11150,
      "paper_id": "An analysis of the use of probabilistic modeling for synaptic connectivity prediction from genomic data",
      "paper_title": "An analysis of the use of probabilistic modeling for synaptic connectivity prediction from genomic data",
      "paper_year": "2012",
      "chunk_content": "An analysis of the use of probabilistic modeling for synaptic connectivity prediction from genomic data Roberto SantanaIntelligent Systems Group.School of Computer ScienceUniversity of the Basque CountryUPV/EHUEmail: roberto.santana@ehu.es Alexander MendiburuIntelligent Systems GroupSchool of Computer ScienceUniversity of the Basque CountryUPV/EHUEmail: alexander.mendiburu@ehu.es Jose A. LozanoIntelligent Systems GroupSchool of Computer ScienceUniversity of the Basque CountryUPV/EHUEmail: ja.lozano@ehu.es #### Abstract The identification of the specific genes that influence particular phenotypes is a common problem in genetic studies. In this paper we address the problem of determining the influence of gene joint expression in synapse predictability. The question is posed as an optimization problem in which the conditional entropy of gene subsets with respect to the synaptic connectivity phenotype is minimized. We investigate the use of single- and multi-objective estimation of distribution algorithms and focus on real data from C. elegans synaptic connectivity. We show that the introduced algorithms are able to compute gene sets that allow an accurate synapse predictability.",
      "question_type": "application",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:37",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Estimation of Distribution Algorithms (EDAs) discussed, what statistical modeling approach is used to represent the distribution of solutions, and how does this approach differ from traditional genetic operators found in Evolutionary Algorithms (EAs)?",
      "topics": [
        "optimization"
      ],
      "answer": "The Estimation of Distribution Algorithms (EDAs) utilize a statistical model of the search space, denoted as P(X), to represent the distribution of solutions. This approach contrasts with Evolutionary Algorithms (EAs) that rely on genetic operators such as crossover and mutation. The EDAs' unique feature lies in their capability of building a principled statistical model of the distribution of solutions.",
      "chunk_id": 11906,
      "paper_id": "Extending a class of continuous estimation of distribution algorithms to dynamic problems",
      "paper_title": "Extending a class of continuous estimation of distribution algorithms to dynamic problems",
      "paper_year": "2008",
      "chunk_content": "Extending a class of continuous estimation of distribution algorithms to dynamic problems Bo Yuan _ Maria Or_owska _ Shazia Sadiq Received: 28 April 2007 / Accepted: 13 November 2007 / Published online: 30 November 2007 (c) Springer-Verlag 2007 #### Abstract In this paper, a class of continuous Estimation of Distribution Algorithms (EDAs) based on Gaussian models is analyzed to investigate their potential for solving dynamic optimization problems where the global optima may change dramatically during time. Experimental results on a number of dynamic problems show that the proposed strategy for dynamic optimization can significantly improve the performance of the original EDAs and the optimal solutions can be consistently located. Keywords Evolutionary algorithms $\\cdot$ Global optimization $\\boxtimes)$ Estimation of distribution algorithms $P(X)$ Dynamic optimization ## 1 Introduction Estimation of Distribution Algorithms (EDAs) [7] refer to a class of Evolutionary Algorithms (EAs) [1] based on statistical modeling of the search space instead of traditional genetic operators such as crossover and mutation. In addition to sharing the robustness and global optimization ability of EAs, the unique feature of EDAs is that they are capable of building a principled statistical model of the distribution of [^0] [^0]: B.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:39",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "In the described hybrid GEP algorithm ($GEP_{H}$), how are offspring generated differently, and what role does the probabilistic model $P$ play in comparison to the application of genetic operators?",
      "topics": [
        "Estimation of Distribution Algorithms"
      ],
      "answer": "The $GEP_{H}$ algorithm generates offspring using two strategies: some offspring are sampled from the probabilistic model $P$, which is initialized as a uniform distribution and updated throughout the algorithm, while others are generated by applying genetic operators to selected individuals. The individual fitness $f_{x}$ is evaluated in terms of the mean squared error.",
      "chunk_id": 20320,
      "paper_id": "Information Fusion in Offspring Generation- A Case Study in Gene Expression Programming",
      "paper_title": "Information Fusion in Offspring Generation: A Case Study in Gene Expression Programming",
      "paper_year": "2020",
      "chunk_content": "Section III presents some experiments on ten datasets for evaluating the performance of our proposed approach. We also discuss the impacts of the parameters on algorithm performance. Section IV summarizes the paper and discusses future works. ## II. ALGORITHM In this section, we introduce the main algorithm, which involves two offspring generation strategies. The main components, including the individual evaluation, model definition, model initialization, model updating, and model sampling, are presented in detail. ## A. ALGORITHMIC FRAMEWORK In this section, we propose a hybrid GEP algorithm $\\left(G E P_{H}\\right)$, which fuses information at the individual level. In other words, in each generation, some offspring are sampled from the probabilistic model, and the others are generated by applying genetic operators to some selected individuals. The pseudo-code of the $G E P_{H}$ algorithm is presented in Algorithm 1, and the flowchart is shown in Fig.3. - Population initialization: In line 1, the population $X$ is randomly initialized by randomly sampling in the search space. - Model initialization: In line 2, the probabilistic model $P$ is initialized as a uniform distribution. - Individual evaluation: In line 3 and 21, the individual fitness $f_{x}$ is evaluated in terms of the mean squared error.",
      "question_type": "COMPARATIVE EVALUATION",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:40",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What programming considerations should be taken into account when setting the `subNum` parameter in an Estimation of Distribution Algorithm (EDA) to ensure adequate statistical characterization of the sample?",
      "topics": [
        "selection",
        "EDAs"
      ],
      "answer": "When implementing an EDA, the size of the subpopulation, determined by the `subNum` parameter, is crucial. If `subNum` is too large, the subpopulation size becomes too small, which can lead to a lack of statistical characterization in the sample. Therefore, careful selection of `subNum` is needed to balance subpopulation diversity and statistical significance. The provided text suggests a value of `subNum=8` as a reasonable setting, implying that values significantly higher may be detrimental.",
      "chunk_id": 26618,
      "paper_id": "Parallel island-based estimation of distribution algorithms for wireless network planning",
      "paper_title": "Parallel island-based estimation of distribution algorithms for wireless network planning",
      "paper_year": "2005",
      "chunk_content": "The cause may be that size of subpopulation becomes too small if subNum is too big, which make the sample lack of statistical characterization. Therefore, we argue that subNum=8 is reasonable. ## V. CONCLUSION This paper proposes a novel transmitter selection model in wireless network-PBCM based on [2] as well as a method to simulate this experiment. A new algorithm-PEDA is proposed to solve the problem of wireless network planning. Experiments prove that our method outperforms commonly used algorithms in wireless network planning. Furthermore, convergence efficiency is observed along with different parameters.",
      "question_type": "implementation focused",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:42",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of the MVP project, what time-related factor is addressed using Gaussian Process Regression (GPRs) in the model?",
      "topics": [
        "heuristic"
      ],
      "answer": "The model uses Gaussian Process Regression (GPRs) to address time concerns, aiming to reduce wasted engineering hours and expedite project completion.",
      "chunk_id": 22147,
      "paper_id": "Multi-mode resource leveling in projects with mode-dependent generalized precedence relations",
      "paper_title": "Multi-mode resource leveling in projects with mode-dependent generalized precedence relations",
      "paper_year": "2018",
      "chunk_content": "When dealing with the MVP project, reducing wasted engineering hours and finishing the project as soon as possible are critical. In our model, the time concerns can be treated by GPRs. The MVP project may also be faced with limited resources availability. In our approach, the resource constraints are implicitly satisfied in constraints (5). Therefore, our heuristics can be used for resource leveling in the MVP project. ### 2.3. Example To illustrate the MRLP-GPR, Fig. 1 gives an example project network. The project consists of 5 activities, with 2 dummy activities (activities 0 and 4) and 3 non-dummy activities (activities 1, 2 and 3). The activity indices are placed inside the nodes. Each nondummy activity has two execution modes. The number(s) appearing above each node indicate(s) the corresponding activity's duration. There is one renewable resource type, and the number(s) appearing below each node indicate(s) the corresponding activ- [IMAGE1] Fig. 2. Optimal schedule of the example project. [IMAGE2] Fig. 3. Solution approach. ity's resource requirement.",
      "question_type": "relationship",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:43",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the primary objective of the methodology described in the context of EDAs, specifically regarding the generation of offspring and addressing potential convergence issues in algorithms like the Bayesian Optimization Algorithm (BOA)?",
      "topics": [
        "performance"
      ],
      "answer": "The primary objective is to generate new offspring by sampling a learned Bayesian Network (BN), where nodes are ordered such that parent nodes are sampled before their children, ensuring that conditional probabilities can be accurately computed for each variable based on its parents' values. Furthermore, to address potential convergence issues in BOA due to poorly distributed initial populations, a modification is proposed that considers increasing the population size to improve the distribution quality of good solutions, acknowledging the trade-off with increased computational time for evaluating the larger population.",
      "chunk_id": 11015,
      "paper_id": "Modified Bayesian optimization algorithm for EMC complex system design",
      "paper_title": "Modified Bayesian Optimization Algorithm for EMC Complex System Design",
      "paper_year": "2012",
      "chunk_content": "B. Generating offspring Once the structure and parameters of BN have been learned, new offspring will be generated by sampling the learned network. The procedure proceeds in two steps: ordering nodes and sampling variable according to the order. In the first step, an order of nodes, where each node is preceded by it parents, is computed. The purpose of this order is to generate variable in a certain sequence so that the value of parents of one node are generated prior to the generation of the node itself. The second step samples all variables according to the computed ordering. Following the ancestral ordering, given values of parents of a variable, the distribution of this variable is computed by the corresponding conditional probability, and new value is generated according to this distribution. ## C. Modified BOA As described above, BOA performance greatly depends on distribution of current good solutions. However, initial population for BOA is randomly generated; there would be some cases when all best solutions would not provide good enough distribution about problem, thus the algorithm hardly converges. To overcome this difficulty, one possibility is to increase the population size; therefore it will increase the quality of distribution of good solution. However, this solution will be time consuming, i.e. evaluating all population and offspring.",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:45",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs using Markov networks, how does the algorithmic mechanism of adjusting the parameter $SP(S)$ in Eq. (13), which controls the threshold of Mutual Information (MI), relate to controlling the complexity of the structure using the parameter $\\bar{d}=\\frac{\\sum_{i=1}^{10} d_{i}}{(S)}$?",
      "topics": [
        "complexity"
      ],
      "answer": "The parameter $SP(S)$ in Eq. (13) within the Markov network EDA controls the threshold of Mutual Information (MI). Adjusting $SP(S)$ influences the connections within the network. Similarly, the parameter $\\bar{d}=\\frac{\\sum_{i=1}^{10} d_{i}}{(S)}$ is used to control the complexity of the structure. The parameter $\\alpha$ is used to define different conditions for the threshold: very low when $\\alpha=0.5$, low when $\\alpha=0.8$, medium when $\\alpha=1.0$, high when $\\alpha=1.2$, and very high when $\\alpha=1.5$. Thus, both $SP(S)$ and $\\bar{d}$ serve as mechanisms to manage the complexity of the Markov network structure, albeit through different means: $SP(S)$ via MI thresholding and $\\bar{d}$ directly influencing structural complexity.",
      "chunk_id": 16696,
      "paper_id": "A hybrid multi-objective EDA for robust resource constraint project scheduling with uncertainty",
      "paper_title": "A hybrid multi-objective EDA for robust resource constraint project scheduling with uncertainty",
      "paper_year": "2019",
      "chunk_content": "Due to the paper limitation, we cannot give exhaustive parameters tuning. From previous literatures, a great number of experiments have performed on GA, but there is few studies conducted on parameters of EDA, especially of Markov network. Thereafter, we select one typical parameter of Markov network to illustrate, the parameter $S P(S)$ in Eq. (13), which is to control the threshold of MI. In the previous section, we have already discussed that he parameter $\\bar{d}=\\frac{\\sum_{i=1}^{10} d_{i}}{(S)}$ is used to control the complexity of structure. We take five different values of $\\alpha(\\alpha=0.5, \\alpha=0.8, \\alpha=1.0, \\alpha=1.2, \\alpha=1.5)$ to [IMAGE8] Fig. 10. Scenario generation. [IMAGE9] Fig. 11. Flowchart of simulation. represent five different conditions: very low threshold, low threshold, medium threshold, high threshold, and very high threshold. For multi-objective optimization problems, the solutions are Pareto set and very difficult to be compared. In order to make the comparison result much more easier to understand, we adopt the coverage (Zitzler \\& Thiele, 1999) to evaluate the optimality. Let $S_{1}$ be a solution set for each algorithm.",
      "question_type": "comparison",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:48",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm with a Multi-Leader Search (MLS) mechanism, how should practitioners utilize the absolute value of $z_{i}$ and the vector $|\\boldsymbol{S}|_{\\max }$ to control the search scope during the optimization process?",
      "topics": [
        "search"
      ],
      "answer": "In the MLS mechanism, the absolute value of $z_{i}$ is used as a truncation setting to ensure that half of the distribution ellipsoid in the descent direction is retained, preserving half of the search scope near the dominant area. The vector $|\\boldsymbol{S}|_{\\max }$ with elements $1 \\cdot D, 2 \\cdot D, 3 \\cdot D, 4 \\cdot D$ decreases as the number of evaluations increases, thereby reducing the search scope. This focuses the search on a more precise local search around the randomly selected leader solution in later stages. Note that $D$ represents the dimensionality of the problem.",
      "chunk_id": 20287,
      "paper_id": "An Estimation of Distribution Algorithm With Multi-Leader Search",
      "paper_title": "An Estimation of Distribution Algorithm With Multi-Leader Search",
      "paper_year": "2020",
      "chunk_content": "The MLS mechanism can generate an efficient mean point around at selected leader. $\\left|z_{i}\\right|$ is used to ensure that half of the distribution ellipsoid in the descent direction is retained. the $14 \\cdot D$ th solution itself and a randomly selected dominant solution. This search scope derived from the first two terms in (15) in the eigen coordinates is illustrated in Fig. 5. The absolute value of $z_{i}$ is used as a truncation setting, which ensures that half of the search scope near the dominant area is preserved. In fact, this process also relies on the use of a leader solution for local search to generate the distribution mean point, thus enhancing the distribution and even the diversity of the population. As presented in (16), the elements of vector $|\\boldsymbol{S}|_{\\max }=1, D, 2 \\cdot D, 3 \\cdot D, 4 \\cdot D$ decrease as the number of evaluations increases, thereby reducing the search scope. Thus, in later search stages, the MLS mechanism focuses on a more precise local search around the randomly selected leader solution. Both search behaviors expressed in (14) and (15) utilize a leader solution as the center around which to execute local exploitation. The critical difference is that the former behavior uses a location difference to generate new solutions, while the latter behavior uses a reduced distribution scope for sampling.",
      "question_type": "process",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:49",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the integrated genetic programming method based on super heuristics to solve the scheduling problem of random resource-constrained items, what specific mathematical properties of the evolutionary priority rules used to construct the solution space guarantee the algorithm's convergence to an optimal or near-optimal solution?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The provided text does not specify any convergence guarantees or mathematical proofs related to the integrated genetic programming method's evolutionary priority rules. It only mentions that the method constructs an appropriate solution space through these rules, but it does not provide any theorems, complexity notations, or specific bounds on the algorithm's performance or convergence behavior.",
      "chunk_id": 6346,
      "paper_id": "Scheduling optimization of underground mine trackless transportation based on improved estimation of distribution algorithm",
      "paper_title": "Scheduling optimization of underground mine trackless transportation based on improved estimation of distribution algorithm",
      "paper_year": "2024",
      "chunk_content": "Huang (Huang, Li, Eugene, Bright, \\& Hu, 2020) proposed incorporating the uncertainty of ore grade into the mine's production plan and developed an optimization model based on the stochastic mixed integer programming framework. Chen (Chen, Ding, Qin, \\& Zhang, 2021) proposed an integrated genetic programming method based on super heuristics to solve the scheduling problem of random resource-constrained items. He constructed appropriate solution space through evolutionary priority rules. Li (Li et al., 2021a) first take the maximum total revenue as the objective function, solve the short-term production plan model of underground mines with the artificial bee colony algorithm, and then take the minimum total waiting time of equipment as the objective function, solve the equipment scheduling model with the non-dominant sorting genetic algorithm, and obtain the scheduling plan to guide the mine production operation. For truck scheduling in underground mine trackless transportation, the primary objective is to solve the path optimization problem between the underground loading point and the surface unloading point of the truck in order to achieve the goal of completing the transportation task while minimizing the time and cost, which includes ore matching and grade requirements between the loading and unloading points. This kind of research on the optimization of transportation truck scheduling is relatively common in open-pit mines.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:51",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "What advantages are suggested when comparing a standard Estimation of Distribution Algorithm (EDA) to a Hybrid Quantum Genetic Algorithm (HQGA) integrated with mixed variants of EDA?",
      "topics": [
        "search"
      ],
      "answer": "Integrating a Hybrid Quantum Genetic Algorithm (HQGA) with mixed variants of EDA is suggested to improve results on large size problems compared to a standard EDA. This integration, along with job encoding in terms of angles, can reduce the complexity of the algorithm and require less computation time and effort.",
      "chunk_id": 1995,
      "paper_id": "An estimation of distribution algorithm (EDA) variant with QGA for Flowshop scheduling problem",
      "paper_title": "An Estimation of Distribution Algorithm (EDA) Variant with QGA for Flowshop Scheduling Problem",
      "paper_year": "2014",
      "chunk_content": "The QGA and EDA has probabilistic in nature and they can exploit and explore search space and keep directed themselves towards promising solution space. The results have shown that it can be improved more if we used their hybrid forms like Hybrid Quantum Genetic Algorithm (HQGA) and integrated with mixed variants of EDA along with the job encoding in terms of angles. This kind of job encoding can reduce the complexity of algorithm and also requires less computation time and effort. The improvement of Q-EDA has validated that if it implemented on large size problems along with hybrid form of QGA, it can produced more better results. Furthermore, if standard versions of other meta-heuristics are integrated instead adding other strategies, they still be able produce better results and remain efficient.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:04:53",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When implementing CauchyEDA, how can a developer determine if the algorithm has reached a target value, given that the expected running time (ERT) is a key performance metric?",
      "topics": [
        "algorithm"
      ],
      "answer": "To determine if CauchyEDA has reached a target value, the developer needs to track the number of function evaluations required to achieve a specific target value \\$\\Delta f\\$. The ERT, expressed in \\$\\log10\\$ of the number of function evaluations, indicates the algorithm's efficiency. If the target value \\$\\Delta f\\$ is never reached, it should be marked accordingly, impacting the ERT calculation for that specific target value.",
      "chunk_id": 15251,
      "paper_id": "Comparison of cauchy EDA and rosenbrock’s algorithms on the BBOB noiseless testbed",
      "paper_title": "Comparison of Cauchy EDA and G3PCX Algorithms on the BBOB Noiseless Testbed",
      "paper_year": "2010",
      "chunk_content": "Symbols indicate the best achieved $0 \\%$-value of one algorithm (ERT gets undefined to the right). The dashed line continues as the fraction of successful trials of the other algorithm, where 0 means $0 \\%$ and the $y$-axis limits mean $100 \\%$, values below zero for CauchyEDA. The line ends when no algorithm reaches $p=0.05$ anymore. The number of successful trials is given, only if it was in $\\{1 \\ldots 9\\}$ for CauchyEDA (1st number) and non-zero for Rosenbrock (2nd number). Results are significant with $p=0.05$ for one star and $p=10^{-\\# *}$ otherwise, with Bonferroni correction within each figure. [IMAGE_1] Figure 2: Expected running time (ERT in $\\log 10$ of number of function evaluations) of CauchyEDA versus Rosenbrock for 46 target values $\\Delta f \\in\\left[10^{-8}, 10\\right]$ in each dimension for functions $f_{\\text {opt }}+\\Delta f$. Markers on the upper or right egde indicate that the target value was never reached by CauchyEDA or Rosenbrock respectively. Markers represent dimension: $2:+, 3: \\nabla, 5: \\mathrm{s}, 10: \\circ, 20: \\square, 40: \\diamond$.",
      "question_type": "implementation focused",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:04:54",
      "generation_style": "implementation_focused"
    },
    {
      "question": "What benchmarking approach is outlined to assess the EDA, specifically regarding the multi-objective problems and stopping criteria?",
      "topics": [
        "performance"
      ],
      "answer": "The Walking Fish Group multi-objective problem toolkit, which includes nine complex problems from WFG1 to WFG9, is used as a benchmark. The number of objective function evaluations serves as the stopping criterion, with the algorithm halting when the number of evaluations reaches $10^{3+\\frac{M}{2}}$, where M represents the number of objectives. The population size, denoted as $n_{pop}$, is automatically adjusted based on the problem dimension, calculated as $50 \\times 10^{\\frac{M}{2}}$.",
      "chunk_id": 24521,
      "paper_id": "SMS-EDA-MEC Extending Copula-based EDAs to Multi-Objective Optimization",
      "paper_title": "SMS-EDA-MEC: Extending Copula-based EDAs to Multi-Objective Optimization",
      "paper_year": "2016",
      "chunk_content": "end while $\\Upsilon \\leftarrow$ precursors $\\left(\\mathcal{P},\\left\\lceil\\varepsilon n_{\\text {pop }}\\right\\rceil\\right)$. $\\mathcal{P}_{t+1} \\leftarrow \\mathcal{O} \\cup \\Upsilon$. $t \\leftarrow t+1$. end while return [FORMULA171], the non-dominated subset of [FORMULA172]. Figure 1. Algorithmic description of SMS-EDA-MEC. See Table ?? for a description of the functions used. based on reference points to yield a better performance in higher objective dimensions. We selected the Walking Fish Group multi-objective problem toolkit [33] as benchmark tests. It describes nine complex problems (from WFG1 to WFG9) that test whether the optimization algorithms are capable of handling different challenges, like separability, multi-modality, deceptive local optima, etc. Each problem was addressed with 3, 5, 7 and 9 objectives. The number of objective function evaluation was used as stopping criterion, stopping the execution of the algorithm when the number of evaluations reached $10^{3+\\frac{M}{2}}$, thus allowing longer runs for more complex problems. Population sizes for the algorithms were to automatically increase with problem dimension as $n_{\\text {pop }}=50 \\times 10^{\\frac{M}{2}}$. In these experiments, SMS-EDA-MEC was run with $n_{\\text {off }}=$ $n_{\\text {pop }}, \\varepsilon=0.1, \\nu=0.1, \\sigma_{\\min }=10^{-5}, \\Delta t_{\\mathrm{OCD}}=15, \\gamma=10^{-5}$ and $\\Delta t_{\\text {restart }}=10$. The parameters of the other algorithms were set as in the respective referenced papers.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:04:56",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of the CEDA algorithm, particularly concerning the learning rate and its impact on performance?",
      "topics": [
        "search"
      ],
      "answer": "The CEDA algorithm's performance is highly sensitive to the learning rate, denoted as alpha. A high learning rate can lead to premature convergence, while a low learning rate increases the time required to find a near-optimal solution. When alpha equals zero, CEDA behaves as a co-evolutionary paradigm with a random search strategy, achieving results superior to traditional EAs. This suggests that CEDA leverages a balance between exploration and exploitation, which is influenced by the setting of alpha.",
      "chunk_id": 1894,
      "paper_id": "The cooperative estimation of distribution algorithm  a novel approach for semiconductor final test scheduling problems",
      "paper_title": "The cooperative estimation of distribution algorithm: a novel approach for semiconductor final test scheduling problems",
      "paper_year": "2014",
      "chunk_content": "If the learning rate value is too high, this may lead to premature convergence. Although the learning rate is too small, the algorithm may use more time to find the near optimal solution (shown in Fig. 8). CEDA degenerates into a co-evolutionary paradigm with a random search strategy when $\\alpha=0$, and it achieved superior results to traditional EAs. ## Conclusion This paper presents the proposed CEDA to overcome the challenges of modeling and evaluating complexity of S-MRS problems. First, CEDA applies a divide-and-conquer strategy to divide the overall problem into partitions, and each partition is mapped to a sub-population (species) following an evolutionary scheme. A positive result indicates that the CEDA approach is suitable for providing decision support in a dynamic operation environment, in which management preferences often change in response to the real-time conditions of the manufacturing system. In our future work, further experiments will be conducted to determine the accuracy of the proposed CEDA in response to variations among the species of the ecosystem. The partnership, which includes the cooperators from each species as search points, is involved in CEDA. The construction and abandonment of the partnership are introduced briefly; however, the policy of abandonment and reconstruction must be further examined.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:00",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the Forward-Backward iteration (FBI) and permutation based local search method (PBLS) contribute to the performance of the hybrid EDA (HEDA) when solving the Resource-Constrained Project Scheduling Problem (RCPSP)?",
      "topics": [
        "scheduling"
      ],
      "answer": "The hybrid EDA (HEDA) utilizes a Forward-Backward iteration (FBI) and a permutation based local search method (PBLS) applied to the best individuals. This mechanism serves to exploit the neighborhood of the best individuals found during the EDA's search process. The RCPSP aims to optimize the makespan, which is achieved through reasonable utilization of resources and scheduling project activities $j=1, \\ldots, J$, where each activity has a duration $d_{j}$. The FBI and PBLS enhance the HEDA's ability to identify promising solutions by refining the best individuals within the search space.",
      "chunk_id": 10419,
      "paper_id": "A hybrid estimation of distribution algorithm for solving the resource-constrained project scheduling problem",
      "paper_title": "A hybrid estimation of distribution algorithm for solving the resource-constrained project scheduling problem",
      "paper_year": "2012",
      "chunk_content": "In this paper, we will propose a hybrid EDA (HEDA) for solving the RCPSP with a criterion to minimize the makespan. In particular, a new encoding scheme based on an extended active list will be proposed, and a novel probability model updating mechanism will be developed in the procedure of HEDA to help identify the most promising area, and a Forward-Backward iteration (FBI) and a permutation based local search method (PBLS) will be applied to the best individuals to exploit the neighborhood of the best individuals. Computational results and comparisons will demonstrate the effectiveness of the HEDA. The remainder of the paper is organized as follows: In Section 2, the RCPSP is described. In Section 3, the basic EDA is introduced. Then, the hybrid EDA for the RCPSP is proposed in Section 4. Computational results and comparisons are provided in Section 5. Finally we end the paper with some conclusions in Section 6. ## 2. Resource-constrained project scheduling problem The RCPSP is to study the reasonable utilization of resource and the scheduling of project activities to optimize the makespan. Generally, the RCPSP can be stated as follows. A project consists of $j$ activities labeled as $j=1, \\ldots, J$, where the duration of activity $d_{j}$ is denoted by $d_{j}$.",
      "question_type": "conceptual deep",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:02",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm using the described procedure, how should practitioners generate a new individual within the population, specifically detailing the steps for constructing $\\pi_{i}(gen+1)$?",
      "topics": [
        "selection",
        "implementation",
        "population generation"
      ],
      "answer": "To generate a new individual $\\pi_{i}(gen+1)$, the procedure involves initializing it to zero and then iteratively filling it with candidate jobs (CJ). First, set $\\pi_{ij}(gen+1) = 0$ for all $j$ from 1 to $n \\times m \\times L$. Then, for each position $j$, a candidate job $CJ$ is obtained using the function $Job(\\pi_{i}(gen+1), j)$. A check is performed to ensure the repeat times $l(CJ, \\pi_{i}(gen+1))$ of $CJ$ in $\\pi_{i}(gen+1)$ does not exceed $m \\times L$. If it does, another $CJ$ is selected. Otherwise, $\\pi_{ij}(gen+1)$ is set to $CJ$, and $j$ is incremented until $j > n \\times m \\times L$. This process is repeated for each individual $i$ from 1 to $PS$, where $PS$ represents the population size.",
      "chunk_id": 2480,
      "paper_id": "A Bayesian Statistical Inference-Based Estimation of Distribution Algorithm for the Re-entrant Job-Shop Scheduling Problem with Sequence-Dependent Setup Times",
      "paper_title": "A Bayesian Statistical Inference-Based Estimation of Distribution Algorithm for the Re-entrant Job-Shop Scheduling Problem with Sequence-Dependent Setup Times",
      "paper_year": "2014",
      "chunk_content": "Without loss of generality, we assume that $P\\left(N_{1, \\beta} \\mid N_{0, \\beta^{\\prime}}\\right)$ $=P\\left(N_{1, \\beta}\\right) \\quad\\left(\\beta, \\beta^{\\prime} \\in 1, \\ldots, n\\right)$. The procedure of $j=1, \\cdots, n \\times m \\times L$ gen $j=1$ is described as follows: Step 1: Set $\\alpha=j, \\beta^{\\prime}=\\pi_{i(j-1)}(\\text { gen })$ Step 2: Randomly create a probabilistic $r$ where $r \\sim[0,1)$. Step 3: Get a candidate job $C J$ by the roulette-wheel selection scheme. Step 3.1: If $r \\sim\\left[0, P\\left(N_{\\alpha, 1} \\mid N_{\\alpha-1, \\beta^{\\prime}}\\right)\\right.$, then set $C J=1$ and go to Step 4. Step 3.2: If $r \\sim\\left\\{\\sum_{\\beta=1}^{w-1} P\\left(N_{\\alpha, \\beta} \\mid N_{\\alpha-1, \\beta^{\\prime}}\\right), \\sum_{\\beta=1}^{w} P\\left(N_{\\alpha, \\beta} \\mid N_{\\alpha-1, \\beta^{\\prime}}\\right)\\right\\} \\quad$ and $\\quad w \\in\\{2$, $\\cdots, n\\}$, then set $C J=w$ and go to Step 4. Step 4: Return $(n \\times m \\times L-1)^{2}$. Let $l\\left(C J, \\pi_{i}(\\operatorname{gen}+1)\\right)$ denote the repeat times of $\\mathrm{e} \\%$ in $\\pi_{i}(\\operatorname{gen}+1)$. Then, the new population generation method is given as the following steps: Step 1: Set $i=1$. Step 2: Generate a new individual $=0$. Step 2.1: Set $\\pi_{i j}(\\operatorname{gen}+1)=0$ for $j=1, \\cdots, n \\times m \\times L$. Step 2.2: Set $j=1$. Step 2.3: $C J=\\operatorname{Job}\\left(\\pi_{i}(\\operatorname{gen}+1), j\\right)$. Step 2.4: If $l\\left(C J, \\pi_{i}(\\operatorname{gen}+1)\\right)=m \\times L$, then go to Step 2.3. Step 2.5: Set $\\pi_{i j}(\\operatorname{gen}+1)=C J$. Step 2.6: Set $j=j+1$. Step 2.7: If $j \\leq n \\times m \\times L$, then go to Step 2.3. Step 3: Set $i=i+1$. Step 4: If $i \\leq P S$, then go to Step 2.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:04",
      "generation_style": "practical_application"
    },
    {
      "question": "How is the computational complexity of the CEDA-GP algorithm implicitly analyzed in relation to the surrogate models used during the evolutionary course?",
      "topics": [
        "complexity"
      ],
      "answer": "The computational complexity of CEDA-GP is implicitly tied to the fitness evaluation using a Gaussian Process (GP) model as a surrogate. The GP model's approximation performance is compared to that of an RBF network within the CEDA-RBF algorithm during the evolutionary course. It is observed that the GP model, used in CEDA-GP, tunes hyperparameters automatically and provides a more accurate approximation of the true fitness (specifically, the 2-D Rosenbrock function) compared to the RBF network. While the chunk doesn't provide the explicit computational complexity, it suggests that the GP model's ability to simulate the shape of the true fitness function more accurately contributes to the overall performance of the CEDA-GP algorithm.",
      "chunk_id": 11232,
      "paper_id": "Gaussian process assisted coevolutionary estimation of distribution algorithm for computationally expensive problems",
      "paper_title": "Gaussian process assisted coevolutionary estimation of distribution algorithm for computationally expensive problems",
      "paper_year": "2012",
      "chunk_content": "6 Computation complexity of CEDA-GP and other algorithms Therefore, among the period, the middle run to validate the surrogate models is chosen. The fitness values estimated from GP and RBF network are shown in Fig. 7. Compared with the true 2-D Rosenbrock function, it is seen that the approximation of GP is more accurate than that of RBF network. At the beginning of the evolution, GP model automatically tunes the hyperparameters and the range of the approximation is wide but close to the true fitness. While for RBF network, many points of the model are far from the true fitness. With evolution going on, the optimization range decreases. GP model can simulate the shape of the true 2-D Rosenbrock function while RBF network has serious errors. [IMAGE_5] Fig. 7 Fitness evaluations compared between GP and RBF network during evolutionary course of CEDA-GP and CEDA-RBF: (a) GP model, iter=3; (b) RBF model, iter=3; (c) GP model, iter=7; (d) RBF model, iter=7; (e) GP model, iter=17; (f) RBF model, iter=17 From evolutionary course of algorithms CEDA-GP and CEDA-RBF, GP model displays better approximation performance than RBF network.",
      "question_type": "theoretical foundation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:06",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does setting $\\alpha_{2}$ to 0 compare to setting it to 1 in the protective routing heuristic, in terms of solution feasibility and link establishment cost?",
      "topics": [
        "heuristic"
      ],
      "answer": "When $\\alpha_{2}$ is set to 0 in the protective routing heuristic, no new links are added, which may lead to unfeasible solutions where the network topology is not closed. Conversely, setting $\\alpha_{2}$ to 1 requires more links, increasing the establishment cost. Adjusting the control parameter percentage in the range $\\left(w_{a v g}-w\\right) / c * 100$ can limit the usage of more links and encourage sharing of spare capacities.",
      "chunk_id": 15471,
      "paper_id": "Two-level evolutionary approach to the survivable mesh-based transport network topological design",
      "paper_title": "Two-level evolutionary approach to the survivable mesh-based transport network topological design",
      "paper_year": "2010",
      "chunk_content": "In case $\\alpha_{2}=0$, no new links will be added in the current network topology. This may result in unfeasible solutions, i.e. the network topology is not closed. On the other hand, in case $\\alpha_{2}=1$, more links will be required which will increase the establishment cost of the links. If we adjust the control parameter $\\%$ in the range $\\left(w_{a v g}-w\\right) / c * 100$, not only the usage of more links can be limited, but also the sharing of the spare capacities will be encouraged. In the protective routing heuristic, the control parameters are $g_{2}, \\alpha_{2}$ and $17.38 \\%$. Given a specific control parameters, the protective link usage matrix $\\Omega=5$ and the protective capacity assignment matrix $$\n\\mathcal{O}\\left(M^{2} \\sum_{i} \\sum_{j}\\left[\\left\\lceil\\frac{\\mathbb{D}_{i j}}{g_{1}}\\right\\rceil+\\left\\lceil\\frac{\\mathbb{D}_{i j}}{g_{2}}\\right\\rceil\\right]\\right)\n$$, will be deterministically obtained. # 4.3 Feasible solution construction and evaluation As we can see from the routing construction process, a solution can be constructed by using the proposed working and spare traffic routing heuristics given the control parameters $\\pi, \\sigma, g_{1}, g_{2}, \\alpha_{1}$ and $$\n\\sum_{i} \\sum_{j}\\left[\\left\\lceil\\frac{\\mathbb{D}_{i j}}{g_{1}}\\right\\rceil+\\left\\lceil\\frac{\\mathbb{D}_{i j}}{g_{2}}\\right\\rceil\\right] \\leq\\left\\lceil\\frac{T}{g_{1}}\\right\\rceil+\\left\\lceil\\frac{T}{g_{2}}\\right\\rceil\n$$. After routing, the working capacities assigned on the links are recorded in the matrix $\\Omega=15$, the spare capacities are recorded in the matrix $T=\\sum_{i} \\sum_{j} \\mathbb{D}_{i j}$, and the network topology can be induced from the link usage matrix $2.5 M^{2} T$, and the protective link usage matrix $\\mathrm{MTRS}_{\\text {cplex }}$.",
      "question_type": "comparison",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:07",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "When addressing extra inaccessible 2-D grids during the execution of this EDA, what specific steps must be taken to modify the circular paths and update the relevant data structures?",
      "topics": [
        "optimization",
        "implementation details",
        "estimation of distribution algorithms"
      ],
      "answer": "To address extra inaccessible 2-D grids, first, apply the function [FORMULA123] to identify path points in the existing circular paths that cover these grids. Next, remove these identified points from the arrays `PPD` and `PPG`, marking them as inactive. Finally, construct new circular paths using the remaining active points in `PPD` and `PPG`, storing these updated paths in new arrays denoted as `PPD'` and `PPG'`, respectively. This ensures the algorithm avoids the inaccessible grids in subsequent iterations.",
      "chunk_id": 18771,
      "paper_id": "Cooperative Path Planning of UAVs & UGVs for a Persistent Surveillance Task in Urban Environments",
      "paper_title": "Cooperative Path Planning of UAVs & UGVs for a Persistent Surveillance Task in Urban Environments",
      "paper_year": "2021",
      "chunk_content": "(8). 5: The index value corresponding to $c p_{i}(j)$ and [FORMULA121] is recorded in an array $V A_{i}(j)$. 6: end 7: end [IMAGE_9] Fig. 8. Strategy of avoiding the extra inaccessible 2-D grids by removing the open points. locally to the existing circular path considering both reducing the influence on the existing circular path and optimizing the index value. ## B. Strategy of Addressing the Extra Inaccessible 2-D Grids Opposite to the situation in Section V-A, some 2-D grids which could be covered by traveling the circular paths may become the inaccessible grids at a certain time. The circular path must be changed to avoid covering those inaccessible grids. By using the function [FORMULA123] again, the path points in the circular paths that cover the extra inaccessible 2-D grids can be obtained, and those points must be removed from the arrays PPD and PPG and are regarded as the inactive points. The remaining points in the arrays PPD and PPG will compose the new circular paths, and the corresponding new arrays are denoted as $P P D^{\\prime}$ and $P P G^{\\prime}$, respectively, as shown in Fig.",
      "question_type": "process",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:09",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the quality of the EDA's output be measured using the criteria of Copula theory in statistics?",
      "topics": [
        "search"
      ],
      "answer": "The quality of the EDA's output, when using Copula theory, can be assessed by modeling univariate margins and selecting a copula. Sampling from the constructed model can then be done using the copula.",
      "chunk_id": 13770,
      "paper_id": "Estimation of distribution algorithm based on archimedean copulas",
      "paper_title": "Estimation of Distribution Algorithm Based on Archimedean Copulas",
      "paper_year": "2009",
      "chunk_content": "Copula theory in statistics provides an easier way for it. The process of modeling the probability distribution can be divided into modeling univariate margins and selecting a copula. Sampling from the constructed model can also be done by use of copula. From the experimental results it is obvious that the Archimedean Copula EDAs proposed in this paper is effective. But two-dimensional Archimedean copula EDAs is only an attempt to join EDAs with copulas. The multi-dimensional algorithm is the next target of our study. ## 6. ACKNOWLEDGMENTS This work was supported in part by the Youth Research Fund of Taiyuan University of Science and Technology (No.2007116), the Chinese Nature Science Fund (No. 60674104) and the Youth Research Fund of ShanXi province (No. 2006021019).",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:10",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for EDAs in the context of flowshop scheduling with limited buffers, and how does it enhance the search procedure?",
      "topics": [
        "search"
      ],
      "answer": "The primary purpose is to minimize makespan in flowshop scheduling problems with buffer constraints using an Estimation of Distribution Algorithm (EDA). The algorithm builds a probabilistic model that considers the order of jobs and similar blocks of jobs. To improve the search, a skewed variable neighbourhood search algorithm is embedded, with its application probability dependent on the quality of the offspring generated. This approach is shown to outperform genetic algorithms and particle swarm algorithms in obtaining optimal solutions efficiently.",
      "chunk_id": 13695,
      "paper_id": "An Estimation of Distribution Algorithm for Flowshop Scheduling with Limited Buffers",
      "paper_title": "An Estimation of Distribution Algorithm for Flowshop Scheduling with Limited Buffers",
      "paper_year": "2009",
      "chunk_content": "An Estimation of Distribution Algorithm for Flowshop Scheduling with Limited Buffers Mansour Eddaly, Bassem Jarboui, Patrick Siarry, and Abdelwaheb Rebai #### Abstract Most of the works that address the flowshop scheduling problem presume unlimited buffers between successive machines. However, with the advent of new technologies in the manufacturing systems, limited capacity storage between machines has become profitable. Aimed at makespan minimization, the flowshop scheduling problem with buffer constraints is NP-hard in the strong sense. Therefore, several approximate algorithms have been proposed in the literature. In this chapter, we propose an Estimation of Distribution Algorithm for solving a flowshop scheduling problem with buffer constraints. The main characteristics of the problem, such as the order of jobs and similar blocks of jobs in the sequence, are taken into account while building the probabilistic model. In order to enrich the search procedure of the algorithm, a skewed variable neighbourhood search algorithm is embedded into it, restricted by a calculated probability which depends on the quality of the created offspring. The computational results show that our algorithm outperforms genetic algorithm and particle swarm algorithm, and can obtain several optimal solutions in a short time. ## 1 Introduction In a classical flowshop scheduling problem (FSP), each job $j(j=1,2, \\ldots, n)$ must be processed on every machine $i(i=1,2, \\ldots, m)$ and all the jobs have to pass through all the machines following the same route.",
      "question_type": "synthesis summary",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:12",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the correction procedure, as used in $\\mathrm{M}_{4}-\\mathrm{EDA}_{2}$, attempt to improve a solution, and what is the computational complexity associated with this procedure?",
      "topics": [
        "algorithmic mechanism",
        "computational complexity",
        "resource allocation",
        "estimation of distribution algorithms"
      ],
      "answer": "The correction procedure in $\\mathrm{M}_{4}-\\mathrm{EDA}_{2}$ attempts to improve the current solution by iteratively adding maximal resource to the job with the largest pseudo utility. The procedure involves calculating $d_{i}$ using the formula $d_{i}=\\sum_{j=1}^{n} p_{j k}+\\varepsilon_{i}\\left(\\sum_{j=1}^{n} p_{j 1}-\\sum_{j=1}^{n} p_{j k}\\right)$, where $d_{i}$ represents a value calculated for each resource, $n$ is the number of jobs, $p_{jk}$ represents processing time of job $j$ with resource $k$, and $\\varepsilon_{i}$ is a resource allocation parameter. The calculation of each $d_{i}$ requires $O(n)$ time. The ordering operation within the procedure has a complexity of $O(n \\log n)$. The overall complexity of the correction procedure for each solution is $\\mathrm{M}_{4}-\\mathrm{EDA}_{2}$.",
      "chunk_id": 8874,
      "paper_id": "A Boltzmann-Based Estimation of Distribution Algorithm for a General Resource Scheduling Model",
      "paper_title": "A Boltzmann-Based Estimation of Distribution Algorithm for a General Resource Scheduling Model",
      "paper_year": "2015",
      "chunk_content": "Step 5: Calculate $C_{n}$ : if $d_{3}$ then the procedure ends; otherwise, set $j \\leftarrow j+1$ and go to step 4. The procedure tries to improve the current solution by adding maximal resource to the job with the largest pseudo utility. In the worst case, if all resource allocations are set to $\\varepsilon_{3}=0.8$ and the solution still cannot comply with the constraint, then the problem has no feasible solution. Note that each $$\nd_{i}=\\sum_{j=1}^{n} p_{j k}+\\varepsilon_{i}\\left(\\sum_{j=1}^{n} p_{j 1}-\\sum_{j=1}^{n} p_{j k}\\right)\n$$ can be obtained by calculating the current reduction of processing time $p_{i r_{i}}-p_{i k}$, which requires $O(n)$ time. Steps 2 and 4 also can be completed in $\\mathrm{M}_{k^{-}}$ time. The ordering operation in step 3 has a complexity of $O(n \\log n)$. Thus the correction procedure for each solution has a complexity of $\\mathrm{M}_{4}-\\mathrm{EDA}_{2}$. [IMAGE_0] Fig. 1. Performance comparisons between ANR and GM on different instances. The curves show the changes of the probability of the optimal value of the first component. The dots denote the first generation where one component can obtain its final probability value. Results for the (a) $5 \\times 5$ instance, (b) $5 \\times 10$ instance, (c) $10 \\times 5$ instance, and (d) $10 \\times 10$ instance.",
      "question_type": "algorithmic",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:14",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners generate a new population and select excellent individuals for further evaluation?",
      "topics": [
        "genetic"
      ],
      "answer": "A new population is generated by sampling according to [FORMULA114]. Frequencies of attribute values are calculated for this new population, and Pareto dominance sorting is performed based on these frequencies. The Top-Nc individuals are then recommended as excellent individuals for further evaluation and serve as a dominant set to update the EDA preference probability vector.",
      "chunk_id": 8358,
      "paper_id": "An Interactive Estimation of the Distribution Algorithm Integrated with Surrogate-Assisted Fitness",
      "paper_title": "An Interactive Estimation of the Distribution Algorithm Integrated with Surrogate-Assisted Fitness",
      "paper_year": "2023",
      "chunk_content": "Finally, a new population is generated by sampling according to [FORMULA114]. For this new population, frequencies of attribute values are calculated, and Pareto dominance sorting is performed based on these frequencies. The Top-Nc individuals are recommended as excellent individuals for further evaluation by the user and will serve as a dominant set used to update the EDA preference probability vector. The cycle continues until the termination condition is satisfied, upon which the \"End\" button can be clicked to end the evolution. The termination conditions for the algorithm are as follows: (1) [FORMULA115], which indicates two colors are within the minimum just-noticeable difference, and the population has produced colors that are very close to the target color or (2) the user has discovered a satisfactory individual, or half of the individuals to be evaluated by users are the same, or the user feels fatigued. In this study, four representative algorithms are selected as comparison algorithms to validate the effectiveness of SAF-IEDA. These algorithms include: (1) The traditional interactive genetic algorithm (IGA); (2) The Kano-integrated interactive genetic algorithm (Kano-IGA), proposed in [23]; (3) The interactive genetic algorithm with BP neural network-based user cognitive surrogate model (BP-IGA), proposed in [24]; (4) An interactive estimation of the distribution algorithm with RBF neural networkbased fitness evaluation (RBF-IEDA), proposed in [16].",
      "question_type": "theory",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:15",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the RM-MEDA algorithm, what is the described limitation concerning the generation of new individuals, and how does the IRM-MEDA algorithm attempt to address this limitation?",
      "topics": [
        "genetic"
      ],
      "answer": "The RM-MEDA algorithm is described as being prone to falling into local Pareto fronts because only elitist solutions are used to generate new individuals. The IRM-MEDA algorithm addresses this by employing two reproduction methods. One method is model-based reproduction, which learns global statistical information to generate a portion of the new solutions. This contrasts with the original RM-MEDA's single reproduction method.",
      "chunk_id": 2531,
      "paper_id": "Improved RM-MEDA with local learning",
      "paper_title": "Improved RM-MEDA with local learning",
      "paper_year": "2014",
      "chunk_content": "However, as only elitist solutions can be used to generate new individuals, it is easy for RM-MEDA to fall into the local Pareto fronts. A hybrid algorithm combining genetic algorithm (GA) with RM-MEDA was proposed to accelerate the convergence speed in Dai et al. (2009). Wang et al. (2012) proposed a reducing redundant cluster operator (RRCO) to build more precise model during the evolution, and combined it with RM-MEDA. In order to enhance the global search ability of RM-MEDA, Zhou and Zhang et al. added two operations, biased crossover and biased initialization to traditional RM-MEDA in Zhang et al. (2007b). It can effectively solve problems with many local Pareto fronts. Although these strategies can enhance the performance of RM-MEDA, the shortcoming caused by the reproduction in RM-MEDA is not really solved. In this paper, a novel local learning strategy is designed for overcoming this drawback in RM-MEDA. We propose an improved RM-MEDA with local learning which is named as IRM-MEDA. Different from the original algorithm, two reproduction methods are employed in this IRM-MEDA. The model-based reproduction learns the global statistic information to generate a part of new solutions.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:17",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the described approach of renewing a single-unit system at each inspection time point compare to renewing via preventive maintenance (PM) once it reaches a predetermined age, specifically concerning point unavailability?",
      "topics": [
        "optimization"
      ],
      "answer": "The text indicates that renewing the system at each inspection time point is one approach, while another involves preventive maintenance (PM) triggered by a predetermined age. The evaluation by Van der Weide and Pandey [6] concluded that point unavailability can be reduced by implementing a PM policy based on age. The comparison suggests that age-based PM is more effective at reducing point unavailability than simply renewing at each inspection.",
      "chunk_id": 21486,
      "paper_id": "Determining the inspection intervals for one-shot systems with support equipment",
      "paper_title": "Determining the inspection intervals for one-shot systems with support equipment",
      "paper_year": "2018",
      "chunk_content": "They determined optimal inspection interval and a preventive maintenance threshold that minimize the expected maintenance cost per unit over an infinite time span. The system is always replaced at interval $(P M)$, and corrective replacement cost is considered higher than preventive replacement cost in the $P M$ policy, they also found the optimal values of regular time interval $[0, t]$ which minimized the cost criterion. Van der Weide and Pandey [6] presented a stochastic alternating renewal process model for a single-unit system. Failures are detected only by periodic inspection, the system is renewed at each inspection time point. It can also be renewed by preventive maintenance $(P M)$ once it reaches a [^0]: * Corresponding author. E-mail addresses: chaoyangxue@gmail.com (Q.Q. Zhao), wonyun@pusan.ac.kr (W.Y. Yun). http://dx.doi.org/10.1016/j.ress.2017.08.007 predetermined age. Renewal function, point unavailability and time average unavailability, and the effect of age based on $P M$ policy were evaluated. They concluded that the point unavailability can be reduced by PM. Cui et al.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:19",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the specific EDA mentioned, what programming considerations should be taken into account to improve sampling efficiency?",
      "topics": [
        "search"
      ],
      "answer": "For improving sampling efficiency in the EDA, the text suggests that in each iteration of EDA sampling, only the best individual should be added to the sample set. This focuses the computational effort on promising solutions.",
      "chunk_id": 4822,
      "paper_id": "An EDA-based Genetic Algorithm for EV Charging Scheduling under Surge Demand",
      "paper_title": "An EDA-based Genetic Algorithm for EV Charging Scheduling under Surge Demand",
      "paper_year": "2022",
      "chunk_content": "The finally case study is used to assess the proposed algorithm with different rounds of EDA sampling. Accordingly, three experiments are carried out. Settings of the objective function, genetic algorithm and EDA-based genetic algorithm are shown in Table II. TABLE II Settings Of Obiect Function and Algorithms It is should be noted that, for improving the sampling efficiency, in each iteration of EDA sampling, only the best individual is added to the sample set. ## C. Results and Discussion Results of all experiments are shown in Table III. As shown in the table, methods in case 1 have poor performances, which implicates that customers oriented methods in surge demand will result in vehicles gathering at charging stations and long queuing times. Moreover, the greedy strategy in charging stations only has a little mitigative effect. As we can see from the table, results of three methods in case 2 are more better. This means that genetic algorithmbased methods can efficiently solve the scheduling problem. However, with a large search space, these methods are easy to premature or have slow speeds of convergence. For example, algorithms 1 and 3 in case 2 are premature (the convergence times are 52 and 30 respectively) and fall into the local optimal. Algorithm 2 in case 2 have slow speeds of convergence, which cannot explore more areas.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:20",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How is the degree ($\\lambda$) of factor nodes used to evaluate the quality of the Estimation of Distribution Algorithm's (EDA) output, and what does a low degree indicate about a factor node?",
      "topics": [
        "search"
      ],
      "answer": "The degree ($\\lambda$) of a factor node, as defined in section 4, is used as an indicator of its importance. A very low degree ($\\lambda$) for a factor node suggests that the corresponding cell in the matrix \\$f_{n k}\\left(X_{0}, \\ldots, X_{n-1}\\right)=\\sum_{i=0}^{n-1} g_{i}\\left(X_{i}, \\Pi\\left(X_{i}\\right)\\right)\\$ is almost zero, implying that the factor node has little influence on the overall model and may be a duplicate or noise.",
      "chunk_id": 1262,
      "paper_id": "Estimation of distribution algorithm using factor graph and Markov blanket canonical factorization",
      "paper_title": "Estimation of Distribution Algorithm using Factor Graph and Markov Blanket Canonical Factorization",
      "paper_year": "2014",
      "chunk_content": "That is due to the fact that some more generations are required after discovering the BBs, to search inside the BBs and mix the BBs in different individuals, in order to find the optimum solution. In this sample only BB-wise crossover is used to find the optimum configuration. It is obvious from the figure that some of the columns (factor nodes) of the matrix $$\nf_{n k}\\left(X_{0}, \\ldots, X_{n-1}\\right)=\\sum_{i=0}^{n-1} g_{i}\\left(X_{i}, \\Pi\\left(X_{i}\\right)\\right)\n$$ (second column of the figure 2) have very low degree $(\\lambda)(\\lambda$ is used as the degree of the factor nodes and is defined in section 4), so that their corresponding cells are almost zero. Along with the generations, the amount of noise in the MIM and $g \\approx n$ are getting smaller. For the first generation, the number of the factor nodes is set to $\\max F N=n=50$. Along with the optimization algorithm, the number of unique and high-degree factor nodes approaches to its actual value which in this example is 10 . The excess factor nodes (other 40 factor nodes) are either duplicates of the true factor nodes or have a very low degree. For example in the bottom row of the figure 2, 43 out of 50 columns have visually distinguishable degrees and these 43 columns are duplicate and there are no false factor nodes among them.",
      "question_type": "definition",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:22",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "Based on the text, what is the main conclusion regarding the performance of the DE/EDA hybrid algorithm compared to standard DE and EDA approaches, and how does the parameter tuning of [FORMULA133] influence this performance?",
      "topics": [
        "performance"
      ],
      "answer": "The DE/EDA hybrid algorithm, which combines Differential Evolution mutation with Estimation of Distribution Algorithm modeling, achieves superior solution quality compared to standalone DE and EDA when solving test problems. The performance of DE/EDA is sensitive to the parameter [FORMULA133]; with a fixed [FORMULA134], increasing [FORMULA133] initially improves performance until an optimal point, after which performance degrades, with the optimal value of [FORMULA137] being problem-dependent. The combination of local and global search information enhances evolutionary algorithm performance.",
      "chunk_id": 26303,
      "paper_id": "DE EDA  A new evolutionary algorithm for global optimization",
      "paper_title": "DE/EDA: A new evolutionary algorithm for global optimization",
      "paper_year": "2005",
      "chunk_content": "In DE/EDA, new promising solution is created by DE/EDA offspring generation scheme, in which local information (obtained by the DE mutation) and global information (extracted from a population of solutions by the EDA modelling) are incorporated together. We executed DE/EDA to solve several commonly utilized test problems with different dimensions and compared with the best version of the DE algorithm described in [5] and the EDA. The experimental results demonstrated that for the test problems, DE/EDA outperforms the DE algorithm and the EDA in terms of solution quality within given objective function evaluations. The effect of the parameter [FORMULA133] with fixed [FORMULA134] to the performance of DE/EDA is experimentally investigated. The computational results show that in case that [FORMULA135] is fixed, as [FORMULA136] increases, DE/EDA firstly performs better until the best performance is achieved, then the performance becomes worse. The value of [FORMULA137] for the best performance depends on the problem to be optimized. The effect of [FORMULA138] is explored as well with fixed value of [FORMULA139]. From the experiments, we can claim that the combination of local information and global information can improve the performance of evolutionary algorithm.",
      "question_type": "comparison",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:23",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of rescheduling methods for job shop scheduling, how is the deviation $Dev_{kh}$ of an operation $O_{kh}$ estimated when using the right-shift method, and what factors contribute to this deviation according to the provided formula?",
      "topics": [
        "scheduling"
      ],
      "answer": "When a disruption occurs and the right-shift rescheduling method is adopted, the deviation $Dev_{kh}$ of operation $O_{kh}$ is estimated by the formula: $Dev_{kh}=\\max \\left\\{\\left[Dev_{kj}-\\left(ST_{kh}^{e}-CT_{kj}^{e}\\right)\\right],\\left[Dev_{ij}-\\left(ST_{kh}^{e}-CT_{ih}^{e}\\right)\\right]\\right\\}+\\varepsilon_{ij}$. In this formula, $Dev_{kj}$ and $Dev_{ij}$ represent the deviations of preceding operations. $ST_{kh}^{e}$ and $CT_{kj}^{e}$ represent the start time and completion time of operations, respectively. The term $\\varepsilon_{ij}$ is also added to the deviation. The $\\max$ function selects the larger of the two bracketed terms to propagate the maximum deviation from preceding operations.",
      "chunk_id": 3063,
      "paper_id": "Resilience-Based Surrogate Robustness Measure and Optimization Method for Robust Job-Shop Scheduling",
      "paper_title": "Resilience-Based Surrogate Robustness Measure and Optimization Method for Robust Job-Shop Scheduling",
      "paper_year": "2022",
      "chunk_content": "The reasonability lies in that the start times and completion times of the succeeding operation are shifted backward due to the adoption of the right-shift rescheduling method when disruption occurs. Therefore, the deviation $D e v_{k h}$ of operation $O_{k h}$ can be estimated by the following equation: $$\nD e v_{k h}=\\max \\left\\{\\left[\\operatorname{Dev}_{k j}-\\left(S T_{k h}^{e}-C T_{k j}^{e}\\right)\\right],\\left[\\operatorname{Dev}_{i j}-\\left(S T_{k h}^{e}-C T_{i h}^{e}\\right)\\right]\\right\\}+\\varepsilon_{i j}\n$$ To generalize the formula for estimating the deviation on the arbitrary operation $S_{1}$, we define $O_{i^{\\prime} j}$ as the preceding operation of the operation $i \\in\\{1, \\ldots, M\\}: f_{i}\\left(S_{1}\\right) \\leq f_{i}\\left(S_{2}\\right)$ on the same machine $h \\in\\{1, \\ldots, m\\}, h \\neq i$ and define $O_{i j^{\\prime}}$ as the preceding operation of the same job $P_{\\text {sizz }}$. There are two cases in which operation $\\mu, \\lambda$ has no preceding operation or has no preceding job. The first case is that the operation $B_{\\text {sizz }}$ is the first operation on a machine $P_{\\mathrm{r}}$ while the second one is that $\\theta$ is the first operation of the job $P_{\\text {sizz }}=N$. To create a generic formula, four cases were discussed by considering whether the operation $P_{\\mathrm{f}}=\\left\\{S_{\\mathrm{i}} \\mid i=1, \\ldots, N ; t=0\\right\\}$ is the first operation on a machine $t$ and whether operation $\\left[f_{\\mathrm{f}}\\left(S_{\\mathrm{i}}\\right), f_{\\mathrm{E}}\\left(S_{\\mathrm{f}}\\right), \\ldots, f_{\\mathrm{M}}\\left(S_{\\mathrm{f}}\\right)\\right] ; M$ is the first operation of the job $\\mathbf{P}(0)$. We used $M_{j}(p)[p=1, \\ldots, n ; j=1, \\ldots, m$ to represent the $p_{\\text {th }}$ job on the machine $\\operatorname{Pop}_{\\text {sizz }}=N$ and used $J_{i}(q)[q=1, \\ldots, m ; i=1, \\ldots, n$ to denote the $q_{\\text {th }}$ operation of job $P_{\\mathrm{f}}$.",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:25",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to reduce the number of input variables in a chemical calibration model, how should practitioners approach the initial variable selection process?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "Feature selection",
        "Chemical calibration models"
      ],
      "answer": "Practitioners should consider initial approaches such as principal components analysis and filtering. These can be combined with prediction techniques like artificial neural networks and partial least squares regression before applying the Estimation of Distribution Algorithm. The EDA is then used to further reduce the number of variables to be used for prediction, ultimately aiming to yield the best models.",
      "chunk_id": 12477,
      "paper_id": "Parallel EDA to create multivariate calibration models for quantitative chemical applications",
      "paper_title": "Parallel EDA to create multivariate calibration models for quantitative chemical applications",
      "paper_year": "2006",
      "chunk_content": "This problem can be seen as a selection + prediction one, where the goal is to obtain good values for the variables to predict while minimizing the number of the input variables needed, taking a small subset of really significant ones. Initial approaches to the problem were principal components analysis and filtering combined with two prediction techniques: artificial neural networks and partial least squares regression. Finally, a parallel estimation of distribution algorithm was used to reduce the number of variables to be used for prediction, yielding the best models for all the considered problems. (c) 2006 Elsevier Inc. All rights reserved. Keywords: Evolutionary algorithms; Estimation of distribution algorithms; Parallel computing; Artificial neural network; Partial least squares regression; Feature extraction and construction; Chemical calibration models ## 1. Introduction In modern laboratories the development of chemical instrumentation has allowed the existence of equipment that can acquire large amounts of data in a short period of time. For instance, whole ultraviolet-visible (UV-Vis) spectra can be obtained at a rate of several samples per second by diode-arrays or charge-coupled devices, and the same happens with mass spectra or nuclear magnetic resonance spectra. Typically, the number of data points in each spectrum ranges between 100 and 1000 , and the number of spectra acquired in a run ranges [^0]between 100 and 200.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:27",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Estimation of Distribution Algorithm (EDA) heuristic, EoDCP, proposed for solving the data center placement problem, what convergence guarantees, or theoretical performance bounds are known specifically for the EoDCP variant of EDAs, considering that its performance is compared to an exhaustive search algorithm, ESDCP, used as a lower bound?",
      "topics": [
        "heuristic",
        "Estimation of Distribution Algorithm"
      ],
      "answer": "The provided information states that the EoDCP heuristic, which is based on the Estimation of Distribution Algorithm (EDA), has a performance that is far better than that of MaxN-MinL and similar to the optimal solution obtained via ESDCP. However, the document does not provide any specific convergence conditions, mathematical proofs, theoretical performance guarantees, or computational complexity analysis for the EoDCP algorithm. Therefore, no concrete mathematical statements about the convergence or performance bounds of EoDCP can be derived from the provided content.",
      "chunk_id": 17780,
      "paper_id": "Power Aware Data Center Placement in WDM Optical Networks",
      "paper_title": "Power Aware Data Center Placement in WDM Optical Networks",
      "paper_year": "2021",
      "chunk_content": "To solve the problem, we propose two heuristics: EoDCP, based on Estimation of Distribution Algorithm (EDA), and MaxN-MinL. An exhaustive search based ESDCP algorithm is used as a lower bound to compare the performance of EoDCP and MaxN-MinL. Moreover, electronic traffic grooming technique is employed to further reduce the total network power consumption. A 20 -Node Random network and a $\\mathbf{1 7}$ Node German network are used to perform comparison of proposed heuristics. Performance of EoDCP algorithm is far better than those of MaxN-MinL, and is similar to the optimal solution obtained via ESDCP. Finally, using electronic traffic grooming improves power savings up-to $15 \\%$ in the two considered topologies. Index Terms--Data Center placement, Estimation of Distribution Algorithm, Traffic Grooming. ## I. INTRODUCTION The increase in network power consumption derived by the need to sustain bandwidth hungry applications has made network energy efficiency an important research topic for several years now. Today, many content and service providers provide services to users by relying on various Data Centers (DCs), installed either in their own private or on third party networks.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:28",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "According to the details in the chunk, what limitation does the Estimation of Gaussian Networks Algorithm (EGNA) have in solving complex problems?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The Estimation of Gaussian Networks Algorithm (EGNA) constructs a Gaussian network in each generation and samples from it. However, EGNA's limitation is that it uses only a single-peak Gaussian model, making it not appropriate for solving complex problems.",
      "chunk_id": 1453,
      "paper_id": "A new real-coded Bayesian optimization algorithm based on a team of learning automata for continuous optimization",
      "paper_title": "A new real-coded Bayesian optimization algorithm based on a team of learning automata for continuous optimization",
      "paper_year": "2014",
      "chunk_content": "These algorithms are stochastic optimization algorithms that construct an explicit probabilistic model from the candidate solutions found from the previous generations. Then, the new solutions are produced by sampling the constructed probabilistic model. Finally, the next generation will be produced from the sampled solutions using the replacement operator. This process is repeated until the stopping criteria have been reached. Several EDAs have been proposed for solving discrete and continuous problems [4, 5]. Extended compact genetic algorithm (ECGA) [6], factorized distribution algorithm (FDA) [7], and Bayesian optimization algorithm (BOA) [8] are examples of EDA algorithms proposed for discrete optimization problems. Estimation of Gaussian networks algorithm (EGNA) [9], mixed Bayesian optimization algorithm (MBOA) [10], (mixed) iterative density-estimation evolutionary algorithms (mIDEAs) [11], real-coded Bayesian optimization algorithm (rBOA) [12], and Bayesian networks and Gaussian mixture model (GMM) [13] are examples of EDA algorithms for continuous optimization problems. In EGNA, a Gaussian network is constructed in each generation by the means of a scoring metric and the new generation is sampled from the constructed network [9]. The EGNA only uses one single-peak Gaussian model and hence is not appropriate for solving complex problems [9].",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:30",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model described, focusing on how the probability model is updated and used to generate new individuals?",
      "topics": [
        "evolutionary"
      ],
      "answer": "The EDA model executes by first building a probability model based on the distribution of \"good individuals\" within the current population. This probability model is then used to generate individuals for the next generation. Crucially, the probability model is not static; it changes as the population evolves throughout the search process. Unlike evolutionary algorithms that rely on mutation and crossover, EDA generates each new individual based on the learned probability distribution, rather than directly modifying individuals from the previous generation.",
      "chunk_id": 18365,
      "paper_id": "An Estimation of Distribution Algorithm With Filtering and Learning",
      "paper_title": "An Estimation of Distribution Algorithm With Filtering and Learning",
      "paper_year": "2021",
      "chunk_content": "The contributions of our proposed algorithm are as follows. 1) A new EDA framework incorporating filtering is proposed. The idea of Kalman filtering is adopted to modify some individuals, which in turn affect the model, using an estimation error matrix from historic information. 2) A sampling strategy with learning is proposed. Using this strategy, the decision on where to sample is made according to the statistical information about the objective values of sampled individuals to avoid completely random sampling. To illustrate the effectiveness, the proposed algorithm is tested through computational experiments on benchmark functions. The rest of this article is organized as follows. Section II presents the background knowledge, including EDA, copulas, and Kalman filtering. Section III describes the fundamental ideas of the EDA-FL algorithm. Experimental studies are shown in Section IV. Finally, conclusions are given in Section V. ## II. BACKGROUND ## A. Estimation of Distribution Algorithm EDA is an evolutionary algorithm that uses a probability model based on the distribution of good individuals in the current population to generate individuals of the next generation. The probability model also changes as the population evolves in the search process. Unlike in other evolutionary algorithms, in EDA, each individual in the new generation is not generated by mutation and crossover operations on the individuals of the old generation.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:31",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of dynamic environments, if an Estimation of Distribution Algorithm (EDA) encounters a problem change at generation $X_{\\text {new }}$, how are the mean ($\\mu_{i}^{t+1}$) and standard deviation ($\\sigma_{i}^{t+1}$) updated to reflect the new search space, considering the potential for the previous optimal solution $B(k)$ to become a 'repeller'?",
      "topics": [
        "search"
      ],
      "answer": "When a problem change occurs at generation $X_{\\text {new }}$, the mean $\\mu_{i}^{t+1}$ for each dimension $i$ is updated by averaging the $i$-th component of $N$ individuals ($x_{i}^{j, i}$) in the current population, calculated as $\\mu_{i}^{t+1}=\\frac{\\sum_{j=1}^{N} x_{i}^{j, i}}{N}$. The standard deviation $\\sigma_{i}^{t+1}$ is then updated using the formula $\\sigma_{i}^{t+1}=\\sqrt{\\frac{\\sum_{j=1}^{N}\\left(x_{i}^{j, i}-\\mu_{i}^{t+1}\\right)^{2}}{N}}$, which measures the spread of the $i$-th component around the updated mean. This adjustment aims to increase diversity and explore new search spaces away from the previous optimal solution $B(k)$, which is now considered a 'repeller'.",
      "chunk_id": 12056,
      "paper_id": "An Improved Estimation of Distribution Algorithm in Dynamic Environments",
      "paper_title": "An Improved Estimation of Distribution Algorithm in Dynamic Environments",
      "paper_year": "2008",
      "chunk_content": "However in the dynamic environments, when the problem changes at generation $X_{\\text {new }}$, the next optimal solution move away from $$\nX_{\\text {new }}=X+\\lambda_{1}(X-B(k))\n$$. Then the information of optimal solution $\\lambda_{1}$ may no longer be true and may actually be misguiding the search as attractor. But it is hoped that the information of $[0,1]$ also can be used to guide the search. The use of the information is discussed in the following. Assume that the problem changes at generation $t \\leftarrow 0$, then the information of $X^{\\prime}$ may no longer be true, but it provides contrary information to us, it will not be the next optimal solution and it can be a trap. In order to find the new optimal solution, the algorithm needs increase the diversity and search new space which is different from the region presented by current population. That is to say we should search space away from the current population. According to the discussion above, we consider the $\\operatorname{Pop}(t)$ as a repeller. Then some selected individuals are diffused by the repeller in order to increase the diversity and find new optimal solution. It is called transfer model. The detail description is as following. Assume that $k$ is the environment change period index, if the environment changes at generation $[0.05,0.15]$ (corresponding to period $D$ ), the optimal solution of the current population is denoted by $B(k) \\quad(B(k)=X^{*}(t)$, $S(t), U=S(t)$ is the optimal solution of generation $$\n\\begin{gathered}\n\\mu_{i}^{t+1}=\\frac{\\sum_{j=1}^{N} x_{i}^{j, i}}{N} \\quad i=1, \\cdots, n \\\\\n\\sigma_{i}^{t+1}=\\sqrt{\\frac{\\sum_{j=1}^{N}\\left(x_{i}^{j, i}-\\mu_{i}^{t+1}\\right)^{2}}{N}} i=1, \\cdots, n\n\\end{gathered}\n$$.",
      "question_type": "evaluation metrics",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:33",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of the specific EDA discussed in relation to GNP and PMBGNP?",
      "topics": [
        "control"
      ],
      "answer": "The PMBGNP algorithm, which inherits advantages from probabilistic model building evolutionary algorithms, aims to find a better trade-off between exploitation and exploration compared to GNP. Simulation results suggest PMBGNP can achieve better performance than GNP, while GNP can sometimes produce good results but with lower average fitness, and PMBGNP may suffer from premature convergence.",
      "chunk_id": 14691,
      "paper_id": "Probabilistic model building Genetic Network Programming using multiple probability vectors",
      "paper_title": "Probabilistic Model Building Genetic Network Programming Using Multiple Probability Vectors",
      "paper_year": "2010",
      "chunk_content": "[^0]: ${ }^{2} 0$ means that no boject is perceived while 1023 means that an object is very close to the sensor (almost touching the sensor). [IMAGE6] Fig. 7. Simulation results of GNP, PMBGNP and PMBGNP [FORMULA110]. PMBGNP [FORMULA111] inherits the advantages of probabilistic model building evolutionary algorithms. A probabilistic model is constructed by learning the promising individuals that guide the more effective evolution than GNP. The simulation results shows that the proposed algorithm [FORMULA112] can find a better trade-off between exploitation and exploration, comparing with GNP and PMBGNP. Fig. 8 shows a successful track of the best solution controlled by [FORMULA113]. The figure shows that the proposed algorithm can solve the wall following problem well. The robot can move straight along the wall and avoid the obstacles. In some trails, GNP can also obtain good track. But in others, good results cannot be obtained, thus GNP achieves lower average fitness than [FORMULA114]. On the other hand, PMBGNP cannot solve the problem due to the premature convergence. [IMAGE7] Fig.",
      "question_type": "evaluation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:34",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "Within the framework of Estimation of Distribution Algorithms, what mathematical principles govern the Mallows Model (MM) and how do its two key parameters, the concentration parameter and location parameter, define the probability distribution on permutations?",
      "topics": [
        "mutation"
      ],
      "answer": "The Mallows Model (MM) is a family of probabilistic models used in Estimation of Distribution Algorithms that operates on the space of permutations of size n, denoted as $\\mathcal{S}^{\\mathrm{n}}$. The MM is defined by two parameters: the concentration parameter, $\\theta \\in \\mathbb{R}^{+}$, which is a real number greater than zero, and the location parameter, $\\sigma_{0} \\in \\mathcal{S}^{\\mathrm{n}}$. The location parameter, also known as the central permutation, represents the mode of the probability distribution, indicating the most likely permutation.",
      "chunk_id": 16048,
      "paper_id": "Approaching the quadratic assignment problem with kernels of mallows models under the hamming distance",
      "paper_title": "Approaching the Quadratic Assignment Problem with Kernels of Mallows Models under the Hamming Distance",
      "paper_year": "2019",
      "chunk_content": "Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. GECCO '19 Companion, July 13-17, 2019, Prague, Czech Republic. 02019 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery. ACM ISBN 978-1-4503-6748-6/19/07... $\\$ 15.00$ https://doi.org/10.1145/3319619.3321976 ## 2 DISTANCE BASED PROBABILITY MODELS The Mallows Models (MM) is a family of probabilistic models on the space of permutations of size $\\mathrm{n}\\left(\\mathcal{S}^{\\mathrm{n}}\\right)$. The MM is one of the most simple and natural distributions on $\\mathcal{S}^{\\mathrm{n}}$, requiring just two parameters: the concentration parameter $\\theta \\in \\mathbb{R}^{+}$and the location parameter, $\\sigma_{0} \\in \\mathcal{S}^{\\mathrm{n}}$ [5]. The location parameter, also known as the central permutation, is the mode of the distribution.",
      "question_type": "relationship",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:36",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm using univariate categorical distributions, how should practitioners standardize fitness values and what conditions necessitate setting all fitness values to zero?",
      "topics": [
        "control"
      ],
      "answer": "Practitioners should standardize fitness values, denoted as \\(f_x\\), using the formula \\(f_{x}:= \\begin{cases}\\frac{f_{x}-\\mu_{f}}{\\sigma_{f}} & \\text { if } \\sigma_{f}>0 \\\\ 0 & \\text { if } \\sigma_{f}==0\\end{cases}\\), where \\(\\mu_{f}\\) represents the mean of fitness values calculated as \\(\\mu_{f}=\\frac{\\sum_{i \\in \\mathcal{K}} f_{x}}{|\\chi|}\\) and \\(\\sigma_{f}\\) represents the standard deviation of fitness values calculated as \\(\\sigma_{f}=\\frac{\\sum_{i \\in \\mathcal{K}}\\left(f_{x}-\\mu_{f}\\right)^{2}}{|\\chi|}\\).  If the standard deviation of fitness values, \\(\\sigma_{f}\\), equals zero, all fitness values should be set to zero to prevent uncontrolled updates to the probabilistic model \\(\\operatorname{Cat}(P)\\), where \\(P\\) is the probability matrix representing the likelihood of primitives on gene points.",
      "chunk_id": 20324,
      "paper_id": "Information Fusion in Offspring Generation- A Case Study in Gene Expression Programming",
      "paper_title": "Information Fusion in Offspring Generation: A Case Study in Gene Expression Programming",
      "paper_year": "2020",
      "chunk_content": "This will cause the updating scale of the probabilistic model becomes uncontrollable. Therefore, a fitness value standardization function can be applied to the fitness value. The standardization function is defined as below: $$\nf_{x}:= \\begin{cases}\\frac{f_{x}-p_{f}}{\\sigma_{f}} & \\text { if } \\sigma_{f}>0 \\\\ 0 & \\text { if } \\sigma_{f}==0\\end{cases}\n$$ where $\\mu_{f}$ represents the mean value of fitness values, i.e., $\\mu_{f}=\\frac{\\sum_{i \\in \\mathcal{K}} f_{x}}{|\\chi|} \\cdot \\sigma_{f}$ represents the variance value of fitness values, i.e., $\\sigma_{f}=\\frac{\\sum_{i \\in \\mathcal{K}}\\left(f_{x}-\\mu_{f}\\right)^{2}}{|\\chi|}$. When the standard deviation of fitness values is zero, the fitness values will not be standardized again. All fitness values should be set as zero in this scenario to avoid updating the probabilistic model. After the standardization, the standardized fitness values satisfy a Gaussian distribution fitness $\\sim \\mathcal{N}(0,1)$. Fitness standardization is not an essential part of the model update process. The effects of fitness standardization are empirically studied in the following section. ## C. PROBABILISTIC MODEL 1) MODEL DEFINITION The algorithm uses a group of univariate categorical distributions as the distribution model of programs. The group of FIGURE 4. An illustration of gene points sampled from a categorical distribution in GEP $P_{N}$. categorical distributions is defined as $\\operatorname{Cat}(P)$, and $x^{1}=\\left(x_{1}^{1}, x_{2}^{1}, x_{3}^{1} \\ldots x_{n}^{1}\\right)$ is the probability matrix, which represents the likelihood of primitives on gene points.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:39",
      "generation_style": "practical_application"
    },
    {
      "question": "How does the Optimal Fisher Linear Discriminant Algorithm (OFLDA) address the potential loss of discriminative information encountered in both DLDA and NLDA, specifically in relation to the subspaces they analyze?",
      "topics": [
        "analysis"
      ],
      "answer": "Optimal Fisher Linear Discriminant Algorithm (OFLDA) aims to preserve all discriminative information by simultaneously applying discriminant analysis in both the range and null subspaces, referred to as Full-space LDA. Unlike DLDA, which discards the null space of \\(S_{k}\\), and NLDA, which focuses on the null space of \\(S_{t}\\), OFLDA uses all eigenvectors in both the range and null subspaces of \\(w_{1}, w_{2}, \\ldots, w_{L}\\) to maximize the criterion \\(J(W_{opt}) = \\arg \\max_{W} \\frac{\\left|W^{T} S_{b} W\\right|}{\\left|W^{T} S_{W} W\\right|}\\), where \\(S_{b}\\) and \\(S_{w}\\) represent the between-class and within-class scatter matrices, respectively. This approach contrasts with DLDA and NLDA, which may lose discriminative information due to their subspace limitations.",
      "chunk_id": 12927,
      "paper_id": "Full-space LDA with evolutionary selection for face recognitiont",
      "paper_title": "Full-space LDA with evolutionary selection for face recognitiont",
      "paper_year": "2006",
      "chunk_content": "DLDA discards the null space of $S_{k}$, since the rank of $X_{i}, m$ is smaller than of $S_{b}$, that might lose some information of null space of $$\n\\begin{aligned}\n& S_{w}=\\sum_{i=1}^{C} \\sum_{k=C_{i}}\\left(x_{k}-m_{i}\\right)\\left(x_{k}-m_{i}\\right)^{T} \\\\\n& S_{b}=\\sum_{i=1}^{C} n_{i}\\left(m_{i}-m\\right)\\left(m_{i}-m\\right)^{T}\n\\end{aligned}\n$$. NLDA extracts discriminant information from the null space of $$\nS_{t}=S_{b}+S_{w}=\\sum_{i=1}^{n}\\left(x_{i}-m\\right)\\left(x_{i}-m\\right)^{T}\n$$, however, when the number of training sample is large, the null space of $W_{n p t}=\\left(w_{1}, w_{2}, \\ldots, w_{L}\\right)$ becomes small, and much discriminative information outside this null space will be lost. Both DLDA and NLDA may lose some discriminative information. In order to solve the S3 problem and still preserve all discriminative information, Optimal Fisher Linear Discriminant Algorithm (OFLDA) [4] and Dual-space LDA [5] are proposed to simultaneously apply discriminant analysis in the range and null subspaces of $$\nJ\\left(W_{o p t}\\right)=\\arg \\max _{W} \\frac{\\left|W^{T} S_{b} W\\right|}{\\left|W^{T} S_{W} W\\right|}\n$$ respectively, here called Full-space LDA. A common drawback of Full-space LDA is that they use all eigenvector in the range and null subspaces of $w_{1}, w_{2}, \\ldots, w_{L}$. They assume that keeping all eigenvector means keeping all of the discriminative information that can improve the classification accuracy efficiently. [^0] [^0]: $S_{W}^{-1} S_{b}$ Correspondence should be addressed to Bin Li(email: binli@ustc.edu.cn).",
      "question_type": "comparison",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:41",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the evolutionary approach of Estimation of Distribution Algorithm (EDA) differ from that of Genetic Algorithm (GA), specifically concerning the search space and convergence speed?",
      "topics": [
        "search"
      ],
      "answer": "EDA predicts the optimal region by searching spatial sampling and statistical learning to generate new individuals, whereas GA uses crossover and mutation. EDA adopts an evolutionary method at the macro-level based on the search space, resulting in a stronger global searching ability and faster convergence speed compared to GA's micro-level gene-based approach.",
      "chunk_id": 14488,
      "paper_id": "Non-Gaussian system identification based on improved estimation of distribution algorithm",
      "paper_title": "Non-Gaussian System Identification Based on Improved Estimation of Distribution Algorithm",
      "paper_year": "2017",
      "chunk_content": "2 Estimation of Distribution Algorithm The concept of Estimation of Distribution Algorithm (EDA) was first proposed by the scholars in 1996 [14]. In the following few years, the algorithm developed rapidly, which became the hotspot in the field of evolutionary computation. There is a significant difference between the EDA algorithm and the genetic algorithm (GA). EDA predicts the optimal region by searching spatial sampling and statistical learning, and then produces excellent new individuals, which differs from GA uses crossover and mutation to generate new individuals. Compared with GA's evolutionary approach based on micro-level of gene, EDA adopts the evolutionary method of macro-level based on search space and has stronger global searching ability and faster convergence speed. The probability distribution model is used to describe [^0]: * This work was supported in part by NSFC (Grant No. 61473025, and 61573050), the open-project grant funded by State Key Laboratory of Management and Control for Complex System at Institute of Automation, CAS (20160107). the distribution of candidate solutions in the search space. The statistical model is used to construct a probability model to describe the solution distribution from the macroscopic point of view and then, the probability model is randomly sampled to generate new population of evolution.",
      "question_type": "comparative analysis",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:43",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the specific EDA mentioned for multi-UAV path planning, what data structures are required to represent the population and individual solutions, and how are these structures encoded?",
      "topics": [
        "data structures",
        "encoding",
        "EDA",
        "path planning"
      ],
      "answer": "The population is represented by a matrix $X$ of size $N \\times L$, where $N$ is the population size. Each individual $X_i$ in the population is a vector representing the paths of $T$ UAVs, denoted as $r_1, r_2, \\ldots r_T$. Each path $r_i$ is encoded as a sequence of coordinates in the $x, y, z$ directions of $H+1$ path points. Therefore, an individual is a vector of size $T \\times (H+1)$, consisting of the $T$ paths. Ineligible individuals that do not satisfy constraints are marked as such to prevent them from entering the next environmental selection operation.",
      "chunk_id": 7895,
      "paper_id": "An Adaptive Evolutionary Multi-Objective Estimation of Distribution Algorithm and Its Application to Multi-UAV Path Planning",
      "paper_title": "An Adaptive Evolutionary Multi-Objective Estimation of Distribution Algorithm and Its Application to Multi-UAV Path Planning",
      "paper_year": "2023",
      "chunk_content": "The population can be represented by the matrix $X=\\left[X_{1}, X_{2}, \\ldots, X_{N}\\right]^{T}$, where [FORMULA204] is the population size, and the individuals [FORMULA205] in the population are encoded as shown in Figure 4. The paths of [FORMULA206] UAVs, denoted by $r_{1}, r_{2}, \\ldots r_{T}$, are encoded in the same way as $r_{1}$, and are composed of a TABLE 1. Comparison results of AEMO-EDA with three multi-objective optimization algorithms. [IMAGE_3] FIGURE 4. Schematic diagram of individual encoding. sequence of coordinates in the $x, y, z$ directions of the $H+$ 1 path points. The individual [FORMULA211] is a vector of size $T \\times(H+1)$, consisting of the [FORMULA213] paths [FORMULA214]. When an individual [FORMULA215] does not satisfy the constraints, it is marked as ineligible to enter the next environmental selection operation. ## B. SIMULATION EXPERIMENT In order to verify the effectiveness of the multi-UAV path planning model and the efficiency of the proposed algorithm, this paper conducted simulation experiments by setting up scenarios, models, and algorithm data.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:44",
      "generation_style": "implementation_focused"
    },
    {
      "question": "When scheduling a project using Estimation of Distribution Algorithms (EDAs), what specific metric is used to evaluate the algorithm's performance related to project completion time?",
      "topics": [
        "scheduling"
      ],
      "answer": "The expected makespan is used as a metric to evaluate the algorithm's performance. For example, when scheduling according to the objective function of the expected makespan, the improved EDA reduced the expected makespan by 22.7 days compared to deterministic scheduling.",
      "chunk_id": 19274,
      "paper_id": "Stochastic resource-constrained project scheduling problem with time varying weather conditions and an improved estimation of distribution algorithm",
      "paper_title": "Stochastic resource-constrained project scheduling problem with time varying weather conditions and an improved estimation of distribution algorithm",
      "paper_year": "2021",
      "chunk_content": "11. Convergence processes of EDAs to minimise the expected makespan. [IMAGE11] Fig. 12. Convergence processes of EDAs to maximise the expected NPV. [IMAGE12] Fig. 13. Pareto solutions for the SRCPSP about windfarm construction. [IMAGE_13] Fig. 14. Distributions of the makespan of hoisting under different objective functions. ### 6.2. Scheduling results and comparisons When the project was scheduled according to the objective function of the expected makespan, the parameters of the original and improved EDA were selected using the orthogonal experiment as $(M=100, \\eta=$ $0.01, n_{W}=20, P_{p a r}=0.05, L_{\\text {rate }}=0.9)$ and $(M=100, \\eta=0.01, n_{W}=$ $5, P_{p a r}=0.2, L_{\\text {rate }}=0.9)$, respectively. The convergence processes of the improved EDA and original EDA are plotted in Fig. 11, the results of the deterministic scheduling are marked in the figure. Compared with the result of deterministic scheduling, the original and improved EDA reduced the expected makespan by 17.5 and 22.7 days, respectively. When the expected NPV was considered an objective function, the parameters of the original and improved EDA were selected as ( $M=$ $200, \\eta=0.02, n_{W}=15, P_{p a r}=0.1, L_{\\text {rate }}=0.9)$ and $(M=150, \\eta=$ $0.01, n_{W}=10, P_{p a r}=0.2, L_{\\text {rate }}=0.9)$, respectively.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:46",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the algorithmic approach detailed for Gaussian network learning in the context of Estimation of Distribution Algorithms, specifically focusing on how the network structure and variable distributions are determined?",
      "topics": [
        "search"
      ],
      "answer": "The algorithmic approach aims to find the Gaussian network model that best explains observed data by maximizing likelihood. This involves searching the space of possible network structures and estimating the probability distribution of each variable. The process typically begins with an empty network and iteratively refines it based on a metric that scores each candidate network structure, as seen in the heuristic search utilized by GAIS.",
      "chunk_id": 14704,
      "paper_id": "GAIS- A Gaussian Artificial Immune System for Continuous Optimization",
      "paper_title": "GAIS: A Gaussian Artificial Immune System for Continuous Optimization",
      "paper_year": "2010",
      "chunk_content": "If $b_{k i}=0$, then there is no relationship between the variables $G$ and $N$. The transformation of $X_{i}, \\boldsymbol{p} \\boldsymbol{a}_{\\boldsymbol{i}}$ and $\\sigma_{i}^{2}$ of the Gaussian network into the precision matrix $X_{1}, \\ldots, X_{i-1} \\forall i, k$ of the equivalent Gaussian distribution is achieved by the following recursive formula [25]: $$\nW(i+1)=\\left(\\begin{array}{cc}\nW(i)+\\frac{b_{i+1} b_{i+1}^{t}}{\\sigma_{i+1}^{2}} & \\frac{-b_{i+1}}{\\sigma_{i+1}^{2}} \\\\\n\\frac{-b_{i+1}^{t}}{\\sigma_{i+1}^{2}} & \\frac{1}{\\sigma_{i+1}^{2}}\n\\end{array}\\right)\n$$ where $W(i)$ is the $i \\times i$ upper left submatrix of $W, W(1)=\\frac{1}{\\sigma_{i}^{2}}, b_{i}$ is the column vector $\\left(b_{1 i}, \\ldots, b_{(i-1) i}\\right)^{t}$ and $b_{i}^{t}$ is the transposed vector. # - Learning The Gaussian network learning from a dataset can be stated as follows. Given a collection of observed data, find the network model to explain these data with maximum likelihood. By finding the network we mean to provide the structure of the graph, as well as the probability distribution of each variable that best fits the data. One usual approach to this task is to adopt a procedure for searching the space of all possible candidate network structures, given a metric that can provide a relative score to each point in the space. The heuristic search utilized by GAIS begins with an empty network, i.e. with no edges. Next, the probability distribution of each variable is estimated using the dataset and the score of the network is computed.",
      "question_type": "application",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:47",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of the IEDA-MsH algorithm relate to the performance metrics HR, AP, and MAP, given the algorithm's focus on personalized search?",
      "topics": [
        "performance"
      ],
      "answer": "The IEDA-MsH algorithm's performance is evaluated using HR (hit ratio), AP (average precision), and MAP (mean average precision). HR reflects recommendation accuracy, defined as the ratio between the number of preferred items in the recommendation list and the number of preferred items in the test dataset. AP expresses the ranking performance of the item recommendation list, with higher values indicating better ranking accuracy, meaning favored items are ranked higher in TopN. MAP is the average AP of all test users, measuring the overall ranking performance, specifically focusing on whether items near the front of the list are the ones users favor. The IEDA-MsH algorithm aims to maximize these metrics through personalized search, incorporating the RBM model and surrogate by integrating multisource heterogeneous UGCs.",
      "chunk_id": 4573,
      "paper_id": "Multisource Heterogeneous User-Generated Contents-Driven Interactive Estimation of Distribution Algorithms for Personalized Search",
      "paper_title": "Multisource Heterogeneous User-Generated Contents-Driven Interactive Estimation of Distribution Algorithms for Personalized Search",
      "paper_year": "2022",
      "chunk_content": "HR refers to the ratio between the number of preferred items in recommendation list and that in the test dataset to reflect the recommendation accuracy. The higher the HR value, the better the search results. AP expresses the ranking performance of the item recommendation list generated by the algorithms, and is sensitive to the position or order of the preferred items in the list. The higher the ranking of the favored items in TopN, the higher the AP. It means that the algorithm has obtained better ranking accuracy. Furthermore, MAP is the average AP of all test users and measures the overall ranking performance TABLE IV EXPERIMENTAL PARAMETERS OF OUR ALGORITHM of the algorithms. In particular, it focuses on whether the items near the front of the list are the ones that users are of favorite. Parameter Settings: After try and errors, the experimental parameters of the proposed IEDA-MsH algorithm are shown in Table IV. Two groups of experiments are designed to investigate the feasibility, effectiveness, and accuracy of the IEDA-MsH algorithm: 1) personalized search without interaction: the effectiveness of the RBM model and surrogate by integrating multisource heterogeneous UGCs; a user is randomly selected to show the results of the personalized searching in different sparsity datasets and 2) interactive personalized search: the comprehensive performance of the proposed algorithm by comparing with other interactive personalized search algorithms.",
      "question_type": "theory",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:49",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm to avoid local optima, how should practitioners manage population diversity?",
      "topics": [
        "search"
      ],
      "answer": "To maintain diversity in the hybrid estimation of distribution algorithm, after every 20 iterations, calculate the difference between the best and worst individuals over three iterations. If this difference remains small and unchanged, assume diversity is compromised. Generate new individuals while partly preserving the best individuals and re-initialize the probability matrixes to expand the search breadth.",
      "chunk_id": 14544,
      "paper_id": "A novel hybrid estimation of distribution algorithm for solving hybrid flowshop scheduling problem with unrelated parallel machine",
      "paper_title": "A novel hybrid estimation of distribution algorithm for solving hybrid flowshop scheduling problem with unrelated parallel machine",
      "paper_year": "2017",
      "chunk_content": "The probability model is adjusted to represent more effective searching region. As a consequence, an updating mechanism is employed to adjust the model at each generation. This is also a updating mechanism for getting new individuals during the iteration. ### 3.3.5 Diversity and rebuild probabilistic model A majority of intelligent optimization algorithms will be faced with the challenge to escape from local minima after a large number of computations. Diversity of the evolutionary population is also an essential role in the algorithm. In the proposed hybrid estimation of distribution algorithm, we bring diversity under control to acquire a better solution for hybrid flow shop scheduling problem with unrelated parallel machine. This method is effective to abandon the local optimize position. During each 20 times iteration of the proposed algorithm, it continuously calculates the difference value between the best individual and worst individual for three iterations. If the difference value is small enough and stays unchanged, the proposed hybrid algorithm assumes that the diversity of the population is damaged to a certain extent. In order to expand the search breadth, we generate some new individuals with partly reserving the best individuals. The probability matrixes are initialed at the same time. According to the information [IMAGE2] Fig. 3 Steps to generate [FORMULA96] during teacher phase [IMAGE3] Fig.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:50",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding the Bayesian network (BN) learned for each expert, how does the compact, graphical representation of the domain, achieved by leveraging conditional independencies, affect the computational tractability of inference compared to using joint probability distributions (JPDs) directly?",
      "topics": [
        "representation"
      ],
      "answer": "The Bayesian network (BN) provides a compact, graphical representation of the domain by leveraging conditional independencies among the six variables. This makes inference computationally tractable for a domain expert, whereas the joint probability distribution (JPD) over the same six variables, while computationally feasible in this specific setting, is described as 'hardly tractable cognitively' for a domain expert due to its size and complexity. The BN's representation simplifies the reasoning process compared to directly using JPDs.",
      "chunk_id": 19545,
      "paper_id": "Bayesian networks for interpretable machine learning and optimization",
      "paper_title": "Bayesian networks for interpretable machine learning and optimization",
      "paper_year": "2021",
      "chunk_content": "5). In particular, they learned a BN for each neuroscientist in order to model his or her reasoning in terms of the six variables. This enabled them to study, for example, how each expert related the morphological features, such as whether the axon was intra- or translaminar, with the interneuron type. As Fig. 5 shows, these networks let us identify similarities and differences in the reasoning among experts. After observing differences among the experts' BNs, Lopez-Cruz et al. [137] sought to identify distinct schools of thought among the neuroscientists. They clustered the experts' BNs into six clusters and then learned from data a representative BN for each cluster (see Fig. 5), thereby modeling the characteristic reasoning patterns of its members. The authors then combined the clusters' BNs into a consensus Bayesian multinet (i.e., a weighted combination of multiple networks) thus modeling the reasoning patterns of all 42 experts. They performed inference with this consensus model in order to, for example, obtain properties of different interneuron types; for example, Martinotti cells were mainly translaminar, displaced, and ascending. Note that, while the JPDs in this setting (six variables) were not prohibitively large in order to be computationally tractable, they are hardly tractable cognitively for a domain expert, unlike with a BN which gives a compact, graphical representation of the domain by leveraging conditional independencies.",
      "question_type": "theoretical foundation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:52",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of FORMULA107-PBIL compare to a simple Genetic Algorithm (GA) with one-point crossover, specifically regarding sensitivity to gene ordering?",
      "topics": [
        "evolutionary"
      ],
      "answer": "FORMULA107-PBIL, relying on cg-recombination, is insensitive to the ordering of genes in the string. In contrast, a GA with one-point crossover depends on the tight linkage of genes, where gene proximity correlates with gene dependence. This difference in sensitivity to gene ordering can lead to performance variations, especially when genes are tightly linked, potentially giving GA an advantage in such scenarios, as suggested by the difference between GA+clearing and FORMULA112-PBIL.",
      "chunk_id": 13071,
      "paper_id": "An Incremental Approach for Niching and Building Block Detection via Clustering",
      "paper_title": "An Incremental Approach for Niching and Building Block Detection via Clustering",
      "paper_year": "2007",
      "chunk_content": "In that particular aspect, [FORMULA107]-PBIL is similar to the simple GA, since both rely on simple recombination operators - crossover in GA and cg-recombination in [FORMULA108]-PBIL. However, an important difference should be pointed out, [IMAGE_1] Figure 3. Scalability of [FORMULA109]-PBIL and BOA for some sizes of the trap-5 problem between [FORMULA110]-PBIL and GA (and also other evolutionary algorithms). [FORMULA111]-PBIL is insensitive to the ordering of the genes in the string. GA with one-point crossover, on the other hand, relies on a tight linkage of genes [8], therefore the ordering of genes is not immaterial. A genetic linkage is said to be tight when there is a correlation between gene proximity and gene dependence. This may be the reason for the great difference between the performance of GA+clearing and [FORMULA112] PBIL, since the genes in the problem being addressed are tight linked, and GA takes advantage from that. The application of unsupervised learning in the GA and EDA context is mostly related to the adoption of clustering algorithms as a niching mechanism [15][20][10]. Alternatively, in [17] a new EDA is proposed, which is based on unsupervised learning of Bayesian networks. That work extends the applicability of unsupervised learning since it shows, for the first time, that those techniques could be employed in the model learning stage of an EDA. In the work herein presented, the application of unsupervised learning in an EDA context follows a similar approach.",
      "question_type": "comparative analysis",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:54",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "What are the precise algorithmic steps for executing the EDA model described, focusing on the initial solution generation and selection of training samples?",
      "topics": [
        "fitness"
      ],
      "answer": "The algorithm starts by randomly generating 'n' initial solutions denoted as p^0. Then, the fitness of these solutions is evaluated using SAW (Simple Additive Weighting). After fitness evaluation, the solutions are sorted, and the 'N' most dominant solutions are selected as training samples, based on the type of composite services. These 'N' training samples are then split into clusters.",
      "chunk_id": 16492,
      "paper_id": "A parallel refined probabilistic approach for QoS-aware service composition",
      "paper_title": "A parallel refined probabilistic approach for QoS-aware service composition",
      "paper_year": "2019",
      "chunk_content": "First, $n$ initial solutions $p^{0}$ are randomly generated (line 1). For the initial solutions, their worthiness is measured by exploiting SAW to compute the fitness values from multiple aspects (line 2). The fitness values of all composite services are sorted in a line ending order and then the $N$ dominant solutions are selected as the training samples by counting the type of composite services (line 4). The N training samples are split into [FORMULA137] clusters, where each cluster has $\\lfloor N / K\\rfloor$ training samples (line 5); For each training sample in one cluster, the genes of the sample is considered as the input vector and clamped to the visible units of [FORMULA139] RB (line 8). Due to the division of [FORMULA140] clusters, [FORMULA141] probability models are constructed and trained at the same time. The probabilistic model of one cluster is trained by implementing the reconstruction of the visible and hit/ + units (lines 10 to 13). After T-steps CD (line 14), the parameters of the probabilistic model are updated by computing the gradient values of these parameters (line 15 to 17).",
      "question_type": "implementation focused",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:05:55",
      "generation_style": "implementation_focused"
    },
    {
      "question": "How can the basin of attraction of the global maximum be used to compare the performance of the compact Genetic Algorithm (cGA) and Population-Based Incremental Learning (PBIL)?",
      "topics": [
        "optimization"
      ],
      "answer": "The size of the basin of attraction of the global maximum can be used as a performance indicator. If the basin of attraction of the global maximum is larger for the cGA than for the PBIL, it suggests that the cGA will converge to the global maximum with a higher probability from different initial values.",
      "chunk_id": 12558,
      "paper_id": "A Step Forward in Studying the Compact Genetic Algorithm",
      "paper_title": "A step forward in studying the compact Genetic Algorithm",
      "paper_year": "2006",
      "chunk_content": "For example if we could show that the basin of attraction of global maximum for the cGA is bigger than the basin of attraction of the same point for the PBIL then we can predict that for different initial values the cGA will converge to the global maxima with a higher probability.",
      "question_type": "comparison",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:05:56",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of the specific EDA discussed, considering the limitations in performance measurement and probabilistic model selection?",
      "topics": [
        "performance"
      ],
      "answer": "The key takeaway is that while the Estimation of Distribution Algorithm (EDA) is applied to makespan optimization using MIPS for VM performance measurement, the exploration of probabilistic models (variable-dependent or variable-independent) and the consideration of other parameters such as bandwidth remain open areas for further research to address more complex scheduling problems.",
      "chunk_id": 22499,
      "paper_id": "Independent tasks scheduling in cloud computing via improved estimation of distribution algorithm",
      "paper_title": "Independent Tasks Scheduling in Cloud Computing via Improved Estimation of Distribution Algorithm",
      "paper_year": "2018",
      "chunk_content": "In this paper, we only consider single objective makespan and use the MIPS to measure VM performance without considering other parameters such as bandwidth. In addition, EDA has many other optional probabilistic models, either variable-dependent or variable-independent. Interested researchers can try other probabilistic models to solve more complex problems. There is still vast space for further research in this area. # ACKNOWLEDGMENTS This work was support in part by the National Natural Science Foundation of China (No.71671168), the China Postdoctoral Science Foundation (No.2015M581707), the Jiangsu Planned Projects for Post-doctoral Research Funds (No.1501006B).",
      "question_type": "synthesis summary",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:05:58",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of pruning in ECGA impact the model building process, and what considerations arise when extending this pruning approach to network-based models using a Bayesian information criterion?",
      "topics": [
        "design"
      ],
      "answer": "The pruning mechanism in ECGA aims to improve the structural accuracy of the model. If the built model does not properly capture the true problem structure, the model-based enhancement mechanism may not work as expected. When extending pruning to network-based models with a Bayesian information criterion, one needs to consider the possible disruption of variable dependencies after pruning a particular variable. A simple solution is to only consider variables that are not depended upon by other variables as candidates for pruning.",
      "chunk_id": 15574,
      "paper_id": "Sensibility of Linkage Information and Effectiveness of Estimated Distributions",
      "paper_title": "Sensibility of Linkage Information and Effectiveness of Estimated Distributions",
      "paper_year": "2010",
      "chunk_content": "Their good functioning crucially depends on the structural accuracy of the built models. Thus, it is conceivable that if the built model does not properly capture the true structure of the underlying problem, the model-based enhancement mechanism will not fully work as expected. Furthermore, as we demonstrated in this paper, the condition that hinders the model building algorithm from constructing models that truly reflect the problem structure may be an inherent property of the underlying problem (e.g., different scales among constitutive subproblems). Thus, we think that adapting pruning mechanisms will provide a more appropriate circumstance for the model-based enhancement techniques to work. # 8.2 Future Work In this paper, we demonstrated a pruning mechanism design and its integration into ECGA. It may also serve as a basis for developing other techniques for more efficient and robust optimization. Some possible extensions of this work are outlined as follows. First of all, the immediate direction is to design pruning mechanisms for other EDAs. As illustrated in Section 7.3, we can extend the pruning metric described in this paper to handle network-based models with a Bayesian information criterion. However, a pruning mechanism for network-based models requires more than that. We also need to consider the possible disruption of variable dependencies after pruning a particular variable. The simplest solution is to consider only those variables that are not depended upon by other variables as possible candidates for pruning.",
      "question_type": "advantage",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:06:00",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm, how should practitioners configure the EDASQ algorithm to leverage its spline-based probability function for online applications?",
      "topics": [
        "design"
      ],
      "answer": "To utilize EDASQ for online applications, practitioners should consider that its faster convergence, stemming from the spline-based probability function (specifically, the Catmull-Rom cubic spline), can be helpful. The algorithm approximates the probability model autonomously using both the spline-based probability function and a Q-learning-based updating rule. The selection of a suitable updating method for the probability model may convert to new optimization problems, requiring experience. Furthermore, the approximation precision can be improved by following the guidelines, as the Catmull-Rom cubic spline function is a suboptimal and adaptive realization of the cubic spline function with the kernels $\\left|x^{3}\\right|$.",
      "chunk_id": 11363,
      "paper_id": "Estimating Biped Gait Using Spline-Based Probability Distribution Function With Q-Learning",
      "paper_title": "Estimating biped gait using spline-based probability distribution function with Q-learning",
      "paper_year": "2008",
      "chunk_content": "Therefore, the Catmull-Rom cubic spline function is applied in EDASQ as the probability distribution function, which has been proved in this paper to be a suboptimal and adaptive realization of the cubic spline function with the kernels $\\left|x^{3}\\right|$ (see Appendix I). We also provide guidelines on how to improve the approximation precision in Appendix II. For the specially-designed probability model, the selection of a suitable updating method needs experience and may convert to new optimization problems. To build and update the probability model from learning instead of exclusively being determined in advance, the idea of reinforcement learning (RL) [4] is incorporated with EDA in this paper. RL can learn the unknown desired distribution by providing an agent with suitable evaluation of its performance. By formulating the relationship between parameters with conditional probability distribution functions, probability models can be updated autonomously by the RL agents, which provide us a new viewpoint to explore the inherent dynamics in biped locomotion. In conclusion, EDASQ is able to find out the preferable solutions in high dimensional coupling space quickly and approximate the probability model autonomously with the help of the spline-based probability function and Q-learning-based updating rule. As indicated by the comparison experiment with traditional EDAs in Section VI, the faster convergence of EDASQ brought by the spline-based probability function may be helpful for online application.",
      "question_type": "practical application",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:06:01",
      "generation_style": "practical_application"
    },
    {
      "question": "Regarding probabilistic models used within Estimation of Distribution Algorithms (EDAs) for fitness function prediction, what criteria are available to compare two such models, specifically focusing on how well they generalize to unseen solutions rather than maximizing likelihood of the training data?",
      "topics": [
        "fitness"
      ],
      "answer": "Within the context of EDAs, probabilistic models of fitness functions can be compared using criteria that assess their ability to generalize and predict fitness values of solutions. The comparison criteria include: the correlation between the probabilities assigned by the models to the solutions and their fitness values, the sum of the probabilities assigned to the solutions, and the model entropy. These criteria aim to evaluate the models as predictors of the fitness function, particularly for identifying 'good enough solutions', rather than merely maximizing the likelihood of the training data.",
      "chunk_id": 15059,
      "paper_id": "Mateda-2.0- Estimation of Distribution Algorithms in MATLAB",
      "paper_title": "Mateda-2.0: Estimation of Distribution Algorithms in MATLAB",
      "paper_year": "2010",
      "chunk_content": "2006; Shakya et al. 2005). - To obtain models of black-box optimization problems. - To unveil and extract problem information that is hidden in the original formulation of the function or optimization problem (Brownlee et al. 2008). - To design improved (local) optimization procedures based on the model structure (Pereira et al. 2000; Sastry et al. 2006). An important question is, which of two given probabilistic models of a fitness function is better? Notice that, in this case, models are not intended to be evaluated in terms of how accurately they represent the selected set (i.e., maximize the likelihood of the data). Instead, we would like to use them as general enough predictors of the fitness function, or at least of the fitness function of good enough solutions. For multi-objective problems, we can conceive multi-models, each model representing a different objective. Mateda-2.0 includes methods that allow the user to compare models in terms of criteria such as the correlation between the probabilities assigned by the models to the solutions and their fitness values, the sum of the probabilities assigned to the solutions, and the model entropy. # 3. General description of the Mateda-2.0 implementation In this section, we describe the input and output parameters used by Mateda-2.0 and the way the probabilistic models are implemented.",
      "question_type": "theoretical foundation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:06:03",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "How does the performance of the HyEDA algorithm compare to the simulated annealing approach in the context of CDMA cellular system configuration design, based on the information provided?",
      "topics": [
        "evolutionary"
      ],
      "answer": "HyEDA outperforms the simulated annealing approach in terms of the quality of solutions found and the number of function evaluations used. However, HyEDA requires longer computational time due to the time needed to construct the probability model using the k-means clustering method.",
      "chunk_id": 11268,
      "paper_id": "A HYBRID ESTIMATION OF DISTRIBUTION ALGORITHM FOR CDMA CELLULAR SYSTEM DESIGN",
      "paper_title": "A HYBRID ESTIMATION OF DISTRIBUTION ALGORITHM FOR CDMA CELLULAR SYSTEM DESIGN",
      "paper_year": "2008",
      "chunk_content": "However, HyEDA does incur slightly more computational time, due to extra time required to construct the probability model in solving a cellular CDMA system configuration design problem. # 5. Conclusion In this paper, we proposed a two-stage hybrid evolutionary approach, HyEDA, for solving the CDMA cellular system configuration design problem. In the first stage of HyEDA, a hybrid estimation of distribution algorithm was proposed, which resorts to the [FORMULA224]-means clustering method for probability model construction, and a simple local search algorithm for solution quality improvement. HyEDA has been applied to tackle a problem given in Ref. 2, as well as some of its derived and more difficult cases. The experimental results have demonstrated that the proposed algorithm outperforms the simulated annealing approach used in Ref. 2, in terms of the quality of the solutions found and the number of function evaluations used, though at the price of long computational time. In the future, HyEDA could be improved in several ways. First, we would like to carry out more experiments to thoroughly understand the effects of the components of HyEDA including the two-stage framework, the clustering method and the Tchebycheff distance metric, and improve HyEDA toward a single stage algorithm by incorporating the local search for the antenna' powers and heights into HEDA.",
      "question_type": "comparative analysis",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:06:05",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the population-based probability distribution estimation for dynamic optimization problems (DOPs), what programming considerations should be taken into account regarding solution migration and penalty assignment techniques?",
      "topics": [
        "genetic"
      ],
      "answer": "When implementing population-based probability distribution estimation for DOPs, special attention should be given to solution migration and penalty assignment. The implementation must include specific mechanisms to handle the migration of solutions between different stages of the complex process. Furthermore, the penalty assignment technique needs to be carefully coded to ensure that constraint violations are appropriately penalized during the optimization process. These techniques are crucial for effectively tackling constrained multistage complex process DOPs.",
      "chunk_id": 1697,
      "paper_id": "Dynamic Optimization of Chemical Engineering Problems Using Affinity Propagation Based Estimation of Distribution Algorithm",
      "paper_title": "Dynamic Optimization of Chemical Engineering Problems Using Affinity Propagation Based Estimation of Distribution Algorithm",
      "paper_year": "2014",
      "chunk_content": "Genetic Algorithm (GA) was the first to solve DOPs. Pham[5] applied GA in DOPs by using several novel reproductive operators including shift, smoothing, extrapolation and swapping. Modified differential evolution algorithm was also tried by Lee[6] through employing a local search to enhance the computational efficiency and modifying heuristic constraints to systematically reduce the size of the search space. Angira[7] applied trigonometric differential evolution algorithm for solving DOPs to enhance convergence speed. Simulated annealing algorithm was also applied[8] in determining operation policies of a binary distillation[9]. As a low computational algorithm, ant colony algorithm had a potential for DOPs[10]. Zhang[11] developed iterative ant-colony algorithm to gradually approximate the optimal control profile. Likewise, population-based probability distribution estimation was applied in tackling constrained multistage complex process DOPs with special solution migration and penalty assignment techniques integrated[12]. Recently, hybrid algorithms such as improved particle swarm optimization with successive quadratic programming[13] were proposed which balanced the speed of convergence and the ability of escaping local optimal. In general, evolutionary algorithms have been widely used in DOPs. Certainly, improvement on evolutionary algorithms should be taken when DOPs need to be solved efficiently and accurately.",
      "question_type": "implementation",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:06:07",
      "generation_style": "implementation_focused"
    },
    {
      "question": "In the context of copula-based Estimation of Distribution Algorithms (EDAs), how is the maximum likelihood estimation method applied to assess the probabilistic model's quality, specifically regarding the copula's parameters?",
      "topics": [
        "optimization"
      ],
      "answer": "The maximum likelihood estimation is used in copula-EDAs to estimate the parameters of the copula function. This estimation is crucial because the copula function, denoted as \\(C(u_1, ..., u_d)\\), models the dependence structure between the variables \\(x_1, ..., x_d\\), where \\(u_d = F_d(x_d)\\) represents the marginal distributions. By maximizing the likelihood function with respect to the copula's parameters, the EDA aims to find the copula model that best fits the observed data, thereby enhancing the accuracy of generating new solutions \\(\\mathbf{x}^{-}(k)\\) in the optimization process.",
      "chunk_id": 18368,
      "paper_id": "An Estimation of Distribution Algorithm With Filtering and Learning",
      "paper_title": "An Estimation of Distribution Algorithm With Filtering and Learning",
      "paper_year": "2021",
      "chunk_content": "Their marginal distributions are Gaussian and empirical distributions, respectively. Other copulas include $t$-copula, Clayton-copula, Gumbel-copula, and Frank-copula [27]. The key theorem was introduced by Nelsen [28] to separate the effect of dependence from the effect of marginal distributions in a copula. For a $$\n\\begin{aligned}\nC\\left(u_{1}, \\ldots, u_{d}\\right) & =F\\left(x_{1}, \\ldots, x_{d}\\right) \\\\\n& =F\\left(F_{1}^{-1}\\left(u_{1}\\right), \\ldots, F_{d}^{-1}\\left(u_{d}\\right)\\right)\n\\end{aligned}\n$$-dimensional random vector $\\mathbf{x}=\\left(x_{1}, x_{2}, \\ldots, x_{D}\\right)$, its joint distribution function being $F(\\mathbf{x})$ with the marginal distributions $u_{d}=F_{d}\\left(x_{d}\\right), d=1, \\ldots, D$, there exists a copula $C(\\mathbf{u})$, which is a multivariate distribution function of vector $\\mathbf{u}=\\left(u_{1}, u_{2}, \\ldots, u_{D}\\right)$ in $\\mathbf{x}(0)$. The following equality was set up and the proof was given in [28]: $$\n\\begin{aligned}\nC\\left(u_{1}, \\ldots, u_{d}\\right) & =F\\left(x_{1}, \\ldots, x_{d}\\right) \\\\\n& =F\\left(F_{1}^{-1}\\left(u_{1}\\right), \\ldots, F_{d}^{-1}\\left(u_{d}\\right)\\right)\n\\end{aligned}\n$$ where $F_{d}^{-1}\\left(u_{d}\\right)=\\sup \\left\\{x_{d} \\mid F_{d}\\left(x_{d}\\right) \\leq u_{d}\\right\\}$. With the analysis above, each $\\mathbf{x}^{-}(k)$ can be generated in two steps. The first step is to generate a random vector $\\mathbf{P}^{-}(k)$ with the copula function. In the second step, vector $\\mathbf{g}(k)$ is generated using the common inverse function of the marginal distribution of each variable, i.e., $x_{d}=F_{d}^{-1}\\left(u_{d}\\right), d=1, \\ldots, D$. In recent years, many EDAs using copulas as probabilistic models (copula-EDAs) have been proposed. Based on the copula theory, a framework of EDA was proposed in [29] to solve 2-D optimization problems. Archimedean-copula, Gumbel-copula, and Gaussian-copula have all been used in multivariate EDAs [30]-[34], which were tested on problems of low dimensions in small scale experiments. The maximum likelihood estimation was used in a copula-EDA to estimate the parameters of copula [35].",
      "question_type": "evaluation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:06:08",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "What is the key takeaway about the application of the specific EDA discussed, RK-EDA, in comparison to other continuous EDAs?",
      "topics": [
        "mutation"
      ],
      "answer": "The RK-EDA algorithm, which uses rescaled Random Keys and a cooling scheme for exploration/exploitation balance, achieves competitive results with a computationally efficient model that only saves the mean of solutions. It outperforms other reviewed continuous EDAs despite the challenges previously associated with Random Key-based EDAs.",
      "chunk_id": 25060,
      "paper_id": "RK-EDA- A Novel Random Key Based Estimation of Distribution Algorithm",
      "paper_title": "RK-EDA: A Novel Random Key Based Estimation of Distribution Algorithm",
      "paper_year": "2016",
      "chunk_content": "There are however no significant difference between the performance of the algorithms on other instances. Results from comparing RK-EDA with GM-EDA as shown in Table 6 also indicate that RK-EDA is competitive and should be further explored to solve bigger and more complex problems. # 5 Conclusions EDAs based on RKs have previously been considered the poorest of permutation based EDAs [3]. One of the problems posed by RKs is attributed to the variety of ways of representing an ordering [13]. In this paper, we introduce a novel RK based EDA (RK-EDA) which addresses this by rescaling the RKs uniformly. This approach improves the information captured by the probabilistic model. Furthermore, RK-EDA uses a cooling scheme to manage the rate of exploration/exploitation of the search space such that there is better exploration at the start of the algorithm and better exploitation of already found good pattern as the search progresses. Furthermore, learning a probability structure is considered the most expensive operation in EDAs [2], we present a simple model, which only saves the mean of solutions in a selected population. This is relatively computationally efficient. RK-EDA whose procedure is comparatively simple produces very competitive results. It outperforms other reviewed continuous EDAs.",
      "question_type": "relationship",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:06:11",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of EDAs, how does the algorithmic mechanism of leave-one-out cross-validation (LOOCV) contribute to estimating the prediction accuracy of a classifier built by a selected optimal gene subset, and what role does this accuracy play in determining the fitness of individuals within the evolutionary process?",
      "topics": [
        "fitness"
      ],
      "answer": "The algorithmic mechanism of leave-one-out cross-validation (LOOCV) estimates the prediction accuracy of the classifier by withholding one sample in the training set. The remaining samples are used to build a classifier to predict the class of the withheld sample. The cumulative error rate, serving as the LOOCV accuracy, is then calculated. In this specific instance, the best solution found has a LOOCV accuracy of 0.048, and this value is used as one of the objectives, alongside F1, F2, and F3, to determine the overall fitness of the individuals within the Estimation of Distribution Algorithm.",
      "chunk_id": 11451,
      "paper_id": "Optimal Genes Selection with a New Multi-objective Evolutional Algorithm Hybriding NSGA-II with EDA",
      "paper_title": "Optimal genes selection with a new multi-objective evolutional algorithm hybriding NSGA-II with EDA",
      "paper_year": "2008",
      "chunk_content": "Thereafter, as for our MOEA-Classifier component, the training data is randomly shuffled and generate two subsets, one subset containing $70 \\%$ of the samples for constructing the classifier and the other $30 \\%$ of the samples serving as tuning set to evaluate the classifier and then calculate the fitness values of individuals. During generating offspring at each iteration, best genes $\\mathrm{M}=10$ selected by crowding tournament selection to establish learning dataset $D_{e}$. After 10 runs, we use leave one out cross validation (LOOCV) to estimated the predication accuracy of the classifier built by our selected optimal gene subset, in which one sample in the training set is withheld, the remaining samples of the training set are used to build a classifier to predicate the class of the withheld sample, and the cumulative error rate is calculated. [IMAGE_0] Figure 1. The fitness average values of population in the 10 iteration. Figure. 1 is the three fitness average values of populations in the 10 iterations. We can see the method decreases the number of genes under the acceptable accuracy. Finally, we find the best soluteion whose objectives [FORMULA67] is $0, F_{2}$ is $0, F_{3}$ is 7 and LOOCV accuracy is 0.048 .",
      "question_type": "definition",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:06:13",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm utilizing Latent Semantic Analysis (LSA) for dimensionality reduction of artifact-feature matrices, what specific implementation steps are required for Singular Value Decomposition (SVD) as it applies to the term-document matrix M?",
      "topics": [
        "genetic"
      ],
      "answer": "To implement SVD for dimensionality reduction within an EDA using LSA, the following steps are required: First, construct an artifact-feature matrix M, where rows represent artifacts and columns represent features. Compute the value for each entry in M using tf-idf. Then, apply SVD to decompose M into three matrices: $M=U S V^{\\mathrm{T}}$, where U represents the left singular vectors, S is a diagonal matrix of singular values, and V represents the right singular vectors.  The dimensionality reduction is achieved by truncating the matrices U, S, and V, preserving the similarity structure among artifacts. According to the text, the rank of the matrix is reduced to 67%.",
      "chunk_id": 16759,
      "paper_id": "Multi-objective search-based software modularization- structural and non-structural features",
      "paper_title": "Multi-objective search-based software modularization: structural and non-structural features",
      "paper_year": "2019",
      "chunk_content": "We use this technique for discovering latent semantic between non-structural features. An LSA uses a mathematical technique from linear algebra named Singular Value Decomposition (SVD) to reveal latent semantic. This technique applies on a term-document matrix and is represented by Eq. 10 and Fig. 2. $M=U S V^{\\mathrm{T}}$ $S$, ( $\\operatorname{mno}(A, B)$ : rank of the matrix); $U_{\\mathrm{mr}}$, ( $B$ : document, $\\max (\\operatorname{mno}(\\forall A, B))$ : rank of the matrix); $V_{\\mathrm{m}}^{\\mathrm{T}}$, ( $n$ : term, $67 \\%$ : rank of the matrix). Dimensionality reduction Let M indicate an artifact-feature matrix containing features counts per artifact (rows represent each artifact and columns represent unique Fig. 2 The form of singular value decomposition after dimensionality reduction [IMAGE_1] Fig. 3 The overall evolutionary process of genetic algorithm features) is formed from the source code. We use tf-idf (Abualigah et al. 2017a, b) to compute the value that each entry in the matrix should take. Then, SVD is used to reduce the number of features (i.e., dimensionality reduction) while preserving the similarity structure among artifacts.",
      "question_type": "practical application",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:06:15",
      "generation_style": "practical_application"
    },
    {
      "question": "Considering the comparison of Estimation of Distribution Algorithms (EDAs) on the 3-deceptive problem, how does the performance of the new algorithm compare to the compact Genetic Algorithm (cGA), pe-cGA, and ne-cGA in terms of the number of function evaluations required to achieve a certain solution quality?",
      "topics": [
        "comparison"
      ],
      "answer": "When tested on a fully deceptive problem consisting of ten concatenated 3-bit trap functions, the new algorithm outperforms cGA, pe-cGA, and ne-cGA when comparing the quality of the solution in terms of the number of function evaluations. The 3-bit trap function is defined as having a deceptive to optimal ratio of 0.7, where the function value f(x_{3-bit}) is 0.7 if sum(3bit) = 0, 0.4 if sum(3bit) = 1, 0.0 if sum(3bit) = 2, and 1.0 if sum(3bit) = 3. In contrast, with low order problems, the new algorithm is close to the cGA performances in terms of population size.",
      "chunk_id": 10743,
      "paper_id": "Improved compact genetic algorithm for EM complex system design",
      "paper_title": "Improved Compact Genetic Algorithm for EM Complex System Design",
      "paper_year": "2012",
      "chunk_content": "(a) Number of correct BBs versus population size. (b) Number of correct BBs versus number of function evaluations. Fig. 3 (a) and (b) report the quality of solution and speed of convergence of all cGAs the MDP with 10 BBs is considered. With low order problems the new algorithm is close to the cGA performances in terms of population size, while, as shown in Fig. 3(b) reporting the comparison of the quality of the solution in terms of the number of function evaluations, the new algorithm outperforms cGA, pe-cGA, and ne-cGA. Fully deceptive problems [3] have been then considered for testing the proposed algorithm. The problems involving trap functions are ideal cases for testing the capability to deal with high order BBs. A simple 3-deceptive problem, formed by concatenating ten copies of the three bits trap function has been considered, each three bits trap function has deceptive to optimal ratio of 0.7 as defined below: $$\n\\begin{gathered}\nf_{3-b i t}=\\sum_{1}^{10} f\\left(x_{3-b i t}\\right) \\\\\n\\text { where } f\\left(x_{3-b i t}\\right)=\\left\\{\\begin{array}{l}\n0.7 \\text { if sum }(3 b i t)=0 \\\\\n0.4 \\text { if sum }(3 b i t)=1 \\\\\n0.0 \\text { if sum }(3 b i t)=2 \\\\\n1.0 \\text { if sum }(3 b i t)=3\n\\end{array}\\right.\n\\end{gathered}\n$$ [IMAGE_1] Fig.4. Performance of different algorithms on 3-deceptive problem. (a) Number of correct BBs vs. population size. (b) Number of correct BBs vs.",
      "question_type": "process",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:06:17",
      "generation_style": "theoretical_foundation"
    },
    {
      "question": "According to the details in the chunk, what advantage does the Multi-Objective Parzen EDA (MOPED) algorithm have over directly sampling from a uniform m-dimensional PDF in the initial population generation?",
      "topics": [
        "fitness",
        "Estimation of Distribution Algorithms (EDAs)"
      ],
      "answer": "The Multi-Objective Parzen EDA (MOPED) algorithm uses the Parzen method to build a probabilistic model and sample from the estimated PDF to obtain new solutions, while the initial step involves sampling N_ind individuals from a uniform m-dimensional PDF. A key advantage arises in subsequent iterations: MOPED refines the search by sampling from a learned distribution based on promising solutions, whereas the initial uniform sampling offers no such refinement, potentially leading to less efficient exploration of the search space after the initial population.",
      "chunk_id": 26855,
      "paper_id": "MOPED  A Multi-objective Parzen-Based Estimation of Distribution Algorithm for Continuous Problems",
      "paper_title": "MOPED: A multi-objective Parzen-based estimation of distribution algorithm for continuous problems",
      "paper_year": "2003",
      "chunk_content": "The above results were later extended to the multi-variate case by Cacoullos [15]. # 3 Parzen EDA The main idea of the work is the use of the Parzen method to build a probabilistic model and to sample from the estimated PDF in order to obtain new promising solutions. A detailed description of the Multi-Objective Parzen EDa (MOPED algorithm) follows, and some results are presented in order to show capabilities and potentialities of the algorithm. Moreover, an extensive use of the Parzen method could lead to simplify the overall optimization procedure towards a parameter-less tool. As a first step in this direction, at the end of section we introduce a different spreading technique for solutions in the Pareto front. # 3.1 General Algorithm As summarized in figure 1, the general optimization procedure can be described as follows: 1. Starting: $\\mathrm{N}_{\\text {ind }}$ individuals are sampled from a uniform $m$-dimensional PDF. 2. Classification \\& Fitness evaluation: by using NSGA-II techniques [13], individuals of current population are ranked and ordered in terms of dominance criterion and crowding distance in the objective function. A fitness value, linearly varying from $2-\\alpha$ (best individual) to $\\alpha$ (worst individual), with $0<\\alpha<1$, is assigned to each individual.",
      "question_type": "advantage",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:06:19",
      "generation_style": "comparative_analysis"
    },
    {
      "question": "To implement the specific EDA mentioned, what data structures are required to represent the Bayesian Network probability model and the variable vector when using tournament selection and individual mutation for a real-variable problem like microwave filter design?",
      "topics": [
        "selection",
        "data structures",
        "Bayesian optimization algorithm",
        "mutation"
      ],
      "answer": "Based on the description, the implementation requires a data structure to represent a Bayesian Network for modeling probabilities, and a variable vector to represent individuals, as opposed to a probability vector. The variable vector is preferred for real-variable problems. Tournament selection will require storing a population of these variable vectors and a mechanism for comparing them based on their fitness. Individual mutation necessitates a data structure that allows for modification of individual variable vectors.",
      "chunk_id": 11016,
      "paper_id": "Modified Bayesian optimization algorithm for EMC complex system design",
      "paper_title": "Modified Bayesian Optimization Algorithm for EMC Complex System Design",
      "paper_year": "2012",
      "chunk_content": "In this paper, we propose a new approach by adding mutation to traditional BOA. By adding mutation, some individual will be used to discover space out of distribution of good ones; therefore, the algorithm will avoid being trapped at local optimum. This is similar to Genetic Algorithms (GA) [5], Population Based Incremental Learning (PBIL) [6], and Compact Genetic Algorithm (CGA) [7], which use mutation as one of most important operator for exploring problem space. However, in proposed algorithm, we use Bayesian Network to represent probability model and generate new offspring. Moreover, we work with variable vector, which is more preferred for realvariable problem as microwave, instead of probability vector as PBIL and CGA. In our work, we implement BOA with tournament selection and individual mutation for design microwave filters. ## III. TEST PROBLEMS In this paper, we present result of modified BOA for Onemax problem as mathematical test function and then apply for filter synthesis. For all test problems, 30 independent runs are performed and the results showed here are averaged values. ## A. Onemax problem Onemax problem is defined as sum of bits in the input binary string. The optimal solution of Onemax is the string of all ones.",
      "question_type": "implementation focused",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:06:20",
      "generation_style": "implementation_focused"
    },
    {
      "question": "When planning for Distributed Generation (DG), what objective is established to guide the optimization process?",
      "topics": [
        "planning"
      ],
      "answer": "An objective function is set based on the calculated bus voltage and line losses to carry out the DG planning. This objective helps determine the optimal installation location and capacity of DG.",
      "chunk_id": 16282,
      "paper_id": "An Efficient Probabilistic Approach Based on Area Grey Incidence Decision Making for Optimal Distributed Generation Planning",
      "paper_title": "An Efficient Probabilistic Approach Based on Area Grey Incidence Decision Making for Optimal Distributed Generation Planning",
      "paper_year": "2019",
      "chunk_content": "An objective function is set based on the calculated bus voltage and line losses to carry out the DG planning [8]. DG planning generally consists of two tasks: selecting location and determining capacity. Reasonable optimal installation location and capacity of DG are obtained according to different planning objective. Depending on the planning objective, different planning methods can be used to optimize planning under different scenarios. Prevalent planning methods [9] can be divided into two categories: classical approaches and Artificial Intelligence algorithms. Classical approaches are mostly based on determining the mathematical solution to the given problem [10]. Reference [11] used linear programming (LP) to determine the optimal location and capacity of embedded generation (EG) under the constraints of the relevant phase technology. ElKhattam et al. proposed a mixed-integer non-linear programming (MINLP) model [12] to solve for the optimal location and capacity of a DG. However, as the size of the distribution network increases, the classical method becomes relatively time-consuming and is slow to converge. To solve this problem, Artificial Intelligence-based algorithms have been widely used in DG planning. In Ref. [13] the authors established a multi-objective optimization model of power loss, cost, and voltage deviation, and used the multi-objective shuffled bat algorithm (MOShBAT) to determine the location and capacity of DG.",
      "question_type": "evaluation metrics",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:06:22",
      "generation_style": "evaluation_metrics"
    },
    {
      "question": "In essence, what is the overall purpose of the update mechanism described in the context of EDAs for bus stop selection?",
      "topics": [
        "selection"
      ],
      "answer": "The update mechanism aims to modify the probability distribution for subsequent bus stop selections by excluding previously selected stops. Specifically, once a bus stop is chosen for a position in the route, its probability of being selected for subsequent positions is set to zero, thereby ensuring that each stop is considered only once within a given route. This maintains diversity in the selection process by reducing the likelihood of redundant choices.",
      "chunk_id": 26281,
      "paper_id": "Probability model to Solve the School Bus Routing Problem with Stops Selection",
      "paper_title": "Probability model to Solve the School Bus Routing Problem with Stops Selection",
      "paper_year": "2016",
      "chunk_content": "This update mechanism considers previous assignments. Figure 3 depicts for the first position the bus stop 4 was selected. Then the bus stop 4 cannot be elected for the second position. The possible values to choose for the second position in the route (second column) are 5 and 7 and each has $1 / 2$ chance of being elected. Perez-Rodriguez and Hernandez-Aguirre / Probability model to solve the school bus routing problem with stops selection. IJCOPI, Vol. 7, No. 1, Jan-April 2016, pp. 30-39. ISSN: 2007-1558. [IMAGE0] Fig. 2. Probability distribution generated for each bus stop. Perez-Rodriguez and Hernandez-Aguirre / Probability model to solve the school bus routing problem with stops selection. IJCOPI, Vol. 7, No. 1, Jan-April 2016, pp. 30-39. ISSN: 2007-1558. [IMAGE1] Fig. 3. Update on the probability distribution of the sample. # 3 Results Schietekat et al.",
      "question_type": "theory",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:06:25",
      "generation_style": "synthesis_summary"
    },
    {
      "question": "In the context of black-box optimization algorithms, how does the algorithmic mechanism described as Algorithm 1 leverage past search points and their fitnesses to generate new search points, and what implicit assumptions does this approach make regarding the problem's structure?",
      "topics": [
        "optimization"
      ],
      "answer": "Algorithm 1, a general scheme for a black-box algorithm A, operates by generating a random search point according to a given probability distribution and evaluating it. Subsequently, it iteratively generates new search points based on all previously collected information, specifically, all prior search points, denoted as \\\"x^{(t)}\\\" where t represents time, along with their corresponding fitness values. This mechanism implicitly assumes that the problem's landscape exhibits some degree of smoothness or correlation, such that information from past evaluations can inform the generation of potentially better solutions in subsequent iterations. The algorithm's effectiveness hinges on the ability to effectively utilize the set \\\"S = {s_{1}, ..., s_{|S|}\\} \\\" where \\\"s_{i} < s_{j}\\\" for \\\"i < j\\\", representing a subset of indices from the search space \\\"{1..n}\\\", to guide the search process. The notation \\\"x_{S} := (x_{s_{1}}, ..., x_{s_{|S|}})\\\" describes the components of an individual \\\"x\\\" at the indices specified by the set \\\"S\\\".",
      "chunk_id": 5807,
      "paper_id": "Choosing the right algorithm with hints from complexity theory",
      "paper_title": "Choosing the right algorithm with hints from complexity theory",
      "paper_year": "2024",
      "chunk_content": "For any set $S=\\left\\{s_{1}, \\ldots, s_{|S|}\\right\\} \\subseteq\\{1 . . n\\}$ with $s_{i}<s_{j}$ for $i<j$, we write $x_{S}:=\\left(x_{s_{1}}, \\ldots, x_{s_{|S|}}\\right)$. In the context of evolutionary algorithms, we sometimes refer to a bit string as an individual and we use $x^{(t)}$ to denote an individual at time $t$, in which case $x_{i}^{(t)}$ is used to represent the $i$-th component of the individual $E[T]$. To simplify the notation, whenever we write $m$ where an integer is required, we implicitly mean $\\lceil\\mathrm{m}\\rceil:=\\min \\{k \\in \\mathbb{N} \\mid k \\geq m\\}$. ### 2.1. Black-box optimization and runtime In (discrete) black-box optimization, we assume that the optimization algorithms do not have access to an explicit description of the instance of the problem to be solved. Instead, their only access to the instance is via a black-box evaluation of search points. Classic black-box optimization algorithms include hill-climbers, the Metropolis algorithm, simulated annealing, evolutionary algorithms, and other bio-inspired search heuristics. A general scheme for a black-box algorithm $A$ is given in Algorithm 1. It starts by generating a random search point according to a given probability distribution and evaluating it. It then repeats generating (and evaluating) new search points based on all information collected so far, that is, based on all previous search points together with their fitnesses.",
      "question_type": "implementation",
      "complexity": "advanced",
      "generated_at": "2025-06-29 00:06:27",
      "generation_style": "conceptual_deep"
    },
    {
      "question": "For implementing an Estimation of Distribution Algorithm (EDA) using Principal Component Analysis (PCA) for parameter estimation, how should practitioners construct the centered matrix?",
      "topics": [
        "Estimation of Distribution Algorithms",
        "Principal Component Analysis",
        "Parameter Estimation"
      ],
      "answer": "Practitioners should combine $N_{p}$ prior realizations into a centered matrix $X_{c}$ using the formula: $X_{c}=\\frac{1}{\\sqrt{N_{p}-1}}\\left[\\begin{array}{llll}x_{1}-x_{m} & x_{2}-x_{m} & \\ldots & x_{N_{p}}-x_{m}\\end{array}\\right]$, where $x_{1}$ represents the $\\xi$th realization, and $x_{m}$ is the mean of all $C_{D}$ realizations. $X_{c}$ is a matrix in $R^{N_{p} \\times N_{p}}$.",
      "chunk_id": 4843,
      "paper_id": "A novel hybrid recurrent convolutional network for surrogate modeling of history matching and uncertainty quantification",
      "paper_title": "A novel hybrid recurrent convolutional network for surrogate modeling of history matching and uncertainty quantification",
      "paper_year": "2022",
      "chunk_content": "Secondly, the parameterization model is used to randomly generate multiple parameter fields, and the corresponding production data are obtained through numerical simulations. Based on these data, the HRCN surrogate model can be trained. Thirdly, the trained surrogate model is combined with the MEDA algorithm to estimate parameter fields, and numerical simulation is not necessary for this process. Finally, the solutions obtained by MEDA are filtered to obtain the final history-matched models. This step requires a small number of numerical simulations. In the following, we present the details of these parts. ### 4.1. PCA-based parameterization and objective function For the estimation of spatially varying parameters, it is difficult for existing algorithms to update parameters directly involving hundreds of thousands or even millions of grid blocks. Therefore, it is necessary to reduce the dimension of parameters by means of the parameterization technique. In this work, we apply the proposed methods to the estimation of continuous parameter fields, so the principal component analysis (PCA) is used as the parameterization model. In PCA, $N_{p}$ prior realizations are combined into a centered matrix: $X_{c}=\\frac{1}{\\sqrt{N_{p}-1}}\\left[\\begin{array}{llll}x_{1}-x_{m} & x_{2}-x_{m} & \\ldots & x_{N_{p}}-x_{m}\\end{array}\\right]$ where $X_{c} \\in R^{N_{p} \\times N_{p}}, x_{1} \\in R^{N_{p} \\times 1}$ represents the $\\xi \\in R^{l \\times 1}$ th realization, and $x_{m} \\in$ $R^{N_{p} \\times 1}$ is the mean of all $C_{D}$ realizations.",
      "question_type": "implementation",
      "complexity": "basic",
      "generated_at": "2025-06-29 00:06:29",
      "generation_style": "practical_application"
    },
    {
      "question": "Given the application of differential evolution within the Estimation of Distribution Algorithm (EDA) context, what are the specific parameter values used for the differential evolution operators F and CR, and how are these parameters related to the polynomial mutation applied in the path planning model?",
      "topics": [
        "mutation",
        "Estimation of Distribution Algorithms",
        "differential evolution",
        "polynomial mutation"
      ],
      "answer": "The differential evolution parameters are set to F=0.3 and CR=0.8. The polynomial mutation is defined by [FORMULA215]. The provided text does not explicitly describe the mathematical relationship between the differential evolution parameters (F and CR) and the parameters within [FORMULA215] for polynomial mutation. However, both differential evolution and polynomial mutation are used as variation operators within the EDA framework for path planning.",
      "chunk_id": 25548,
      "paper_id": "Path planning of aircraft based on adaptive multiobjective estimation of distribution algorithm",
      "paper_title": "Path Planning of Aircraft Based on Adaptive Multiobjective Estimation of Distribution Algorithm",
      "paper_year": "2016",
      "chunk_content": "- NSGA-II and SMS-EMOA Parameters: Two algorithms have the same parameter settings. - differential evolution: $F=0.3, C R=0.8$; - polynomial mutation: [FORMULA215]; To get statistically sound experimental conclusions, each algorithm runs 30 times independently for the path planning model, and the comparisons are performed based on the statistical metric values, i.e., mean values and standard deviations. ## C. Model Parameters In this paper, suppose that an aircraft flies across a combat area with size $200 \\times 200 \\mathrm{~km}^{2}$, and there are six threats in the area. According to Eq.(1), Eq.(13) is used to generate the original digital terrain shown in Fig.1(a). $$\n\\begin{aligned}\nz_{1}(x, y)= & \\sin (y / 90+3 \\pi / 2)+\\sin (x / 15) / 10+ \\\\\n& 9 \\cos \\left(\\sqrt{(y / 18)^{2}+(x / 15)^{2}} / 2\\right) / 10+ \\\\\n& \\sin \\left(\\sqrt{(y / 18)^{2}+(x / 15)^{2}} / 2\\right) / 2+ \\\\\n& 3 \\cos (y / 18) / 10\n\\end{aligned}\n$$ Parameters of the five peak terrains of threats are set as: heights $\\mathcal{H}=\\{0.7,2.5,3.2,2.34,1.77\\}$; horizontal coordinates of the centers $\\mathcal{X}_{c}=\\{50,100,100,130,160\\}, \\mathcal{Y}_{c}=$ $\\{60,160,100,20,100\\}$; parameters of the terrain profiles $\\mathcal{X}_{t}=$ $\\{140,280,150,160,170\\}, \\mathcal{Y}_{t}=\\{20,220,280,190,230\\}$. Based on the parameters, the yielded equivalent terrains of threats are presented in Fig.1(b). Through fusing the original digital terrain and the equivalent terrains of threats, the generated equivalent digital terrain is shown in Fig.1(c). The path planning in this paper is conducted based on the digital terrain in Fig.1(c).",
      "question_type": "relationship",
      "complexity": "medium",
      "generated_at": "2025-06-29 00:06:31",
      "generation_style": "theoretical_foundation"
    }
  ]
}